<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On gradient regularizers for MMD GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arbel</surname></persName>
							<email>michael.n.arbel@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikołaj</forename><surname>Bińkowski</surname></persName>
							<email>mikbinkowski@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
							<email>arthur.gretton@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Gatsby Computational</orgName>
								<orgName type="institution">Neuroscience Unit University College London</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Gatsby Computational Neuroscience Unit University College London</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Gatsby Computational Neuroscience Unit University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On gradient regularizers for MMD GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on 160 × 160 CelebA and 64 × 64 unconditional ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been an explosion of interest in implicit generative models (IGMs) over the last few years, especially after the introduction of generative adversarial networks (GANs) <ref type="bibr" target="#b15">[16]</ref>. These models allow approximate samples from a complex high-dimensional target distribution P, using a model distribution Q θ , where estimation of likelihoods, exact inference, and so on are not tractable. GANtype IGMs have yielded very impressive empirical results, particularly for image generation, far beyond the quality of samples seen from most earlier generative models [e.g. 18, 22, 23, 24, 38].</p><p>These excellent results, however, have depended on adding a variety of methods of regularization and other tricks to stabilize the notoriously difficult optimization problem of GANs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref>. Some of this difficulty is perhaps because when a GAN is viewed as minimizing a discrepancy D GAN (P, Q θ ), its gradient ∇ θ D GAN (P, Q θ ) does not provide useful signal to the generator if the target and model distributions are not absolutely continuous, as is nearly always the case [2].</p><p>An alternative set of losses are the integral probability metrics (IPMs) <ref type="bibr" target="#b35">[36]</ref>, which can give credit to models Q θ "near" to the target distribution P [3, 8, Section 4 of 15]. IPMs are defined in terms of a critic function: a "well behaved" function with large amplitude where P and Q θ differ most. The IPM is the difference in the expected critic under P and Q θ , and is zero when the distributions agree. The Wasserstein IPMs, whose critics are made smooth via a Lipschitz constraint, have been particularly successful in IGMs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. But the Lipschitz constraint must hold uniformly, which can be hard to enforce. A popular approximation has been to apply a gradient constraint only in expectation [18]: the critic's gradient norm is constrained to be small on points chosen uniformly between P and Q.</p><p>Another class of IPMs used as IGM losses are the Maximum Mean Discrepancies (MMDs) <ref type="bibr" target="#b16">[17]</ref>, as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>. Here the critic function is a member of a reproducing kernel Hilbert space (except in <ref type="bibr" target="#b49">[50]</ref>, who learn a deep approximation to an RKHS critic). Better performance can be obtained, * These authors contributed equally.</p><p>The MMD is continuous in the weak topology for any bounded kernel with Lipschitz embeddings [46, Theorem 3.2(b)], meaning that if P n converges in distribution to P, P n D − → P, then MMD(P n , P) → 0.</p><p>(W is continuous in the slightly stronger Wasserstein topology [51, Definition 6.9]; P n W −→ P implies</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>however, when the MMD kernel is not based directly on image pixels, but on learned features of images. Wasserstein-inspired gradient regularization approaches can be used on the MMD critic when learning these features: <ref type="bibr" target="#b26">[27]</ref> uses weight clipping <ref type="bibr" target="#b2">[3]</ref>, and <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> use a gradient penalty <ref type="bibr" target="#b17">[18]</ref>.</p><p>The recent Sobolev GAN <ref type="bibr" target="#b32">[33]</ref> uses a similar constraint on the expected gradient norm, but phrases it as estimating a Sobolev IPM rather than loosely approximating Wasserstein. This expectation can be taken over the same distribution as <ref type="bibr" target="#b17">[18]</ref>, but other measures are also proposed, such as (P + Q θ ) /2. A second recent approach, the spectrally normalized GAN <ref type="bibr" target="#b31">[32]</ref>, controls the Lipschitz constant of the critic by enforcing the spectral norms of the weight matrices to be 1. Gradient penalties also benefit GANs based on f -divergences <ref type="bibr" target="#b36">[37]</ref>: for instance, the spectral normalization technique of <ref type="bibr" target="#b31">[32]</ref> can be applied to the critic network of an f -GAN. Alternatively, a gradient penalty can be defined to approximate the effect of blurring P and Q θ with noise <ref type="bibr" target="#b39">[40]</ref>, which addresses the problem of non-overlapping support <ref type="bibr" target="#b1">[2]</ref>. This approach has recently been shown to yield locally convergent optimization in some cases with non-continuous distributions, where the original GAN does not <ref type="bibr" target="#b29">[30]</ref>.</p><p>In this paper, we introduce a novel regularization for the MMD GAN critic of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref>, which directly targets generator performance, rather than adopting regularization methods intended to approximate Wasserstein distances <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. The new MMD regularizer derives from an approach widely used in semi-supervised learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">Section 2]</ref>, where the aim is to define a classification function f which is positive on P (the positive class) and negative on Q θ (negative class), in the absence of labels on many of the samples. The decision boundary between the classes is assumed to be in a region of low density for both P and Q θ : f should therefore be flat where P and Q θ have support (areas with constant label), and have a larger slope in regions of low density. Bousquet et al. <ref type="bibr" target="#b9">[10]</ref> propose as their regularizer on f a sum of the variance and a density-weighted gradient norm.</p><p>We adopt a related penalty on the MMD critic, with the difference that we only apply the penalty on P: thus, the critic is flatter where P has high mass, but does not vanish on the generator samples from Q θ (which we optimize). In excluding Q θ from the critic function constraint, we also avoid the concern raised by [32] that a critic depending on Q θ will change with the current minibatch -potentially leading to less stable learning. The resulting discrepancy is no longer an integral probability metric: it is asymmetric, and the critic function class depends on the target P being approximated.</p><p>We first discuss in Section 2 how MMD-based losses can be used to learn implicit generative models, and how a naive approach could fail. This motivates our new discrepancies, introduced in Section 3. Section 4 demonstrates that these losses outperform state-of-the-art models for image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning implicit generative models with MMD-based losses</head><p>An IGM is a model Q θ which aims to approximate a target distribution P over a space X ⊆ R d . We will define Q θ by a generator function G θ : Z → X , implemented as a deep network with parameters θ, where Z is a space of latent codes, say R 128 . We assume a fixed distribution on Z, say Z ∼ Uniform [−1, 1] 128 , and call Q θ the distribution of G θ (Z). We will consider learning by minimizing a discrepancy D between distributions, with D(P, Q θ ) ≥ 0 and D(P, P) = 0, which we call our loss. We aim to minimize D(P, Q θ ) with stochastic gradient descent on an estimator of D.</p><p>In the present work, we will build losses D based on the Maximum Mean Discrepancy,</p><formula xml:id="formula_0">MMD k (P, Q) = sup f : f H k ≤1 E X∼P [f (X)] − E Y ∼Q [f (Y )],<label>(1)</label></formula><p>an integral probability metric where the critic class is the unit ball within H k , the reproducing kernel Hilbert space with a kernel k. The optimization in (1) admits a simple closed-form optimal critic, f</p><formula xml:id="formula_1">* (t) ∝ E X∼P [k(X, t)] − E Y ∼Q [k(Y, t)].</formula><p>There is also an unbiased, closed-form estimator of MMD 2 k with appealing statistical properties <ref type="bibr" target="#b16">[17]</ref> -in particular, its sample complexity is independent of the dimension of X , compared to the exponential dependence <ref type="bibr" target="#b51">[52]</ref> of the Wasserstein distance W(P, Q) = sup</p><formula xml:id="formula_2">f : f Lip≤1 E X∼P [f (X)] − E Y ∼Q [f (Y )].<label>(2)</label></formula><p>P n D − → P, and the two notions coincide if X is bounded.) Continuity means the loss can provide better signal to the generator as Q θ approaches P, as opposed to e.g. Jensen-Shannon where the loss could be constant until suddenly jumping to 0 [e.g. 3, Example 1]. The MMD is also strict, meaning it is zero iff P = Q θ , for characteristic kernels <ref type="bibr" target="#b44">[45]</ref>. The Gaussian kernel yields an MMD both continuous in the weak topology and strict. Thus in principle, one need not conduct any alternating optimization in an IGM at all, but merely choose generator parameters θ to minimize MMD k .</p><p>Despite these appealing properties, using simple pixel-level kernels leads to poor generator samples <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>. More recent MMD GANs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref> achieve better results by using a parameterized family of kernels, {k ψ } ψ∈Ψ , in the Optimized MMD loss previously studied by <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>:</p><formula xml:id="formula_3">D Ψ MMD (P, Q) := sup ψ∈Ψ MMD k ψ (P, Q).<label>(3)</label></formula><p>We primarily consider kernels defined by some fixed kernel K on top of a learned low-dimensional</p><formula xml:id="formula_4">representation φ ψ : X → R s , i.e. k ψ (x, y) = K(φ ψ (x), φ ψ (y)), denoted k ψ = K • φ ψ . In practice,</formula><p>K is a simple characteristic kernel, e.g. Gaussian, and φ ψ is usually a deep network with output dimension say s = 16 <ref type="bibr" target="#b6">[7]</ref> or even s = 1 (in our experiments). If φ ψ is powerful enough, this choice is sufficient; we need not try to ensure each k ψ is characteristic, as did <ref type="bibr" target="#b26">[27]</ref>.</p><formula xml:id="formula_5">Proposition 1. Suppose k = K • φ ψ , with K characteristic and {φ ψ } rich enough that for any P = Q, there is a ψ ∈ Ψ for which φ ψ #P = φ ψ #Q. 2 Then if P = Q, D Ψ MMD (P, Q) &gt; 0.</formula><p>Proof. Letψ ∈ Ψ be such that φψ(P) = φψ(Q). Then, since K is characteristic,</p><formula xml:id="formula_6">D Ψ MMD (P, Q) = sup ψ∈Ψ MMD K (φ ψ #P, φ ψ #Q) ≥ MMD K (φψ#P, φψ#Q) &gt; 0.</formula><p>To estimate D Ψ MMD , one can conduct alternating optimization to estimate aψ and then update the generator according to MMD kψ , similar to the scheme used in GANs and WGANs. (This form of estimator is justified by an envelope theorem <ref type="bibr" target="#b30">[31]</ref>, although it is invariably biased <ref type="bibr" target="#b6">[7]</ref>.) Unlike D GAN or W, fixing aψ and optimizing the generator still yields a sensible distance MMD kψ .</p><p>Early attempts at minimizing D Ψ MMD in an IGM, though, were unsuccessful <ref type="bibr">[48, footnote 7]</ref>. This could be because for some kernel classes, D Ψ MMD is stronger than Wasserstein or MMD. Example 1 (DiracGAN <ref type="bibr" target="#b29">[30]</ref>). We wish to model a point mass at the origin of R, P = δ 0 , with any possible point mass, Q θ = δ θ for θ ∈ R. We use a Gaussian kernel of any bandwidth, which can be written as</p><formula xml:id="formula_7">k ψ = K • φ ψ with φ ψ (x) = ψx for ψ ∈ Ψ = R and K(a, b) = exp − 1 2 (a − b) 2 . Then MMD 2 k ψ (δ 0 , δ θ ) = 2 1 − exp − 1 2 ψ 2 θ 2 , D Ψ MMD (δ 0 , δ θ ) = √ 2 θ = 0 0 θ = 0 .</formula><p>Considering D Ψ MMD (δ 0 , δ 1/n ) = √ 2 → 0, even though δ 1/n W −→ δ 0 , shows that the Optimized MMD distance is not continuous in the weak or Wasserstein topologies.</p><p>This also causes optimization issues. <ref type="figure" target="#fig_1">Figure 1</ref> </p><formula xml:id="formula_8">(a) shows gradient vector fields in parameter space, v(θ, ψ) ∝ − ∇ θ MMD 2 k ψ (δ 0 , δ θ ), ∇ ψ MMD 2 k ψ (δ 0 , δ θ )</formula><p>. Some sequences following v (e.g. A) converge to an optimal solution (0, ψ), but some (B) move in the wrong direction, and others (C) are stuck because there is essentially no gradient. <ref type="figure" target="#fig_1">Figure 1</ref> (c, red) shows that the optimal D Ψ MMD critic is very sharp near P and Q; this is less true for cases where the algorithm converged.</p><p>We can avoid these issues if we ensure a bounded Lipschitz critic: 3</p><formula xml:id="formula_9">Proposition 2. Assume the critics f ψ (x) = (E X∼P k ψ (X, x) − E Y ∼Q k ψ (Y, x))/ MMD k ψ (P, Q)</formula><p>are uniformly bounded and have a common Lipschitz constant: sup x∈X ,ψ∈Ψ |f ψ (x)| &lt; ∞ and sup ψ∈Ψ f ψ Lip &lt; ∞. In particular, this holds when k ψ = K • φ ψ and</p><formula xml:id="formula_10">sup a∈R s K(a, a) &lt; ∞, K(a, ·) − K(b, ·) H K ≤ L K a − b R s , sup ψ∈Ψ φ ψ Lip ≤ L φ &lt; ∞.</formula><p>Then D Ψ MMD is continuous in the weak topology: if P n D − → P, then D Ψ MMD (P n , P) → 0.  Proof. The main result is <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Corollary 11.3.4]</ref>. To show the claim for</p><formula xml:id="formula_11"> MMD  SMMD  GCMMD  LipMMD</formula><formula xml:id="formula_12">k ψ = K • φ ψ , note that |f ψ (x) − f ψ (y)| ≤ f ψ H k ψ k ψ (x, ·) − k ψ (y, ·) H k ψ , which since f ψ H k ψ = 1 is K(φ ψ (x), ·) − K(φ ψ (y), ·) H K ≤ L K φ ψ (x) − φ ψ (y) R s ≤ L K L φ x − y R d .</formula><p>Indeed, if we put a box constraint on ψ <ref type="bibr" target="#b26">[27]</ref> or regularize the gradient of the critic function <ref type="bibr" target="#b6">[7]</ref>, the resulting MMD GAN generally matches or outperforms WGAN-based models. Unfortunately, though, an additive gradient penalty doesn't substantially change the vector field of <ref type="figure" target="#fig_1">Figure 1</ref> (a), as shown in <ref type="figure" target="#fig_8">Figure 5</ref> (Appendix B). We will propose distances with much better convergence behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">New discrepancies for learning implicit generative models</head><p>Our aim here is to introduce a discrepancy that can provide useful gradient information when used as an IGM loss. Proofs of results in this section are deferred to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lipschitz Maximum Mean Discrepancy</head><p>Proposition 2 shows that an MMD-like discrepancy can be continuous under the weak topology even when optimizing over kernels, if we directly restrict the critic functions to be Lipschitz. We can easily define such a distance, which we call the Lipschitz MMD: for some λ &gt; 0,</p><formula xml:id="formula_13">LipMMD k,λ (P, Q) := sup f ∈H k : f 2 Lip +λ f 2 H k ≤1 E X∼P [f (X)] − E Y ∼Q [f (Y )] .<label>(4)</label></formula><p>For a universal kernel k, we conjecture that lim λ→0 LipMMD k,λ (P, Q) → W(P, Q). But for any k and λ, LipMMD is upper-bounded by W, as (4) optimizes over a smaller set of functions than (2). Thus D Ψ,λ LipMMD (P, Q) := sup ψ∈Ψ LipMMD k ψ ,λ (P, Q) is also upper-bounded by W, and hence is continuous in the Wasserstein topology. It also shows excellent empirical behavior on Example 1 <ref type="figure" target="#fig_1">(Figure 1 (d)</ref>, and <ref type="figure" target="#fig_8">Figure 5</ref> in Appendix B). But estimating LipMMD k,λ , let alone D Ψ,λ LipMMD , is in general extremely difficult (Appendix D), as finding f Lip requires optimization in the input space. Constraining the mean gradient rather than the maximum, as we will do next, is far more tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradient-Constrained Maximum Mean Discrepancy</head><p>We define the Gradient-Constrained MMD for λ &gt; 0 and using some measure µ as GCMMD µ,k,λ (P, Q) := sup</p><formula xml:id="formula_14">f ∈H k : f S(µ),k,λ ≤1 E X∼P [f (X)] − E Y ∼Q [f (Y )] ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_15">f 2 S(µ),k,λ := f 2 L 2 (µ) + ∇f 2 L 2 (µ) + λ f 2 H k .<label>(6)</label></formula><p>· 2 L 2 (µ) = · 2 µ(dx) denotes the squared L 2 norm. Rather than directly constraining the Lipschitz constant, the second term ∇f 2 L 2 (µ) encourages the function f to be flat where µ has mass. In experiments we use µ = P, flattening the critic near the target sample. We add the first term following <ref type="bibr" target="#b9">[10]</ref>: in one dimension and with µ uniform, · S(µ),·,0 is then an RKHS norm with the kernel κ(x, y) = exp(− x − y ), which is also a Sobolev space. The correspondence to a Sobolev norm is lost in higher dimensions <ref type="bibr" target="#b52">[53,</ref><ref type="bibr">Ch. 10</ref>], but we also found the first term to be beneficial in practice.</p><p>We can exploit some properties of H k to compute <ref type="bibr" target="#b4">(5)</ref> analytically. Call the difference in kernel mean </p><formula xml:id="formula_16">embeddings η := E X∼P [k(X, ·)] − E Y ∼Q [k(Y, ·)] ∈ H k ; recall MMD(P, Q) = η H k .</formula><formula xml:id="formula_17">GCMMD 2 µ,k,λ (P, Q) = 1 λ MMD 2 (P, Q) −P (η) P (η) = η(X) ∇η(X) T K G T G H + M λI M +M d −1 η(X) ∇η(X) , where K is the kernel matrix K m,m = k(X m , X m ), G is the matrix of left derivatives 5 G (m,i),m = ∂ i k(X m , X m ), and H that of derivatives of both arguments H (m,i),(m ,j) = ∂ i ∂ j+d k(X m , X m ).</formula><p>As long as P and Q have integrable first moments, and µ has second moments, Assumptions (A) to (D) are satisfied e.g. by a Gaussian or linear kernel on top of a differentiable φ ψ . We can thus estimate the GCMMD based on samples from P, Q, and µ by using the empirical meanη for η.</p><p>This discrepancy indeed works well in practice: Appendix F.2 shows that optimizing our estimate of D µ,Ψ,λ GCMMD = sup ψ∈Ψ GCMMD µ,k ψ ,λ yields a good generative model on MNIST. But the linear system of size M + M d is impractical: even on 28 × 28 images and using a low-rank approximation, the model took days to converge. We therefore design a less expensive discrepancy in the next section.</p><p>The GCMMD is related to some discrepancies previously used in IGM training. The Fisher GAN <ref type="bibr" target="#b33">[34]</ref> uses only the variance constraint f 2 L 2 (µ) ≤ 1. The Sobolev GAN <ref type="bibr" target="#b32">[33]</ref> constrains ∇f 2 L 2 (µ) ≤ 1, along with a vanishing boundary condition on f to ensure a well-defined solution (although this was not used in the implementation, and can cause very unintuitive critic behavior; see Appendix C). The authors considered several choices of µ, including the WGAN-GP measure <ref type="bibr" target="#b17">[18]</ref> and mixtures (P + Q θ ) /2. Rather than enforcing the constraints in closed form as we do, though, these models used additive regularization. We will compare to the Sobolev GAN in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scaled Maximum Mean Discrepancy</head><p>We will now derive a lower bound on the Gradient-Constrained MMD which retains many of its attractive qualities but can be estimated in time linear in the dimension d. </p><formula xml:id="formula_18">f ∈ H k , f S(µ),k,λ ≤ σ −1 µ,k,λ f H k , where σ µ,k,λ := 1 λ + k(x, x)µ(dx) + d i=1 ∂ 2 k(y, z) ∂y i ∂z i (y,z)=(x,x) µ(dx).</formula><p>We then define the Scaled Maximum Mean Discrepancy based on this bound of Proposition 4: SMMD µ,k,λ (P, Q) := sup</p><formula xml:id="formula_19">f : σ −1 µ,k,λ f H ≤1 E X∼P [f (X)]−E Y ∼Q [f (Y )] = σ µ,k,λ MMD k (P, Q).<label>(7)</label></formula><p>Because the constraint in the optimization of (7) is more restrictive than in that of (5), we have that SMMD µ,k,λ (P, Q) ≤ GCMMD µ,k,λ (P, Q). The Sobolev norm f S(µ),λ , and a fortiori the gradient norm under µ, is thus also controlled for the SMMD critic. We also show in Appendix F.1 that SMMD µ,k,λ behaves similarly to GCMMD µ,k,λ on Gaussians.</p><formula xml:id="formula_20">If k ψ = K • φ ψ and K(a, b) = g(− a − b 2 ), then σ −2 k,µ,λ = λ + g(0) + 2|g (0)| E µ ∇φ ψ (X) 2 F . Or if K is linear, K(a, b) = a T b, then σ −2 k,µ,λ = λ + E µ φ ψ (X) 2 + ∇φ ψ (X) 2 F .</formula><p>Estimating these terms based on samples from µ is straightforward, giving a natural estimator for the SMMD.</p><p>Of course, if µ and k are fixed, the SMMD is simply a constant times the MMD, and so behaves in essentially the same way as the MMD. But optimizing the SMMD over a kernel family Ψ, D µ,Ψ,λ SMMD (P, Q) := sup ψ∈Ψ SMMD µ,k ψ ,λ (P, Q), gives a distance very different from D Ψ MMD (3). <ref type="figure" target="#fig_1">Figure 1</ref> (b) shows the vector field for the Optimized SMMD loss in Example 1, using the WGAN-GP measure µ = Uniform(0, θ). The optimization surface is far more amenable: in particular the location C, which formerly had an extremely small gradient that made learning effectively impossible, now converges very quickly by first reducing the critic gradient until some signal is available. <ref type="figure" target="#fig_1">Figure 1</ref> </p><formula xml:id="formula_21">(d) demonstrates that D µ,Ψ,λ SMMD , like D µ,Ψ,λ GCMMD and D Ψ,λ LipMMD but in sharp contrast to D Ψ</formula><p>MMD , is continuous with respect to the location θ and provides a strong gradient towards 0. We can establish that D µ,Ψ,λ SMMD is continuous in the Wasserstein topology under some conditions:</p><formula xml:id="formula_22">Theorem 1. Let k ψ = K • φ ψ , with φ ψ : X → R s a fully-connected L-layer network with</formula><p>Leaky-ReLU α activations whose layers do not increase in width, and K satisfying mild smoothness conditions Q K &lt; ∞ (Assumptions (II) to (V) in Appendix A.2). Let Ψ κ be the set of parameters where each layer's weight matrices have condition number cond(</p><formula xml:id="formula_23">W l ) = W l / σ min (W l ) ≤ κ &lt; ∞. If µ has a density (Assumption (I)), then D µ,Ψ κ ,λ SMMD (P, Q) ≤ Q K κ L/2 √ d L α L/2 W(P, Q). Thus if P n W −→ P, then D µ,Ψ κ ,λ SMMD (P n , P) → 0, even if µ is chosen to depend on P and Q.</formula><p>Uniform bounds vs bounds in expectation Controlling ∇f ψ 2 L 2 (µ) = E µ ∇f ψ (X) 2 does not necessarily imply a bound on f Lip ≥ sup x∈X ∇f ψ (X) , and so does not in general give continuity via Proposition 2. Theorem 1 implies that when the network's weights are well-conditioned, it is sufficient to only control ∇f ψ 2 L 2 (µ) , which is far easier in practice than controlling f Lip . If we instead tried to directly controlled f Lip with e.g. spectral normalization (SN) <ref type="bibr" target="#b31">[32]</ref>, we could significantly reduce the expressiveness of the parametric family. In Example 1, constraining φ ψ Lip = 1 limits us to only Ψ = {1}. Thus D {1} MMD is simply the MMD with an RBF kernel of bandwidth 1, which has poor gradients when θ is far from 0 ( <ref type="figure" target="#fig_1">Figure 1 (c)</ref>, blue). The Cauchy-Schwartz bound of Proposition 4 allows jointly adjusting the smoothness of k ψ and the critic f , while SN must control the two independently. Relatedly, limiting φ Lip by limiting the Lipschitz norm of each layer could substantially reduce capacity, while ∇f ψ L 2 (µ) need not be decomposed by layer. Another advantage is that µ provides a data-dependent measure of complexity as in <ref type="bibr" target="#b9">[10]</ref>: we do not needlessly prevent ourselves from using critics that behave poorly only far from the data.</p><p>Spectral parametrization When the generator is near a local optimum, the critic might identify only one direction on which Q θ and P differ. If the generator parameterization is such that there is no local way for the generator to correct it, the critic may begin to single-mindedly focus on this difference, choosing redundant convolutional filters and causing the condition number of the weights to diverge. If this occurs, the generator will be motivated to fix this single direction while ignoring all other aspects of the distributions, after which it may become stuck. We can help avoid this collapse by using a critic parameterization that encourages diverse filters with higher-rank weight matrices. Miyato et al. <ref type="bibr" target="#b31">[32]</ref> propose to parameterize the weight matrices as W = γW / W op , where W op is the spectral norm ofW . This parametrization works particularly well with D µ,Ψ,λ SMMD ; <ref type="figure" target="#fig_5">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated unsupervised image generation on three datasets: CIFAR-10 <ref type="bibr" target="#b25">[26]</ref> (60 000 images, 32 × 32), CelebA <ref type="bibr" target="#b28">[29]</ref> (202 599 face images, resized and cropped to 160 × 160 as in <ref type="bibr" target="#b6">[7]</ref>), and the more challenging ILSVRC2012 (ImageNet) dataset <ref type="bibr" target="#b40">[41]</ref> (1 281 167 images, resized to 64 × 64). Code for all of these experiments is available at github.com/MichaelArbel/Scaled-MMD-GAN.</p><p>Losses All models are based on a scalar-output critic network φ ψ : X → R, except MMDGAN-GP where φ ψ : X → R 16 as in <ref type="bibr" target="#b6">[7]</ref>. The WGAN and Sobolev GAN use a critic f = φ ψ , while the GAN uses a discriminator D ψ (x) = 1/(1 + exp(−φ ψ (x))). The MMD-based methods use a kernel k ψ (x, y) = exp(−(φ ψ (x) − φ ψ (y)) 2 /2), except for MMDGAN-GP which uses a mixture of RQ kernels as in <ref type="bibr" target="#b6">[7]</ref>. Increasing the output dimension of the critic or using a different kernel didn't substantially change the performance of our proposed method. We also consider SMMD with a linear top-level kernel, k(x, y) = φ ψ (x)φ ψ (y); because this becomes essentially identical to a WGAN (Appendix E), we refer to this method as SWGAN. SMMD and SWGAN use µ = P; Sobolev GAN uses µ = (P + Q)/2 as in <ref type="bibr" target="#b32">[33]</ref>. We choose λ and an overall scaling to obtain the losses:</p><formula xml:id="formula_24">SMMD: MMD 2 k ψ (P, Q θ ) 1 + 10 EP [ ∇φ ψ (X) 2 F ]</formula><p>, SWGAN:</p><formula xml:id="formula_25">EP [φ ψ (X)] − EQ θ [φ ψ (X)] 1 + 10 EP [|φ ψ (X)| 2 ] + EP [ ∇φ ψ (X) 2 F ]</formula><p>.</p><p>Architecture For CIFAR-10, we used the CNN architecture proposed by <ref type="bibr" target="#b31">[32]</ref> with a 7-layer critic and a 4-layer generator. For CelebA, we used a 5-layer DCGAN discriminator and a 10-layer ResNet generator as in <ref type="bibr" target="#b6">[7]</ref>. For ImageNet, we used a 10-layer ResNet for both the generator and discriminator.</p><p>In all experiments we used 64 filters for the smallest convolutional layer, and double it at each layer (CelebA/ImageNet) or every other layer (CIFAR-10). The input codes for the generator are drawn from Uniform [−1, 1] 128 . We consider two parameterizations for each critic: a standard one where the parameters can take any real value, and a spectral parametrization (denoted SN-) as above <ref type="bibr" target="#b31">[32]</ref>. Models without explicit gradient control (SN-GAN, SN-MMDGAN, SN-MMGAN-L2, SN-WGAN) fix γ = 1, for spectral normalization; others learn γ, using a spectral parameterization.</p><p>Training All models were trained for 150 000 generator updates on a single GPU, except for ImageNet where the model was trained on 3 GPUs simultaneously. To limit communication overhead we averaged the MMD estimate on each GPU, giving the block MMD estimator <ref type="bibr" target="#b53">[54]</ref>. We always used 64 samples per GPU from each of P and Q, and 5 critic updates per generator step. We used initial learning rates of 0.0001 for CIFAR-10 and CelebA, 0.0002 for ImageNet, and decayed these rates using the KID adaptive scheme of <ref type="bibr" target="#b6">[7]</ref>: every 2 000 steps, generator samples are compared to those from 20 000 steps ago, and if the relative KID test <ref type="bibr" target="#b8">[9]</ref> fails to show an improvement three consecutive times, the learning rate is decayed by 0.8. We used the Adam optimizer <ref type="bibr" target="#b24">[25]</ref> with β 1 = 0.5, β 2 = 0.9.</p><p>Evaluation To compare the sample quality of different models, we considered three different scores based on the Inception network <ref type="bibr" target="#b48">[49]</ref> trained for ImageNet classification, all using default parameters in the implementation of <ref type="bibr" target="#b6">[7]</ref>. The Inception Score (IS) <ref type="bibr" target="#b41">[42]</ref> is based on the entropy of predicted labels; higher values are better. Though standard, this metric has many issues, particularly on datasets other than ImageNet <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. The FID <ref type="bibr" target="#b19">[20]</ref> instead measures the similarity of samples from the generator and the target as the Wasserstein-2 distance between Gaussians fit to their intermediate representations.</p><p>It is more sensible than the IS and becoming standard, but its estimator is strongly biased <ref type="bibr" target="#b6">[7]</ref>. The KID [7] is similar to FID, but by using a polynomial-kernel MMD its estimates enjoy better statistical properties and are easier to compare. (A similar score was recommended by <ref type="bibr" target="#b20">[21]</ref>.)</p><p>Results <ref type="table" target="#tab_1">Table 1a</ref> presents the scores for models trained on both CIFAR-10 and CelebA datasets. On CIFAR-10, SN-SWGAN and SN-SMMDGAN performed comparably to SN-GAN. But on CelebA, SN-SWGAN and SN-SMMDGAN dramatically outperformed the other methods with the same architecture in all three metrics. It also trained faster, and consistently outperformed other methods over multiple initializations <ref type="figure" target="#fig_5">(Figure 2 (a)</ref>). It is worth noting that SN-SWGAN far outperformed WGAN-GP on both datasets.    proposed methods substantially outperformed both methods in FID and KID scores. <ref type="figure" target="#fig_2">Figure 3</ref> shows samples on ImageNet and CelebA; Appendix F.4 has more.</p><p>Spectrally normalized WGANs / MMDGANs To control for the contribution of the spectral parametrization to the performance, we evaluated variants of MMDGANs, WGANs and Sobolev-GAN using spectral normalization (in <ref type="table" target="#tab_2">Table 2</ref>, Appendix F.3). WGAN and Sobolev-GAN led to unstable training and didn't converge at all ( <ref type="figure" target="#fig_1">Figure 11</ref>) despite many attempts. MMDGAN converged on CIFAR-10 ( <ref type="figure" target="#fig_1">Figure 11</ref>) but was unstable on CelebA ( <ref type="figure" target="#fig_1">Figure 10</ref>). The gradient control due to SN is thus probably too loose for these methods. This is reinforced by <ref type="figure" target="#fig_5">Figure 2</ref> (c), which shows that the expected gradient of the critic network is much better-controlled by SMMD, even when SN is used. We also considered variants of these models with a learned γ while also adding a gradient penalty and an L 2 penalty on critic activations <ref type="bibr">[7, footnote 19]</ref>. These generally behaved similarly to MMDGAN, and didn't lead to substantial improvements. We ran the same experiments on CelebA, but aborted the runs early when it became clear that training was not successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank collapse</head><p>We occasionally observed the failure mode for SMMD where the critic becomes low-rank, discussed in Section 3.3, especially on CelebA; this failure was obvious even in the training objective. <ref type="figure" target="#fig_5">Figure 2</ref> (b) is one of these examples. Spectral parametrization seemed to prevent this behavior. We also found one could avoid collapse by reverting to an earlier checkpoint and increasing the RKHS regularization parameter λ, but did not do this for any of the experiments here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We studied gradient regularization for MMD-based critics in implicit generative models, clarifying how previous techniques relate to the D Ψ MMD loss. Based on these insights, we proposed the Gradient-Constrained MMD and its approximation the Scaled MMD, a new loss function for IGMs that controls gradient behavior in a principled way and obtains excellent performance in practice.</p><p>One interesting area of future study for these distances is their behavior when used to diffuse particles distributed as Q towards particles distributed as P. Mroueh et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">Appendix A.1]</ref> began such a study for the Sobolev GAN loss; <ref type="bibr" target="#b34">[35]</ref> proved convergence and studied discrete-time approximations.</p><p>Another area to explore is the geometry of these losses, as studied by Bottou et al. <ref type="bibr" target="#b7">[8]</ref>, who showed potential advantages of the Wasserstein geometry over the MMD. Their results, though, do not address any distances based on optimized kernels; the new distances introduced here might have interesting geometry of their own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>We first review some basic properties of Reproducing Kernel Hilbert Spaces. We consider here a separable RKHS H with basis (e i ) i∈I , where I is either finite if H is finite-dimensional, or I = N otherwise. We also assume that the reproducing kernel k is continuously twice differentiable.</p><p>We use a slightly nonstandard notation for derivatives: ∂ i f (x) denotes the ith partial derivative of f evaluated at x, and ∂ i ∂ j+d k(x, y) denotes ∂ 2 k(a,b) ∂ai∂bj | (a,b)=(x,y) . Then the following reproducing properties hold for any given function f in H <ref type="bibr" target="#b46">[47,</ref><ref type="bibr">Lemma 4.34]</ref>:</p><formula xml:id="formula_26">f (x) = f, k(x, .) H (8) ∂ i f (x) = f, ∂ i k(x, .) H .<label>(9)</label></formula><p>We say that an operator A :</p><formula xml:id="formula_27">H → H is Hilbert-Schmidt if A 2 HS = i∈I Ae i 2 H is finite.</formula><p>A HS is called the Hilbert-Schmidt norm of A. The space of Hilbert-Schmidt operators itself a Hilbert space with the inner product A, B HS = i∈I Ae i , Be i H . Moreover, we say that an operator A is trace-class if its trace norm is finite, i.e. A 1 = i∈I e i , (A * A)</p><formula xml:id="formula_28">1 2 e i H &lt; ∞. The outer product f ⊗ g for f, g ∈ H gives an H → H operator such that (f ⊗ g)v = g, v H f for all v in H.</formula><p>Given two vectors f and g in H and a Hilbert-Schmidt operator A we have the following properties:</p><p>(i) The outer product f ⊗ g is a Hilbert-Schmidt operator with Hilbert-Schmidt norm given by:</p><formula xml:id="formula_29">f ⊗ g HS = f H g H . (ii) The inner product between two rank-one operators f ⊗ g and u ⊗ v is f ⊗ g, u ⊗ v HS = f, u H g, v H . (iii) The following identity holds: f, Ag H = f ⊗ g, A HS .</formula><p>Define the following covariance-type operators:</p><formula xml:id="formula_30">D x = k(x, ·) ⊗ k(x, ·) + d i=1 ∂ i k(x, ·) ⊗ ∂ i k(x, ·) D µ = E X∼µ D X D µ,λ = D µ + λI; (10)</formula><p>these are useful in that, using <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_26">(9)</ref></p><formula xml:id="formula_31">, f, D x g = f (x)g(x) + d i=1 ∂ i f (x) ∂ i g(x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Definitions and estimators of the new distances</head><p>We will need the following assumptions about the distributions P and Q, the measure µ, and the kernel k:</p><p>(A) P and Q have integrable first moments.</p><p>(B) k(x, x) grows at most linearly in x: for all x in X , k(x, x) ≤ C( x + 1) for some constant C. <ref type="figure">Assumption (B)</ref> is automatically satisfied by a K such as the Gaussian; when K is linear, it is true for a quite general class of networks φ ψ [7, Lemma 1].</p><formula xml:id="formula_32">(C) The kernel k is twice continuously differentiable. (D) The functions x → k(x, x) and x → ∂ i ∂ i+d k(x, x) for 1 ≤ i ≤ d are µ-integrable. When k = K • φ ψ ,</formula><p>We will first give a form for the Gradient-Constrained MMD (5) in terms of the operator (10): Proposition 5. Under Assumptions (A) to (D), the Gradient-Constrained MMD is given by</p><formula xml:id="formula_33">GCMMD µ,k,λ (P, Q) = η, D −1 µ,λ η H .<label>(11)</label></formula><p>Proof of Proposition 5. Let f be a function in H. We will first express the squared λ-regularized Sobolev norm of f (6) as a quadratic form in H. Recalling the reproducing properties of (8) and <ref type="formula" target="#formula_26">(9)</ref>, we have:</p><formula xml:id="formula_34">f 2 S(µ),k,λ = f, k(x, ·) 2 H µ(dx) + d i=1 f, ∂ i k(x, ·) 2 H µ(dx) + λ f 2 H .</formula><p>Using Property (ii) and the operator <ref type="formula" target="#formula_0">(10)</ref>, one further gets</p><formula xml:id="formula_35">f 2 S(µ),k,λ = f ⊗ f, D x HS µ(dx) + λ f 2 H .</formula><p>Under Assumption (D), and using Lemma 6, one can take the integral inside the inner product, which leads to f 2 S(µ),k,λ = f ⊗ f, D µ HS + λ f 2 H . Finally, using Property (iii) it follows that</p><formula xml:id="formula_36">f 2 S(µ),k,λ = f, D µ,λ f H .</formula><p>Under Assumptions (A) and (B), Lemma 6 applies, and it follows that k(x, ·) is also Bochner integrable under P and Q. Thus</p><formula xml:id="formula_37">E P [ f, k(x, ·) H ] − E Q [ f, k(x, ·) H ] = f, E P [k(x, ·)] − E P [k(x, ·)] H = f, η H ,</formula><p>where η is defined as this difference in mean embeddings.</p><p>Since D µ,λ is symmetric positive definite, its square-root D µ,λ f , so that f, D µ,λ f H = g 2 H . Note that for any g ∈ H, there is a corresponding f = D − 1 2 µ,λ g. Thus we can re-express the maximization problem in (5) in terms of g: </p><formula xml:id="formula_38">GCMMD µ,k,λ (P, Q) := sup f ∈H f,D µ,λ f H ≤1 f, η H = sup g∈H g H ≤1 D − 1 2 µ,λ g, η H = sup g∈H g H ≤1 g, D − 1 2 µ,λ η H = D − 1 2 µ,λ η H = η, D −1 µ,λ η H .</formula><formula xml:id="formula_39">GCMMD 2 µ,k,λ (P, Q) = 1 λ MMD 2 (P, Q) −P (η) P (η) = η(X) ∇η(X) T K G T G H + M λI M +M d −1 η(X) ∇η(X) ,</formula><p>where K is the kernel matrix K m,m = k(X m , X m ), G is the matrix of left derivatives G (m,i),m = ∂ i k(X m , X m ), and H that of derivatives of both arguments H (m,i),(m ,j) = ∂ i ∂ j+d k(X m , X m ).</p><p>Before proving Proposition 3, we note the following interesting alternate form. Letē i be the ith standard basis vector for R M +M d , and define T : H → R M +M d as the linear operator</p><formula xml:id="formula_40">T = M m=1ē m ⊗ k(X m , ·) + M m=1 d i=1ē m+(m,i) ⊗ ∂ i k(X m , ·).</formula><p>Then η(X) ∇η(X) = T η, and K G T G H = T T * . Thus we can write</p><formula xml:id="formula_41">GCMMD 2 µ,k,λ = 1 λ η, I − T * (T T * + M λI) −1 T η H .</formula><p>Proof of Proposition 3. Let g ∈ H be the solution to the regression problem D µ,λ g = η:</p><formula xml:id="formula_42">1 M M m=1 g(X m )k(X m , ·) + d i=1 ∂ i g(X m )∂ i k(X m , ·) + λg = η g = 1 λ η − 1 λM M m=1 g(X m )k(X m , ·) + d i=1 ∂ i g(X m )∂ i k(X m , ·) .<label>(12)</label></formula><p>Taking the inner product of both sides of (12) with k(X m , ·) for each 1 ≤ m ≤ M yields the following M equations:</p><formula xml:id="formula_43">g(X m ) = 1 λ η(X m ) − 1 λM M m=1 g(X m )K m,m + d i=1 ∂ i g(X m ) G (m,i),m .<label>(13)</label></formula><p>Doing the same with ∂ j k(X m , ·) gives M d equations:</p><formula xml:id="formula_44">∂ j g(X m ) = 1 λ ∂ j η(X m ) − 1 λM M m=1 g(X m )G (m ,j),m + d i=1 ∂ i g(X m )H (m,i),(m ,j) .<label>(14)</label></formula><p>From <ref type="formula" target="#formula_0">(12)</ref>, it is clear that g is a linear combination of the form:</p><formula xml:id="formula_45">g(x) = 1 λ η(x) − 1 λM M m=1 α m k(X m , x) + d i=1 β m,i ∂ i k(X m , x) ,</formula><p>where the coefficients α := (α m = g(X m )) 1≤m≤M and β := (β m,i = ∂ i g(X m )) 1≤m≤M 1≤i≤d satisfy the system of equations <ref type="formula" target="#formula_0">(13)</ref> and <ref type="bibr" target="#b13">(14)</ref>. We can rewrite this system as</p><formula xml:id="formula_46">K + M λI M G T G H + M λI M d α β = M η(X) ∇η(X) ,</formula><p>where I M , I M d are the identity matrices of dimension M , M d. Since K and H must be positive semidefinite, an inverse exists. We conclude by noticing that</p><formula xml:id="formula_47">GCMMDμ ,k,λ (P, Q) 2 = η, g H = 1 λ η 2 H − 1 λM M m=1 α m η(X m ) + d i=1 β m,i ∂ i η(X m ) .</formula><p>The following result was key to our definition of the SMMD in Section 3.3. Proposition 4. Under Assumptions (A) to (D), we have for all f ∈ H that</p><formula xml:id="formula_48">f S(µ),k,λ ≤ σ −1 µ,k,λ f H k , where σ k,µ,λ := 1/ λ + k(x, x)µ(dx) + d i=1 ∂ i ∂ i+d k(x, x)µ(dx).</formula><p>Proof of Proposition 4. The key idea here is to use the Cauchy-Schwarz inequality for the Hilbert-Schmidt inner product.</p><formula xml:id="formula_49">Letting f ∈ H, f 2 S(µ),k,λ is f (x) 2 µ(dx) + ∇f (x) 2 µ(dx) + λ f 2 H (a) = f, k(x, ·) ⊗ k(x, ·)f H µ(dx) + d i=1 f, ∂ i k(x, ·) ⊗ ∂ i k(x, ·)f H µ(dx) + λ f 2 H (b) = f ⊗ f, k(x, ·) ⊗ k(x, ·) HS µ(dx) + d i=1 f ⊗ f, ∂ i k(x, ·) ⊗ ∂ i k(x, ·) HS µ(dx) + λ f 2 H (c) ≤ f 2 H k(x, x) µ(dx) + d i=1 ∂ i ∂ i+d k(x, x) µ(dx) + λ .</formula><p>(a) follows from the reproducing properties <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_26">(9)</ref> and Property (ii). (b) is obtained using Property (iii), while (c) follows from the Cauchy-Schwarz inequality and Property (i). Under Assumptions (A) and (B), k(x, ·) is Bochner integrable with respect to any probability distribution P with finite first moment and the following relation holds:</p><formula xml:id="formula_50">f, E P [k(x, ·)] H = E P [ f, k(x, ·) H ] for all f in H.</formula><p>Proof. The operator D x is positive self-adjoint. It is also trace-class, as by the triangle inequality</p><formula xml:id="formula_51">D x 1 ≤ k(x, ·) ⊗ k(x, ·) 1 + d i=1 ∂ i k(x, ·) ⊗ ∂ i k(x, ·) 1 = k(x, ·) 2 H + d i=1 ∂ i k(x, ·) 2 H &lt; ∞.</formula><p>By Assumption (D), we have that </p><formula xml:id="formula_52">D x 1 µ(dx) &lt; ∞ which implies that D x is µ-integrable in</formula><formula xml:id="formula_53">f, E P [k(x, ·)] H = E P [ f, k(x, ·) H ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Continuity of the Optimized Scaled MMD in the Wasserstein topology</head><p>To prove Theorem 1, we we will first need some new notation.</p><p>We assume the kernel is k = K • φ ψ , i.e. k ψ (x, y) = K(φ ψ (x), φ ψ (y)), where the representation function φ ψ is a network φ ψ (X) : R d → R d L consisting of L fully-connected layers:</p><formula xml:id="formula_54">h 0 ψ (X) = X h l ψ (X) = W l σ l−1 (h l−1 ψ (X)) + b l for 1 ≤ l ≤ L (15) φ ψ (X) = h L ψ (X).</formula><p>The intermediate representations h l ψ (X) are of dimension d l , the weights W l are matrices in R d l ×d l−1 , and biases b l are vectors in R d l . The elementwise activation function σ is given by σ 0 (x) = x, and for l &gt; 0 the activation σ l is a leaky ReLU with leak coefficient 0 &lt; α &lt; 1:</p><formula xml:id="formula_55">σ l (x) = σ(x) = x x &gt; 0 αx x ≤ 0 for l &gt; 0.<label>(16)</label></formula><p>The parameter ψ is the concatenation of all the layer parameters:</p><formula xml:id="formula_56">ψ = (W L , b L ), (W L−1 , b L−1 ), . . . , (W 1 , b 1 ) .</formula><p>We denote by Ψ the set of all such possible parameters, i.e.</p><formula xml:id="formula_57">Ψ = R d L ×d L−1 ×R d L ×· · ·×R d1×d ×R d1 .</formula><p>Define the following restrictions of Ψ:</p><formula xml:id="formula_58">Ψ κ := ψ ∈ Ψ | ∀1 ≤ l ≤ L, cond(W l ) ≤ κ<label>(17)</label></formula><formula xml:id="formula_59">Ψ κ 1 := ψ ∈ Ψ κ | ∀1 ≤ l ≤ L, W l = 1 .<label>(18)</label></formula><p>Ψ κ is the set of those parameters such that W l have a small condition number, cond(W ) = σ max (W )/σ min (W ). Ψ κ 1 is the set of per-layer normalized parameters with a condition number bounded by κ.</p><p>Recall the definition of Scaled MMD, <ref type="bibr" target="#b6">(7)</ref>, where λ &gt; 0 and µ is a probability measure:</p><formula xml:id="formula_60">SMMD µ,k,λ (P, Q) := σ µ,k,λ MMD k (P, Q) σ k,µ,λ := 1/ λ + k(x, x) µ(dx) + d i=1 ∂ i ∂ i+d k(x, x) µ(dx).</formula><p>The Optimized SMMD over the restricted set Ψ κ is given by:</p><formula xml:id="formula_61">D µ,Ψ κ ,λ SMMD (P, Q) := sup ψ∈Ψ κ SMMD µ,k ψ ,λ .</formula><p>The constraint to ψ ∈ Ψ κ is critical to the proof. In practice, using a spectral parametrization helps enforce this assumption, as shown in <ref type="figure" target="#fig_5">Figures 2 and 9</ref>. Other regularization methods, like orthogonal normalization <ref type="bibr" target="#b10">[11]</ref>, are also possible.</p><p>We will use the following assumptions:</p><p>(I) µ is a probability distribution absolutely continuous with respect to the Lebesgue measure. (II) The dimensions of the weights are decreasing per layer: d l+1 ≤ d l for all 0 ≤ l ≤ L − 1.</p><p>(III) The non-linearity used is Leaky-ReLU, <ref type="bibr" target="#b15">(16)</ref>, with leak coefficient α ∈ (0, 1). (IV) The top-level kernel K is globally Lipschitz in the RKHS norm: there exists a positive constant L K &gt; 0 such that K(a, .</p><formula xml:id="formula_62">) − K(b, .) ≤ L K a − b for all a and b in R d L . (V) There is some γ K &gt; 0 for which K satisfies ∇ b ∇ c K(b, c) (b,c)=(a,a) γ 2 I for all a ∈ R d L .<label>(19)</label></formula><p>Assumption (I) ensures that the points where φ ψ (X) is not differentiable are reached with probability 0 under µ. This assumption can be easily satisfied e.g. if we define µ by adding Gaussian noise to P.</p><p>Assumption (II) helps ensure that the span of W l is never contained in the null space of W l+1 . Using Leaky-ReLU as a non-linearity, Assumption (III), further ensures that the network φ ψ is locally full-rank almost everywhere; this might not be true with ReLU activations, where it could be always 0. Assumptions (II) and (III) can be easily satisfied by design of the network.</p><p>Assumptions (IV) and (V) only depend on the top-level kernel K and are easy to satisfy in practice.</p><p>In particular, they always hold for a smooth translation-invariant kernel, such as the Gaussian, as well as the linear kernel.</p><p>We are now ready to prove Theorem 1. Theorem 1. Under Assumptions (I) to (V),</p><formula xml:id="formula_63">D µ,Ψ κ ,λ SMMD (P, Q) ≤ L K κ L/2 γ √ d L α L/2 W(P, Q), which implies that if P n W −→ P, then D µ,Ψ κ ,λ SMMD (P n , P) → 0.</formula><p>Proof. Define the pseudo-distance corresponding to the kernel k ψ</p><formula xml:id="formula_64">d ψ (x, y) = k ψ (x, ·) − k ψ (y, ·) H ψ = k ψ (x, x) + k ψ (y, y) − 2k ψ (x, y).</formula><p>Denote by W d ψ (P, Q) the optimal transport metric between P and Q using the cost d ψ , given by</p><formula xml:id="formula_65">W d ψ (P, Q) = inf π∈Π(P,Q) E (X,Y )∼π [d ψ (X, Y )] .</formula><p>where Π is the set of couplings with marginals P and Q. By Lemma 7,</p><formula xml:id="formula_66">MMD ψ (P, Q) ≤ W d ψ (P, Q).</formula><p>Recall that φ ψ is Lipschitz, φ ψ Lip &lt; ∞, so along with Assumption (IV) we have that</p><formula xml:id="formula_67">d ψ (x, y) ≤ L K φ ψ (x) − φ ψ (y) ≤ L K φ ψ Lip x − y . Thus W d ψ (P, Q) ≤ inf π∈Π(P,Q) E (X,Y )∼π [L K φ ψ Lip X − Y ] = L K φ ψ Lip W(P, Q),</formula><p>where W is the standard Wasserstein distance (2), and so MMD ψ (P, Q) ≤ L k φ ψ Lip W(P, Q).</p><p>We have that</p><formula xml:id="formula_68">∂ i ∂ i+d k(x, y) = [∂ i φ ψ (x)] T ∇ a ∇ b K(a, b) (a,b)=(φ ψ (x),φ ψ (y)) [∂ i φ ψ (y)]</formula><p>, where the middle term is a d L × d L matrix and the outer terms are vectors of length d L . Thus Assumption (V)</p><formula xml:id="formula_69">implies that ∂ i ∂ i+d k(x, x) ≥ γ 2 K ∂ i φ ψ (x) 2 , and hence σ −2 µ,k,λ ≥ γ 2 K E[ ∇φ ψ (X) 2 F ] so that SMMD 2 ψ (P, Q) = σ 2 µ,k,λ MMD 2 ψ (P, Q) ≤ L 2 K φ ψ 2 Lip γ 2 K E [ ∇φ ψ (X) 2 F ] W 2 (P, Q).</formula><p>Using Lemma 8, we can write φ ψ (X) = α(ψ)φψ(X) withψ ∈ Ψ κ 1 . Then we have</p><formula xml:id="formula_70">φ ψ 2 Lip E µ ∇φ ψ (X) 2 F = α(ψ) 2 φψ 2 Lip α(ψ) 2 E µ ∇φψ(X) 2 F ≤ 1 E µ ∇φψ(X) 2 F , where we used φψ Lip ≤ L l=1</formula><p>W l = 1. But by Lemma 9, for Lebesgue-almost all X,</p><formula xml:id="formula_71">∇φψ(X) 2 F ≥ d L (α/κ) L . Using Assumption (I), this implies that φ ψ 2 Lip E µ ∇φ ψ (X) 2 F ≤ 1 E µ ∇φψ(X) 2 F ] ≤ κ L d L α L .</formula><p>Thus for any ψ ∈ Ψ κ ,</p><formula xml:id="formula_72">SMMD ψ (P, Q) ≤ L K κ L/2 γ K √ d L α L/2 W(P, Q).</formula><p>The desired bound on D µ,Ψ κ ,λ SMMD follows immediately. Lemma 7. Let (x, y) → k(x, y) be the continuous kernel of an RKHS H defined on a Polish space X , and define the corresponding pseudo-distance d k (x, y) := k(x, ·) − k(y, ·) H . Then the following inequality holds for any distributions P and Q on X , including when the quantities are infinite:</p><formula xml:id="formula_73">MMD k (P, Q) ≤ W d k (P, Q).</formula><p>Proof. Let P and Q be two probability distributions, and let Π(P, Q) be the set of couplings between them. Let π * ∈ argmin (X,Y )∼π [c k (X, Y )] be an optimal coupling, which is guaranteed to exist [51, Theorem 4.1]; by definition</p><formula xml:id="formula_74">W d k (P, Q) = E (X,Y )∼π * [d k (X, Y )]. When W d k (P, Q) = ∞ the inequality trivially holds, so assume that W d k (P, Q) &lt; ∞.</formula><p>Take a sample (X, Y ) ∼ π and a function f ∈ H with f H ≤ 1. By the Cauchy-Schwarz inequality,</p><formula xml:id="formula_75">f (X) − f (Y ) ≤ f H k(X, ·) − k(Y, ·) H ≤ k(X, ·) − k(Y, ·) H .</formula><p>Taking the expectation with respect to π , we obtain</p><formula xml:id="formula_76">E π [|f (X) − f (Y )|] ≤ E π [ k(X, ·) − k(Y, ·) H ].</formula><p>The right-hand side is just the definition of W d k (P, Q). By Jensen's inequality, the left-hand side is lower-bounded by</p><formula xml:id="formula_77">|E π * [f (X) − f (Y )]| = |E X∼P [f (X)] − E Y ∼Q [f (Y )]|</formula><p>since π has marginals P and Q. We have shown so far that for any f ∈ H with f H ≤ 1,</p><formula xml:id="formula_78">|E P [f (X)] − E Q [f (Y )]| ≤ W c k (P, Q);</formula><p>the result follows by taking the supremum over f .</p><formula xml:id="formula_79">Lemma 8. Let ψ = ((W L , b L ), (W L−1 , b L−1 ), . . . , (W 1 , b 1 )) ∈ Ψ κ .</formula><p>There exists a corresponding scalar α(ψ) andψ = ((W L ,b L ), (W L−1 ,b L−1 ), . . . , (W 1 ,b 1 )) ∈ Ψ κ 1 , defined by <ref type="bibr" target="#b17">(18)</ref>, such that for all X, φ ψ (X) = α(ψ) φψ(X).</p><formula xml:id="formula_80">Proof. SetW l = 1 W l W l ,b l = 1 l m=1 W m b l , and α(ψ) = L l=1 W l .</formula><p>Note that the condition number is unchanged, cond(W l ) = cond(W l ) ≤ κ, and W l = 1, soψ ∈ Φ κ 1 . It is also easy to see from <ref type="bibr" target="#b15">(16)</ref> that</p><formula xml:id="formula_81">h lψ (X) = 1 l m=1 W m h l ψ (X) so that α(ψ)h L ψ (X) = L l=1 W l L l=1 W l h L ψ (X) = φ ψ (X).</formula><p>Lemma 9. Make Assumptions (II) and (III), and let ψ ∈ Ψ κ 1 . Then the set of inputs for which any intermediate activation is exactly zero,</p><formula xml:id="formula_82">N ψ = L l=1 d l k=1 X ∈ R d | h l ψ (X) k = 0 ,</formula><p>has zero Lebesgue measure. Moreover, for any X / ∈ N ψ , ∇ X φ ψ (X) exists and</p><formula xml:id="formula_83">∇ X φ ψ (X) 2 F ≥ d L α L κ L .</formula><p>Proof. First, note that the network representation at layer l is piecewise affine. Specifically, define M l X ∈ R d l by, using Assumption (III),</p><formula xml:id="formula_84">(M l X ) k = σ l (h l k (X)) = 1 h l k (X) &gt; 0 α h l k (X) &lt; 0 ;</formula><p>it is undefined when any h l k (X) = 0, i.e. when X ∈ N ψ . Let V l X := W l diag M l−1 X . Then</p><formula xml:id="formula_85">h l ψ (X) = W l σ l−1 (h l−1 ψ (X)) + b l = V l X X + b l , and thus h l ψ (X) = W l X X + b l X ,<label>(20)</label></formula><formula xml:id="formula_86">where b 0 X = 0, b l X = V l X b l−1 + b l , and W l X = V l X V l−1 X · · · V 1 X , so long as X / ∈ N ψ .</formula><p>Because ψ ∈ Ψ κ 1 , we have W l = 1 and σ min (W l ) ≥ 1/κ; also, M l X ≤ 1, σ min (M l X ) ≥ α. Thus W l X ≤ 1, and using Assumption (II) with Lemma 10 gives σ min (W l X ) ≥ (α/κ) l . In particular, each W l X is full-rank. Next, note that b l X and W l X each only depend on X through the activation patterns M l X . Letting H l X = (M l X , M l−1 X , . . . , M 1 X ) denote the full activation patterns up to level l, we can thus write</p><formula xml:id="formula_87">h l ψ (X) = W H l X X + b H l X .</formula><p>There are only finitely many possible values for H l X ; we denote the set of such values as H l . Then we have that</p><formula xml:id="formula_88">N ψ ⊆ L l=0 d L k=1 H l ∈H l X ∈ R d | W H l k X + b H l k = 0 .</formula><p>Because each W H l k is of rank d l , each set in the union is either empty or an affine subspace of dimension d − d l . As each d l &gt; 0, each set in the finite union has zero Lebesgue measure, and N ψ also has zero Lebesgue measure.</p><p>We will now show that the activation patterns are piecewise constant, so that ∇ X h l ψ (X) = W H l X for all X / ∈ N ψ . Because ψ ∈ Ψ κ 1 , we have h l ψ Lip ≤ 1, and in particular</p><formula xml:id="formula_89">h l ψ (X) k − h l ψ (X ) k ≤ X − X .</formula><p>Thus, take some X / ∈ N ψ , and find the smallest absolute value of its activations, = min l=1,...,L min k=1,...,d l h l ψ (X) k ; clearly &gt; 0. For any X with X − X &lt; , we know that for all l and k,</p><formula xml:id="formula_90">sign h l ψ (X) k = sign h l ψ (X ) k ,</formula><p>implying that H l X = H l X as well as X / ∈ N ψ . Thus for any point X / ∈ N ψ , ∇φ ψ (X) = W H L X . Finally, we obtain</p><formula xml:id="formula_91">∇φ ψ (X) 2 F = W H L X 2 F ≥ d L σ min W H L X 2 ≥ d L α L κ L . Lemma 10. Let A ∈ R m×n , B ∈ R n×p , with m ≥ n ≥ p. Then σ min (AB) ≥ σ min (A) σ min (B).</formula><p>Proof. A more general version of this result can be found in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">Theorem 2]</ref>; we provide a proof here for completeness.</p><p>If B has a nontrivial null space, σ min (B) = 0 and the inequality holds. Otherwise, let R n * denote R n \ {0}. Recall that for C ∈ R m×n with m ≥ n,</p><formula xml:id="formula_92">σ min (C) = λ min (C T C) = inf x∈R n * x T C T Cx x T x = inf x∈R n * Cx x .</formula><p>Thus, as Bx = 0 for x = 0, Condition Number: We start by a first example where the condition number can be arbitrarily high. We consider a two-layer network on R 2 , defined by</p><formula xml:id="formula_93">φ α (X) = [1 −1] σ(W α X) W α = 1 1 1 1 + α<label>(21)</label></formula><p>where α &gt; 0. As α approaches 0 the matrix W α becomes singular which means that its condition number blows up. We are interested in analyzing the behavior of the Lipschitz constant of φ and the expected squared norm of its gradient under µ as α approaches 0.</p><p>One can easily compute the squared norm of the gradient of φ which is given by</p><formula xml:id="formula_94">∇φ α (X) 2 =        α 2 X ∈ A 1 γ 2 α 2 X ∈ A 2 (1 − γ) 2 + (1 + α − γ) 2 X ∈ A 3 (1 − γ) 2 + (γα + γ − 1) 2 X ∈ A 4<label>(22)</label></formula><p>Here A 1 , A 2 , A 3 and A 4 are defined by <ref type="bibr" target="#b22">(23)</ref> and are represented in <ref type="figure" target="#fig_3">Figure 4</ref>: <ref type="figure" target="#fig_3">Figure 4</ref>: Decomposition of R 2 into 4 regions A 1 , A 2 , A 3 and A 4 as defined in <ref type="bibr" target="#b22">(23)</ref>. As α approaches 0, the area of sets A 3 and A 4 becomes negligible.</p><formula xml:id="formula_95">A 1 := {X ∈ R 2 |X 1 + X 2 ≥ 0 X 1 + (1 + α)X 2 ≥ 0} A 2 := {X ∈ R 2 |X 1 + X 2 &lt; 0 X 1 + (1 + α)X 2 &lt; 0} A 3 := {X ∈ R 2 |X 1 + X 2 &lt; 0 X 1 + (1 + α)X 2 ≥ 0} A 4 := {X ∈ R 2 |X 1 + X 2 ≥ 0 X 1 + (1 + α)X 2 &lt; 0}<label>(23)</label></formula><p>It is easy to see that whenever µ has a density, the probability of the sets A 3 and A 4 goes to 0 are α → 0. Hence one can deduce that E µ [ ∇φ α (X) 2 ] → 0 when α → 0. On the other hand, the squared Lipschitz constant of φ is given by (1 − γ) 2 + (1 + α − γ) 2 which converges to 2(1 − γ) 2 . This shows that controlling the expectation of the gradient doesn't allow to effectively control the Lipschitz constant of φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monotonicity of the dimensions:</head><p>We would like to consider a second example where Assumption (II) doesn't hold. Consider the following two layer network defined by:</p><formula xml:id="formula_96">φ(X) = [−1 0 1] σ(W β X) W β := 1 0 0 1 1 β<label>(24)</label></formula><p>for β &gt; 0. Note that W β is a full rank matrix, but Assumption (II) doesn't hold. Depending on the sign of the components of W β X one has the following expression for ∇φ α (X) 2 :</p><formula xml:id="formula_97">∇φ α (X) 2 =                  β 2 X ∈ B 1 γ 2 β 2 X ∈ B 2 β 2 X ∈ B 3 (1 − γ) 2 + γ 2 β 2 X ∈ B 4 (1 − γ) 2 + β 2 X ∈ B 5 γ 2 β 2 X ∈ B 6<label>(25)</label></formula><p>where (B i ) 1≤i≤6 are defined by (26)</p><formula xml:id="formula_98">B 1 := {X ∈ R 2 |X 1 ≥ 0 X 2 ≥ 0} B 2 := {X ∈ R 2 |X 1 &lt; 0 X 2 &lt; 0} B 3 := {X ∈ R 2 |X 1 ≥ X 2 &lt; 0 X 1 + βX 2 ≥ 0} B 4 := {X ∈ R 2 |X 1 ≥ X 2 &lt; 0 X 1 + βX 2 &lt; 0} B 5 := {X ∈ R 2 |X 1 &gt; 0 X 2 ≥ 0 X 1 + βX 2 ≥ 0} B 6 := {X ∈ R 2 |X 1 &gt; 0 X 2 ≥ 0 X 1 + βX 2 &lt; 0}<label>(26)</label></formula><p>20</p><p>The squared Lipschitz constant is given by φ 2 L (1 − γ) 2 + β 2 while the expected squared norm of the gradient of φ is given by:</p><formula xml:id="formula_99">E µ [ φ(X) 2 ] = 3β 2 (p(B 1 ∪ B 3 ∪ B 5 ) + γ 2 p(B 2 ∪ B 4 ∪ B 6 )) + (1 − γ) 2 p(B 4 ∪ B 5 ). (27)</formula><p>Again the set B 4 ∪ B 5 becomes negligible as β approaches 0 which implies that E µ [ φ(X) 2 ] → 0. On the other hand φ 2 L converges to (1 − γ) 2 . Note that unlike in the first example in <ref type="bibr" target="#b20">(21)</ref>, the matrix W β has a bounded condition number. In this example, the columns of W 0 are all in the null space of [−1 0 1], which implies ∇φ 0 (X) = 0 for all X ∈ R 2 , even though all matrices have full rank.</p><p>B DiracGAN vector fields for more losses <ref type="figure" target="#fig_8">Figure 5</ref>: Vector fields for different losses with respect to the generator parameter θ and the feature representation parameter ψ; the losses use a Gaussian kernel, and are shown in <ref type="bibr" target="#b27">(28)</ref>. Following <ref type="bibr" target="#b29">[30]</ref>, P = δ 0 , Q = δ θ and φ ψ (x) = ψx. The curves show the result of taking simultaneous gradient steps in (θ, ψ) beginning from three initial parameter values. <ref type="figure" target="#fig_8">Figure 5</ref> shows parameter vector fields, like those in <ref type="figure" target="#fig_9">Figure 6</ref>, for Example 1 for a variety of different losses:</p><formula xml:id="formula_100">MMD: − MMD 2 ψ MMD-GP: − MMD 2 ψ +λ E P [( ∇f (X) − 1) 2 ] MMD-GP-Unif: − MMD 2 ψ +λ E X µ * [( ∇f ( X) − 1) 2 ] SN-MMD: − 2 MMD 1 (P, Q) 2 Sobolev-MMD: − MMD 2 ψ +λ(E (P+Q)/2 [ ∇f (X) 2 ] − 1) 2 CenteredSobolev-MMD: − MMD 2 ψ +λ(E (P+Q)/2 [ ∇f (X) 2 ]) 2 LipMMD: − LipMMD 2 k ψ ,λ GC-MMD: − GCMMD 2 N (0,10 2 ),k ψ ,λ SMMD: − SMMD 2 k ψ ,P,λ<label>(28)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21</head><p>The squared MMD between δ 0 and δ θ under a Gaussian kernel of bandwidth 1/ψ and is given by</p><formula xml:id="formula_101">2(1 − e − ψ 2 θ 2 2</formula><p>). MMD-GP-unif uses a gradient penalty as in <ref type="bibr" target="#b6">[7]</ref> where each samples from µ * is obtained by first sampling X and Y from P and Q and then sampling uniformly between X and Y . MMD-GP uses the same gradient penalty, but the expectation is taken under P rather than µ * . SN-MMD refers to MMD with spectral normalization; here this means that ψ = 1. Sobolev-MMD refers to the loss used in <ref type="bibr" target="#b32">[33]</ref> with the quadratic penalty only. GCMMD µ,k,λ is defined by (5), with µ = N (0, 10 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Vector fields of Gradient-Constrained MMD and Sobolev GAN critics</head><p>Mroueh et al. <ref type="bibr" target="#b32">[33]</ref> argue that the gradient of the critic (...) defines a transportation plan for moving the distribution mass (from generated to reference distribution) and present the solution of Sobolev PDE for 2-dimensional Gaussians. We observed that in this simple example the gradient of the Sobolev critic can be very high outside of the areas of high density, which is not the case with the Gradient-Constrained MMD. <ref type="figure" target="#fig_9">Figure 6</ref> presents critic gradients in both cases, using µ = (P + Q)/2 for both.   <ref type="figure" target="#fig_9">Figure 6</ref>: Vector fields of critic gradients between two Gaussians. The grey arrows show normalized gradients, i.e. gradient directions, while the black ones are the actual gradients. Note that for the Sobolev critic, gradients norms are orders of magnitudes higher on the right hand side of the plot than in the areas of high density of the given distributions.</p><p>This unintuitive behavior is most likely related to the vanishing boundary condition, assummed by Sobolev GAN. Solving the actual Sobolev PDE, we found that the Sobolev critic has very high gradients close to the boundary in order to match the condition; moreover, these gradients point in opposite directions to the target distribution. and the corresponding critic function is</p><formula xml:id="formula_102">η(t) η H = E X∼P φ(X)φ(t) − E Y ∼Q φ(Y )φ(t) |E P φ(X) − E Q φ(Y )| = sign (E X∼P φ(X) − E Y ∼Q φ(Y )) φ(t).</formula><p>Thus if we assume E X∼P φ(X) &gt; E Y ∼Q φ(Y ), as that is the goal of our critic training, we see that the MMD becomes identical to the WGAN loss, and the gradient penalty is applied to the same function.</p><p>(MMD GANs, however, would typically train on the unbiased estimator of MMD 2 , giving a very slightly different loss function. <ref type="bibr" target="#b6">[7]</ref> also applied the gradient penalty to η rather than the true critic η/ η .)</p><p>The SMMD with a linear kernel is thus analogous to applying the scaling operator to a WGAN; hence the name SWGAN.  <ref type="figure">Figure 7</ref> shows the behavior of the MMD, the Gradient-Constrained SMMD, and the Scaled MMD when comparing Gaussian distributions. We can see that MMD ∝ SMMD and the Gradient-Constrained MMD behave similarly in this case, and that optimizing the SMMD and the Gradient-Constrained MMD is also similar. Optimizing the MMD would yield an essentially constant distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 IGMs with Optimized Gradient-Constrained MMD loss</head><p>We implemented the estimator of Proposition 3 using the empirical mean estimator of η, and sharing samples for µ = P. To handle the large but approximately low-rank matrix system, we used an incomplete Cholesky decomposition <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">Algorithm 5.12</ref>] to obtain R ∈ R ×M (1+d) such that</p><formula xml:id="formula_103">K G T G H ≈ R T R.</formula><p>Then the Woodbury matrix identity allows an efficient evaluation:</p><formula xml:id="formula_104">R T R + M λI −1 = 1 M λ I − R(RR T + M λI) −1 R .</formula><p>Even though only a small is required for a good approximation, and the full matrices K, G, and H need never be constructed, backpropagation through this procedure is slow and not especially GPU-friendly; training on CPU was faster. Thus we were only able to run the estimator on MNIST, and even that took days to conduct the optimization on powerful workstations.</p><p>The learned models, however, were reasonable. Using a DCGAN architecture, batches of size 64, and a procedure that otherwise agreed with the setup of Section 4, samples with and without spectral normalization are shown in <ref type="figure" target="#fig_16">Figures 8a and 8b</ref>. After the points in training shown, however, the same rank collapse as discussed in Section 4 occurred. Here it seems that spectral normalization may have delayed the collapse, but not prevented it. <ref type="figure" target="#fig_16">Figure 8c</ref> shows generator loss estimates through training, including the obvious peak at collapse; <ref type="figure" target="#fig_16">Figure 8d</ref> shows KID scores based on the MNIST-trained convnet representation <ref type="bibr" target="#b6">[7]</ref>, including comparable SMMD models for context. The fact that SMMD models converged somewhat faster than Gradient-Constrained MMD models here may be more related to properties of the estimator of Proposition 3 rather than the distances; more work would be needed to fully compare the behavior of the two distances. <ref type="figure">Figure 9</ref> shows the distribution of critic weight singular values, like <ref type="figure" target="#fig_5">Figure 2</ref>, at more layers. <ref type="figure" target="#fig_1">Figure 11</ref> and <ref type="table" target="#tab_2">Table 2</ref> show results for the spectral normalization variants considered in the experiments. MMDGAN, with neither spectral normalization nor a gradient penalty, did surprisingly well in this case, though it fails badly in other situations. <ref type="figure">Figure 9</ref> compares the decay of singular values for layer of the critic's network at both early and later stages of training in two cases: with or without the spectral parametrization. The model was trained on CelebA using SMMD. <ref type="figure" target="#fig_1">Figure 11</ref> shows the evolution per iteration of Inception score,     <ref type="figure" target="#fig_1">Figure 11</ref>: Evolution per iteration of different scores for variants of methods, mostly using spectral normalization, on CIFAR-10.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Spectral normalization and Scaled MMD</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The setting of Example 1. (a, b): parameter-space gradient fields for the MMD and the SMMD (Section 3.3); the horizontal axis is θ, and the vertical 1/ψ. (c): optimal MMD critics for θ = 20 with different kernels. (d): the MMD and the distances of Section 3 optimized over ψ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 3 .</head><label>3</label><figDesc>Letμ = M m=1 δ Xm . Define η(X) ∈ R M with mth entry η(X m ), and ∇η(X) ∈ R M d with (m, i)th entry 4 ∂ i η(X m ). Then under Assumptions (A) to (D) in Appendix A.1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 4 .</head><label>4</label><figDesc>Make Assumptions (A) to (D). For any</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(b) shows the singular values of the second layer of a critic's network (and Figure 9, in Appendix F.3, shows more layers), while Figure 2 (d) shows the evolution of the condition number during training. The conditioning of the weight matrix remains stable throughout training with spectral parametrization, while it worsens through training in the default case.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>The training process on CelebA. (a) KID scores. We report a final score for SN-GAN slightly before its sudden failure mode; MMDGAN and SN-MMDGAN were unstable and had scores around 100. (b) Singular values of the second layer, both early (dashed) and late (solid) in training. (c) σ −2 µ,k,λ for several MMD-based methods. (d) The condition number in the first layer through training. SN alone does not control σ µ,k,λ , and SMMD alone does not control the condition number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>(a) Scaled MMD GAN with SN (b) SN-GAN (c) Boundary Seeking GAN (d) Scaled MMD GAN with SN (e) Scaled WGAN with SN (f) MMD GAN with GP+L2 Samples from various models. Top: 64 × 64 ImageNet; bottom: 160 × 160 CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 2 µ</head><label>12</label><figDesc>,λ is well-defined and is also invertible. For any f ∈ H, let g = D 1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Proposition 5 ,</head><label>5</label><figDesc>though, involves inverting the infinite-dimensional operator D µ,λ and thus doesn't directly give us a computable estimator. Proposition 3 solves this problem in the case where µ is a discrete measure: Proposition 3. Letμ = M m=1 δ Xm be an empirical measure of M points. Let η(X) ∈ R M have mth entry η(X m ), and ∇η(X) ∈ R M d have (m, i)th entry 7 ∂ i η(X m ). Then under Assumptions (A) to (D), the Gradient-Constrained MMD is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Lemma 6 .</head><label>6</label><figDesc>Under Assumption (D), D x is Bochner integrable and its integral D µ is a trace-class symmetric positive semi-definite operator with D µ,λ = D+λI invertible for any positive λ. Moreover, for any Hilbert-Schmidt operator A we have: A, D µ HS = A, D x HS µ(dx).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>the Bochner sense [39, Definition 1 and Theorem 2]. Its integral D µ is trace-class and satisfies D µ 1 ≤ D x 1 µ(dx). This allows to have A, D µ HS = A, D x HS µ(dx) for all Hilbert-Schmidt operators A. Moreover, the integral preserves the symmetry and positivity. It follows that D µ,λ is invertible. The Bochner integrability of k(x, ·) under a distribution P with finite moment follows directly from Assumptions (A) and (B), since k(x, ·) P(dx) ≤ C ( x + 1) P(dx) &lt; ∞. This allows us to write</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Bx x= σ min (A) σ min (B).A.2.1 When some of the assumptions don't holdHere we analyze through simple examples what happens when the condition number can be unbounded, and when Assumption (II), about decreasing widths of the network, is violated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Gradient-Constrained MMD critic gradient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Sobolev IPM critic gradient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>F Additional experiments F. 1</head><label>experiments1</label><figDesc>Comparison of Gradient-Constrained MMD to Scaled MMD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 : 25 (</head><label>725</label><figDesc>Plots of various distances between one dimensional Gaussians, where P = N (0, 0.1 2 ), and the colors show log D(P, N (µ, σ 2 )). All distances use λ = 1. Top left: MMD with a Gaussian kernel of bandwidth ψ = 0.1. Top right: MMD with bandwidth ψ = 10. Middle left: Gradient-Constrained MMD with bandwidth ψ = 0.1. Middle right: Gradient-Constrained MMD with bandwidth ψ = 10. Bottom left: Optimized SSMD, allowing any ψ ∈ R. Bottom right: Optimized Gradient-Constrained MMD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 :</head><label>8</label><figDesc>The MNIST models with Optimized Gradient-Constrained MMD loss.FID and KID for Sobolev-GAN, MMDGAN and variants of MMDGAN and WGAN using spectral normalization. It is often the case that this parametrization alone is not enough to achieve good results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 :F. 4 Figure 10 :</head><label>9410</label><figDesc>Singular values at different layers, for the same setup as Figure 2. Additional samplesFigures 12 and 13 give extra samples from the models. Evolution of various quantities per generator iteration on CelebA during training. 4 models are considered: (SMMDGAN, SN-SMMDGAN, MMDGAN, SN-MMDGAN). (a) Loss: SMMD 2 = σ 2 µ,k,λ MMD 2 k for SMMDGAN and SN-SMMDGAN, and MMD 2 k for MMDGAN and SN-MMDGAN. The loss saturates for MMDGAN (green); spectral normalization allows some improvement in loss, but training is still unstable (orange). SMMDGAN and SN-SMMDGAN both lead to stable, fast training (blue and red). (b) SMMD controls the critic complexity well, as expected (blue and red); SN has little effect on the complexity (orange). (c) Ratio of the highest singular value to the smallest for the first layer of the critic network: σ max /σ min . SMMD tends to increase the condition number of the weights during training (blue), while SN helps controlling it (red). (d) KID score during training: Only variants using SMMD lead to stable training in this case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Samples from a generator trained on ImageNet dataset using Scaled MMD with Spectral Normalization: SN-SMMDGAN. 28 (a) SNGAN (b) SobolevGAN (c) MMDGAN-GP-L2 (d) SN-SMMD GAN (e) SN SWGAN (f) SMMD GAN Comparison of samples from different models trained on CelebA with 160×160 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1b</head><label>1b</label><figDesc></figDesc><table><row><cell>30 35</cell><cell></cell><cell></cell><cell>(a) : KID×10 3</cell><cell cols="2">SMMDGAN SN-SMMDGAN MMDGAN-GP-L2 Sobolev-GAN SN-GAN</cell><cell>1.0 0.8</cell><cell cols="5">(b) : Singular Values: Layer 2 SMMDGAN : 10K iterations SMMDGAN : 150K iterations SN-SMMDGAN : 10K iterations SN-SMMDGAN : 150K iterations</cell></row><row><cell>25</cell><cell></cell><cell></cell><cell></cell><cell cols="2">WGAN-GP SN-SWGAN</cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>2</cell><cell></cell><cell>4 generator iterations 6 8</cell><cell>10</cell><cell>×10 4</cell><cell>0</cell><cell>20</cell><cell cols="2">40</cell><cell>60 i th singular value 80</cell><cell>100</cell><cell>120</cell></row><row><cell>10 4</cell><cell>SMMDGAN</cell><cell cols="2">(c) : Critic Complexity</cell><cell></cell><cell cols="2">700</cell><cell cols="5">(d) : Condition Number: Layer 1 SMMDGAN</cell></row><row><cell></cell><cell cols="3">SN-SMMDGAN</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SN-SMMDGAN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MMDGAN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">600</cell><cell>MMDGAN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 3</cell><cell cols="2">SN-MMDGAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SN-MMDGAN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3 generator iterations 4</cell><cell>5</cell><cell>6 ×10 4</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell cols="2">3 generator iterations 4</cell><cell>5</cell><cell>6 ×10 4</cell></row></table><note>presents the scores for SMMDGAN and SN-SMMDGAN trained on ImageNet, and the scores of pre-trained models using BGAN [6] and SN-GAN [32].6 The</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean (standard deviation) of score estimates, based on 50 000 samples from each model. 2±0.1 47.5±0.1 44.4±2.2 SMMDGAN 10.7±0.2 38.4±0.3 39.3±2.5 SN-SMMDGAN 10.9±0.1 36.6±0.2 34.6±1.6</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) CIFAR-10 and CelebA.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CelebA</cell><cell></cell></row><row><cell>Method</cell><cell>IS</cell><cell>FID</cell><cell cols="2">KID×10 3 IS</cell><cell>FID</cell><cell>KID×10 3</cell></row><row><cell>WGAN-GP</cell><cell cols="3">6.9±0.2 31.1±0.2 22.2±1.1</cell><cell cols="3">2.7±0.0 29.2±0.2 22.0±1.0</cell></row><row><cell cols="4">MMDGAN-GP-L2 6.9±0.1 31.4±0.3 23.3±1.1</cell><cell cols="3">2.6±0.0 20.5±0.2 13.0±1.0</cell></row><row><cell>Sobolev-GAN</cell><cell cols="3">7.0±0.1 30.3±0.3 22.3±1.2</cell><cell cols="3">2.9±0.0 16.4±0.1 10.6±0.5</cell></row><row><cell>SMMDGAN</cell><cell cols="3">7.0±0.1 31.5±0.4 22.2±1.1</cell><cell cols="3">2.7±0.0 18.4±0.2 11.5±0.8</cell></row><row><cell>SN-GAN</cell><cell cols="3">7.2±0.1 26.7±0.2 16.1±0.9</cell><cell cols="3">2.7±0.0 22.6±0.1 14.6±1.1</cell></row><row><cell>SN-SWGAN</cell><cell cols="3">7.2±0.1 28.5±0.2 17.6±1.1</cell><cell cols="2">2.8±0.0 14.1±0.2</cell><cell>7.7±0.5</cell></row><row><cell>SN-SMMDGAN</cell><cell cols="3">7.3±0.1 25.0±0.3 16.6±2.0</cell><cell cols="2">2.8±0.0 12.4±0.2</cell><cell>6.1±0.4</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) ImageNet.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>IS</cell><cell>FID</cell><cell cols="2">KID×10 3</cell></row><row><cell></cell><cell>BGAN</cell><cell cols="4">10.7±0.4 43.9±0.3 47.0±1.1</cell></row><row><cell></cell><cell>SN-GAN</cell><cell>11.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean (standard deviation) of score evaluations on CIFAR-10 for different methods using Spectral Normalization.</figDesc><table><row><cell></cell><cell>IS</cell><cell>FID</cell><cell>KID×10 3</cell></row><row><cell>Method</cell><cell></cell><cell></cell></row><row><cell>MMDGAN</cell><cell cols="2">5.5±0.0 73.9±0.1</cell><cell>39.4±1.5</cell></row><row><cell>SN-WGAN</cell><cell cols="3">2.2±0.0 208.5±0.2 178.9±1.5</cell></row><row><cell>SN-WGAN-GP</cell><cell cols="3">2.5±0.0 154.3±0.2 125.3±0.9</cell></row><row><cell>SN-Sobolev-GAN</cell><cell cols="3">2.9±0.0 140.2±0.2 130.0±1.9</cell></row><row><cell>SN-MMDGAN-GP</cell><cell cols="2">4.6±0.1 96.8±0.4</cell><cell>59.5±1.4</cell></row><row><cell>SN-MMDGAN-L2</cell><cell cols="2">7.1±0.1 31.9±0.2</cell><cell>21.7±0.9</cell></row><row><cell>SN-MMDGAN</cell><cell cols="2">6.9±0.1 31.5±0.2</cell><cell>21.7±1.0</cell></row><row><cell cols="3">SN-MMDGAN-GP-L2 6.9±0.2 32.3±0.3</cell><cell>20.9±1.1</cell></row><row><cell>SN-SMMDGAN</cell><cell cols="2">7.3±0.1 25.0±0.3</cell><cell>16.6±2.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">f #P denotes the pushforward of a distribution: if X ∼ P, then f (X) ∼ f #P.<ref type="bibr" target="#b2">3</ref> <ref type="bibr" target="#b26">[27,</ref> Theorem 4] makes a similar claim to Proposition 2, but its proof was incorrect: it tries to uniformly bound MMD k ψ ≤ W 2 , but the bound used is for a Wasserstein in terms of k ψ (x, ·) − k ψ (y, ·) H k ψ .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.<ref type="bibr" target="#b4">5</ref> We use ∂ik(x, y) to denote the partial derivative with respect to xi, and ∂ i+d k(x, y) that for yi.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">These models are courtesy of the respective authors and also trained at 64 × 64 resolution. SN-GAN used the same architecture as our model, but trained for 250 000 generator iterations; BS-GAN used a similar 5-layer ResNet architecture and trained for 74 epochs, comparable to SN-GAN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use (m, i) to denote (m − 1)d + i; thus ∇η(X) stacks ∇η(X1), . . . , ∇η(XM ) into one vector.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D An estimator for Lipschitz MMD</head><p>We now describe briefly how to estimate the Lipschitz MMD in low dimensions. Recall that LipMMD k,λ (P, Q) = sup</p><p>For f ∈ H k , it is the case that</p><p>Thus we can approximate the constraint f 2 Lip + λ f 2 H k ≤ 1 by enforcing the constraint on a set of m points {Z i } reasonably densely covering the region around the supports of P and Q, rather 22 than enforcing it at every point in X . An estimator of the Lipschitz MMD based on X ∼ P n X and</p><p>By the generalized representer theorem, the optimal f for (29) will be of the form</p><p>Writing δ = (α, β, γ), the objective function is linear in δ,</p><p>The constraints are quadratic, built from the following matrices, where the X and Y samples are concatenated together, as are the derivatives with each dimension of the Z samples:</p><p>Given these matrices, and letting O j = d i=1 e (i,j) e T (i,j) where e (i,j) is the (i, j)th standard basis vector in R md , we have that</p><p>Thus the optimization problem <ref type="formula">(29)</ref> is a linear problem with convex quadratic constraints, which can be solved by standard convex optimization software. The approximation is reasonable only if we can effectively cover the region of interest with densely spaced {Z i }; it requires a nontrivial amount of computation even for the very simple 1-dimensional toy problem of Example 1.</p><p>One advantage of this estimator, though, is that finding its derivative with respect to the input points or the kernel parameterization is almost free once we have computed the estimate, as long as our solver has computed the dual variables µ corresponding to the constraints in <ref type="bibr" target="#b28">(29)</ref>. We just need to exploit the envelope theorem and then differentiate the KKT conditions, as done for instance in <ref type="bibr" target="#b0">[1]</ref>. The differential of (29) ends up being, assuming the optimum of (29) is atδ ∈ R n X +n Y +md and µ ∈ R m , </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Near-equivalence of WGAN and linear-kernel MMD GANs</head><p>For an MMD GAN-GP with kernel k(x, y) = φ(x)φ(y), we have that</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">OptNet: Differentiable Optimization as a Layer in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00443</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ICLR. 2017.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Note on the Inception Score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Cramer Distance as a Solution to Biased Wasserstein Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10743</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BEGAN: Boundary Equilibrium Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Demystifying MMD GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Braverman Readings in Machine Learning: Key Iedas from Inception to Current State</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07822</idno>
		<editor>L. Rozonoer, B. Mirkin, and I. Muchnik. LNAI</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">11100</biblScope>
			<biblScope unit="page" from="229" to="268" />
		</imprint>
	</monogr>
	<note>Geometrical Insights for Implicit Generative Modeling</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Test of Relative Similarity For Model Selection in Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bounliphone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">ICLR.2016.arXiv:1511.04581</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measure Based Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural Photo Editing with Introspective Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07093</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Real Analysis and Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Dudley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training generative neural networks via Maximum Mean Discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03906</idno>
	</analytic>
	<monogr>
		<title level="j">UAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Generative Models with Sinkhorn Divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00292</idno>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strictly proper scoring rules, prediction, and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gneiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASA</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="359" to="378" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Kernel Two-Sample Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Some bounds for the product of singular values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Güngör</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Contemporary Mathematical Sciences</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An empirical study on evaluation metrics of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07755</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal Unsupervised Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04732</idno>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards the Automatic Anime Characters Creation with Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05509</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">ICLR.2015.arXiv:1412.6980</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">MMD GAN: Towards Deeper Understanding of Moment Matching Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08584</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generative Moment Matching Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02761</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ICML. 2015.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7766</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Which Training Methods for GANs do actually Converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Envelope theorems for arbitrary choice sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milgrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="583" to="601" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05927</idno>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sobolev GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04894</idno>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fisher GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09675</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Regularized Kernel and Neural Sobolev Descent: Dynamic MMD Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12062</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integral Probability Metrics and their Generating Classes of Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00709</idno>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ICLR. 2016.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vector measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="1978-07" />
		</imprint>
	</monogr>
	<note>Review: J. Diestel</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Stabilizing Training of Generative Adversarial Networks through Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09367</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Kernel choice and classifiability for RKHS embeddings of probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Universality, Characteristic Kernels and RKHS Embedding of Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1003.0887</idno>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2389" to="2410" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the optimal estimation of probability mesaures in weak and strong topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.8240</idno>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1839" to="1893" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Christmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">ICLR.2017.arXiv:1611.04488</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08819</idno>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Optimal Transport: Old and New</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00087</idno>
	</analytic>
	<monogr>
		<title level="m">Bernoulli (forthcoming)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Scattered Data Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wendland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">B-tests: Low Variance Kernel Two-Sample Tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.1954</idno>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
