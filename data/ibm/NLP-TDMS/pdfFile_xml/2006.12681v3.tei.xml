<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ContraGAN: Contrastive Learning for Conditional Image Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguk</forename><surname>Kang</surname></persName>
							<email>mgkang@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Artificial Intelligence POSTECH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
							<email>jaesik.park@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Artificial Intelligence POSTECH</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ContraGAN: Contrastive Learning for Conditional Image Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional image generation is the task of generating diverse images using class label information. Although many conditional Generative Adversarial Networks (GAN) have shown realistic results, such methods consider pairwise relations between the embedding of an image and the embedding of the corresponding label (data-to-class relations) as the conditioning losses. In this paper, we propose ContraGAN that considers relations between multiple image embeddings in the same batch (data-to-data relations) as well as the data-to-class relations by using a conditional contrastive loss. The discriminator of ContraGAN discriminates the authenticity of given samples and minimizes a contrastive objective to learn the relations between training images. Simultaneously, the generator tries to generate realistic images that deceive the authenticity and have a low contrastive loss. The experimental results show that ContraGAN outperforms state-of-the-artmodels by 7.3% and 7.7% on Tiny ImageNet and ImageNet datasets, respectively. Besides, we experimentally demonstrate that ContraGAN helps to relieve the overfitting of the discriminator. For a fair comparison, we re-implement twelve state-of-the-art GANs using the PyTorch library. The software package is available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks (GAN) <ref type="bibr" target="#b0">[1]</ref> have introduced a new paradigm for realistic data generation. Many approaches have shown impressive improvements in un/conditional image generation tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The studies on non-convexity of objective landscapes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and gradient vanishing problems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> emphasize the instability of the adversarial dynamics. Therefore, many approaches have tried to stabilize the training procedure by adopting well-behaved objectives <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref> and regularization techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>. In particular, spectral normalization <ref type="bibr" target="#b3">[4]</ref> with a projection discriminator <ref type="bibr" target="#b16">[17]</ref> made the first success in generating images of ImageNet dataset <ref type="bibr" target="#b17">[18]</ref>. SAGAN <ref type="bibr" target="#b4">[5]</ref> shows using spectral normalization on both the generator and discriminator can alleviate training instability of GANs. BigGAN <ref type="bibr" target="#b5">[6]</ref> dramatically advances the quality of generated images by scaling up the number of network parameters and batch size.</p><p>On this journey, conditioning class information for the generator and discriminator turns out to be the secret behind realistic image generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. ACGAN <ref type="bibr" target="#b18">[19]</ref> validates this direction by training a softmax classifier along with the discriminator. ProjGAN <ref type="bibr" target="#b16">[17]</ref> utilizes a projection discriminator with probabilistic model assumptions. Especially, ProjGAN shows surprising image synthesis results and becomes the basic model adopted by SNGAN <ref type="bibr" target="#b3">[4]</ref>, SAGAN <ref type="bibr" target="#b6">[7]</ref>, BigGAN <ref type="bibr" target="#b5">[6]</ref>, CRGAN <ref type="bibr" target="#b6">[7]</ref>, and LOGAN <ref type="bibr" target="#b8">[9]</ref>. However, GANs with the projection discriminator have overfitting issues, which lead to the collapse of adversarial training <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. The ACGAN is known to be unstable when the number of classes increases <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In this paper, we propose a new conditional generative adversarial network framework, namely Contrastive Generative Adversarial Networks (ContraGAN). Our approach is motivated by an interpretation that ACGAN and ProjGAN utilize data-to-class relation as the conditioning losses. Such losses only consider relations between the embedding of an image and the embedding of the corresponding label. In contrast, ContraGAN is based on a conditional contrastive loss (2C loss) to consider data-to-data relations in the same batch. ContraGAN pulls the multiple image embeddings closer to each other when the class labels are the same, but it pushes far away otherwise. In this manner, the discriminator can capture not only data-to-class but also data-to-data relations between samples.</p><p>We perform image generation experiments on CIFAR10 <ref type="bibr" target="#b23">[24]</ref>, Tiny ImageNet <ref type="bibr" target="#b24">[25]</ref>, and ImageNet <ref type="bibr" target="#b17">[18]</ref> datasets using various backbone architectures, such as DCGAN <ref type="bibr" target="#b1">[2]</ref>, ResGAN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref>, and Big-GAN <ref type="bibr" target="#b5">[6]</ref> equipped with spectral normalization <ref type="bibr" target="#b3">[4]</ref>. Through exhaustive experiments, we verify that the proposed ContraGAN improves the state-of-the-art-models by 7.3% and 7.7% on Tiny ImageNet and ImageNet datasets respectively, in terms of Frechet Inception Distance (FID) <ref type="bibr" target="#b26">[27]</ref>. Also, Con-traGAN gives comparable results (1.3% lower FID) on CIFAR10 with the art model <ref type="bibr" target="#b5">[6]</ref>. Since ContraGAN can learn plentiful data-to-data relations from a properly sized batch, it reduces FID significantly without hard negative and positive mining. Furthermore, we experimentally show that 2C loss alleviates the overfitting problem of the discriminator. In the ablation study, we demonstrate that ContraGAN can benefit from consistency regularization <ref type="bibr" target="#b6">[7]</ref> that uses data augmentations.</p><p>In summary, the contributions of our work are as follows:</p><p>• We propose novel Contrastive Generative Adversarial Networks (ContraGAN) for conditional image generation. ContraGAN is based on a novel conditional contrastive loss (2C loss) that can learn both data-to-class and data-to-data relations. • We experimentally demonstrate that ContraGAN improves state-of-the-art-results by 7.3% and 7.7% on Tiny ImageNet and ImageNet datasets, respectively. ContraGAN also helps to relieve the overfitting problem of the discriminator. • ContraGAN shows favorable results without data augmentations for consistency regularization. If consistency regularization is applied, ContraGAN can give superior image generation results. • We provide implementations of twelve state-of-the-art GANs for a fair comparison. Our implementation of the prior arts for CIFAR10 dataset achieves even better performances than FID scores reported in the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Adversarial Networks</head><p>Generative adversarial networks (GAN) <ref type="bibr" target="#b0">[1]</ref> are implicit generative models that use a generator and a discriminator to synthesize realistic images. While the discriminator (D) should distinguish whether the given images are synthesized or not, the generator (G) tries to fool the discriminator by generating realistic images from noise vectors. The objective of the adversarial training is as follows:</p><formula xml:id="formula_0">min G max D E x∼p real (x) [log(D(x))] + E z∼p(z) [log(1 − D(G(z)))],<label>(1)</label></formula><p>where p real (x) is the real data distribution, and p z (z) is a predefined prior distribution, typically multivariate Gaussian. Since the dynamics between the generator and discriminator is unstable, and it is hard to achieve the Nash equilibrium <ref type="bibr" target="#b27">[28]</ref>, there are many objective functions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> and regularization techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> to help networks to converge to a proper equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional GANs</head><p>One of the widely used strategies to synthesize realistic images is utilizing class label information.</p><p>Early approaches in this category are conditional variational auto-encoder (CVAE) <ref type="bibr" target="#b29">[30]</ref> and conditional generative adversarial networks <ref type="bibr" target="#b30">[31]</ref>. These approaches concatenate a latent vector with the label to manipulate the semantic characteristics of the generated image. Since DCGAN <ref type="bibr" target="#b1">[2]</ref> demonstrated high-resolution image generation, GANs utilizing class label information has shown advanced performances <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Class embedding ( )</p><formula xml:id="formula_1">! ! Adversarial loss ! !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial loss Classification loss</head><p>Input image ( )</p><formula xml:id="formula_2">" ! ( ) " "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner product</head><p>Input image ( )</p><formula xml:id="formula_3">Input label ( ) " ! ( ) Classifier Input label ( ) " "</formula><p>(a) ACGAN <ref type="bibr" target="#b18">[19]</ref> Class embedding ( )</p><formula xml:id="formula_4">! ! Adversarial loss ! !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial loss Classification loss</head><p>Input image ( )</p><formula xml:id="formula_5">" ! ( ) " "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner product</head><p>Input image ( )  <ref type="figure">Figure 1</ref>: Schematics of discriminators of three conditional GANs. (a) ACGAN <ref type="bibr" target="#b18">[19]</ref> has an auxiliary classifier to guide the generator to synthesize well-classifiable images. (b) ProjGAN <ref type="bibr" target="#b16">[17]</ref> improves ACGAN by adding the inner product of an embedded image and the corresponding class embedding. (c) Our approach extends ACGAN and ProjGAN with a conditional contrastive loss (2C loss). ContraGAN considers multiple positive and negative pairs in the same batch. ContraGAN utilizes 2C loss to update the generator as well.</p><formula xml:id="formula_6">Input label ( ) " ! ( ) Classifier Input label ( ) " " (b) ProjGAN [17]</formula><p>The most common approach of conditional GANs is to inject label information into the generator and discriminator. ACGAN <ref type="bibr" target="#b18">[19]</ref> attaches an auxiliary classifier on the top of convolutional layers in the discriminator to distinguish the classes of images. An illustration of ACGAN is shown in <ref type="figure">Fig. 1a</ref>. ProjGAN <ref type="bibr" target="#b16">[17]</ref> points out that ACGAN is likely to generate easily classifiable images, and the generated images are not diverse. ProjGAN proposes a projection discriminator to relieve the issues (see <ref type="figure">Fig. 1b</ref>). However, these approaches do not explicitly consider data-to-data relations in the training phase. Besides, the recent study by Wu et al. <ref type="bibr" target="#b8">[9]</ref> discovers that BigGAN with the projection discriminator <ref type="bibr" target="#b5">[6]</ref> still suffers from the discriminator's overfitting and training collapse problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We begin with analyzing that conditioning functions of ACGAN and ProjGAN can be interpreted as pair-based losses that look at only data-to-class relations of training examples (Sec. 3.1). Then, in order to consider both data-to-data and data-to-class relations, we devise a new conditional contrastive loss (2C loss) (Sec. 3.2). Finally, we propose Contrastive Generative Adversarial Networks (ContraGAN) for conditional image generation (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional GANs and Data-to-Class Relations</head><p>The goal of the discriminator in ACGAN is to classify the class of a given image and the sample's authenticity. Using data-to-class relations, i.e., information about which class a given data belongs to, the generator tries to generate fake images that can deceive the authenticity and are classified as the target labels. Since ACGAN uses a cross-entropy loss to classify the class of an image, we can regard the conditioning loss of ACGAN as a pair-based loss that can consider only data-to-class relations (see <ref type="figure" target="#fig_1">Fig. 2d</ref>). ProjGAN tries to maximize inner-product values between embeddings of real images and the corresponding target embeddings while minimizing the inner-product values when the images are fake. Since the discriminator of ProjGAN pushes and pulls the embeddings of images according to the authenticity and class information, we can think of the conditioning objective of ProjGAN as a pair-based loss that considers data-to-class relations (see <ref type="figure" target="#fig_1">Fig. 2e</ref>). Unlike ACGAN, which looks at relations between a fixed one-hot vector and a sample, ProjGAN can consider more flexible relations using a learnable class embedding, namely Proxy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Contrastive Loss</head><p>To exploit data-to-data relations, we can adopt loss functions used in self-supervised <ref type="bibr" target="#b33">[34]</ref> learning or metric learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. In other words, our approach is to add a metric learning or self-supervised learning objective in the discriminator and generator to explicitly control distances between embedded image features depending on the labels. Several metric learning losses, such (a) Triplet <ref type="bibr" target="#b31">[32]</ref> (b) P-NCA <ref type="bibr" target="#b32">[33]</ref> (c) NT-Xent <ref type="bibr" target="#b33">[34]</ref> (d) ACGAN <ref type="bibr" target="#b18">[19]</ref> (e) ProjGAN <ref type="bibr" target="#b16">[17]</ref>  as contrastive loss <ref type="bibr" target="#b34">[35]</ref>, triplet loss <ref type="bibr" target="#b31">[32]</ref>, quadruplet loss <ref type="bibr" target="#b35">[36]</ref>, and N-pair loss <ref type="bibr" target="#b36">[37]</ref> could be good candidates. However, it is known that 1) mining informative triplets or quadruplets requires higher training complexity, and 2) poor tuples make the training time longer. While the proxy-based losses <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> relieves mining complexity using trainable class embedding vectors, such losses do not explicitly take data-to-data relations <ref type="bibr" target="#b39">[40]</ref> into account.</p><p>Before introducing the proposed 2C loss, we bring NT-Xent loss <ref type="bibr" target="#b33">[34]</ref> to express our idea better. Let X = {x 1 , ..., x m }, where x ∈ R W ×H be a randomly sampled minibatch of training images and y = {y 1 , ..., y m }, where y ∈ R be the collection of corresponding class labels. Then, we define a deep neural network encoder S(x) ∈ R k and a projection layer that embeds onto a new unit hypersphere h : R k −→ S d . Then, we can map the data space to the hypersphere using the composition of l = h(S(·)). NT-Xent loss conducts random data augmentations T on the training data X, and we denote it as A = {x 1 , T (x 1 ), ..., x m , T (x m )} = {a 1 , ..., a 2m }. Using the above, we can formulate NT-Xent loss as follows:</p><formula xml:id="formula_7">(a i , a j ; t) = −log exp(l(a i ) l(a j )/t) 2m k=1 1 k =i · exp(l(a i ) l(a k )/t) ,<label>(6)</label></formula><p>where the scalar value t is a temperature to control push and pull force. In this work, we use the part of the discriminator network (D φ1 ) before the fully connected layer as the encoder network (S) and use multi-layer perceptrons parameterized by ϕ as the projection head (h). As a result, we can map the data space to the unit hypersphere using l = h(D φ1 (·)).</p><p>However, Eq. (6) requires proper data augmentations and can not consider data-to-class relations of training examples. To resolve these issues, we propose to use the embeddings of class labels instead of using data augmentations. With a class embedding function e(y) : R −→ R d , Eq. (6) can be formulated as follows:</p><formula xml:id="formula_8">(x i , y i ; t) = −log exp(l(x i ) e(y i )/t) exp(l(x i ) e(y i )/t) + m k=1 1 k =i · exp(l(x i ) l(x k )/t) .<label>(7)</label></formula><p>Eq. (7) pulls a reference sample x i nearer to the class embedding e(y i ) and pushes the others away. This scheme may push negative samples which have the same label as y i . Therefore, we make an exception by adding cosine similarities of such negative samples in the numerator of Eq. <ref type="bibr" target="#b6">(7)</ref>. The final loss function is as follows:</p><formula xml:id="formula_9">2C (x i , y i ; t) = −log exp(l(x i ) e(y i )/t) + m k=1 1 y k =yi · exp(l(x i ) l(x k )/t) exp(l(x i ) e(y i )/t) + m k=1 1 k =i · exp(l(x i ) l(x k )/t) .<label>(8)</label></formula><p>Eq. <ref type="formula" target="#formula_9">(8)</ref> is the proposed conditional contrastive loss (2C loss). Minimizing 2C loss will reduce distances between the embeddings of images with the same labels while maximizing the others. 2C loss explicitly considers the data-to-data relations l(x i ) l(x k ) and data-to-class relations l(x i ) e(y i ) without comprehensive mining of the training dataset and augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 : Training ContraGAN</head><p>Input: Learning rate: α 1 , α 2 . Adam hyperparameters <ref type="bibr" target="#b40">[41]</ref>: β 1 , β 2 . Batch size: m. Temperature: t. # of discriminator iterations per single generator iteration: n dis . Contrastive coefficient: λ. Parameters of the generator, the discriminator, and the projection layer: (θ, φ, ϕ). Output: Optimized (θ, φ, ϕ).</p><formula xml:id="formula_10">1: Initialize (θ, φ, ϕ) 2: for {1, ..., # of training iterations} do 3: for {1, ..., n dis } do 4: Sample {(x i , y real i )} m i=1 ∼ p real (x, y) 5: Sample {z i } m i=1 ∼ p(z) and {y fake i } m i=1 ∼ p(y) 6: L real C ←− 1 m m i=1 2C (x i , y real i ; t)</formula><p>Eq. (8) with real images. <ref type="bibr" target="#b6">7</ref>:</p><formula xml:id="formula_11">L D ←− 1 m m i=1 {D φ (G θ (z i , y fake i ), y fake i ) − D φ (x i , y real i )} + λL real C 8: φ, ϕ ←− Adam(L D , α 1 , β 1 , β 2 ) 9:</formula><p>end for 10:</p><formula xml:id="formula_12">Sample {z i } m i=1 ∼ p(z) and {y fake i } m i=1 ∼ p(y) 11: L fake C ←− 1 m m i=1 2C (G θ (z i , y fake i ), y fake i ; t)</formula><p>Eq. (8) with fake images. <ref type="bibr" target="#b11">12</ref>:</p><formula xml:id="formula_13">L G ←− − 1 m m i=1 {D φ (G θ (z i , y fake i ), y fake i )} + λL fake C 13: θ ←− Adam(L G , α 2 , β 1 , β 2 ) 14: end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive Generative Adversarial Networks</head><p>With proposed 2C loss, we describe the framework, called ContraGAN and introduce training procedures. Like the typical training procedures of GANs, ContraGAN has a discriminator training step and a generator training step that compute an adversarial loss. With this foundation, ContraGAN additionally calculates 2C loss using a set of real or fake images. Algorithm 1 shows the training procedures of the proposed ContraGAN. A notable aspect is that 2C loss is computed using m real images in the discriminator training step and m generated images in the generator training step.</p><p>In this manner, the discriminator updates itself by minimizing the distances between real image embeddings from the same class while maximizing it otherwise. By forcing the embeddings to relate via 2C loss, the discriminator can learn the fine-grained representations of real images. Similarly, the generator exploits the knowledge of the discriminator, such as intra-class characteristics and higher-order representations of the real images, to generate more realistic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Differences between 2C Loss and NT-Xent Loss</head><p>NT-Xent loss <ref type="bibr" target="#b33">[34]</ref> is intended for unsupervised learning. It regards the augmented image as the positive sample to consider data-to-data relations between an original image and the augmented image. On the other hand, 2C loss utilizes weak supervision of label information. Therefore, compared with 2C loss, NT-Xent hardly gathers image embeddings of the same class, since there is no supervision from the label information. Besides, NT-Xent loss requires extra data augmentations and additional forward and backward propagations, which induce a few times of longer training time than the model with 2C loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metric</head><p>We perform conditional image generation experiments with CIFAR10 <ref type="bibr" target="#b23">[24]</ref>, Tiny ImageNet <ref type="bibr" target="#b24">[25]</ref>, and ImageNet <ref type="bibr" target="#b17">[18]</ref> datasets to compare the proposed approach with other approaches.</p><p>CIFAR10 <ref type="bibr" target="#b23">[24]</ref> is a widely used benchmark dataset in many image generation works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>, and it contains 32 × 32 pixels of color images for 10 different classes. The dataset consists of 60,000 images in total. It is divided into 50,000 images for training and 10,000 images for testing.</p><p>Tiny ImageNet <ref type="bibr" target="#b24">[25]</ref> provides 120,000 color images in total. Image size is 64 × 64 pixels, and the dataset consists of 200 categories. Each category has 600 images divided into 500, 50, and 50 samples for training, validation, and testing, respectively. Tiny ImageNet has 10× smaller number of images per class than CIFAR10, but it provides 20× larger number of classes than CIFAR10. Compared to CIFAR10, Tiny ImageNet is selected to test a more challenging scenario -the number of images per class is not plentiful, but the network needs to learn more categories.</p><p>ImageNet <ref type="bibr" target="#b17">[18]</ref> provides 1,281,167 and 50,000 color images for training and validation respectively, and the dataset consists of 1,000 categories. We crop each image using a square box whose length is the same as the shorter side of the image. The cropped images are rescaled to 128 × 128 pixels.</p><p>Frechet Inception Distance (FID) is an evaluation metric used in all experiments in this paper. The FID proposed by Heusel et al. <ref type="bibr" target="#b41">[42]</ref> calculates Wasserstein-2 distance <ref type="bibr" target="#b42">[43]</ref> between the features obtained from real images and generated images using Inception-V3 network <ref type="bibr" target="#b43">[44]</ref>. Since FID is the distance between two distributions, lower FID indicates better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Software</head><p>There are various approaches that report strong FID scores, but it is not easy to reproduce the results because detailed specifications for training or ways to measure the results are not clearly stated. For instance, FID could be different depending on the choice of the reference images (training, validation, or testing datasets could be used). Besides, FID score of prior work is not consistent, depending on TensorFlow versions <ref type="bibr" target="#b44">[45]</ref>. Therefore, we re-implement twelve state-ofthe-art GANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> to validate the proposed ContraGAN under the same condition. Our implementation carefully follows the principal concepts and the available specifications in the prior work. Experimental results show that the results from our implementation are superior to the numbers in the original papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> for the experiments using CIFAR10 dataset. We hope that our implementation would relieve efforts to compare various GAN pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup</head><p>To conduct a reliable assessment, all experiments that use CIFAR10 and Tiny ImageNet datasets are performed three times with different random seeds, and we report means and standard deviations of FIDs. Experiments using ImageNet are executed once, and we report the best performance during the training. We calculate FID using CIFAR10's test images and the same amount of generated images. For the experiments using Tiny ImageNet and ImageNet, we use the validation set with the same amount of generated images. All FID values reported in our paper are calculated using the PyTorch FID implementation <ref type="bibr" target="#b45">[46]</ref>.</p><p>Since spectral normalization <ref type="bibr" target="#b3">[4]</ref> has become an essential element in modern GAN training, we use hinge loss <ref type="bibr" target="#b14">[15]</ref> and apply spectral normalization on all architectures used in our experiments. We adopt modern architectures used in the papers: DCGAN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, ResGAN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref>, and BigGAN <ref type="bibr" target="#b5">[6]</ref>, and all details about the architectures are described in the supplement.</p><p>Since the conditioning strategy used in the generator of ACGAN differs from that of ProjGAN, we incorporate the generator's conditioning method in all experiments for a fair comparison. We use the conditional coloring transform <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref>, which is the method adopted by the original ProjGAN.</p><p>Before conducting the main experiments, we investigate performance changes according to the type of projection layer h in Eq. (8) and batch size. Although Chen et al. <ref type="bibr" target="#b33">[34]</ref> reports that contrastive learning can benefit from a higher-dimensional projection and a larger batch size, we found that the linear projection with batch size 64 for CIFAR10 and 1,024 for Tiny ImageNet performs the best. For the dimension of the projection layer, we select 512 for CIFAR10, 768 for Tiny ImageNet, and 1,024 for ImageNet experiments. We do a grid search to find a proper temperature (t) used in Eq. 8 and experimentally found that the temperature of 1.0 gives the best results. Detailed hyperparameter settings used in our experiments are described in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Results</head><p>Effectiveness of 2C loss. We compare 2C loss with P-NCA loss <ref type="bibr" target="#b32">[33]</ref>, NT-Xent loss <ref type="bibr" target="#b33">[34]</ref>, and the objective function formulated in Eq. 7. P-NCA loss <ref type="bibr" target="#b23">[24]</ref> does not explicitly look at data-to-data relations, and NT-Xent loss <ref type="bibr" target="#b24">[25]</ref> (equivalent to Eq. 6) does not take data-to-class relations into account.   Our 2C loss can take advantage of both losses. Compared with the Eq. 7 loss, 2C loss considers cosine similarities of negative samples whose labels are the same as the positive image. The experimental results show that considering both data-to-class and data-to-data relations is effective and largely enhances image generation performance on CIFAR10 and Tiny ImageNet dataset. Besides, removing degenerating negative samples gives slightly better performances on CIFAR10 and Tiny ImageNet datasets.</p><p>Comparison with other conditional GANs. We compare ContraGAN with ACGAN <ref type="bibr" target="#b18">[19]</ref> and ProjGAN <ref type="bibr" target="#b16">[17]</ref>, since these approaches are representative models using class information conditioning. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our approach shows favorable performances in CIFAR10, but our approach exhibits larger variations. Examples of generated images is shown in <ref type="figure" target="#fig_4">Fig. 4 (left)</ref>. Experiments with Tiny ImageNet indicate that our ContraGAN is more effective when the target dataset is in the higher-dimensional space and has large inter-class variations.</p><p>Comparison with state-of-the-art models. We compare our method with SNResGAN <ref type="bibr" target="#b3">[4]</ref>, SAGAN <ref type="bibr" target="#b4">[5]</ref>, and BigGAN <ref type="bibr" target="#b5">[6]</ref>. All of these approaches adopt ProjGAN <ref type="bibr" target="#b16">[17]</ref> for class information conditioning. We conduct all experiments on Tiny ImageNet and ImageNet datasets using the hyperparameter setting used in SAGAN <ref type="bibr" target="#b4">[5]</ref>. We use our implementation of BigGAN for a fair comparison and report the best FID values during training.</p><p>If we consider the most recent work, CRGAN <ref type="bibr" target="#b6">[7]</ref>, ICRGAN <ref type="bibr" target="#b7">[8]</ref>, and LOGAN <ref type="bibr" target="#b8">[9]</ref> can generate more realistic images than BigGAN. Compared to such approaches, we show that our framework outperforms BigGAN by just adopting the proposed 2C loss. CRGAN and ICRGAN conduct explicit data augmentations during the training, which requires additional gradient calculations for backpropagation. Also, LOGAN needs one more feedforward and backpropagation processes for latent optimization. It takes twice as much time to train the model than standard GANs.</p><p>As a result, we identify how ContraGAN performs without data augmentations or latent optimization. <ref type="table" target="#tab_2">Table 3</ref> quantitatively shows that ContraGAN can synthesize images better than other state-of-the- art GAN models under the same conditions. Compared to BigGAN, ContraGAN improves the performances by 1.3% on CIFAR10, 7.3% on Tiny ImageNet, 7.7% on ImageNet. If we use the reported number in BigGAN paper <ref type="bibr" target="#b5">[6]</ref>, the improvement is 29.9% on CIFAR10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training Stability of ContraGAN</head><p>This section compares the training stability of ContraGAN and ProjGAN <ref type="bibr" target="#b16">[17]</ref> for the experiments using Tiny ImageNet. We compute the difference between the authenticity accuracies on the training and validation dataset. It is because the difference between training and validation performance is a good estimator for measuring the overfitting. Also, as Brock et al. mentioned in his work <ref type="bibr" target="#b5">[6]</ref>, the sudden change in network parameters' largest singular values (spectral norms) can indicate the collapse of adversarial training. Following this idea, we plot the trends of spectral norms of the discriminator's parameters to monitor the training collapse.</p><p>As shown in the first column of <ref type="figure" target="#fig_2">Fig. 3</ref>, ProjGAN shows the rapid increase of the accuracy difference, and ProjGAN reaches the collapse point earlier than ContraGAN. Moreover, the trend of FIDs and spectral analysis show that ContraGAN is more robust to training collapse. We speculate that ContraGAN is harder to reach undesirable status since ContraGAN jointly considers data-to-data and data-to-label relations. We discover that an increase in the accuracy on the validation dataset can indicate training collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>We study how ContraGAN can be improved further with a larger batch size and data augmentations. We use ProjGAN with BigGAN architecture on Tiny ImageNet for this study. We use consistency regularization (CR) <ref type="bibr" target="#b6">[7]</ref> to identify that our ContraGAN can benefit from regularization that uses data augmentations. Also, to identify that 2C loss is not only computationally cheap but also effective to train GANs, we replace the class embeddings with augmented positive samples (APS). APS is widely used in the self-supervised contrastive learning community <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">49]</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows the experiment settings, FID, and time per iteration. We indicate the number of parameters as Param. and denote three ablations -(the 2C loss, augmented positive samples (APS), and consistency regularization (CR)) as Reg.  Comparison with APS. From the experiments (E, F), we can see that the 2C loss performs better than 2C loss + APS, although the latter takes about 12.9% more time. We speculate this is because each class embedding can become the representatives of the class, and it serves as the anchor that pulls corresponding images. Without the class embeddings, images in a minibatch are collected depending on a sampling state, and this may lead to training instability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we formulate a conditional contrastive loss (2C loss) and present new Contrastive Generative Adversarial Networks (ContraGAN) for conditional image generation. Unlike previous conditioning losses, the proposed 2C loss considers not only data-to-class but also data-to-data relations between training examples. Under the same conditions, we demonstrate that ContraGAN outperforms state-of-the-art conditional GANs on Tiny ImageNet and ImageNet datasets. Also, we identify that ContraGAN helps to relieve the discriminator's overfitting problem and training collapse. As future work, we would like to theoretically and experimentally analyze how adversarial training collapses as the authenticity accuracy on the validation dataset increases. Also, we think that exploring advanced regularization techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> is necessary to understand ContraGAN further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We proposed a new conditional image generation model that can synthesize more realistic and diverse images. Our work can contribute to image-to-image translations <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>, generating realistic human faces <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, or any task that utilizes adversarial training.</p><p>Since conditional GANs can expand to various image processing applications and can learn the representations of high-dimensional datasets, scientists can enhance the quality of astronomical images <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, design complex architectured materials <ref type="bibr" target="#b56">[57]</ref>, and efficiently search chemical space for developing materials <ref type="bibr" target="#b57">[58]</ref>. We can do so many beneficial tasks with conditional GANs, but we should be concerned that conditional GANs can be used for deepfake techniques <ref type="bibr" target="#b58">[59]</ref>. Modern generative models can synthesize realistic images, making it more difficult to distinguish between real and fake. This can trigger sexual harassment <ref type="bibr" target="#b59">[60]</ref>, fake news <ref type="bibr" target="#b60">[61]</ref>, and even security issues of face recognition systems <ref type="bibr" target="#b61">[62]</ref>.</p><p>To avoid improper use of conditional GANs, we need to be aware of generative models' strengths and weaknesses. Besides, it would be good to study the general characteristics of generated samples <ref type="bibr" target="#b62">[63]</ref> and how we can distinguish fake images from unknown generative models <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Network Architectures</head><p>Since DCGAN <ref type="bibr" target="#b1">[2]</ref> showed astonishing image generation ability, several generator and discriminator architectures have been proposed to stabilize and enhance the generation quality. Representatively, Miyato et al. <ref type="bibr" target="#b3">[4]</ref> have used a modified version of DCGAN <ref type="bibr" target="#b1">[2]</ref> and ResNet-style GAN <ref type="bibr" target="#b15">[16]</ref> architectures with spectral normalization (We abbreviate it SNDCGAN and SNResGAN, respectively). Brock et al. <ref type="bibr" target="#b5">[6]</ref> have expanded the capacity of SNResGAN with a shared embedding and skip connections from the noise vector (BigGAN). As a result, we tested the aforementioned frameworks to validate the proposed approach. To provide details of the main experiments in our paper, we introduce the network architectures in this section.</p><p>We start by defining some notations: m is a batch size, FC(in_features, out_features) is a fully connected layer, CONV(in_channels, out_channels, kernel_size, strides) is a convolutional layer, DECONV(in_channels, out_channels, kernel_size, strides) is a deconvolutional layer, BN is a batch normalization <ref type="bibr" target="#b66">[67]</ref>, CBN is a conditional batch normalization <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17]</ref>, RELU, LRELU, and TANH indicate ReLU <ref type="bibr" target="#b67">[68]</ref>, Leaky ReLU <ref type="bibr" target="#b68">[69]</ref>, and hyperbolic tangent functions, respectively. GBLOCK(in channels, out channels, upsampling) is a generator block used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>, BIGGBLOCK(in channels, out channels, upsampling, z split dim, shared dim) is a modified version of the GBLOCK used in <ref type="bibr" target="#b5">[6]</ref>, DBLOCK(in channels, out channels, downsampling) is a discriminator block used in <ref type="bibr" target="#b5">[6]</ref>, SELF-ATTENTION is a self-attention block used in <ref type="bibr" target="#b4">[5]</ref>, NORMALIZE is a normalize operation to project given embeddings onto a unit hypersphere, and GSP is a global sum pooling layer <ref type="bibr" target="#b69">[70]</ref>. For more details about the GBLOCK, BIGGBLOCK, DBLOCK, and the SELF-ATTENTION block, please refer to the papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> or the code of our PyTorch implementation.   <ref type="table" target="#tab_2">Table A3</ref>: Generator of SNResGAN <ref type="bibr" target="#b3">[4]</ref> used for CIFAR10 <ref type="bibr" target="#b23">[24]</ref> image synthesis. <ref type="table">Table A6</ref>: Discriminator of BigGAN <ref type="bibr" target="#b5">[6]</ref> used for CIFAR10 <ref type="bibr" target="#b23">[24]</ref> image synthesis. <ref type="table">Table A9</ref>: Generator of BigGAN <ref type="bibr" target="#b5">[6]</ref> for ImageNet <ref type="bibr" target="#b17">[18]</ref> image synthesis. <ref type="table" target="#tab_0">Table A11</ref>: Hyperparameter values used for experiments. Settings (B, C, E) and (F) are the settings used in <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref> and <ref type="bibr" target="#b4">[5]</ref>, respectively. we conduct experiments with CIFAR10 <ref type="bibr" target="#b23">[24]</ref> using the settings (A, B, C, D, E) and with Tiny ImageNet <ref type="bibr" target="#b24">[25]</ref> and ImageNet <ref type="bibr" target="#b17">[18]</ref> using the setting (F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameter Setup</head><p>Setting Choosing a proper hyperparameter setup is crucial to train GANs. In this paper, we conduct experiments using six settings with Adam optimizer <ref type="bibr" target="#b40">[41]</ref>. α 1 and α 2 are the learning rates of the discriminator and generator. β 1 and β 2 are the hyperparameters of Adam optimizer to control exponential decay rates of moving averages. n dis is the number of discriminator iterations per single generator iteration. For the contrastive coefficient λ (see Algorithm 1), the value is fixed at 1.0 for a fair comparison with <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>. In all experiments, we use the temperature t = 1.0. Experiments over temperature are displayed in <ref type="figure" target="#fig_7">Fig. A1</ref>. Besides, we apply moving average update of the generator's weights used in <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74]</ref> after 20,000 generator iterations with the decay rate of 0.9999. The settings (B, C, E) are known to give satisfactory performances on CIFAR10 <ref type="bibr" target="#b23">[24]</ref>  in previous papers <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref>. Since Heusel et al. <ref type="bibr" target="#b41">[42]</ref> and Zhang et al. <ref type="bibr" target="#b4">[5]</ref> have shown that two time-scale update (TTUR) can converge to a stationary local Nash equilibrium <ref type="bibr" target="#b27">[28]</ref>, we adopt the hyperparameter setup used in <ref type="bibr" target="#b4">[5]</ref> (setting F) to generate realistic images on Tiny ImageNet <ref type="bibr" target="#b24">[25]</ref> and ImageNet <ref type="bibr" target="#b17">[18]</ref> datasets.</p><formula xml:id="formula_14">α 1 α 2 β 1 β 2 n dis</formula><p>Experimental setup used for <ref type="table" target="#tab_0">Table 1</ref> in the main paper: Experiments on CIFAR10 dataset are conducted three times with different random seeds using the setting (E) with the batch size of 64 until 80k generator updates. Experiments on Tiny ImageNet dataset are performed three times until 100k generator updates using the setting (F) with the batch size of 256 and BigGAN architecture (see <ref type="table">Table A7</ref> and <ref type="table">Table A8</ref>).</p><p>Experimental setup used for <ref type="table" target="#tab_1">Table 2</ref> in the main paper: Experiments on CIFAR10 dataset are performed three times with different random seeds using the settings (A, B, C, D, E) with the batch size of 64. We stop training GANs with SNDCGAN, SNResGAN, and BigGAN architectures after 200k, 100k, and 80k generator updates, respectively. Also, we report performances of the hyperparameter settings that showed the lowest FID values by mean. Experiments on Tiny ImageNet dataset are conducted three times until 100k generator updates using the setting (F) with the batch size of 256 and BigGAN architecture (see <ref type="table">Table A7</ref> and <ref type="table">Table A8</ref>). The hyperparameter settings: C, D, E, show the best performance in SNDCGAN <ref type="bibr" target="#b3">[4]</ref>, SNResGAN <ref type="bibr" target="#b3">[4]</ref>, and BigGAN <ref type="bibr" target="#b5">[6]</ref>, respectively. We reason that as the model's capacity increases, training GANs becomes more difficult; thus, it requires more discriminator updates. Moreover, we experimentally identify that updating discriminator more times does not always produce better performance, but it might be related to the model capacity.</p><p>Experimental setup used for <ref type="table" target="#tab_2">Table 3</ref> in the main paper: FID values on CIFAR10 dataset are reported using the setting (E) with the batch size of 64. The experiments on the Tiny ImageNet are conducted using the setting (F) with the batch size of 1024. Experiments on ImageNet dataset are executed once until 250k generator updates using the setting (F) with the batch size of 256 and BigGAN architecture (see <ref type="table">Table A9</ref> and <ref type="table" target="#tab_0">Table A10</ref>). All other settings not noticed here are the same as the experimental setup for <ref type="table" target="#tab_1">Table 2</ref> above.</p><p>Experimental setup used for <ref type="table" target="#tab_3">Table 4</ref> in the main paper: All ablation results are reported using the setting (F), and models with consistency regularization (CR) <ref type="bibr" target="#b6">[7]</ref> are trained with the coefficient of 10.0. We use an Intel(R) Xeon(R) Silver 4114 CPU, four NVIDIA Geforce RTX 2080 Ti GPUs, and PyTorch DataParallel library to measure time per iteration. All other settings not noticed here are the same as the experimental settings used for <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Nonlinear Projection and Batch Size</head><p>We study the effect of a projection layer h : R k −→ S d that is introduced in Sec. 3.2. We change the types of the layer (linear vs. nonlinear) and increase the dimensionality of projected embeddings, d on CIFAR10 dataset. <ref type="figure" target="#fig_1">Fig. A2a</ref> shows the overview of FID values. All experiments are conducted using three different architectures: DCGAN, ResGAN, and BigGAN that are equipped with spectral normalization. We also run the experiments using three different random seeds and do not apply moving average update of the generator's weights. SNDCGAN with the liner projection layer projects latent features onto the 1,024 dimensional space. This configuration shows higher FID than the nonlinear counterpart, but ContraGANs with a linear projection layer generally give lower FIDs. Although GANs are known to need careful hyperparameter selection, our ContraGAN does not seem to be sensitive to the type and dimensionality of the projection layer. <ref type="figure" target="#fig_1">Figure A2b</ref> shows the change in FID values as the batch size increases. Experiments conducted by Brock et al. <ref type="bibr" target="#b5">[6]</ref> have demonstrated that increasing the batch size enhances image generation performance on ImageNet dataset <ref type="bibr" target="#b17">[18]</ref>. However, as shown in <ref type="figure" target="#fig_1">Fig. A2b</ref>, optimal batch sizes for CIFAR10 and Tiny ImageNet are 64 and 1,024, respectively. Based on these results, we can deduce that increasing batch size does not always give the best synthesis results. We presume that this phenomenon is related to the number of classes used for the training. ContraGAN FID implementation CIFAR10 <ref type="bibr" target="#b23">[24]</ref> Tiny ImageNet <ref type="bibr" target="#b24">[25]</ref> TensorFlow <ref type="bibr" target="#b26">[27]</ref> 10.308 26.924 PyTorch <ref type="bibr" target="#b45">[46]</ref> 10.304 <ref type="bibr">27.131</ref> FID is a widely used metric to evaluate the performance of a GAN model. Since calculating FID requires a pre-trained inception-V3 network <ref type="bibr" target="#b43">[44]</ref>, many implementations use Tensorflow <ref type="bibr" target="#b44">[45]</ref> or PyTorch <ref type="bibr" target="#b74">[75]</ref> libraries. Among them, the TensorFlow implementation <ref type="bibr" target="#b26">[27]</ref> for FID measurement is widely used. We use the PyTorch implementation for FID measurement <ref type="bibr" target="#b45">[46]</ref>, instead. In this section, we show that the PyTorch-based FID implementation <ref type="bibr" target="#b45">[46]</ref> used in our work provides almost the same results as the TensorFlow implementation. The results are summarized in <ref type="table" target="#tab_0">Table A12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D FID Implementations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Multiple Runs of the Stability Experiment</head><p>In this section, we provide the additional results of the stability test performed in Sec. 4.5 of the main paper. The third and fourth row of <ref type="figure" target="#fig_2">Fig. A3</ref> shows the another run from ProjGAN and ContraGAN. <ref type="figure" target="#fig_2">Figure A3</ref>: Authenticity classification accuracies on the training and validation datasets (left), trends of FID values (middle), and trends of the largest singular values of the discriminator's convolutional parameters (right). To specify the starting point where the difference between the training and validation accuracies is greater than 0.5, we use a solid black line. The first and second black dotted lines indicate when the performance is best and when training collapse occurs, respectively.</p><p>As shown in the third row of <ref type="figure" target="#fig_2">Fig. A3</ref>, training collapse does not occur in training ProjGAN <ref type="bibr" target="#b16">[17]</ref>. However, the best FID value of the ProjGAN is 34.831, which is much higher than that of Contra-GAN (25 ≤ FID ≤ 27). The above results show that ContraGAN is more robust to the overfitting and training collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Qualitative Results</head><p>This section presents images generated by various conditional image generation frameworks. <ref type="figure" target="#fig_4">Fig. A4</ref>, A5, and A6 show the synthesized images using CIFAR10 dataset. <ref type="figure" target="#fig_11">Fig. A7</ref> and A8 show the synthesized images using Tiny ImageNet dataset. <ref type="figure" target="#fig_13">Fig. A9</ref> and A10 show the generated images using ImageNet dataset. As shown in <ref type="figure" target="#fig_12">Fig. A8</ref> and A10, our approach can achieve favorable FID compared to the other baseline approaches. <ref type="figure" target="#fig_4">Figure A4</ref>: Examples generated by ACGAN <ref type="bibr" target="#b18">[19]</ref> trained on CIFAR10 dataset <ref type="bibr" target="#b23">[24]</ref> (FID=11.111).      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(f) 2C loss (Ours) Illustrative figures visualize metric learning losses (a,b,c) and conditional GANs (d,e,f). The color indicates the class label and the shape represents the role. (Square) the embedding of an image. (Diamond) the embedding of an augmented image. (Circle) a reference image embedding. Each loss is applied to the reference. (Star) the embedding of a class label. (Triangle) the one-hot encoding of a class label. The thicknesses of red and blue lines represent the strength of pull and push force, respectively. The loss function of ProjGAN lets the reference and the corresponding class embedding be close to each other when the reference is real, but it pushes far away otherwise. Compared to ACGAN and ProjGAN, 2C loss can consider both data-to-class and data-to-data relations between training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Authenticity classification accuracies on the training and validation datasets (left), trends of FID values (middle), and trends of the largest singular values of the discriminator's convolutional parameters (right). To specify the starting point where the difference between the training and validation accuracies is greater than 0.5, we use a solid black line. The first and second black dotted lines indicate when the performance is best and when training collapse occurs, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Large batch size. (A, C) and (E, H) show that ContraGAN can benefit from large batch size. Effect of the proposed 2C loss. (A, E) and (C, H) show that the proposed 2C loss significantly reduces FID scores of the vanilla networks (A, C) by 21.6% and 11.2%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Examples of generated images using the proposed ContraGAN. (left) CIFAR10<ref type="bibr" target="#b23">[24]</ref>, FID: 10.322, (right) ImageNet<ref type="bibr" target="#b17">[18]</ref>, FID: 19.443. In the case of ImageNet experiment, we select and plot well-generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Comparison with CR. (A, E, G) and (C, H, I) show that vanilla + 2C loss + CR can reduce FIDs of either the results from vanilla networks (A, C) and vanilla + 2C loss (E, H). Note that the synergy is only observable if CR is used with 2C loss, and vanilla + 2C loss + CR beats vanilla + CR (B, D) with a large margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A1 :</head><label>A1</label><figDesc>Change of FID values as the temperature increases. Experiments are executed three times, and the means and standard deviations are represented by the blue dots and solid lines, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure</head><label></label><figDesc>A2: (a) FID values of ContraGANs with different projection layers and embedding dimensionalities. (b) The change in FID values as the batch size increases. The experiments (a) and (b) are conducted using the setting (D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A5 :</head><label>A5</label><figDesc>Examples generated by ProjGAN [17] on CIFAR10 dataset [24] (FID=10.933).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A6 :</head><label>A6</label><figDesc>Examples generated by ContraGAN (Ours) on CIFAR10 dataset<ref type="bibr" target="#b23">[24]</ref> (FID=10.188).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A7 :</head><label>A7</label><figDesc>Examples generated by ProjGAN<ref type="bibr" target="#b16">[17]</ref> on Tiny ImageNet dataset<ref type="bibr" target="#b24">[25]</ref> (FID=34.090).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A8 :</head><label>A8</label><figDesc>Examples generated by ContraGAN (Ours) on Tiny ImageNet dataset<ref type="bibr" target="#b24">[25]</ref> (FID=30.286).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A9 :</head><label>A9</label><figDesc>Examples generated by ProjGAN [17] on ImageNet dataset [18] (FID=21.072).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure A10 :</head><label>A10</label><figDesc>Examples generated by ContraGAN (Ours) on ImageNet dataset [18] (FID=19.443).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experiments on the effectiveness of 2C loss. Considering both data-to-data and data-to-class relations largely improves image generation results based on FID values. Mean±variance of FID is reported, and lower is better.DatasetUncond. GAN<ref type="bibr" target="#b5">[6]</ref> with P-NCA loss<ref type="bibr" target="#b32">[33]</ref> with NT-Xent loss<ref type="bibr" target="#b33">[34]</ref> with Eq.7 loss with 2C loss(ContraGAN)</figDesc><table><row><cell>CIFAR10 [24]</cell><cell>15.550±1.955</cell><cell>15.350±0.017</cell><cell>14.832±0.695</cell><cell>10.886±0.072</cell><cell>10.597±0.273</cell></row><row><cell>Tiny ImageNet [25]</cell><cell>56.297±1.499</cell><cell>47.867±1.813</cell><cell>54.394±9.982</cell><cell>33.488±1.006</cell><cell>32.720±1.084</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiments using CIFAR10 and Tiny ImageNet datasets. Using three backbone architectures (DCGAN, ResGAN, and BigGAN), we test three approaches using different class conditioning models (ACGAN, ProjGAN, and ContraGAN (ours)).</figDesc><table><row><cell>Dataset</cell><cell>Backbone</cell><cell cols="2">Method for class information conditioning ACGAN [19] ProjGAN [17] ContraGAN (Ours)</cell></row><row><cell></cell><cell>DCGAN [2, 4]</cell><cell>21.439±0.581 19.524± 0.249</cell><cell>18.788±0.571</cell></row><row><cell>CIFAR10 [24]</cell><cell cols="2">ResGAN [26, 16] 11.588±0.093 11.025± 0.177</cell><cell>11.334±0.126</cell></row><row><cell></cell><cell>BigGAN [6]</cell><cell>10.697±0.129 10.739±0.016</cell><cell>10.597±0.273</cell></row><row><cell>Tiny ImageNet [25]</cell><cell>BigGAN [6]</cell><cell>88.628±5.523 37.563±4.390</cell><cell>32.720±1.084</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>SNResGAN [4]</cell><cell>SAGAN [5]</cell><cell>BigGAN [6]</cell><cell>ContraGAN (Ours)</cell><cell>Improvement</cell></row><row><cell>CIFAR10 [24]</cell><cell>*17.5</cell><cell cols="2">17.127±0.220 *14.73/10.739±0.016</cell><cell>10.597±0.273</cell><cell>*+28.1%/+1.3%</cell></row><row><cell cols="3">Tiny ImageNet [25] 47.055±3.234 46.221±3.655</cell><cell>31.771±3.968</cell><cell>29.492±1.296</cell><cell>+7.2%</cell></row><row><cell>ImageNet [18]</cell><cell>-</cell><cell>-</cell><cell>21.072</cell><cell>19.443</cell><cell>+7.7%</cell></row></table><note>Comparison with state-of-the-art GAN models. We mark '*' to FID values reported in the original papers [4, 5, 7]. The other FID values are obtained from our implementation. We conduct ImageNet [18] experiments with a batch size of 256.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on various batch sizes, losses, and regularizations. In Reg. row, we mark − if an approach not applied and mark otherwise (in order of 2C loss, Augmented Positive Samples (APS), and Consistency Regularization<ref type="bibr" target="#b6">[7]</ref> (CR)). Please refer Sec. 4.6 for the details.</figDesc><table><row><cell>ID</cell><cell>(A)</cell><cell>(B)</cell><cell>(C)</cell><cell>(D)</cell><cell>(E)</cell><cell>(F)</cell><cell>(G)</cell><cell>(H)</cell><cell>(I)</cell></row><row><cell>Batch</cell><cell>256</cell><cell>256</cell><cell>1024</cell><cell>1024</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Param.</cell><cell>48.1</cell><cell>48.1</cell><cell>48.1</cell><cell>48.1</cell><cell>49.0</cell><cell>49.0</cell><cell>49.0</cell><cell>49.0</cell><cell>49.0</cell></row><row><cell>Reg.</cell><cell>---</cell><cell>--</cell><cell>---</cell><cell>--</cell><cell>--</cell><cell>-</cell><cell>-</cell><cell>--</cell><cell>-</cell></row><row><cell>FID</cell><cell cols="9">40.981 36.434 34.090 38.231 32.094 33.151 28.631 30.286 27.018</cell></row><row><cell>Time</cell><cell>0.901</cell><cell>1.093</cell><cell>3.586</cell><cell>4.448</cell><cell>0.967</cell><cell>1.110</cell><cell>1.121</cell><cell>3.807</cell><cell>4.611</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; communications TechnologyPlanning &amp; Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-01906, Artificial Intelligence Graduate School Program(POSTECH)). The supercomputing resources for this work was partially supported by Grand Challenging Project of Supercomuting Bigdata Center, DGIST.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A1 :</head><label>A1</label><figDesc>Generator of SNDCGAN<ref type="bibr" target="#b3">[4]</ref> used for CIFAR10<ref type="bibr" target="#b23">[24]</ref> image synthesis.</figDesc><table><row><cell>Layer</cell><cell>Input</cell><cell>Output</cell><cell>Operation</cell></row><row><cell>Input Layer</cell><cell>(m, 128)</cell><cell>(m, 8192)</cell><cell>FC(128, 8192)</cell></row><row><cell cols="2">Reshape Layer (m, 8192)</cell><cell>(m, 4, 4, 512)</cell><cell>RESHAPE</cell></row><row><cell>Hidden Layer</cell><cell>(m, 4, 4, 512)</cell><cell cols="2">(m, 8, 8, 256) DECONV(512, 256, 4, 2),CBN,LRELU</cell></row><row><cell>Hidden Layer</cell><cell>(m, 8, 8, 256)</cell><cell cols="2">(m, 16, 16, 128) DECONV(256, 128, 4, 2),CBN,LRELU</cell></row><row><cell>Hidden Layer</cell><cell>(m, 16, 16, 128)</cell><cell cols="2">(m, 32, 32, 64) DECONV(128, 64, 4, 2),CBN,LRELU</cell></row><row><cell>Hidden Layer</cell><cell>(m, 32, 32, 64)</cell><cell>(m, 32, 32, 3)</cell><cell>CONV(64, 3, 3, 1)</cell></row><row><cell>Output Layer</cell><cell>(m, 32, 32, 3)</cell><cell>(m, 32, 32, 3)</cell><cell>TANH</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A2 :</head><label>A2</label><figDesc>Discriminator of SNDCGAN<ref type="bibr" target="#b3">[4]</ref> used for CIFAR10<ref type="bibr" target="#b23">[24]</ref> image synthesis.</figDesc><table><row><cell>Layer</cell><cell>Input</cell><cell>Output</cell><cell>Operation</cell></row><row><cell>Input Layer</cell><cell>(m, 32, 32, 3)</cell><cell>(m, 32, 32, 64)</cell><cell>CONV(3, 64, 3, 1), LRELU</cell></row><row><cell cols="2">Hidden Layer (m, 32, 32, 64)</cell><cell>(m, 16, 16, 64)</cell><cell>CONV(64, 64, 4, 2), LRELU</cell></row><row><cell cols="2">Hidden Layer (m, 16, 16, 64)</cell><cell cols="2">(m, 16, 16, 128) CONV(64, 128, 3, 1), LRELU</cell></row><row><cell cols="2">Hidden Layer (m, 16, 16, 128)</cell><cell cols="2">(m, 8, 8, 128) CONV(128, 128, 4, 2), LRELU</cell></row><row><cell cols="2">Hidden Layer (m, 8, 8, 128)</cell><cell cols="2">(m, 8, 8, 256) CONV(128, 256, 3, 1), LRELU</cell></row><row><cell cols="2">Hidden Layer (m, 8, 8, 256)</cell><cell cols="2">(m, 4, 4, 256) CONV(256, 256, 4, 2), LRELU</cell></row><row><cell cols="2">Hidden Layer (m, 4, 4, 256)</cell><cell cols="2">(m, 4, 4, 512) CONV(256, 512, 3, 1), LRELU</cell></row><row><cell cols="2">Hidden Layer (m, 4, 4, 512)</cell><cell>(m, 512)</cell><cell>GSP</cell></row><row><cell cols="2">Output Layer (m, 512)</cell><cell>(m, 1)</cell><cell>FC(512, 1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A12 :</head><label>A12</label><figDesc>Comparison of TensorFlow and PyTorch FID implementations.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>arXiv 1511.06434</idno>
		<title level="m">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno>arXiv 1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-Attention Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Consistency Regularization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv 2002.04724</idno>
		<title level="m">Improved Consistency Regularization for GANs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
		<idno>arXiv 1912.00953</idno>
		<title level="m">Latent Optimisation for Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">D</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<title level="m">On Convergence and Stability of GANs. arXiv preprint arXiv 1705.07215</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the Limitations of First-Order Approximation in GAN Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient descent GAN optimization is locally stable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Zico</forename><surname>Vaishnavh Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5585" to="5595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Least Squares Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno>arXiv 1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">cGANs with Projection Discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional Image Synthesis with Auxiliary Classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Whitening and Coloring Batch Transform for GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Photo Editing with Introspective Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for dataefficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>arXiv 2006.06676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tiny ImageNet Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<ptr target="https://tiny-imagenet.herokuapp.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Two time-scale update rule for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="https://github.com/bioinf-jku/TTUR" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-Cooperative Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of mathematics</title>
		<imprint>
			<biblScope unit="page" from="286" to="295" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training Generative Neural Samplers using Variational Divergence Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno>arXiv 1411.1784</idno>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Metric Learning Using Triplet Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIMBAD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">No Fuss Distance Metric Learning Using Proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>arXiv 2002.05709</idno>
		<title level="m">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quadruplet-Wise Image Similarity Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble Deep Manifold Similarity Learning Using Hard Proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Aziere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="276" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Proxy Anchor Loss for Deep Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>arXiv 1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Markov processes over denumerable products of spaces, describing large systems of automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Nisonovich Vaserstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Peredachi Informatsii</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas</editor>
		<meeting><address><addrLine>Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner; Pete Warden, Martin Wattenberg, Martin Wicke</addrLine></address></meeting>
		<imprint>
			<publisher>Yuan Yu</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A Port of Fréchet Inception Distance (FID score) to PyTorch</title>
		<ptr target="https://github.com/mseitzer/pytorch-fid" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Learned Representation For Artistic Style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno>arXiv 2004.11362</idno>
		<title level="m">Supervised Contrastive Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4396" to="4405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Analyzing and Improving the Image Quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>arXiv 1912.04958</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse Image Synthesis for Multiple Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">Antonio</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Schawinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokula Krishnan</forename><surname>Santhanam</surname></persName>
		</author>
		<idno>arXiv 1702.00403</idno>
		<title level="m">Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Designing complex architectured materials with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunwei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhe</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generative adversarial networks (GAN) based efficient sampling of chemical composition space for inverse design of inorganic materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabo</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Materials</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Protecting World Leaders Against Deep Fakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairgan</surname></persName>
		</author>
		<title level="m">Fairness-aware Generative Adversarial Networks. International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="570" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Defending Against Neural Fake News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9054" to="9065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep learning and face recognition: the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Balaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric and Surveillance Technology for Human and Activity Identification XII</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">CNN-generated images are surprisingly easy to spot... for now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deepfake Video Detection Using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Güera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deepfake Video Detection through Optical Flow Based CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Amerini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Caldelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuezun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Rectified Activations in Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv 1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Network In Network. arXiv preprint arXiv 1312</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4400</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Improving Generative Adversarial Networks with Denoising Feature Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>arXiv 1710.10196</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Which Training Methods for GANs do actually Converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The Unusual Effectiveness of Averaging in GAN Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yazıcı</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
