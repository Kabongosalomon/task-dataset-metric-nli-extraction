<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DVDNET: A FAST NETWORK FOR DEEP VIDEO DENOISING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris Descartes † GoPro</orgName>
								<address>
									<postCode>MAP5</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris Descartes † GoPro</orgName>
								<address>
									<postCode>MAP5</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris Descartes † GoPro</orgName>
								<address>
									<postCode>MAP5</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DVDNET: A FAST NETWORK FOR DEEP VIDEO DENOISING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-video denoising</term>
					<term>CNN</term>
					<term>residual learning</term>
					<term>neural networks</term>
					<term>image restoration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a state-of-the-art video denoising algorithm based on a convolutional neural network architecture. Previous neural network based approaches to video denoising have been unsuccessful as their performance cannot compete with the performance of patch-based methods. However, our approach outperforms other patch-based competitors with significantly lower computing times. In contrast to other existing neural network denoisers, our algorithm exhibits several desirable properties such as a small memory footprint, and the ability to handle a wide range of noise levels with a single network model. The combination between its denoising performance and lower computational load makes this algorithm attractive for practical denoising applications. We compare our method with different state-of-art algorithms, both visually and with respect to objective quality metrics. The experiments show that our algorithm compares favorably to other state-of-art methods. Video examples, code and models are publicly available at https://github.com/ m-tassano/dvdnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We introduce a network for Deep Video Denoising: DVDnet. The algorithm compares favorably to other state-of-theart methods, while it features fast running times. The outputs of our algorithm present remarkable temporal coherence, very low flickering, strong noise reduction, and accurate detail preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Image Denoising</head><p>Compared to image denoising, video denoising appears as a largely underexplored domain. Recently, new image denoising methods based on deep learning techniques have drawn considerable attention due to their outstanding performance. 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. <ref type="bibr">Schmidt</ref> and Roth proposed in <ref type="bibr" target="#b0">[1]</ref> the cascade of shrinkage fields method that unifies the random field-based model and half-quadratic optimization into a single learning framework. Based on this method, Chen and Pock proposed in <ref type="bibr" target="#b1">[2]</ref> a trainable nonlinear reaction diffusion model. This model can be expressed as a feed-forward deep network by concatenating a fixed number of gradient descent inference steps. Methods such as these two attain denoising performances comparable to those of well-known algorithms such as BM3D <ref type="bibr" target="#b2">[3]</ref> or non-local Bayes (NLB <ref type="bibr" target="#b3">[4]</ref>). However, their performance is restricted to specific forms of prior. Additionally, many handtuned parameters are involved in the training process. In <ref type="bibr" target="#b4">[5]</ref>, a multi-layer perceptron was successfully applied for image denoising. Nevertheless, a significant drawback of all these algorithms is that a specific model must be trained for each noise level.</p><p>Another popular approach involves the use of convolutional neural networks (CNN), e.g. RBDN <ref type="bibr" target="#b5">[6]</ref>, DnCNN <ref type="bibr" target="#b6">[7]</ref>, and FFDNet <ref type="bibr" target="#b7">[8]</ref>. Their performance compares favorably to other state-of-the-art image denoising algorithms, both quantitatively and visually. These methods are composed of a succession of convolutional layers with nonlinear activation functions in between them. This type of architecture has been applied to the problem of joint denoising and demosaicing of RGB and raw images by Gharbi et al. in <ref type="bibr" target="#b8">[9]</ref>. Contrary to other deep learning denoising methods, one of the remarkable features that these CNN-based methods present is the ability to denoise several levels of noise with only one trained model. Proposed by Zhang et al. in <ref type="bibr" target="#b6">[7]</ref>, DnCNN is an end-to-end trainable deep CNN for image denoising. This method is able to denoise different noise levels (e.g. with standard deviation σ ∈ [0, 55]) with only one trained model. One of its main features is that it implements residual learning <ref type="bibr" target="#b9">[10]</ref>, i.e. it estimates the noise existent in the input image rather than the denoised image. In a following paper <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr">Zhang et al. proposed</ref> FFDNet, which builds upon the work done for DnCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Video Denoising</head><p>As for video denoising, the method proposed by Chen et al. in <ref type="bibr" target="#b10">[11]</ref> is one of the few to approach this problem with neural networks-recurrent neural networks in their case. However, their algorithm only works on grayscale images and it does not achieve satisfactory results, probably due to the difficulties associated with training recurring neural networks <ref type="bibr" target="#b11">[12]</ref>. Vogels et al. proposed in <ref type="bibr" target="#b12">[13]</ref> an architecture based on kernelpredicting neural networks able to denoise Monte Carlo rendered sequences. The state-of-the-art in video denoising is mostly defined by patch-based methods. Kokaram et al. proposed in <ref type="bibr" target="#b13">[14]</ref> a 3D Wiener filtering scheme. We note in particular an extension of the popular BM3D to video denoising, V-BM4D <ref type="bibr" target="#b14">[15]</ref>, and Video non-local Bayes (VNLB <ref type="bibr" target="#b15">[16]</ref>). Nowadays, VNLB is the best video denoising algorithm in terms of quality of results, as it outperforms V-BM4D by a large margin. Nonetheless, its long running times render the method impractical-it could take several minutes to denoise a single frame. The performance of our method compares favorably to that of VNLB for moderate to large values of noise, while it features significantly faster inference times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OUR METHOD</head><p>Methods based on neural networks are nowadays state-of-theart in image denoising. However, state-of-the-art in video denoising still consists of patch-based methods. Generally speaking, most previous approaches based on deep learning have failed to employ the temporal information existent in image sequences effectively. Temporal coherence and the lack of flickering are vital aspects in the perceived quality of a video. Most state-of-the-art algorithms in video denoising are extensions of their image denoising counterparts. Such is the case, for example, of V-BM4D and BM3D, or VNLB and NLB. There are mainly two factors in these video denoising approaches which enforce temporal coherence in the results, namely the extension of search regions from spatial neighborhoods to volumetric neighborhoods, and the use of motion estimation. In other words, the former implies that when denoising a given pixel (or patch), the algorithm is going to look for similar pixels (patches) not only in the same frame, but also in adjacent frames of the sequence. Secondly, the use of motion estimation and/or compensation has been shown to help improving video denoising performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref>. We thus incorporated these two elements into our algorithm, as well as different aspects of other relevant CNN-based denoising architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. Thanks to all these characteristics, our algorithm improves the state-of-the-art results, while featuring fast inference times. <ref type="figure" target="#fig_0">Figure 1</ref> displays a simplified diagram of the architecture of our method. When denoising a given frame, its 2T neighboring frames are also taken as inputs. The denoising process of our algorithm can be split in two stages. Firstly, the 2T + 1 frames are individually denoised with a spatial denoiser. Although each individual frame output at this stage features relatively good image quality, they present evident flickering when considered as a sequence. In the second stage of the algorithm, the 2T denoised temporal neighbors are registered with respect to the central frame. We use optical flow for this purpose. Splitting denoising in two stages allows for an individual pre-processing of each frame. On top of this, motion compensation is performed on pre-denoised images, which facilitates the task. Finally, the 2T + 1 aligned frames are concatenated and input into the temporal denoising block. Using temporal neighbors when denoising each frame helps to reduce flickering as the residual error in each frame will be correlated. We also add a noise map as input to the spatial and temporal denoisers. The inclusion of the noise map as input allows the processing of spatially varying noise <ref type="bibr" target="#b17">[18]</ref>. Contrary to other denoising algorithms, our denoiser takes no other parameters as inputs apart from the image sequence and the estimation of the input noise.</p><p>Observe that experiments presented in this paper focus on the case of additive white Gaussian noise (AWGN). Nevertheless, this algorithm can be straightforwardly extended to other types of noise, e.g. spatially varying noise (e.g. Poissonian). Let I be a noiseless image, whileĨ is its noisy version corrupted by a realization of zero-mean white Gaussian noise N of standard deviation σ, theñ</p><formula xml:id="formula_0">I = I + N .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Spatial and Temporal Denoising Blocks</head><p>The design characteristics of the spatial and temporal blocks make a good compromise between performance and fast running times. Both blocks are implemented as standard feedforward networks, as shown in <ref type="figure" target="#fig_1">fig. 2</ref>. The architecture of the spatial denoiser is inspired by the architectures in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, while the temporal denoiser also borrows some elements from <ref type="bibr" target="#b12">[13]</ref>. The spatial and temporal denoising blocks are composed of D spa = 12, and D temp = 6 convolutional layers, respectively. The number of feature maps is set to W = 96. The outputs of the convolutional layers are followed by pointwise ReLU <ref type="bibr" target="#b18">[19]</ref> activation functions ReLU (·) = max(·, 0). At training time, batch normalization layers (BN <ref type="bibr" target="#b19">[20]</ref>) are placed between the convolutional and ReLU layers. At evaluation time, the batch normalization layers are removed, and replaced by an affine layer that applies the learned normalization. The spatial size of the convolutional kernels is 3 × 3, and the stride is set to 1. In both blocks, the inputs are first downscaled to a quarter resolution. The main advantage of performing the denoising in a lower resolution is the large reduction in running times and memory requirements, without sacrificing denoising performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. The upscaling back to full resolution is performed with the technique described in <ref type="bibr" target="#b20">[21]</ref>. Both blocks feature residual connections <ref type="bibr" target="#b9">[10]</ref>, which have been observed to ease the training process <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TRAINING DETAILS</head><p>The spatial and temporal denoising parts are trained separately, with the spatial denoiser trained first as its outputs are  which are generated by adding AWGN with standard deviation σ ∈ [0, 55] to the clean patches I j and building the corresponding noise map M j (which is in this case constant with all its elements equal to σ). A total of m s = 1024000 patches are extracted from the Waterloo Exploration Database <ref type="bibr" target="#b21">[22]</ref>. The patch size is 50×50. Patches are randomly cropped from randomly sampled images of the training dataset. Residual learning is used, which implies that if the network outputs an estimation of the input noise F spa (Ĩ; θ spa ) =N, then the denoised image is computed by subtracting the output noise to the noisy input</p><formula xml:id="formula_1">I(Ĩ; θ spa ) =Ĩ − F spa (Ĩ; θ spa ) .<label>(2)</label></formula><p>The loss function of the spatial denoiser writes</p><formula xml:id="formula_2">L spa (θ spa ) = 1 2m s ms j=1 Î j (Ĩ j ; θ spa ) − I j 2 ,<label>(3)</label></formula><p>where θ spa is the collection of all learnable parameters. As for the temporal denoiser, the training dataset consists of input-output pairs</p><formula xml:id="formula_3">P j t = ( ( wÎj t−T , . . . ,Î j t , . . . , wÎj t+T ), M j ), I j t mt j=0 ,</formula><p>where ( wÎ j t−T , . . . ,Î j t , . . . , wÎ j t+T ) is a collection of 2T + 1 spatial patches cropped at the same location in contiguous frames. These are generated by adding AWGN of σ ∈ [0, 55] to clean patches of a given sequence, and denoising them using the spatial denoiser. Then, the 2T patches contiguous to the central reference patch I j t are motion-compensated with respect to the latter, i.e. wÎ j l = compensate(Î j l ,Î j t ). To compensate frames, we use the DeepFlow algorithm <ref type="bibr" target="#b22">[23]</ref> for the estimation of the optical flow between frames. The noise map M j is the same as the one used in the spatial denoising stage. A total of m t = 450000 training samples are extracted from the training set of the DAVIS database <ref type="bibr" target="#b23">[24]</ref>. The spatial size of the patches is 44 × 44, while the temporal size is 2T + 1 = 5. The loss function for the temporal denoiser is</p><formula xml:id="formula_4">L temp (θ temp ) = 1 2m t mt j=1 Î j temp, t − I j t 2 ,<label>(4)</label></formula><p>whereÎ j temp, t = F temp (P j t ; θ temp ). In both cases, the ADAM algorithm <ref type="bibr" target="#b24">[25]</ref> is applied to minimize the loss function, with all its hyper-parameters set to their default values. The number of epochs is set to 80, and the mini-batch size is 128. The scheduling of the learning rate is also common to both cases. It starts at 1e−3 for the first 50 epochs, then changes to 1e−4 for the following 10 epochs, and finally switches to 1e−6 for the remaining of the training. Data is augmented five times by introducing rescaling by different scale factors and random flips. During the first 60 epochs, the orthogonalization of the convolutional kernels is applied as a means of regularization. It has been observed that initializing the training with orthogonalization may be beneficial to performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>Two different testsets were used for benchmarking our method: the DAVIS-test testset, and Set8, which is com- In all cases, sequences were limited to a maximum of 85 frames. We used the DeepFlow algorithm to compute flow maps for DVDnet and VNLB. We also compare our method to a commercial blind denoising software, Neat Video (NV <ref type="bibr" target="#b25">[26]</ref>). In general, DVDnet outputs sequences which feature remarkable temporal coherence. Flickering rendered by our method is notably small, especially in flat areas, where patchbased algorithms often leave behind low-frequency residual noise. An example can be observed in <ref type="figure" target="#fig_2">fig. 3</ref> (which is best viewed in digital format). Temporally decorrelated lowfrequency noise in flat areas appears as particularly annoying in the eyes of the viewer. More video examples can be found in the website of the algorithm. The reader is encouraged to watch these examples to compare the visual quality of the results of our method. <ref type="table" target="#tab_0">Tables 1 and 2</ref> show a comparison of P SN R on the Set8 and DAVIS dataset, respectively. It can be observed that for smaller values of noise, VNLB performs better. In effect, DVDnet tends to over denoise in some of these cases. However, for larger values of noise DVDnet surpasses VNLB.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Running times</head><p>Our method achieves fast inference times, thanks to its design characteristics and simple architecture. DVDnet takes less than 8s to denoise a 960 × 540 color frame, which is about 20 times faster than V-BM4D, and about 50 times faster than VNLB. Even running on CPU, DVDnet is about an order of magnitude faster than these methods. Of the 8s it takes to denoise a frame, 6s are spent on compensating motion of the temporal neighboring frames. <ref type="table" target="#tab_2">Table 3</ref> compares the running times of different state-of-the-art algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we presented DVDnet, a video denoising algorithm which improves the state-of-the-art. Denoising results of DVDnet feature remarkable temporal coherence, very low flickering, and excellent detail preservation. The algorithm achieves running times which are at least an order of magnitude faster than other state-of-the-art competitors. Although the results presented in this paper hold for Gaussian noise, our method could be extended to denoise other types of noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Simplified architecture of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Simplified architecture of the spatial (top) and temporal (bottom) denoising blocks. used to train the temporal denoiser. Both blocks are trained using crops of images, or patches. The size of the patches should be larger than the receptive field of the networks. In the case of the spatial denoiser, the training dataset is composed of pairs of input-output patches (Ĩ j , M j ), I j ms j=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of results. Left to right: noisy frame (PSNR seq = 14.15dB), output by V-BM4D (PSNR seq = 24.91dB), output by VNLB (PSNR seq = 26.34dB), output by Neat Video (PSNR seq = 23.11dB), output by DVDnet (PSNR seq = 26.62dB). Note the clarity of the denoised text, and the lack of low-frequency residual noise and chroma noise for DVDnet. Best viewed in digital format. posed of 4 color sequences from the Derfs Test Media collection 1 and 4 color sequences captured with a GoPro camera. The DAVIS set contains 30 color sequences of resolution 854 × 480. The sequences of Set8 have been downscaled to a resolution of 960 × 540.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of P SN R on the Set8 testset.</figDesc><table><row><cell></cell><cell cols="3">DVDnet VNLB V-BM4D</cell><cell>NV</cell></row><row><cell>σ = 10</cell><cell>36.08</cell><cell>37.26</cell><cell>36.05</cell><cell>35.67</cell></row><row><cell>σ = 20</cell><cell>33.49</cell><cell>33.72</cell><cell>32.19</cell><cell>31.69</cell></row><row><cell>σ = 30</cell><cell>31.79</cell><cell>31.74</cell><cell>30.00</cell><cell>28.84</cell></row><row><cell>σ = 40</cell><cell>30.55</cell><cell>30.39</cell><cell>28.48</cell><cell>26.36</cell></row><row><cell>σ = 50</cell><cell>29.56</cell><cell>29.24</cell><cell>27.33</cell><cell>25.46</cell></row></table><note>1 https://media.xiph.org/video/derf</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of P SN R on the DAVIS testset.</figDesc><table><row><cell></cell><cell cols="3">DVDnet VNLB V-BM4D</cell></row><row><cell>σ = 10</cell><cell>38.13</cell><cell>38.85</cell><cell>37.58</cell></row><row><cell>σ = 20</cell><cell>35.70</cell><cell>35.68</cell><cell>33.88</cell></row><row><cell>σ = 30</cell><cell>34.08</cell><cell>33.73</cell><cell>31.65</cell></row><row><cell>σ = 40</cell><cell>32.86</cell><cell>32.32</cell><cell>30.05</cell></row><row><cell>σ = 50</cell><cell>31.85</cell><cell>31.13</cell><cell>28.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of running times. Time to denoise a color frame of resolution 960 × 540. Note: values displayed for VNLB do not include the time required to estimate motion.</figDesc><table><row><cell cols="5">Method V-BM4D VNLB DVDnet DVDnet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(CPU)</cell><cell>(GPU)</cell></row><row><cell>Time (s)</cell><cell>156</cell><cell>420</cell><cell>19</cell><cell>8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3D transformation-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katkovnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Nonlocal Bayesian Image Denoising Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal IS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1665" to="1688" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalized Deep Image to Image Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep rnns for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing XXXIX. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9971</biblScope>
			<biblScope unit="page">99711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denoising with kernel prediction and asymmetric loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thijs</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Röthlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Harvill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Novák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Motion picture restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Christopher Kokaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-D nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video denoising via empirical Bayesian estimation of space-time patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="93" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Patch-Based Video Denoising With Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose-Luis</forename><surname>Lisani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2573" to="2586" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Analysis and Implementation of the FFDNet Image Denoising Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPOL</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
		<respStmt>
			<orgName>JMLR.org</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Waterloo Exploration Database: New Challenges for Image Quality Assessment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV, Sydney</title>
		<meeting><address><addrLine>Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video object segmentation with language referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ADAM: a Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neat Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Absoft</surname></persName>
		</author>
		<ptr target="https://www.neatvideo.com" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
