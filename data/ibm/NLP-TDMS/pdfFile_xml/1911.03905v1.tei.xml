<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Noise Matters for Neural Natural Language Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10">October 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
							<email>odusek@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Howcroft</surname></persName>
							<email>d.howcroft@hw.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
							<email>v.t.rieser@hw.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics Prague</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">The Interaction Lab, MACS Heriot-Watt University Edinburgh</orgName>
								<address>
									<country key="GB">Scotland, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Noise Matters for Neural Natural Language Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of INLG</title>
						<meeting>INLG <address><addrLine>Tokyo, Japan</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2019-10">October 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-theart NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Natural Language Generation (NNLG) is promising for generating text from Meaning <ref type="bibr">Representations (MRs)</ref> in an 'end-to-end' fashion, i.e. without needing alignments <ref type="bibr" target="#b23">(Wen et al., 2015</ref><ref type="bibr" target="#b22">(Wen et al., , 2016</ref><ref type="bibr" target="#b2">Dušek and Jurčíček, 2016;</ref><ref type="bibr" target="#b15">Mei et al., 2016)</ref>. However, NNLG requires large volumes of indomain data, which is typically crowdsourced (e.g. <ref type="bibr" target="#b14">Mairesse et al., 2010;</ref><ref type="bibr" target="#b17">Novikova et al., 2016;</ref><ref type="bibr" target="#b23">Wen et al., 2015</ref><ref type="bibr" target="#b22">Wen et al., , 2016</ref><ref type="bibr" target="#b8">Howcroft et al., 2017)</ref>, introducing noise. For example, up to 40% of the E2E Generation Challenge 1 data contains omitted or additional information <ref type="bibr" target="#b4">(Dušek et al., 2019)</ref>.</p><p>In this paper, we examine the impact of this type of semantic noise on two state-of-the-art NNLG models with different semantic control mechanisms: TGen <ref type="bibr" target="#b2">(Dušek and Jurčíček, 2016)</ref> and <ref type="bibr">SC-LSTM (Wen et al., 2015)</ref>. In particular, we investigate the systems' ability to produce fact-accurate text, i.e. without omitting or hallucinating information, in the presence of semantic noise. <ref type="bibr">2</ref> We find that: * Denotes equal contribution. 1 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ 2 Also see https://ehudreiter.com/2018/11/ 12/hallucination-in-neural-nlg/ • training on cleaned data reduces slot-error rate up to 97% on the original evaluation data; • testing on cleaned data is challenging, even for models trained on cleaned data, likely due to increased MR diversity in the cleaned dataset; and • TGen performs better than SC-LSTM, even when cleaner training data is available. We hypothesise that this is due to differences in how the two systems handle semantic input and the degree of delexicalization that they expect. In addition, we release our code and a cleaned version of the E2E data with this paper. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mismatched Semantics in E2E Data</head><p>The E2E dataset contains input MRs and corresponding target human-authored textual references in the restaurant domain. MRs here are sets of attribute-value pairs (see <ref type="figure">Figure 1</ref>). Most MRs in the dataset have multiple references (8.1 on average). These were collected using crowdsourcing, leading to noise when crowd workers did not verbalise all attributes or added information not present in the MR. According to <ref type="bibr" target="#b4">Dušek et al. (2019)</ref>, the multiple references should help NLG systems abstract from the noise. However, most NLG systems in the E2E challenge in fact produced noisy outputs, suggesting that they were unable to learn to ignore noise in the training input.</p><p>Problems with the semantic accuracy in training data is not unique to the E2E dataset. <ref type="bibr" target="#b8">Howcroft et al. (2017)</ref> collected a corpus of paraphrases differing with respect to information density for use in training NLG systems and found that subjects' paraphrases dropped about 5% of the slot-value pairs from the original texts and changed the val-  <ref type="figure">Figure 1</ref>: MR and references from the E2E corpus. The first reference is accurate and verbalises all attributes, but the remaining ones contain inaccuracies. Corrected MRs were automatically produced by our slot matching script (see Section 3). Note that HR 2 is not fixed properly since the script's patterns are not perfect.  ues for approximately 10% of the slot-value pairs. As a result of these changes and the insertion of new facts, only 61% of the corpus contained all and only the intended propositions. This is similar to what <ref type="bibr" target="#b6">Eric et al. (2019)</ref> found in their work on the MultiWOZ 2.0 dataset: correcting the dialogue state annotations resulted in changes to about 40% of the dialogue turns in their dataset. These findings suggest that efforts to create more accurate training data-whether through stricter crowdsourcing protocols, conducting follow-up annotations (cf. <ref type="bibr" target="#b6">Eric et al., 2019)</ref>, or automated cleanup heuristics like we report here-are likely necessary in the NLG and dialogue systems communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cleaning the Meaning Representations</head><p>To produce a cleaned version of the E2E data, we used the original human textual references, but paired them with correctly matching MRs. <ref type="bibr">4</ref> To this end, we reimplemented the slot matching script of <ref type="bibr" target="#b19">Reed et al. (2018)</ref>, which tags MR slots and values using regular expressions. We tuned our expressions based on the first 500 instances from the E2E development set and ran the script on the full dataset, producing corrected MRs for all human references (see <ref type="figure">Figure 1</ref>). The differences against the original MRs allow us to compute the semantic/slot error rate (SER; <ref type="bibr" target="#b23">Wen et al., 2015;</ref><ref type="bibr" target="#b19">Reed et al., 2018;</ref><ref type="bibr" target="#b4">Dušek et al., 2019)</ref>:</p><formula xml:id="formula_0">SER = #added + #missing + #wrong value #slots</formula><p>To guarantee the integrity of the test set, we removed instances from the TRAIN (training) and DEV (development) sets that overlapped the TEST set. This resulted in 20% reduction for TRAIN and ca. 8% reduction for DEV in terms of references (see <ref type="table" target="#tab_2">Table 1</ref>). On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes. This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs. We manually evaluated 200 randomly chosen instances from the cleaned TRAIN set to check the accuracy of the slot matching script. We found that the slot matching script itself has a SER of 4.2%, with 39 instances (19.5%) not 100% correctly rated. This is much lower than the E2E dataset authors' own manual assessment of ca. 40% noisy instances <ref type="bibr" target="#b4">(Dušek et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TGen</head><p>TGen <ref type="bibr" target="#b2">(Dušek and Jurčíček, 2016</ref>) is the baseline system used in the E2E challenge. 6 TGen is in essence a vanilla sequence-to-sequence (seq2seq) model with attention <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> using LSTM cells where input MRs are encoded as sequences of triples in the form (dialogue act, slot, value). 7 TGen adds to the standard seq2seq setup a reranker that selects the output with the lowest SER from the decoder output beam (n-best list). SER is estimated based on a classifier trained to identify the MR corresponding to a given text. We use the default TGen parameters for the E2E data, experimenting with three variants:</p><p>• TGen without reranker: a vanilla seq2seq model with attention (TGen−); • TGen with default reranker: the same augmented with an LSTM encoder and binary classifier for individual slot-value pairs; • TGen with oracle reranker: directly uses the slot matching script to compute SER (TGen+). We fixed the parameters of the main seq2seq generator to see the direct influence of each reranker, without the added effect of random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SC-LSTM</head><p>In contrast to seq2seq architecture used by TGen, the Semantically Controlled LSTM (SC-LSTM, Wen et al., 2015) uses a learned gating mechanism to selectively express parts of the MR during generation. We use the SC-LSTM model provided as part of the RNNLG repository 8 with minor changes to improve comparability to TGen. Most importantly, we incorporate the tokenization and normalization used by TGen into RNNLG. Since the word embeddings provided with RNNLG only cover about half of the tokens in the E2E dataset, we use randomly initialised word embeddings (dimension 50; same as TGen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Results</head><p>To measure the effect of noisy data, we compare systems trained on the original data against systems trained using cleaned TRAIN and validation (=DEV) sets; we perform the comparisons both on the original and the cleaned TEST sets. Note that only scores on the same test set are directly comparable as the cleaned TEST set has more diverse MRs and fewer references per MR (i.e. numbers in <ref type="table" target="#tab_4">Tables 2 and 3 cannot be compared across tables;</ref> cf. Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Metrics</head><p>We use freely available word-overlap-based evaluation metrics (WOM) scripts that come with the E2E data <ref type="bibr" target="#b4">(Dušek et al., 2019)</ref>, 9 supporting BLEU <ref type="bibr" target="#b18">(Papineni et al., 2002)</ref>, NIST <ref type="bibr" target="#b1">(Doddington, 2002)</ref>, ROUGE-L <ref type="bibr" target="#b13">(Lin, 2004)</ref>, METEOR <ref type="bibr" target="#b11">(Lavie and Agarwal, 2007)</ref> and CIDEr <ref type="bibr" target="#b21">(Vedantam et al., 2015)</ref>. In addition, we use our slot matching script for SER (cf. Section 3). We also show detailed results for the percentages of added and missed slots and wrong slot values. <ref type="bibr">10</ref> The results in <ref type="table" target="#tab_4">Table 2</ref> (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset. We hypothesise that this is mainly due to the amount of delexicalisation required. However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without. 11 In other words, just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER). WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams. This suggests better preservation of content at the expense of slightly lower fluency.</p><p>The results for testing on cleaned data <ref type="table" target="#tab_5">(Table 3</ref>, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf. Section 3), as reflected in the lower WOMs. This raises the question whether the improved results from clean training data are due to seeing more challenging examples at training time. However, the improved results for training and testing on clean data (i.e. seeing equally challenging examples at training and test time), suggest the increase in performance can be attributed to data accuracy rather than diversity. 9 https://github.com/tuetschek/e2emetrics 10 Absolute numbers of errors and number of completely correct instances are shown in <ref type="table">Table 5</ref>     Looking at the detailed results for the number of added, missing, and wrong-valued slots (Add, Miss, Wrong), we observe more deletions than insertions, i.e. the models more often fail to realise part of the MR, rather than hallucinating additional information. To investigate whether this effect stems from the training data, we partially cleaned the data of missing or added information only. <ref type="bibr">12</ref> However, the results in bottom halves <ref type="bibr">12</ref> We only performed these experiments on TGen because of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Again, one possible explanation is that cleaning the missing slots provided more complex training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Manual Error Analysis</head><p>We carried out a detailed manual error analysis of selected systems to confirm the automatic metrics results, performing a blind annotation of semantic and fluency errors (not a human preference rating). We evaluated a sample of 100 outputs on the original test set produced by TGen with the default reranker trained using all four cleaning settings (original data, cleaned missing slots, cleaned added slots, fully cleaned). The results in <ref type="table" target="#tab_6">Table 4</ref> confirm the findings of the automatic of the low performance of SC-LSTM in general. metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other. The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency. All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem.</p><p>The manual error analysis also served to assess the accuracy of the SER measuring script on system outputs. Since NNLG tends to use more frequent phrasing, we expected better performance than on the dataset itself, and this proved true: we only found 2 errors in the 400 system outputs (i.e. 99.5% of instances and 99.93% of slots were matched correctly). This confirms that the automatic SER numbers reflect the semantic accuracy of individual systems very closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Related Work</head><p>We present a detailed study of semantic errors in NNLG outputs and how these relate to noise in training data. We found that even imperfectly cleaned input data significantly improves semantic accuracy for seq2seq-based generators (up to 97% relative error reduction with the reranker), while only causing a slight decrease in fluency.</p><p>Contemporaneous with our work is the effort of <ref type="bibr" target="#b16">Nie et al. (2019)</ref>, who focus on automatic data cleaning using a NLU iteratively bootstrapped from the noisy data. Their analysis similarly finds that omissions are more common than hallucinations. Correcting for missing slots, i.e. forcing the generator to verbalise all slots during training, leads to the biggest performance improvement. This phenomenon is also observed by <ref type="bibr" target="#b3">Dušek et al. (2018</ref><ref type="bibr" target="#b4">Dušek et al. ( , 2019</ref> for systems in the E2E NLG challenge, but stands in contrast to work on related tasks, which mostly reports on hallucinations (i.e. adding information not grounded in the input), as observed for image captioning <ref type="bibr" target="#b20">(Rohrbach et al., 2018)</ref>, sports report generation <ref type="bibr" target="#b24">(Wiseman et al., 2017)</ref>, machine translation <ref type="bibr" target="#b10">(Koehn and Knowles, 2017;</ref><ref type="bibr" target="#b12">Lee et al., 2019)</ref>, and question answering <ref type="bibr" target="#b7">(Feng et al., 2018)</ref>. These previous works suggest that the most likely case of hallucinations is an over-reliance on language priors, i.e. memorising 'which words go together'. Similar priors could equally exist in the E2E data for omitting a slot; this might be connected with the fact that the E2E test set MRs tend to be longer than training MRs (6.91 slots on average for test MRs vs. 5.52 for training MRs) and that a large part of them is 'saturated', i.e. contains all possible 8 attributes.</p><p>Furthermore, in accordance with our observations, related work also reports a relation between hallucinations and data diversity: <ref type="bibr" target="#b20">Rohrbach et al. (2018)</ref> observe an increase for "novel compositions of objects at test time", i.e. non-overlapping test and training sets (cf. Section 3); whereas <ref type="bibr" target="#b12">Lee et al. (2019)</ref> reports data augmentation as one of the most efficient counter measures. In future work, we plan to experimentally manipulate these factors to disentangle the relative contributions of data cleanliness and diversity.  <ref type="table">Table 5</ref>: Absolute numbers of errors (added slots/missed slots/wrong slot values) and numbers of completely correct instances in all our experiments (compare to Tables 2 and 3 in the paper). Note that (1) the numbers are averages over 5 runs with different random network initializations, hence the non-integer values;</p><p>(2) only numbers in the top half and the bottom half (with the same test set) are comparable. The original test set has 630 MRs and 4,352 slots in total. The cleaned test set has 1,847 MRs and 11,547 slots; however, for the runs with SC-LSTM these counts are 1,800 and 11,101, respectively, since some items had to be dropped due to preprocessing issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and the script's rating of the whole dataset (mean SER: 16.37%),and comparable to the slot matching script of Juraska et al. (2018) evaluated on the same data. 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Original MR: name[Cotto], eatType[coffee shop], food[English], priceRange[less than £20], customer rating[low], area[riverside], near[The Portland Arms] Human reference 1 (accurate): At the riverside near The Portland Arms, Cotto is a coffee shop that serves English food at less than £20 and has low customer rating. HR 2: Located near The Portland Arms in riverside, the Cotto coffee shop serves English food with a price range of £20 and a low customer rating. Corrected MR: name[Cotto], eatType[coffee shop], food[English], customer rating[low], area[riverside], near[The Portland Arms] (removed price range)HR 3: Cotto is a coffee shop that serves English food in the city centre. They are located near the Portland Arms and are low rated.</figDesc><table><row><cell>Corrected MR: name[Cotto], eatType[coffee shop], food[English],</cell></row><row><cell>customer rating[low], area[city centre], near[The Portland Arms]</cell></row><row><cell>(removed price range, changed area)</cell></row><row><cell>HR 4: Cotto is a cheap coffee shop with one-star located near The Portland</cell></row><row><cell>Arms.</cell></row><row><cell>Corrected MR: name[Cotto], eatType[coffee shop], priceRange[less than</cell></row><row><cell>£20], customer rating[low], near[The Portland Arms]</cell></row><row><cell>(removed area)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Data statistics comparison for the original E2E</cell></row><row><cell>data and our cleaned version (number of distinct MRs,</cell></row><row><cell>total number of textual references, SER as measured by</cell></row><row><cell>our slot matching script, see Section 3).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>in the Supplementary.</figDesc><table><row><cell>TRAIN</cell><cell cols="2">TEST System</cell><cell>BLEU</cell><cell cols="4">NIST METEOR ROUGE-L CIDEr</cell><cell>Add</cell><cell cols="2">Miss Wrong SER</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell>63.37</cell><cell>7.7188</cell><cell>41.99</cell><cell>68.53</cell><cell cols="3">1.9355 00.06 15.77</cell><cell>00.11</cell><cell>15.94</cell></row><row><cell>Original</cell><cell></cell><cell>TGen TGen+</cell><cell>66.41 67.06</cell><cell>8.5565 8.5871</cell><cell>45.07 45.83</cell><cell>69.17 69.73</cell><cell cols="3">2.2253 00.14 04.11 2.2681 00.04 01.75</cell><cell>00.03 00.01</cell><cell>04.27 01.80</cell></row><row><cell></cell><cell></cell><cell>SC-LSTM</cell><cell>39.11</cell><cell>5.6704</cell><cell>36.83</cell><cell>50.02</cell><cell cols="3">0.6045 02.79 18.90</cell><cell>09.79</cell><cell>31.51</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell>65.87</cell><cell>8.6400</cell><cell>44.20</cell><cell>67.51</cell><cell cols="3">2.1710 00.20 00.56</cell><cell>00.21</cell><cell>00.97</cell></row><row><cell>Cleaned Cleaned missing</cell><cell>Original</cell><cell>TGen TGen+ SC-LSTM TGen− TGen TGen+</cell><cell>66.24 65.97 38.52 66.28 67.00 66.74</cell><cell>8.6889 8.6630 5.7125 8.5202 8.6889 8.6649</cell><cell>44.66 44.45 37.45 43.96 44.97 44.84</cell><cell>67.85 67.59 48.50 67.83 68.19 67.95</cell><cell cols="3">2.2181 00.10 00.02 2.1855 00.02 00.00 0.4343 03.85 17.39 2.1375 00.14 02.26 2.2228 00.06 00.44 2.2018 00.00 00.21</cell><cell>00.00 00.00 08.12 00.22 00.03 00.03</cell><cell>00.12 00.03 29.37 02.61 00.53 00.24</cell></row><row><cell>Cleaned added</cell><cell></cell><cell>TGen− TGen TGen+</cell><cell>64.40 66.23 65.96</cell><cell>7.9692 8.5578 8.5238</cell><cell>42.81 45.12 45.49</cell><cell>68.87 68.87 68.79</cell><cell cols="3">2.0563 00.01 13.08 2.2548 00.04 03.04 2.2456 00.00 01.44</cell><cell>00.00 00.00 00.00</cell><cell>13.09 03.09 01.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">11 0.12 4.27 = 0.028 and 0.97 15.94 = 0.061</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable toTable 3as the test set is different.</figDesc><table><row><cell>TRAIN</cell><cell cols="2">TEST System</cell><cell>BLEU</cell><cell cols="4">NIST METEOR ROUGE-L CIDEr</cell><cell>Add</cell><cell cols="2">Miss Wrong SER</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell>36.85</cell><cell>5.3782</cell><cell>35.14</cell><cell>55.01</cell><cell cols="3">1.6016 00.34 09.81</cell><cell>00.15</cell><cell>10.31</cell></row><row><cell>Original</cell><cell></cell><cell>TGen TGen+</cell><cell>39.23 40.25</cell><cell>6.0217 6.1448</cell><cell>36.97 37.50</cell><cell>55.52 56.19</cell><cell cols="3">1.7623 00.40 03.59 1.8181 00.21 01.99</cell><cell>00.07 00.05</cell><cell>04.05 02.24</cell></row><row><cell></cell><cell></cell><cell>SC-LSTM</cell><cell>23.88</cell><cell>3.9310</cell><cell>32.11</cell><cell>39.90</cell><cell cols="3">0.5036 07.73 17.76</cell><cell>09.52</cell><cell>35.03</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell>40.19</cell><cell>6.0543</cell><cell>37.38</cell><cell>55.88</cell><cell cols="3">1.8104 00.17 01.31</cell><cell>00.25</cell><cell>01.72</cell></row><row><cell>Cleaned Cleaned missing</cell><cell>Cleaned</cell><cell>TGen TGen+ SC-LSTM TGen− TGen TGen+</cell><cell>40.73 40.51 23.66 40.48 41.57 41.56</cell><cell>6.1711 6.1226 3.9511 6.0269 6.2830 6.2700</cell><cell>37.76 37.61 32.93 37.26 37.99 37.94</cell><cell>56.09 55.98 39.29 56.19 56.36 56.38</cell><cell cols="3">1.8518 00.07 00.72 1.8286 00.02 00.63 0.3855 07.89 15.60 1.7999 00.43 02.84 1.8849 00.37 01.40 1.8827 00.21 01.04</cell><cell>00.08 00.06 08.44 00.26 00.09 00.07</cell><cell>00.87 00.70 31.94 03.52 01.86 01.31</cell></row><row><cell>Cleaned added</cell><cell></cell><cell>TGen− TGen TGen+</cell><cell>35.99 40.07 40.80</cell><cell>5.0734 6.1243 6.2197</cell><cell>34.74 37.45 37.86</cell><cell>54.79 55.81 56.13</cell><cell cols="3">1.5259 00.02 11.58 1.8026 00.05 03.23 1.8422 00.01 01.87</cell><cell>00.02 00.01 00.01</cell><cell>11.62 03.29 01.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results evaluated on the cleaned test set (cf.Table 2for column details; note that the numbers are not comparable toTable 2as the test set is different).</figDesc><table><row><cell>Training data</cell><cell cols="4">Add Miss Wrong Disfl</cell></row><row><cell>Original</cell><cell>0</cell><cell>22</cell><cell>0</cell><cell>14</cell></row><row><cell>Cleaned added</cell><cell>0</cell><cell>23</cell><cell>0</cell><cell>14</cell></row><row><cell>Cleaned missing</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>2</cell></row><row><cell>Cleaned</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Semantic Noise Matters for Neural Natural Language Generation: Supplementary</figDesc><table><row><cell>TRAIN</cell><cell cols="2">TEST System</cell><cell>Add</cell><cell cols="3">Miss Wrong InstOK</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell>2.8</cell><cell>686.2</cell><cell>4.8</cell><cell>192.2</cell></row><row><cell>Original</cell><cell></cell><cell>TGen TGen+</cell><cell>6.0 1.6</cell><cell>178.8 76.2</cell><cell>1.2 0.4</cell><cell>496.4 558.2</cell></row><row><cell></cell><cell></cell><cell cols="2">SC-LSTM 121.6</cell><cell>823.6</cell><cell>426.2</cell><cell>7.8</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell>8.8</cell><cell>24.2</cell><cell>9.0</cell><cell>591.6</cell></row><row><cell>Cleaned Cleaned missing</cell><cell>Original</cell><cell cols="2">TGen TGen+ SC-LSTM 167.6 4.2 1.0 TGen− 6.0 TGen 2.6 TGen+ 0.0</cell><cell>0.8 0.2 757.2 98.2 19.0 9.0</cell><cell>0.2 0.2 353.4 9.4 1.4 1.4</cell><cell>624.8 628.6 14.0 525.2 608.0 620.6</cell></row><row><cell>Cleaned added</cell><cell></cell><cell>TGen− TGen TGen+</cell><cell>0.4 2.0 0.2</cell><cell>569.2 132.2 62.8</cell><cell>0.2 0.2 0.2</cell><cell>234.0 501.6 567.0</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell cols="2">39.4 1135.6</cell><cell>17.8</cell><cell>1089.4</cell></row><row><cell>Original</cell><cell></cell><cell>TGen TGen+</cell><cell>45.6 23.6</cell><cell>415.4 230.2</cell><cell>7.8 5.2</cell><cell>1469.8 1608.8</cell></row><row><cell></cell><cell></cell><cell cols="3">SC-LSTM 858.6 1972.2</cell><cell>1057.6</cell><cell>39.0</cell></row><row><cell></cell><cell></cell><cell>TGen−</cell><cell>19.0</cell><cell>151.2</cell><cell>28.6</cell><cell>1667.8</cell></row><row><cell>Cleaned Cleaned missing</cell><cell>Cleaned</cell><cell cols="3">TGen TGen+ SC-LSTM 876.2 1732.4 7.8 83.0 1.8 72.6 TGen− 49.4 328.4 TGen 42.8 162.0 TGen+ 24.0 120.0</cell><cell>9.6 7.0 937.4 30.0 10.8 8.0</cell><cell>1751.4 1768.8 78.0 1482.6 1643.2 1702.8</cell></row><row><cell>Cleaned added</cell><cell></cell><cell>TGen− TGen TGen+</cell><cell cols="2">2.2 1340.2 6.0 373.6 0.8 216.6</cell><cell>2.8 1.8 0.8</cell><cell>959.8 1518.6 1646.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Data cleaning scripts, the resulting cleaned data and links to code are available at https://github.com/ tuetschek/e2e-cleaning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Evaluating the Impact on Neural NLGWe chose two recent neural end-to-end NLG systems, which represent two different approaches to semantic control and have been widely used and extended by the research community.4  Note that this can be done automatically, unlike fixing the references to match the original MRs.5  <ref type="bibr" target="#b9">Juraska et al. (2018)</ref>'s script reaches 6.2% SER and 60 instances with errors, most of which is just omitting the eat-Type[restaurant] value. If we ignore this value, it gets 1.9% SER and 20 incorrect instances. We did not use this script as it was not available to us until very shortly before the camera-ready deadline. The script is now accessible under https://github.com/jjuraska/slug2slug. We plan to further improve our slot matching script based on errors found during the manual evaluation and comparison to<ref type="bibr" target="#b9">Juraska et al. (2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/UFAL-DSG/tgen 7 The dialogue act is constant/ignored for the E2E dataset since it's not part of the MRs there. 8 https://github.com/shawnwun/RNNLG</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research received funding from the EP-SRC projects DILiGENt (EP/M005429/1) and MaDrIgAL (EP/N017536/1) and Charles University project PRIMUS/19/SCI/10. The authors would also like to thank Prof. Ehud Reiter, whose blog 13 inspired some of this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR2015)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Language Technology Research</title>
		<meeting>the Second International Conference on Human Language Technology Research<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequenceto-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the E2E NLG Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Evaluating the State-of-the-Art of End-to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<ptr target="https://ehudreiter.com/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural Language Generation: The E2E NLG Challenge</title>
		<idno type="arXiv">arXiv:1901.07931</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="123" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MultiWOZ 2.1: Multi-Domain Dialogue State Corrections and State Tracking Baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyag</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno type="DOI">10.17863/CAM.41572</idno>
		<idno type="arXiv">arXiv:1907.01669</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pathologies of Neural Models Make Interpretations Difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Grissom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3719" to="3728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Extended SPaRKy Restaurant Corpus: Designing a Corpus with Variable Information Density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Howcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2017</title>
		<meeting>Interspeech 2017<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3757" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juraj</forename><surname>Juraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Six Challenges for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hallucinations in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Fannjiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phrase-based statistical language generation using graphical models and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1552" to="1561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00838</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple recipe towards reducing hallucination in neural surface realisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2673" to="2679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crowd-sourcing NLG Data: Pictures Elicit Better Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00339</idno>
	</analytic>
	<monogr>
		<title level="m">The 9th International Natural Language Generation conference INLG</title>
		<meeting><address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shereen</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="284" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object hallucination in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4035" to="4045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-domain Neural Network Language Generation for Spoken Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01232</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Challenges in Data-to-Document Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08052</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2243" to="2253" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
