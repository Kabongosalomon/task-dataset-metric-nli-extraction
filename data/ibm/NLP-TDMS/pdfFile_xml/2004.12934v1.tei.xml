<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
							<email>xiangk@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gangal</surname></persName>
							<email>vgangal@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a humancreated sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers. Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood. The blanks require joint solving and significantly impair each other's context. Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging. Our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap. 1 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cloze questions were first proposed by <ref type="bibr" target="#b7">Taylor (1953)</ref> as a readability test, motivated by Gestalt psychology. They become an efficient way of testing reading for public exams, overtaking the dominant paradigm of subjective questions <ref type="bibr">(Fotos, 1991;</ref><ref type="bibr">Jonz, 1991)</ref>. Cloze datasets <ref type="bibr" target="#b11">(Zweig and Burges, 2011;</ref><ref type="bibr">Hermann et al., 2015;</ref><ref type="bibr">Hill et al., 2015;</ref><ref type="bibr">Paperno et al., 2016;</ref><ref type="bibr">Onishi et al., 2016;</ref><ref type="bibr" target="#b10">Xie et al., 2018)</ref> became prevalent as questionanswering (QA) benchmarks since they are convenient either to be generated automatically or by annotators. These datasets could be split into two clear types:</p><p>1. Where the context is a complete text, and there is an explicit question posed which is a statement with a cloze gap. The answer is either generated freely or is a span * Equal Contribution 1 Data: vgtomahawk.github.io/sced.html 2 Code: https://github.com/shawnkx/SCDE from the context, e.g. Childrens Books Test (CBT) <ref type="bibr">(Hill et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Where the context itself comes with cloze</head><p>gaps. There is no explicit question. The answer is generated freely or chosen from a set of candidates, e.g. CLOTH <ref type="bibr" target="#b10">(Xie et al., 2018)</ref>.</p><p>Herein, we focus on the 2nd category. A common property of these datasets is that they have gaps at the level of words, entities or short syntactic spans. The entity and span-based clozes may sometimes be multi-token, but they do not extend beyond a few tokens. Nevertheless, none of these datasets have cloze gaps at the level of full sentences. Since many syntactic and semantic cues are present in the same sentence, this makes the gap easier to fill compared to the sentence level cloze case where models would have to rely on "discourse" cues beyond the same sentence.</p><p>Besides lack of intra-sentence cues, sentencelevel cloze may require comparing candidates of very different lengths. For instance, the example in <ref type="table" target="#tab_0">Table 1</ref> has a standard deviation of 7.6 with candidate lengths between 3 to 25. A model that only represents words well may not get comparable probabilities at sentence level for very different sentence lengths. Therefore, robust sentence representation models are also required to solve this question. In this paper, we present SCDE, a dataset of sentence-level cloze questions sourced from public school examinations. Each dataset example consists of a passage with multiple sentence-level blanks and a shared set of candidates. Besides the right answer to each cloze in the passage, the candidate set also contains ones which don't answer any cloze, a.k.a., distractors. Both cloze positions and distractors are authored by teachers who design the public school examinations carefully. §3.2 explains our data collection. A representative example from SCDE is shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2004.12934v1 [cs.CL] 27 Apr 2020</head><p>Passage: A student's life is never easy. And it is even more difficult if you will have to complete your study in a foreign land. <ref type="bibr">1</ref> The following are some basic things you need to do before even seizing that passport and boarding on the plane. Knowing the country. You shouldn't bother researching the country's hottest tourist spots or historical places. You won't go there as a tourist, but as a student.</p><p>2 In addition, read about their laws. You surely don't want to face legal problems, especially if you're away from home.</p><p>3 Don't expect that you can graduate abroad without knowing even the basics of the language. Before leaving your home country, take online lessons to at least master some of their words and sentences. This will be useful in living and studying there. Doing this will also prepare you in communicating with those who can't speak English. Preparing for other needs. Check the conversion of your money to their local currency.</p><p>4. The Internet of your intended school will be very helpful in findings an apartment and helping you understand local currency. Remember, you're not only carrying your own reputation but your country's reputation as well. If you act foolishly, people there might think that all of your countrymen are foolish as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates:</head><p>A. Studying their language. B. That would surely be a very bad start for your study abroad program. C. Going with their trends will keep it from being too obvious that you're a foreigner. D. Set up your bank account so you can use it there, get an insurance, and find an apartment. E. It'll be helpful to read the most important points in their history and to read up on their culture. F. A lot of preparations are needed so you can be sure to go back home with a diploma and a bright future waiting for you. G. Packing your clothes. Answers with Reasoning Type: 1→F (Summary) , 2→E (Inference) , 3→A (Paraphrase) , 4→D (WordMatch), 5→B (Inference) (C and G are distractors) Discussion: Blank 3 is the easiest to solve, since "Studying their language" is a near-paraphrase of "Knowing even the basics of the language". Blank 2 needs to be reasoned out by Inference -specifically E can be inferred from the previous sentence. Note however that C is also a possible inference from the previous sentence -it is only after reading the entire context, which seems to be about learning various aspects of a country, that E seems to fit better. Blank 1 needs Summary → it requires understanding several later sentences and abstracting out that they all refer to lots of preparations. Finally, Blank 5 can be mapped to B by inferring that people thinking all your countrymen are foolish is bad, while Blank 4 is a easy WordMatch on apartment to D. The other distractor G, although topically related to preparation for going abroad, does not directly fit into any of the blank contexts Another salient aspect of our dataset is that more than 40% of blanks belong to the reasoning category "Inference" (more on this in §3.3 and <ref type="table" target="#tab_7">Table 4</ref>) which require models to compare plausibility of competing hypotheses given a premise (whether the previous or last sentence(s), or even a combination of information from the two). Filling these blanks requires the model to reason by using commonsense knowledge, factual knowledge, time gaps, etc. Some of these can be thought of as simple entailment, but more generally, many of these can be seen as requiring abductive reasoning, which is of recent interest <ref type="bibr" target="#b1">(Bhagavatula et al., 2019;</ref><ref type="bibr">Sap et al., 2019a,b)</ref> to the NLP community. In summary, our contributions are as follows 1. We introduce the task of sentence level cloze completion with multiple sentence blanks and a shared candidate set with distractors. 2. We release SCDE, a sentence level cloze dataset of ≈ 6k passages and ≈ 30k blanks. 3. We estimate human performance on SCDE, and benchmark several models, including state-of-the-art contextual embeddings (Table 5). We find a significant gap of &gt; 15% for future models to close in order to match human performance.</p><p>4. Through several ablations described in §5.6, we show that distractors designed by English teachers are of high quality and make the task more challenging. 5. We show that extra sentence level cloze questions generated automatically from an external corpus can be used to further improve model performance through data augmentation (See §5.7).   <ref type="bibr">et al., 2016)</ref> is the closest we could find to a sentence level cloze dataset. In this task, the first 4 sentences of a 5-sentence story are provided, and the task is to choose the correct ending from a pair of candidate ending sentences. However, there are several key differences between SCDE and ROCStories. Firstly, there are multiblanks in SCDE which are not in a fixed position and require learning cues from bidirectional contexts of varying lengths. Secondly, the endings in ROCStories have been found to contain "annotation artifacts" (Gururangan et al., 2018) which makes a large fraction of them predictable independent of context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In contrast, SCDE is by design independent of artifacts, since a) given a blank, only some of our candidates are distractors, the rest being answers for other blanks. Even if one were to learn a classifier to distinguish distractors without context, the non-distractor candidates would be unresolvable without context. b) we further check how distinguishable our distractors are from non-distractors without context by training a strong classifier in this setting, as described in §5.6. The classifier obtains a reasonably low F1 score of 0.38.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we summarize the comparison of SCDE with cloze datasets from prior art to show its attractive aspects.</p><p>Public school examinations have been used as a data source by many earlier QA works, two prominent examples being the CLEF QA tracks <ref type="bibr" target="#b2">(Penas et al., 2014;</ref><ref type="bibr" target="#b3">Rodrigo et al., 2015)</ref> and <ref type="bibr">RACE (Lai et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCDE Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Cloze Test with distractors</head><p>In this task, each question consists of a passage, S, multiple sentence level blanks B, and a shared set of candidates C with distractors D, where D ⊂ C.</p><p>Problem Complexity 3 For our case, given the typical value of |C| and |B| being 7 and 5 respectively, the size of the answer space, |A| is 2520. Thus, the chance of guessing all blanks correctly at random is only 0.04%. Moreover, there is a 48.2% probability of being entirely wrong with randomly guessing. Finally, given an answer list chosen uniformly at random, the expectation of number of distractors in the answer list is 1.4, i.e. on average, roughly one and half answers are distractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Collection and Statistics</head><p>Raw sentence cloze problems are crawled from public websites 4 which curate middle and high school English exams designed by teachers. In total, 14,062 raw passages and 68,515 blank questions are crawled from these websites and the following steps are used to clean them. Firstly, duplicate passages are removed. Secondly, when the official answer to the problems are images, two OCR toolkits 5 are employed to convert these images to text and the questions with different results from these two programs will be discarded. Finally, we remove examples which have 1) answers pointing to non-existent candidates, 2) missing or null candidates, 3) number of blanks &gt; number of candidates, 4) missing answers.</p><p>After cleaning, we obtain our SCDE dataset with 5,959 passages and 29,731 blanks. They are  randomly split into training, validation and test sets with 4790, 511 and 658 passages respectively. The detailed statistics are presented in <ref type="table" target="#tab_4">Table 3</ref>. We find that candidates have very different lengths and passages have long context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">In-Depth Analysis &amp; Categorization</head><p>In order to evaluate students mastery of a language, teachers usually design tests in a way that questions cover different aspects of a language.</p><p>Reasoning Types As illustrated with examples in <ref type="table" target="#tab_7">Table 4</ref>, we set a four-fold categorization for the reasoning which leads to a ground truth candidate being assigned to a blank. Our reasoning type taxonomy is motivated by categorization of question types in earlier works in QA such as <ref type="bibr">(Chen et al., 2016;</ref><ref type="bibr">Trischler et al., 2017) 6</ref> . Strictly speaking, these reasoning types could co-exist. But for simplicity, we classify each blank into only one of the four.</p><p>• WORDMATCH: If the candidate has word overlap, especially of non-stopwords or infrequent phrases, with context around the blank. be just strict entailment <ref type="bibr">(Bowman et al., 2015;</ref><ref type="bibr">Marelli et al., 2014)</ref> but could also involve abductive reasoning <ref type="bibr" target="#b1">(Bhagavatula et al., 2019)</ref>, where the candidate is just one of many likely hypothesis (premise) given the left (right) context as premise (hypothesis). • SUMMARY: If the candidate is a summary, introduction, or conclusion of multiple sentences before or after it. In this type, unlike INFERENCE, there is no requirement to deduce and reason about new hypotheses/possibilities not present in the premiseonly consolidation and rearranging of information is required.</p><p>A sample of 100 passages containing 500 blanks are manually categorized into these four categories. Examples and statistics of these four types are listed in <ref type="table" target="#tab_7">Table 4</ref>. More than 40% blanks need inference to be solved, denoting the high difficulty of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Context Length</head><p>We experiment with giving our models different amounts of context. Through this, we can explore how context length affects model performance. 3: May is a great month.</p><p>You can have a good time with your family. × Candidate: E. All the students can come to their schools.</p><p>Candidate: F. From May 1st to 7th, we don't need to come to school. × Candidate: G. On May 20th, a famous sports star YaoMing comes to our school. Explanation: Need to infer that not coming to school → one is at home with family. Simply matching for words May or school will also match wrong candidates.</p><p>Sum. (20.08%) 4: How to Enjoy Life As a Teen? Are high school days equal to the "best years of your life"? Maybe not, but you can learn to make the most of your high school days</p><p>Whether it 's having a computer, having friends, having a good supply of food, a bed to sleep on, family that loves you, having a decent education or simply being born in this world. Be happy, and life will reward you. × Candidate: A. Remember that the point of life is for you to enjoy it.</p><p>Candidate: C. Learn to appreciate small things. Explanation: After summarizing sentences after the blank [which describe a list of "small things"], the answer should be C. A is a strong distractor since both "enjoy" and "life" appear in the context, besides being pertinent to the topic. Indeed, our best-performing BERT-ft model chooses A as the answer. We estimate PMI counts <ref type="bibr">(Church and Hanks, 1990</ref>) from all consecutive sentence pairs in our training split. Let f denote frequency</p><formula xml:id="formula_0">PMI(w s , w c ) = f(w s ∈ S, w c ∈ C) f(w s ∈ S)f(w c ∈ C)</formula><p>Note that our PMI definition diverges from typical PMI since its asymmetric between w s and w c .</p><p>Since S and C are the sets of non-terminating and non-starting sentences respectively, they overlap but aren't identical. For a pair of sentences, we find aggregate PMI(S, C) as:</p><formula xml:id="formula_1">PMI(S, C) = 1 |C||S| wc∈C ws∈S PMI(w s , w c )</formula><p>This definition can be extended to all n-grams upto a certain n. We denote this by PMI n . We notice that PMI n performance saturates after n = 2. Hence, in our experiments, we use PMI 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language Modelling</head><p>One intuitive way to solve this task is to generate the blank sentence given the context by advanced pre-trained language models (LM). Formally, suppose the blank is the ith sentence, s i , and s 1 , . . . , s i−1 , s i+1 , . . . , s n are the context. Our goal is to choose c k from the candidate set which could maximize the joint probability p(s 1 , . . . , s i−1 , c k , s i+1 , . . . , s n ).</p><p>Due to limited number of passages available to train a robust LM, Transformer-XL (TR.XL) Base <ref type="bibr">(Dai et al., 2019)</ref>, trained on WikiText-103, is employed to address this task. In order to make decoding time tractable, context length is limited to three sentences before and after the blank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Coherence</head><p>Coherence models assign a continuous score to a sentence sequence indicative of its coherence. This score is usually unnormalized and not needed to be a probability [unlike language models].</p><p>We use the local coherence approaches implemented by the COHERE 7 framework <ref type="bibr" target="#b6">(Smith et al., 2016)</ref>. Roughly, this model works on the intuition that successive sentences exhibit regularities in syntactic patterns. Specifically, it uses ngram patterns on linearized syntactic parses (e.g. S NP VP . . . ) of consecutive sentences. Once trained, this model can return a "coherence score" for any sentence sequence.</p><p>The COHERE model is first trained on all ground-truth passages from our training set, with the ground truth answers filled into the blanks. At test-time, we score each possible answer permutation using the trained COHERE model and pick the highest scoring one. Note that decoding for COHERE is by definition exhaustive, and doesn't make any assumptions by answering the blanks in a particular order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">InferSent</head><p>Conneau et al. <ref type="formula" target="#formula_3">(2017)</ref> use textual inference supervision as a signal to train a shared sentence encoder for premises and hypotheses, which can later be used as a sentence representor. We refer to this approach as INFST. Context features of a given blank and one candidate feed to two encoders in INFST respectively and classify whether this candidate is suitable to this blank. The maximum tokens of context features is set as 256. Bi-directional LSTMs with the max pooling operation are employed as our encoders. We follow the training procedure described in <ref type="bibr">Conneau et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">BERT Models</head><p>Input Representations Let c k denotes the kth candidate. s −i and s +i denote the ith sentence before and after the blank respectively and |P | and |N | represent total number of sentences before and after the current blank respectively. Following the input convention in <ref type="bibr">Devlin et al. (2018)</ref>, the input sequence given various context lengths and c k is:</p><formula xml:id="formula_2">1. P : [CLS]s −1 [SEP]c k 2. N : [CLS]c k [SEP]s +1 3. AP : [CLS]s −|P | . . . s −1 [SEP]c k 4. AN : [CLS]c k [SEP]s +1 . . . s +|N |</formula><p>To retain sentence sequentiality, the order between the context and the candidate follows that in the original passage. Furthermore, for (A)P+(A)N, we create and score one input sample for each of the context directions during prediction. The average of these two scores is taken as the final score. The maximum tokens of input is set as 256 in our experiments and only the context is truncated to meet this requirement.</p><p>BERT Next Sentence Prediction (NSP) One of the objectives in BERT pre-training stage is  understanding the relationship between two sentences, which is highly correlated with our task. Therefore, we use the pre-trained BERT-Largeuncasedd with its NSP layer to predict the most appropriate candidate for each blank given its context. Specifically, BERT is employed to predict the probability of the context and the candidate being consecutive.</p><p>Finetuning BERT A wide range of NLP tasks have greatly benefited from the pre-trained BERT model. Therefore, we also finetune the pre-trained BERT-Large model on our task through sequence pair classification schema. Specifically, for each blank, its correct candidate will be labelled as 0 and the label of all other wrong candidates is 1. Batch size and number of epochs for all models are 32 and 3. We employ Adam (Kingma and Ba, 2014) as the optimizer with three different learning rates {1e −5 , 2e −5 , 3e −5 }. Best model selection is based on validation performance. All BERT finetuning experiments including ablation study follow this training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Decoding Strategy</head><p>The decoding strategy decides how exactly we assign a candidate to each blank in the passage. Due to shared candidates, we have two strategies:   constituent blank-candidate pairs. The highest scoring permutation is the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We design two metrics to evaluate models. Both of these metrics are reported as percentage.</p><p>Blank accuracy (BA): The fraction of blanks answered correctly, averaged over all passages.</p><p>Passage Accuracy (PA): PA is 1 iff the model gets all blanks in a passage correct, and 0 otherwise. The average of PA over all passages is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human Performance</head><p>We questions (61.0%). Human performance is reported in <ref type="table" target="#tab_9">Table 5</ref>. Annotators achieve BA of 87% which we take as the ceiling performance for models to match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Model Performance</head><p>All models are trained with AP+AN context and decoded by EXH 9 . Results are shown in Table 5. Finetuning BERT achieves the best performance among other models, though it still lags behind human performance significantly. Unsupervised models could only solve one third of all blanks. Surprisingly, PMI 2 and COHERE performs worse than the unsupervised models. We conjecture that it is difficult for COHERE, using syntactic regularities alone, to distinguish between the ground truth answer for a particular blank and another candidate which is a ground truth answer for another nearby blank. As noted, PMI 2 suffers due to inability of incorporating larger context. To explore effects of various context length and decoding strategies, models are trained with different context lengths and inferred by both decoding methods. Results are shown in <ref type="table" target="#tab_11">Table 6</ref>.</p><p>INC vs EXH EXH is better than INC for most approaches, indicating that human created blanks are interdependent and need joint answering.</p><p>Context Length Increasing the context length, such as (P vs. AP), could significantly improve model performance, showing that this task needs discourse-level context to be answered. Furthermore, models with bidirectional context, such as (P+N), perform better than single-direction context, e.g., P, indicating that this task needs global context. Lastly, we observe that PMI-based approaches which do not explicitly encode sentences are unable to incorporate larger context levels, showing best performance with P+N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">BERT-ft vs. Human</head><p>BERT after finetuning (BERT-ft) can perform reasonably well (72%) but there is still a gap comparing with human performance (87%). In this section, we would like to analyze the strength and weakness of BERT-ft compared with HUMAN. Therefore, we analyze their performance across different reasoning categories on test set. From <ref type="figure" target="#fig_0">Figure 1</ref>, inference questions are the most difficult for both HUMAN and BERT-ft and questions needing WordMatch are relatively easy. Compared with human performance, BERT-ft could achieve comparable BA on WordMatch and paraphrasing problems. However, BERT-ft performs much worse on questions needing inference and summary. We also refer to some examples from <ref type="table" target="#tab_7">Table 4</ref>.</p><p>In Example 4, BERT-ft prefers A but the answer is C. The reason why BERT-ft chooses A may be that "enjoy life" happens in the context, but summarizing the next sentence is necessary to achieve the correct answer. Therefore, it is necessary to improve the ability of BERT to represent meaning at the sentence level beyond representing individual words in context.</p><p>We also explore how the system performance corresponds to the human judgement of difficulty. Since evaluates rate the problems into 5 difficulty levels, we report the system BA/PA for each level in <ref type="table" target="#tab_15">Table 8</ref>. For BA (blank-level accuracy), we see that, overall, the system accuracy decreases as difficulty increases from VeryEasy (0.75) to Very-Hard (0.68). However, the decrease is not exactly monotonic (there is a small increase from VeryEasy to Easy, as also from Moderate to Hard).</p><p>We conjecture that non-monotonicity could be due to two reasons:</p><p>• Our difficulty annotations are at passage level rather than blank level. There might be some hard blanks in a passage marked overall Easy. Conversely, there might be easy blanks in a passage marked overall Hard.</p><p>•  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Distractor Quality</head><p>An attractive aspect of this task is distractors designed by English teachers. We verify distractor quality through the following experiments.   averaged score in <ref type="table" target="#tab_12">Table 7</ref>. Comparing with distractors designed by teachers, models could discern these distractors more easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Performance w/o Distractors</head><p>Annotation artifacts of distractors Annotation artifacts (Gururangan et al., 2018) occurs in many datasets created by human annotators. A potential artifact type for our task is whether we could detect distractors without passages. Therefore, we finetune BERT-Large as a binary classifier, the input of which is just distractors and other correct candidates. With this model, we could only obtain 38% F1 score on the test set, showing that it is difficult to filter distractors out without any context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distractor Error (DE)</head><p>We define DE as the number of predicted answers per passage which are actually distractors. Through DE, we measure a model's ability to exclude distractors during prediction. Results are shown in <ref type="table" target="#tab_17">Table 9</ref>. HUMAN has the lowest DE and BERT-ft could discern distractors to some extent. However, DE of PMI 2 is more than 1, meaning that on average, there is atleast one distractor in the predicted answer list. In summary, distractors created by teachers are high quality and increase task difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Automatically Generated Sentence Cloze Questions</head><p>To explore automatic generation of examples for the task, we construct sentence cloze questions by randomly choosing five sentences in a passage as blanks. We defer automatically generating distractors to future work since non-trivial distractor generation is a hard problem in itself. Specifically, we extract all passages from <ref type="bibr">RACE (Lai et al., 2017)</ref> (which is also from exams) and filter out passages which have less than 10 sentences or more than 30 sentences. While choosing blank positions, we prevent three or more blanks consecutive to each other in generated questions. Finally, 16,706 examples are obtained automatically. Here, questions generated automatically and collected from examinations are called Q A and Q H respectively. We leverage Q A in three ways: 1). train models only on Q A , 2) first train models on Q A and finetune models on Q H , i.e., Q A ; Q H , 3) train models on the concatenation of Q A and Q H , i.e., Q A + Q H . BERT-Large is finetuned through these ways and results are shown in <ref type="table" target="#tab_0">Table 10</ref>. The model trained only on Q A has worst performance and we attribute this to the difficulty of distinguishing distractors without seeing them during training. Therefore, this model has the highest DE. However, models trained on Q H and Q A could achieve better performance. We conjecture this is because Q A assists the model to have better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce SCDE, a sentence cloze dataset with high quality distractors carefully designed by English teachers. SCDE requires use of discourselevel context and different reasoning types. More importantly, the high quality distractors make this task more challenging. Human performance is found to exceed advanced contextual embedding and language models by a significant margin. Through SCDE, we aim to encourage the development of more advanced language understanding models. With |B| = 5 blanks and |C| = 7 candidates, the size of answer space, |A|, is number of permutations |B| objects taken |C| at a time, i.e., P(7, 5) = 2520. Therefore, the probability of answering all blanks correctly is 1 2520 = 0.03% What are the chances of getting answers partially correct? What are the chances of getting answers partially correct? If we have the same number of candidates as blanks, this is equivalent to |B|! − D |B| , where D |B| is the number of derangements 10 of |B| elements. In the presence of more candidates than blanks i.e distractors, this expression becomes more involved to derive. Therefore, here, we enumerate all the permutation of answer lists given a correct answer. With |C| = 7 and |B| = 5, ζ(|C|, |B|) = 51.8%. In other words, there is a 48.2% probability of being entirely wrong with a randomly chosen set of answers to each blank in the passage.</p><p>What are the chances of getting distractors as predicted answers? For the expectation of number of distractors choosing by uniform model, it</p><formula xml:id="formula_3">should be E[DE], where DE denotes distractors errors. 2 d=0 p(DE = d) × d<label>(1)</label></formula><p>where p(DE = d) denotes the probability of d predicated answers are distractors. Since there are two distractors in candidates, the maximum of d is 2. Furthermore, p(DE = 1) is P(5, 4)C(5, 4)C(2, 1)/|A| = 0.476 <ref type="formula">(2)</ref> and p(DE = 2) is P(5, 3)C(5, 3)A(2, 2)/|A| = 0.476</p><p>where C(·, ·) and P(·, ·) is combination and permutation respectively. Therefore, the expectation of number of distractors is 1.429.</p><p>9 Additional Experiment Specifications 9.1 Specific BERT Model Used</p><p>We use uncased BERT models for all our experiments. We use the BERT models trained by the canonical pytorch implementation of Wolf et al. You can have a good time with your family. Candidate: F. From May 1st to 7th, we don't need to come to school. × Candidate: G. On May 20th, a famous sports star YaoMing comes to our school. × Candidate: E. All the students can come to their schools. Explanation: Need to infer that not coming to school → one is at home with family. Simply matching for words May or school will also match wrong candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>The Colosseum in Rome was built during the time of the Roman Empire, in the first century AD.</p><p>. It is a popular tourist attraction today.</p><p>Candidate: D. It could seat 50K people, who went to see fights between animals and people. × Candidate: B. The country used to depend on agriculture. × Candidate: C. Mountains cover about three-fourths of the country. Explanation: World knowledge that Colosseum or -eum suffix relates to building with seating facility. Also coreference with the It in It is a popular . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>American students usually get to school at about 8 : 30 in the morning.</p><p>In class, American students can sit in their seats when they answer teachers' questions.</p><p>Candidate: B. School starts at 9:00 a.m. × Candidate: D. Then they take part in different kinds of after-school activities. Explanation: Requires inference about time. Activity starts at 9 after participants get there before.</p><p>Sum. (20.08%) 8: Around water, adults should watch children at all times to make sure they are safe. Those who don't know how to swim should wear life jackets. But by themselves they are not enough, so an adult should always be present. If you have to rescue a child from drowning, a few seconds can make a big difference. Make sure you have a friend with you whenever you swim.</p><p>. That person can make sure you get help. Drink a lot water. The sun's heat and the physical activity may make you sweat more than you realize. By following these simple rules, you can make sure your swim time is safe as well as fun.</p><p>Candidate: B. Now get out there, and enjoy the water. × Candidate: D. Make sure everyone in your family swim well. Explanation: B is a good conclusion pertinent to the content of the passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>. Whenever you are worried, write down the questions that make you worry. And write out all the various steps you could take and then the probable consequences of each step. For example, "What am l worrying about?", What can I do about it? Here is what I'm going to do about it. After carefully weighing all the facts, you can calmly come to a decision.</p><p>Candidate: A. Analyze the facts. × Candidate: C. Decide how much anxiety a thing may be worth. Explanation: A is a more appropriate option to summarize its succeeding context. 10: Expect to work, . If you are not working you are not learning. You are wasting your time at school. Teachers can not make everything enjoyable.</p><p>Candidate: F. School is not a holiday camp. × Candidate: E. Because it means that you are enjoying school and learning more. Explanation: After summarizing the sentences after the blank, the blank should be filled by F. 2 Students in my group are from different cities of Britain and their dialects are different too! Some of their accents are quite strong and they also have their own words and expressions.</p><p>3 Before I came to England I had thought that fish and chips were eaten every day. That's quite wrong! I get rather annoyed now when I hear all the foolish words about typical English food. I had expected to see "London fog". Do you remember our texts about it ? We had no idea that most of this "thick fog" disappeared many years ago when people stopped using coal in their homes. But the idea to speak about weather was very helpful.</p><p>4 On the other hand , habits are different . People tell me what is typical British here in London is not always typical in Wales or <ref type="bibr">Scotland. 5</ref> But what is ordinary for all British is that they follow traditions. Probably Britain has more living signs of its past than many other countries. And people have always been proud of having ancient buildings in capitals, big cities and the countryside. I will tell you more about Britain in my other letters. Love from Britain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Test blank accuracy of BERT-ft and Human on each reasoning type category introduced in §3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>A Representative Example from SCDE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparing SCDE with previous cloze datasets. Exhaustive denotes the case where the entire vocabulary is a candidate for a word level cloze. For the single-blank case, candidate sharing is irrelevant. SL and MB mean sentence level and multi-blanks respectively. Context w is the average token length of the context.</figDesc><table><row><cell>word level blanks. The CLOTH (Xie et al., 2018)</cell></row><row><cell>dataset collects word level cloze questions from</cell></row><row><cell>English exams designed by teachers. MRSCC</cell></row><row><cell>(Zweig and Burges, 2011) consists of 1,040 word</cell></row><row><cell>level cloze questions created by human annotators.</cell></row><row><cell>Among recent cloze datasets, ROCStories</cell></row><row><cell>(Mostafazadeh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>SCDE Statistics. For Consecutive Blanks, either of previous or next sentences is also a blank.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>•</head><label></label><figDesc>PARAPHRASE: If the candidate doesn't have an explicit word overlap with the context, but nevertheless contains words or phrases which are paraphrases of those in the context. • INFERENCE: If the candidate is a valid hypothesis conditioned on the left context [as premise], or a necessary precondition/premise based on the right con-</figDesc><table /><note>text. Note that the candidate in this case doesn't contain word overlap/paraphrases which would obviate need for inferential rea- soning. The reasoning required needs not6 See Section 4.2 from both respective papers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>One day, a teacher was giving a speech to his student. He held up a glass of water and asked the class The students answers ranged from 20g to 500g. Candidate: B. How heavy do you think this glass of water is? × Candidate: D. It does not matter on the weight itself. Explanation: WordMatch based on glass of water. If you want time to have breakfast with your family, save some time the night before by setting out clothes, shoes and bags.That's a quarter-hour more you could be sleeping if you bought a coffee maker with a timer. × Candidate: D. And consider setting a second alarm. × Candidate: F. Stick to your set bedtime and wake-up time, no matter the day.Candidate: G. Reconsider the 15 minutes you spend in line at the cafe. Explanation: Need to match 15 minutes, quarter-hour and coffee, cafe.</figDesc><table><row><cell>Type</cell><cell>Examples with Excerpts From Blank Context</cell></row><row><cell cols="2">WM (18.47%) 1: Para. (19.48%) 2: Infer.</cell></row><row><cell>(41.97%)</cell><cell></cell></row></table><note>1. P(N): Immediate previous (next) sentence 2. P+N: Immediate previous and next sentence 3. AP(AN): All previous (next) sentences 4. AP+AN: All previous and next sentences AP+AN is the unablated setting, where all pas- sage sentences are available to the model.4.2 PMI Before exploring deep representational ap- proaches, we would like to find how well symbolic ones perform at this task. Starting with works such as Iyyer et al. (2015) and Arora et al. (2017), it has become convention to first benchmark simple baselines of this kind. PMI merely encodes how likely it is for a word pair to occur in consecutive sentences. It does not consider the internal sentence structures, or the relative position of the words in their respective sentence. Intuitively, it can be called a "surface- level" approach. A high performance by PMI would indicate that candidates can be matched to blanks by simple ngram statistics, without requiring sentence representation, which would make SCDE uninteresting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Blanks in a sample of 100 passages are manually categorized into four categories. For the ease of illustration, we've shown only limited context around the blanks , and 1-2 wrong candidates. WM, Para., Infer. and Sum denote WordMatch, Paraphrase, Inference and Summary respectively. More examples are in Appendix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Test BA/PA of various model types with EXH decoding and AP+AN context.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>+EXH 47.2/8.5 54.2/11.2 60.0/17.5 60.0/17.5 66.5/25.2 71.7/29.9    </figDesc><table><row><cell>Type</cell><cell>Model P</cell><cell>N</cell><cell>AP</cell><cell>AN</cell><cell>P+N</cell><cell>AP+AN</cell></row><row><cell>UNSUP</cell><cell cols="2">BERT+INC 33.0/2.1 34.7/4.1 +EXH 34.2/3.2 40.2/4.7</cell><cell>29.8/2.1 31.5/2.6</cell><cell>15.7/0.3 14.7/0.0</cell><cell>34.7/2.3 40.2/4.7</cell><cell>27.3/1.4 36.9/3.5</cell></row><row><cell>FT</cell><cell cols="2">BERT+INC 44.3/6.8 48.0/9.6</cell><cell>50.4/9.9</cell><cell cols="3">56.9/16.1 61.0/20.4 66.6/25.1</cell></row><row><cell>SUP</cell><cell cols="2">PMI 2 +INC 23.4/1.2 24.4/1.5 +EXH 24.7/1.5 28.2/1.5</cell><cell>16.2/0.3 20.6/0.9</cell><cell>17.5/0.1 13.3/0.0</cell><cell>26.2/1.7 29.7/2.6</cell><cell>17.1/0.0 25.2/0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">1. INC: Answering each blank from left to right</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">in order. Once a blank is answered with a</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">candidate, this candidate is unavailable for</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">later blanks.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">2. EXH: Exhaustively scoring all permutations</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">of candidates to answer the blanks. The score</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">of a permutation is simply the sum of each its</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Test BA/PA of various model types unsupervised (UNSUP), finetuned (FT) and supervised (SUP) across varying context levels, with INC or EXH decoding.</figDesc><table><row><cell></cell><cell>BERT-Un TR.XL</cell><cell>BERT-ft</cell></row><row><cell cols="3">RemoveDt 47.4/17.2 39.7/9.1 80.9/62.0</cell></row><row><cell cols="3">RandomDt 44.6/12.4 36.0/6.8 77.9/50.9</cell></row><row><cell>Unablated</cell><cell cols="2">40.2/4.7 32.3/2.6 71.7/29.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Test BA/PA with distractor ablations on test set. RemoveDt and RandomDt represent removing and sampling distractors respectively. BERT-Un and BERT-ft denotes pre-trained and finetuned BERT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>BERT-ft performance in terms of human judgement of diffculty.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>All distractors in the test set are removed and models are evaluated on this non-distracted test set. Results are shown inTable 7. It is clear to see that after removing these distracting candidates, models can get better scores, showing that models find it hard to exclude distractors during prediction.</figDesc><table><row><cell cols="4">Model Uni. PMI 2 BERT-ft HUMAN</cell></row><row><cell>DE</cell><cell>1.429 1.204</cell><cell>0.661</cell><cell>0.375</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Randomly Sampled Distractors After remov-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ing human-created distractors, we further ran-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>domly sample sentences from other passages as</cell></row><row><cell></cell><cell></cell><cell></cell><cell>new distractors. To mitigate sampling variance,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>we run this experiment with 8 seeds and report the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Distractor error on test set of different models. Uni. denotes the uniform model.</figDesc><table><row><cell cols="2">Training Strategy PA</cell><cell>BA</cell><cell>DE</cell></row><row><cell>Q A</cell><cell cols="3">65.2 26.1 0.792</cell></row><row><cell>Q H</cell><cell cols="3">71.7 29.9 0.661</cell></row><row><cell>Q A ; Q H</cell><cell cols="3">74.2 33.9 0.624</cell></row><row><cell>Q A + Q H</cell><cell cols="3">74.5 34.3 0.637</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Test performance of models with Q A and Q</figDesc><table /><note>H .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory representations. arXiv preprint arXiv:1511.02301.</figDesc><table><row><cell></cell><cell>Felix Hill, Jennifer Hill and Rahul Simha. 2016. Automatic</cell></row><row><cell></cell><cell>generation of context-based fill-in-the-blank exer-</cell></row><row><cell></cell><cell>cises using co-occurrence likelihoods and google n-</cell></row><row><cell></cell><cell>grams. In Proceedings of the 11th Workshop on In-</cell></row><row><cell></cell><cell>novative Use of NLP for Building Educational Ap-</cell></row><row><cell></cell><cell>plications, pages 23-30.</cell></row><row><cell>Samuel R Bowman, Gabor Angeli, Christopher Potts,</cell><cell></cell></row><row><cell>and Christopher D Manning. 2015. A large anno-</cell><cell>Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,</cell></row><row><cell>tated corpus for learning natural language inference.</cell><cell>and Hal Daumé III. 2015. Deep unordered com-</cell></row><row><cell>In Proceedings of the 2015 Conference on Empiri-</cell><cell>position rivals syntactic methods for text classifica-</cell></row><row><cell>cal Methods in Natural Language Processing, pages</cell><cell>tion. In Proceedings of the 53rd Annual Meeting of</cell></row><row><cell>632-642.</cell><cell>the Association for Computational Linguistics and</cell></row><row><cell></cell><cell>the 7th International Joint Conference on Natural</cell></row><row><cell>Danqi Chen, Jason Bolton, and Christopher D Man-</cell><cell>Language Processing (Volume 1: Long Papers), vol-</cell></row><row><cell>ning. 2016. cnn/daily mail reading comprehension task. In Pro-A thorough examination of the</cell><cell>ume 1, pages 1681-1691.</cell></row><row><cell>ceedings of the 54th Annual Meeting of the Associa-</cell><cell>Jon Jonz. 1991. Cloze item types and second language</cell></row><row><cell>tion for Computational Linguistics (Volume 1: Long</cell><cell>comprehension. Language testing, 8(1):1-22.</cell></row><row><cell>Papers), pages 2358-2367.</cell><cell></cell></row><row><cell>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicog-raphy. Computational linguistics, 16(1):22-29.</cell><cell>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</cell></row><row><cell>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364.</cell><cell>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. In Pro-ceedings of the 2017 Conference on Empirical Meth-ods in Natural Language Processing, pages 785-794.</cell></row><row><cell>Zihang Dai, Zhilin Yang, Yiming Yang, William W</cell><cell></cell></row><row><cell>Cohen, Jaime Carbonell, Quoc V Le, and Ruslan</cell><cell>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-</cell></row><row><cell>Salakhutdinov. 2019. Transformer-xl: Attentive lan-</cell><cell>faella Bernardi, Stefano Menini, and Roberto Zam-</cell></row><row><cell>guage models beyond a fixed-length context. arXiv</cell><cell>parelli. 2014. Semeval-2014 task 1: Evaluation of</cell></row><row><cell>preprint arXiv:1901.02860.</cell><cell>compositional distributional semantic models on full</cell></row><row><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep</cell><cell>sentences through semantic relatedness and textual entailment. In Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014),</cell></row><row><cell>bidirectional transformers for language understand-ing. arXiv preprint arXiv:1810.04805.</cell><cell>pages 1-8.</cell></row><row><cell>Sandra S Fotos. 1991. The cloze test as an integrative measure of efl proficiency: A substitute for essays on college entrance examinations? Language learn-ing, 41(3):313-336.</cell><cell>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016</cell></row><row><cell>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural lan-</cell><cell>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849.</cell></row><row><cell>guage inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), volume 2, pages 107-112.</cell><cell>Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-pel, and David McAllester. 2016. Who did what: A large-scale person-centered cloze dataset. arXiv preprint arXiv:1608.05457.</cell></row><row><cell>Karl Moritz Hermann, Tomas Kocisky, Edward</cell><cell>Denis Paperno, Germán Kruszewski, Angeliki Lazari-</cell></row><row><cell>Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-</cell><cell>dou, Quan Ngoc Pham, Raffaella Bernardi, San-</cell></row><row><cell>leyman, and Phil Blunsom. 2015. Teaching ma-</cell><cell>dro Pezzelle, Marco Baroni, Gemma Boleda, and</cell></row><row><cell>chines to read and comprehend. In Advances in</cell><cell>Raquel Fernández. 2016. The lambada dataset:</cell></row><row><cell>neural information processing systems, pages 1693-</cell><cell>Word prediction requiring a broad discourse context.</cell></row><row><cell>1701.</cell><cell>arXiv preprint arXiv:1606.06031.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>We show more examples belonging to different reasoning categories inTable 11. Also, some completed questions with strong distractors, multiblank logic and diverse reasoning types are shown inTable 12, 13 and 14. One day, a teacher was giving a speech to his student. He held up a glass of water and asked the class. The students answers ranged from 20g to 500g.Candidate: B. How heavy do you think this glass of water is? × Candidate: D. It does not matter on the weight itself. Explanation: Match based on glass of water 2: Begin the sleep adjustment for your school schedule as early as possible.But if you feel you will need some extra time to adjust, start earlier. Candidate: C. Starting a few days early will be enough. × Candidate: A. Relax before you go to bed. Explanation: Match based on early, start If you want time to have breakfast with your family, save some time the night before by setting out clothes, shoes, and bags.That's a quarter-hour more you could be sleeping if you bought a coffee maker with a timer.Candidate: G. Reconsider the 15 minutes you spend in line at the cafe. × Candidate: F. Stick to your set bedtime and wake-up time, no matter the day. × Candidate: D. And consider setting a second alarm Explanation: Need to match 15 minutes, quarter-hour and coffee, cafe 4: Riding a London subway, a person from China will notice one major difference: In London, commuters do not look at each other.That's not rudeness-people are just too busy to bother looking. Candidate: E. In fact, eye contact is avoided at all times. × Candidate: F. Apple must earn a fortune from London commuters. × Candidate: G. Modern Londoner are fancy victims. Explanation: Need to match looking and eye contact</figDesc><table><row><cell>Reasoning</cell><cell>Examples with Excerpts From Blank Context</cell><cell>10 More examples</cell></row><row><cell cols="2">WM (18.47%) 1: Para. (19.47%) 3: Infer. 5: May is a great month.</cell><cell></cell></row><row><cell>(41.16%)</cell><cell></cell><cell></cell></row></table><note>.10 en.wikipedia.org/wiki/Derangement</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>More examples of reasoning categories. After I had spent a week with my English family, I slowly began to understand their English a little better.</figDesc><table><row><cell>Dear David</cell></row></table><note>1</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We defer the derivation to Appendix §1 4 http://www.21cnjy.com/;http://5utk.ks5u.com/; http://zujuan.xkw.com/; https://www.gzenxx.com/Html/rw/. 5 tesseract; ABBYY FineReader</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">github.com/karins/CoherenceFramework</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Unless stated otherwise, models decode with EXH and are trained with full context i.e AP+AN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We thank Qizhe Xie, Hiroaki Hayashi and the 3 anonymous reviewers for valuable comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates:</head><p>A. But it's not the language that's different and surprising. B. Thanks for your nice letter. C. I have difficulty in understanding my classmates. D. The family I live with are friendly. E. It 's very different from what I learned at school. F. Local habits and traditions are not the same as what we knew. G. The weather in London is really changeable.</p><p>Answers: 1→B , 2→E, 3→A , 4→G, 5→F (C and D are distractors) Discussion: C is a strong distractor -not only does it have strong word overlap with the contexts of many blanks -it also has words which can make it rank high in terms of the possible inferences (dialects are different implies difficulty in understanding. Though not as strong as C, D also has a key word matching and is similar in content to the topic.</p><p>How to Enjoy Life As a Teen. Are high school days equal to the "best years of your life"? Maybe not, but you can learn to make the most of your high school days.</p><p>1 Whether it's having a computer, having friends, having a good supply of food, a bed to sleep on, family that loves you, having a decent education or simply being born in this world. Be happy, and life will reward you. Remember that these are the last few years you will be able to enjoy yourself without having to worry about the responsibility of an adult, but make sure you prepare yourself for when you do become one. Choose your friends wisely. Unlike what many articles state, you don't have to be popular and have a gazillion friends to be happy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Try to have friends that like you who you are, not just because you are wearing a certain brand of shoes or something like that. These are people who shop at the same store as you; not someone who will sympathize with you when your dog dies.</p><p>3 Participating in clubs, activities, and sports increases your chances of meeting new friends. While you only need 4 or 5 close friends, that doesn't mean you shouldn't try to meet new people. Participating gives you something to do instead of sitting bored at home and wallowing in self-pity. You can pursue interests you enjoy. Video games, for example, are good if you're the type who can get into that kind of thing. Use your "hobby time" either to gain practical skills for college apps, job resumes, and scholarships or get into something else in the creative field like painting or dance.</p><p>4 Work at a job you can enjoy. Working is a great way to gain experience and to meet other people. When you do get out of college, interviewing companies will look at your prior work experience.</p><p>5 If you can't find work, especially in this hard economic time, volunteer or make your own job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates:</head><p>A.Remember that the point of life is for you to enjoy it. B. In fact, many of the "friends" you have when you are popular are not true friends. C. Learn to appreciate small things. D. Be sociable. E. This will look great on your resume. F. This is the time to start developing passions. G. You should also find a hobby that is meaningful or practical.</p><p>Answers: 1→C , 2→B, 3→D , 4→F, 5→E (A and G are distractors) Discussion: Both A and G are strong distractors especially for 4. Both of them overlap on key words, and do fit in the local context, though they are less coherent w.r.t F (which doesn't have any overlapping words) when placed in the broader narrative. <ref type="table">Table 12</ref>: Examples with strong distractors The demand for ways to improve memory is higher in students than it is in adults. Students often come across new knowledge in different areas that they need to store for exams.</p><p>1 Here are three effective ways to improve your memory as a student.</p><p>2 Research shows that learning activities that take more than two hours without a break are less productive when compared to those that take one hour or 30 minutes. Students are likely to remember things they learn over a short period of time. Make sure you take breaks between learning sessions to help improve your memory. Try to relax. Relaxing should be an essential part of your learning process. Scientists have proven that stronger and lasting memories can be achieved when a person relaxes.</p><p>3 Deep breathing is one of the most popular relaxation techniques. Establish a quiet environment and find a comfortable position. Then go through a deep breathing process for at least 15 minutes. Train the brain Students should give their brains a workout in order to improve their memory. At times the brain needs the right stimulation to keep growing and developing. You need to come up with a brain boosting activity that is suitable for you.</p><p>4 Write a short story and then try to use seven to nine words to describe it. You can also do games and puzzles to help improve your memory.</p><p>5 The techniques discussed above will help you to improve your memory significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates:</head><p>A. Distribute learning. B. Enrich learning activities. C. Some students suffer with memory problems. D. Like a muscle memory can stretch and grow with a workout. E. For instance you can prepare a list of items and try to memorize them. F. You need to use different relaxation techniques in order to improve your memory. G. In summary a good memory is an important advantage to any student who wants to improve his or her grades.</p><p>Answers: 1→C, 2→A, 3→F , 4→E, 5→G (B and D are distractors)</p><p>Discussion: The candidate F can actually go into three possible blanks and fit well into their context -Blanks 1, 3 and 5. This can be seen from the several overlapping phrases/paraphrases F shares with all three, as shown by the three colors (one per concept). However, G (which starts with the phrase In summary, can only fit into Blank 5. A is also difficult to place in any blank other than Blank 1. Hence , candidate F has to be placed into Blank 3. <ref type="table">Table 13</ref>: Examples which require multi-blank logic A student's life is never easy. And it is even more difficult if you will have to complete your study in a foreign land.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>The following are some basic things you need to do before even seizing that passport and boarding on the plane. Knowing the country. You shouldn't bother researching the country's hottest tourist spots or historical places. You won't go there as a tourist, but as a student. 2 In addition, read about their laws. You surely don't want to face legal problems, especially if you're away from home.</p><p>3 Don't expect that you can graduate abroad without knowing even the basics of the language. Before leaving your home country, take online lessons to at least master some of their words and sentences. This will be useful in living and studying there. Doing this will also prepare you in communicating with those who can't speak English. Preparing for other needs. Check the conversion of your money to their local currency. <ref type="bibr">4</ref> The Internet of your intended school will be very helpful in findings an apartment and helping you understand local currency. Remember, you're not only carrying your own reputation but your country's reputation as well. If you act foolishly, people there might think that all of your countrymen are foolish as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates:</head><p>A. Studying their language. B. That would surely be a very bad start for your study abroad program. C. Going with their trends will keep it from being too obvious that you're a foreigner. D. Set up your bank account so you can use it there , get an insurance , and find an apartment. E. It'll be helpful to read the most important points in their history and to read up on their culture. F. A lot of preparations are needed so you can be sure to go back home with a diploma and a bright future waiting for you. G. Packing your clothes.</p><p>Answers with Reasoning Type: 1→F (Summary), 2→E (Inference), 3→A (Paraphrase), 4→D (WordMatch), 5→B (Inference) (C and G are distractors) Discussion: Blank 3 is the easiest to solve, since Studying their language is a near-paraphrase of Knowing even the basics of the language. Blank 2 needs to be reasoned out by Inference -specifically E can be inferred from the previous sentence. Note however that C is also a possible inference from the previous sentence -it is only after reading the entire context, which seems to be about learning various aspects of a country, that E seems to fit better. Blank 1 needs to be reasoned out by Summary → it requires understanding several later sentences and abstracting out that they all refer to lots of preparations. Finally, Blank 5 can be mapped to B by inferring that people thinking all your countrymen are foolish is bad, while Blank 4 is a easy WordMatch on apartment to D. Latest news and comment on Street art from guardian.co.uk...</p><p>1 You can find it on buildings sidewalks street signs and trash cans from Tokyo to Paris from Moscow to Cape Town. Street art has become a global culture and even art museums and galleries are collecting the works of street artist. Street art started out very secretly because it was illegal to paint on public and private property without permission.</p><p>2 Some think it is a crime and others think it is a very beautiful new form of culture. Art experts claim that the street art movement began in New York in the 1960s. Young adults painted words and other images on the walls and trains. This colorful style of writing became known as graffiti whose art showed that young people wanted to rebel against society. Street artists do their work for different reasons.</p><p>3 They choose street art because it is closer to the people. Some artists try to express their political opinion in their work. Others like to do things that are forbidden and hope they don't caught. Advertising companies also use street art in their ads because it gives people the impressions of youth and energy.</p><p>4 Artists can show their pictures to an audience all over the world. Many city residents however say that seeing a picture on the Internet is never as good as seeing it alive.</p><p>5. There it will continue to change and grow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates:</head><p>A. Street art is a very popular form of art that is spreading quickly all over the world. B. Today the Internet has a big influence on street art. C. With the development of science and technology different art styles come into the Internet. D. The street art movement lives with the energy and life of a big city. E. People often have different opinions about street art. F. Street art used to be illegal but now has become popular. G. Some of them do not like artists who make so much money in galleries and museums.</p><p>Answers with Reasoning Type: 1→A (Summary), 2→E (Inference), 3→G (Inference), 4→B (Inference), 5→D (Inference) (C and F are distractors) Discussion: Blank 1 requires an answer which makes an overall broad statement to introduce the topic. Working backwards, this requires summarizing or finding a broad topic given the latter sentences. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05739</idno>
		<title level="m">Abductive commonsense reasoning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Overview of CLEF QA Entrance Exams Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1194" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview of CLEF QA Entrance Exams Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09728</idno>
		<title level="m">Socialiqa: Commonsense reasoning about social interactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cohere: A toolkit for local coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><forename type="middle">Sim</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4111" to="4114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">cloze procedure: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale cloze test dataset created by teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2344" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The microsoft research sentence completion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2011-129</idno>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Microsoft Research, Redmond, WA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
