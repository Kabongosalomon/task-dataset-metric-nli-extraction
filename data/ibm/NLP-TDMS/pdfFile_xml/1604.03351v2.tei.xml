<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Orientation-boosted Voxel Nets for 3D Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Sedaghat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
							<email>zolfagha@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Amiri</surname></persName>
							<email>amirie@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Orientation-boosted Voxel Nets for 3D Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SEDAGHAT et al.: ORIENTATION-BOOSTED VOXEL NETS FOR 3D OBJECT RECOGNITION -BMVC 2017 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More specifically, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields significant improvements in the classification results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images. We report state-of-the-art results on classification as well as significant improvements in precision and speed over the baseline on 3D detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Various devices producing 3D point clouds have become widely applicable in recent years, e.g., range sensors in cars and robots or depth cameras like the Kinect. Structure from motion and SLAM approaches have become quite mature and generate reasonable point clouds, too. With the rising popularity of deep learning, features for recognition are no longer designed manually but learned by the network. Thus, moving from 2D to 3D recognition requires only small conceptual changes in the network architecture <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">38]</ref>.</p><p>In this work, we elaborate on 3D recognition using 3D convolutional networks, where we focus on the aspect of auxiliary task learning. Usually, a deep network is directly trained on the task of interest, i.e., if we care about the class labels, the network is trained to produce correct class labels. There is nothing wrong with this approach. However, it requires the network to learn the underlying concepts, such as object pose, that generalize to variations in the data. Oftentimes, the network does not learn the full underlying concept but some representation that only partially generalizes to new data. c 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1604.03351v2 [cs.CV] 19 Oct 2017 <ref type="figure">Figure 1</ref>: Adding orientation classification as an auxiliary task to a 3D classification network improves its category-level classification accuracy.</p><p>In the present paper, we focus on the concept of object orientation. The actual task only cares about the object label, not its orientation. However, to produce the correct class label, at least some part of the network representation must be invariant to the orientation of the object, which is not trivial in 3D. Effectively, to be successful on the classification task, the network must also solve the orientation estimation task, but the loss function does not give any direct indication that solving this auxiliary task is important. We show that forcing the network to produce the correct orientation during training increases its classification accuracy significantly - <ref type="figure">Figure 1</ref>.</p><p>We introduce a network architecture that implements this idea and evaluate it on 4 different datasets representing the large variety of acquisition methods for point clouds: laser range scanners, RGB-D images, and CAD models. The input to the network is an object candidate obtained from any of these data sources, which is fed to the network as an occupancy grid. We compare the baseline without orientation information to our orientation-boosted version and obtain improved results in all experiments. We also compare to the existing 3D classification methods and achieve state-of-the-art results using our shalow orientation-boosted networks in most of the experiments. In the scope of our experiments, we extended the Mod-elnet40 dataset, which consists of more than 12k objects, with per-class alignments by using some automated alignment procedure <ref type="bibr" target="#b28">[28]</ref>. We will provide the additional annotation.</p><p>We also applied the classifier in a 3D detection scenario using a simple 3D sliding box approach. In this context, the orientation estimation is no longer just an auxiliary task but also determines the orientation of the box, which largely reduces the runtime of the 3D detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most previous works on 3D object recognition rely on handcrafted feature descriptors, such as Point Feature Histograms <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref>, 3D Shape Context <ref type="bibr" target="#b18">[19]</ref>, or Spin Images <ref type="bibr" target="#b17">[18]</ref>. Descriptors based on surface normals have been very popular, too <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">23]</ref>. Yulanguo et al. <ref type="bibr" target="#b40">[40]</ref> gives an extensive survey on such descriptors.</p><p>Feature learning for 3D recognition has first appeared in the context of RGB-D images, where depth is treated as an additional input channel <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. Thus, the approaches are conceptually very similar to feature learning in images. Gupta et al. <ref type="bibr" target="#b11">[12]</ref> fits and projects 3D synthetic models into the image plane.</p><p>3D convolutional neural networks (CNNs) have appeared in the context of videos. Tran et al. <ref type="bibr" target="#b35">[35]</ref> use video frame stacks as a 3D signal to approach multiple video classification tasks using their 3D CNN, called C3D. 3D CNNs are not limited to videos, but can be applied also to other three-dimensional inputs, such as point clouds, as in our work.</p><p>The most closely related works are by Wu et al. <ref type="bibr" target="#b38">[38]</ref> and Maturana &amp; Sherer <ref type="bibr" target="#b20">[21]</ref>, namely 3D ShapeNets and VoxNet. Wu et al. use a Deep Belief Network to represent geometric 3D shapes as a probability distribution of binary variables on a 3D voxel grid. They use their method for shape completion from depth maps, too. The ModelNet dataset was introduced along with their work. The VoxNet <ref type="bibr" target="#b20">[21]</ref> is composed of a simple but effective CNN, accepting as input voxel grids similar to Wu et al. <ref type="bibr" target="#b38">[38]</ref>. In both of these works the training data is augmented by rotating the object to make the network learn a feature representation that is invariant to rotation. However, in contrast to the networks proposed in this paper, the network is not enforced to output the object orientation, but only its class label. While in principle, the loss on the class label alone should be sufficient motivation for the network to learn an invariant representation, our experiments show that an explicit loss on the orientation helps the network to learn such representation.</p><p>Su et al. <ref type="bibr" target="#b34">[34]</ref> take advantage of the object pose explicitly by rendering the 3D objects from multiple viewpoints and using the projected images in a combined architecture of 2D CNNs to extract features. However, this method still relies on the appearance of the objects in images, which only works well for dense surfaces that can be rendered. For sparse and potentially incomplete point clouds, the approach is not applicable. Song et al. <ref type="bibr" target="#b32">[32]</ref> focus on 3D object detection in RGB-D scenes. They utilize a 3D CNN for 3D object bounding box suggestion. For the recognition part, they combine geometric features in 3D and color features in 2D.</p><p>Several 3D datasets have become available recently. Sydney Urban Objects <ref type="bibr" target="#b6">[7]</ref> includes point-clouds obtained from range-scanners. SUN-RGBD <ref type="bibr" target="#b33">[33]</ref> and SUN-3D <ref type="bibr" target="#b39">[39]</ref> gather some reconstructed point-clouds and RGB-D datasets in one place and in some cases they also add extra annotations to the original datasets. We use the annotations provided for NYU-Depth V2 dataset <ref type="bibr" target="#b30">[30]</ref> by SUN-RGBD in this work. ModelNet is a dataset consisting of synthetic 3D object models <ref type="bibr" target="#b38">[38]</ref>. Sedaghat &amp; Brox <ref type="bibr" target="#b28">[28]</ref> created a dataset of annotated 3D point-clouds of cars from monocular videos using structure from motion and some assumptions about the scene structure.</p><p>Quite recently and parallel to this work, there have been several works utilizing 2D or 3D CNNs merely on 3D CAD models of ModelNet <ref type="bibr" target="#b38">[38]</ref> in supervised <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>, semisupervised <ref type="bibr" target="#b24">[24]</ref> and unsupervised <ref type="bibr" target="#b37">[37]</ref> fashions. Although our main architecture is rather shallow compared to most of them, and we use a rather low resolution compared to methods relying on high-resolution input images, we still provide state-of-the-art results on the aligned ModelNet10 subset, and on-par results on a roughly and automatically aligned version of the ModelNet40 subset.</p><p>Most of the published works on detection try to detect objects directly in the 2D space of the image. Wang &amp; Posner <ref type="bibr" target="#b36">[36]</ref> were among the first ones to utilize point clouds to obtain object proposals. In another line of work Gonzalez et al. <ref type="bibr" target="#b10">[11]</ref> and Chen et al. <ref type="bibr" target="#b4">[5]</ref> mix both 2D and 3D data for detection. Many authors, including Li et al. <ref type="bibr" target="#b19">[20]</ref> and Huang et al. <ref type="bibr" target="#b14">[15]</ref> approach the task with a multi-tasking method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The core network architecture is based on VoxNet <ref type="bibr" target="#b20">[21]</ref> and is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. It takes a 3D voxel grid as input and contains two convolutional layers with 3D filters followed by two fully connected layers. Although this choice may not be optimal, we keep it to be able to directly compare our modifications to VoxNet. In addition, we experimented with a slightly deeper network that has four convolutional layers.</p><p>Point clouds and CAD models are converted to voxel grids (occupancy grids). For the NYUv2 dataset we used the provided tools for the conversion; for the other datasets we implemented our own version. We tried both binary-valued and continuous-valued occupancy grids. In the end, the difference in the results was negligible and thus we only report the results of the former one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task learning</head><p>We modify the baseline architecture by adding orientation estimation as an auxiliary parallel task. We call the resulting architecture the ORIentation-boosted vOxel Net -ORION. Without loss of generality, we only consider rotation around the z-axis (azimuth) as the most varying component of orientation in practical applications. Throughout this paper we use the term 'orientation' to refer to this component.</p><p>Orientation is a continuous variable and the network could be trained to provide such an output. However, the idea is to treat different orientations of an object differently, and therefore we cast the orientation estimation as a classification problem. This also serves as a relaxation on dataset constraints, as a rough alignment of data obviates the need for strict orientation annotations. The network has output nodes for the product label space of classes and orientations and learns the mapping</p><formula xml:id="formula_0">x i → (c i , o i )<label>(1)</label></formula><p>where x i are the input instances and c i , o i are their object class and orientation class, respectively. We do not put the same orientations from different object classes into the same orientation class, because we do not seek to extract any information from the absolute pose of the objects. Sharing the orientation output for all classes would make the network learn features shared among classes to determine the orientation, which is the opposite of what we want: leveraging the orientation estimation as an auxiliary task to improve on object classification. For example, a table from the 45 • orientations class is not expected to share any useful information with a car of the same orientation.</p><p>We choose multinomial cross-entropy losses <ref type="bibr" target="#b25">[25]</ref> for both tasks, so we can combine them by summing them up:</p><formula xml:id="formula_1">L = (1 − γ)L C + γL O<label>(2)</label></formula><p>where L C and L O indicate losses for object classification and orientation estimation tasks respectively. We used equal loss weights (γ = 0.5) and found in our classification experiments that the results do not depend on the exact choice of the weight γ around this value. However, in one of the detection experiments, where the orientation estimation is not an auxiliary task anymore, we used a higher weight for the orientation output to improve its accuracy. Due to various object symmetries, the number of orientation labels differs per object class - <ref type="figure" target="#fig_0">Figure 2</ref>. The idea is that we do not want the network to try to differentiate between, e.g., a table and its 180 • rotated counterpart. For the same reason, to rotationally symmetric objects, such as poles, or rotationally neutral ones, such as trees to which no meaningful azimuth label can be assigned, we dedicate only a single node. This is decided upon manually in the smaller datasets. However, during the auto-alignment of the bigger Modelnet40 dataset, the number of orientations are also automatically assigned to different classes. Details are given in the supplementary material.</p><p>Voting Object orientations can be leveraged at the entry of the network, too. During the test phase we feed multiple rotations of the test object to the network and obtain a final consensus on the class label based on the votes we obtain from each inference pass, as follows:</p><formula xml:id="formula_2">c f inal = arg max k ∑ r S k (x r )<label>(3)</label></formula><p>where S k is the score the network assigns to the object at its k th node of the main (object category) output layer. x r is the test input with the rotation index r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>We train and test our networks on four datasets, three of which are illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>We have chosen the datasets such that they represent different data sources.</p><p>Sydney Urban Objects -LiDAR/Pointcloud This dataset consists of LiDAR scans of 631 objects in 26 categories. The objects' point-clouds in this dataset are always incomplete, as they are only seen by the LiDAR sensor from a single viewpoint. Therefore the quality of the objects are by no means comparable to synthetic 3D objects, making classification a challenging task, even for human eyes; see <ref type="figure" target="#fig_1">Figure 3</ref>. This dataset is also of special interest in our category-level classification, as it provides a tough categorization of vehicles: 4wd, bus, car, truck, ute and van are all distinct categories. We use the same settings as <ref type="bibr" target="#b20">[21]</ref> to make our results comparable to theirs. The point clouds are converted to voxel-grids of size 32x32x32, in which the object occupies a 28x28x28 space. Zero-paddings of size 2 is used on each side to enable displacement augmentation during training. We also annotated the orientations to make the data suitable for our method. These we will provide to the public. NYUv2 -Kinect/RGBD This dataset consists of an overall number of 2808 RGBD images, corresponding to 10 object classes. The class types are shared with the ModelNet10 dataset. We used voxel grids of size 32x32x32, which contain the main object in the size of 28x28x28. The rest includes the context of the object and each object has a maximum number of 12 rotations. The dataset does not provide orientation annotations and therefore we used the annotations provided by the SUN-RGBD benchmark <ref type="bibr" target="#b33">[33]</ref>.</p><p>ModelNet -Synthetic/CAD This dataset is composed of synthetic CAD models. The ModelNet10 subset consists of uniformly aligned objects of the same classes as in the NYUv2 dataset. The object meshes in this dataset are converted to voxel grids of size 28x28x28, similar to the NYUv2 setting. The ModelNet40 subset does not come with alignments (or orientation annotations). Thus, we provided manual annotation of orientation that we will make publicly available. In addition, we ran an unsupervised automated procedure to align the samples of ModelNet40. Please refer to supplemental material for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI -LiDAR/Pointcloud</head><p>The KITTI dataset <ref type="bibr" target="#b9">[10]</ref> contains 7481 training images and 7518 test images in its object detection task. Each image represents a scene which also comes with a corresponding Velodyne point cloud. 2D and 3D bounding box annotations are provided in the images. Using the provided camera calibration parameters they can be converted into the coordinates of the Velodyne scanner. We use this dataset only in the detection experiment. To be able to report and analyze the effects of our method at multiple levels, we split the publicly available training set to 80% and 20% subsets for training and testing, respectively.  <ref type="table">Table 1</ref>: Classification results and comparison to the state-of-the-art on three datasets. We report the overall classification accuracy, except for the Sydney dataset where we report the weighted average over F1 score. The auxiliary task on orientation estimation clearly improves the classification accuracy on all datasets. † We report the single-network results for this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification</head><p>The classification results on all datasets are shown in <ref type="table">Table 1</ref>. For the Sydney Urban Objects dataset, we report the average F1 score weighted by class support as in <ref type="bibr" target="#b38">[38]</ref> to be able to compare to their work. This weighted average takes into account that the classes in this dataset are unbalanced. For the other datasets we report the average accuracy. The Sydney dataset provides 4 folds/subsets to be used for cross-validation; in each experiment three folds are for training and one for testing. Also due to the small size of this dataset, we run each experiment three times with different random seeds, and report the average over all the 12 results. We achieve clear improvements over the baseline and report state-of-the-art results in all the three datasets, with a far shallower architecture compared to previous state-of-the-art (2 vs. 43 conv. layers) and a big saving on number of parameters (1M vs.18M).</p><p>We also experimented with a slightly deeper network (last row of <ref type="table">Table 1</ref>), but found that the network starts to overfit on the smaller datasets. Details of this extended architecture can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Non-aligned Dataset</head><p>Since the Modelnet40 dataset does not come with alignments we manually annotated the orientation. As an alternative, we also aligned the objects, class by class, in an unsupervised fashion using the method introduced in Sedaghat &amp; Brox <ref type="bibr" target="#b28">[28]</ref>. Details of the steps of this process can be found in the supplementary material.  <ref type="table" target="#tab_1">Table 2</ref>: Classification accuracy on Modelnet40. Orientation information during training clearly helps boost the classification accuracy even when orientation labels are obtained by unsupervised alignment <ref type="bibr" target="#b28">[28]</ref>. In fact, manually assigned labels do not yield any significant improvement. Batch normalization and two additional convolutional layers improve results.</p><p>is almost as good as the tedious manual orientation labeling. This shows that the network can benefit even from coarse annotations. Since the number of training samples is large, the deeper network with four convolutional layers performed even better than its counterpart with only two convolutional layers.</p><p>Batch normalization (BN) is known to help with problems during network training <ref type="bibr" target="#b15">[16]</ref>. Adding batch normalization to the convolutional layers yielded consistent improvements in the results; e.g. see <ref type="table" target="#tab_1">Table 2</ref>. We conjecture that batch normalization causes the error from the second task to propagate deeper back into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Detection</head><p>We tested the performance of our suggested method in a detection scenario, where the orientation sensitive network is used as a binary object classifier to assign a score to 3D bounding box proposals in a sliding window fashion. We tested the 3D detector to detect cars in the KITTI dataset. <ref type="figure" target="#fig_2">Figure 5</ref> quantifies the improvements of the ORION architecture in such an exemplar detection scenario. The mere use of our architecture as a binary classifier significantly pulls up the PR curve and increases the mean average precision. In this case, we only relied on the object classification output of the network and performed an exhaustive search over the rotations -18 rotation steps to cover 360 degrees. The main benefit is achieved when we also leveraged the orientation output of the network to directly predict the orientation of the object. This resulted in an 18× run time improvement. We also noticed that by increasing the loss weight of the orientation output, thus emphasizing on the orientation, the detection results improved further. It is worth noting that in contrast to most of the detectors which run detection in the RGB image of the KITTI dataset, we do not use the RGB image but only use the 3D point cloud. We also limited the search in the scale and aspect-ratio spaces by obtaining statistical measures of the car sizes in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>To analyze the behavior of the orientation-boosted network, we compare it to its corresponding baseline network. To find a correspondence, we first train the baseline network long enough, so that it reaches a stable state. Then we use the trained net to initialize the weights of ORION, and continue training with low learning rate. We found that some filters tend to become more sensitive to orientation-specific features of the objects. We also found that in the baseline network some filters behave as the dominant ones for all the possible rotations of the objects in a class, while ORION has managed to spread the contributions over different filters for different orientations. Details of these experiments and visualizations are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We showed for the task of 3D object classification that learning of certain concepts, such as invariance to object orientation, can be supported by adding the concept as an auxiliary task during training. By forcing the network to produce also the object orientation during training, it achieved better classification results at test time. This finding was consistent on all datasets and enabled us to establish state-of-the-art results on most of them. The approach is also applicable to 3D detection in a simple sliding 3D box fashion. In this case, the orientation output of the network avoids the exhaustive search over the object rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>8 Auto-Alignment of the Modelnet40 dataset Modelnet40 <ref type="bibr" target="#b38">[38]</ref> consists of more than 12000 non-aligned objects in 40 classes. We used the method of Sedaghat &amp; Brox <ref type="bibr" target="#b28">[28]</ref> to automatically align the objects class by class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh to Point-Cloud Conversion</head><p>The auto-alignment method of <ref type="bibr" target="#b28">[28]</ref> uses point-cloud representations of objects as input. Thus we converted the 3D mesh grids of Modelnet40 to point-clouds by assigning uniformly distributed points to object faces.</p><p>Hidden faces in the mesh grids needed to be removed, as the so called Hierarchical Orientation Histogram (HOH) of <ref type="bibr" target="#b28">[28]</ref> mainly relies on the exterior surfaces of the objects. We tackled this issue using the Jacobson's implementation <ref type="bibr" target="#b16">[17]</ref> of the "ambient occlusion" method <ref type="bibr" target="#b22">[22]</ref>.</p><p>We tried to distribute the points roughly with the same density across different faces, regardless of their shape and size, to avoid a bias towards bigger/wider ones. Our basic point-clouds consist of around 50000 points per object, which are then converted to lighter models using the Smooth Signed Distance surface reconstruction method (SSD) <ref type="bibr" target="#b2">[3]</ref> as used in <ref type="bibr" target="#b28">[28]</ref>.</p><p>Auto-Alignment We first created a "reference-set" in each class, consisting of a random subset of its objects, with an initial size of 100. This number was then decreased, as the low-quality objects were automatically removed from the reference set, according to <ref type="bibr" target="#b28">[28]</ref>. This reference set was then used to align the remaining objects of the class one by one.</p><p>For the HOH descriptor, we used 32 and 8 divisions in φ and θ dimensions respectively, for the root component. We also used 8 child components with 16 divisions for φ and 4 for θ -see <ref type="bibr" target="#b28">[28]</ref>.</p><p>Automatic Assignment of Number of Orientation Classes As pointed out in the main paper, we do not use the same number of orientation classes for all the object categories. We implemented the auto-alignment procedure in a way that this parameter is automatically decided upon for each category: During generation of the reference-set in each class, the alignment procedure was run with 3 different configurations, for which the search space spanned over 360, 180 and 90 degrees of rotations respectively. Each run resulted in an error measure representing the overall quality of the models selected as the reference-set, and we designated respectively 12, 6 and 3 orientation levels to each category, whenever possible. When none of these worked, e.g. for the 'flower_pot' class, we assigned 1 orientation class which is equivalent to discarding the orientation information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Analysis</head><p>To analyze the behavior of the orientation-boosted network, we compare it to its corresponding baseline network. We would like to know the differences between corresponding filters in the two networks. To find this correspondence, we first train a baseline network, without orientations outputs, for long enough so that it reaches a stable state. Then we use this trained net to initialize the weights of the ORION network, and continue training with a low learning rate. This way we can monitor how the learned features change in the transition from the baseline to the orientation-aware network.</p><p>In <ref type="figure" target="#fig_4">Figure 6</ref> transition of a single exemplar filter is depicted, and its responses to different rotations of an input object are illustrated. It turns out that the filter tends to become more sensitive to the orientation-specific features of the input object. Additionally some parts of the object, such as the table legs, show stronger response to the filter in the orientation-aware network.</p><p>With such an observation, we tried to analyze the overall behavior of the network for specific object classes with different orientations. To this end we introduce the "dominant signal-flow path" of the network. The idea is that, although all the nodes and connections of the network contribute to the formation of the output, in some cases there may exist a set of nodes, which have a significantly higher effect in this process for an specific type of object/orientation. To test this, we take this step-by-step approach: First in a forward pass, the class, c, of the object is found. Then we seek to find the highest contributing node of the last hidden layer:</p><formula xml:id="formula_3">l n−1 = arg max k {w n−1 k,c a n−1 k }<label>(4)</label></formula><p>where n is the number of layers, a n−1 k are the activations of layer n − 1, and w n−1 k,c is the weight connecting a n−1 k to the c th node of layer n. This way we naively assume there is a significant maximum in the contributions and assign its index to l n−1 . Later we will see that this assumption proves to be true in many of our observations. We continue "back-tracing" the signal, to the previous layers. Extension of (4) to the convolutional layers is straightforward, as we are just interested in finding the index of the node/filter in each layer. In the end, letting l n = c, gives us the vector l with length equal to the number of network layers, keeping the best contributors' indices in it. Now to depict the "dominant signal-flow path" for a group of objects, we simply obtain l for every member of the group, and plot the histogram of the l i s as a column. <ref type="figure">Figure 7(a)</ref> shows such an illustration for a specific class-rotation of the objects. It is clearly visible that for many objects of that group, specific nodes have been dominant.</p><p>In <ref type="figure">Figure 7(b)</ref>, the dominant paths of the baseline and ORION networks for some sample object categories of the Modelnet10 dataset are illustrated. It can be seen that in the baseline network, the dominant paths among various rotations of a class mostly share a specific set of nodes. This is mostly visible in the convolutional layers -e.g. see the red boxes. On the contrary, the dominant paths in the ORION network rarely follow this rule and have more distributed path nodes. We interpret this as one of the results of orientation-boosting, and a helping factor in better classification abilities of the network.  The input is always the same object, which has been fed to the network in its possible discretized rotations (columns) at each step (row). We simulated this transition by first training the baseline network and then fine-tuning our orientation-aware architecture on top of the learned weights. To be able to depict the 3D feature maps, we had to cut out values below a specific threshold. It can be seen that the encoded filter detects more orientation-specific aspects of the object, as it moves forward in learning the orientations. In addition, it seems that the filter is becoming more sensitive to a table rather than only a horizontal surface -notice the table legs appearing in the below rows.  <ref type="figure">Figure 7</ref>: (a) shows the "dominant signal-flow path" of the network, for an exemplar object category-orientation. Each column contains the activations of one layer's nodes. Obviously the columns are of different sizes. Higher intensities show dominant nodes for the specific group of objects. Details of the steps taken to form such an illustration are explained in the text. In (b), rows represent object classes, while in different columns we show rotations of the objects. So each cell is a specific rotation of a specific object category. It can be seen that in the baseline network, many of the rotations of a class, share nodes in their dominant path (e.g. see the red boxes), whereas, in the ORION network the paths are more distributed over all the nodes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Orientation Estimation Results</head><p>Although the orientation estimation was used merely as an auxiliary task, here in <ref type="table">Table 4</ref> we report the accuracies of the estimated orientation classes. Note that getting better results on orientation estimation would be possible by emphasizing on this task -e.g. see the detection experiment in the main article.  <ref type="table">Table 4</ref>: Orientation estimation accuracies on different datasets. The extended architecture of the second row, is the one introduced in the main article and detailed in <ref type="table" target="#tab_3">Table 3</ref> of this document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Basic Orientation Boosting. Class labels and orientation labels are two separate outputs. The number of orientation labels assigned to each class can be different to the others. Both outputs contribute to the training equally -with the same weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples from the various 3D datasets we used in experiments. On top, two exemplar scenes from the NYUv2 [30] &amp; Sydney [7] datasets are depicted. On the bottom samples from the Modelnet dataset are displayed. The KITTI dataset is similar to the Sydney dataset and is not shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>On the top left, detected boxes of an exemplar scene are displayed in its 3D pointcloud representation. 3D boxes are then projected to the 2D image plane -top right. Green boxes are the ground truth cars. Blue and red show true and false positives respectively. The bottom row illustrates the Precision-Recall curves for multiple detection experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ORION iterations ←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−baseline ←− Object Orientations −→ − higher values −→</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The picture illustrates the activations of one of the nodes of the first layer, while the network transitions from a baseline network to ORION.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows the large improvement obtained by using the extra annotation during training. Interestingly, the automatic alignment</figDesc><table><row><cell>Ground Truth</cell><cell>4wd</cell><cell>building</cell><cell>bus</cell><cell>desk</cell><cell>monitor</cell></row><row><cell>Baseline</cell><cell>car</cell><cell>bus</cell><cell>car</cell><cell>sofa</cell><cell>chair</cell></row><row><cell>Ours</cell><cell>4wd</cell><cell>building</cell><cell>bus</cell><cell>desk</cell><cell>monitor</cell></row><row><cell>Ground Truth</cell><cell>table</cell><cell>chair</cell><cell>ute</cell><cell>toilet</cell><cell>bathtub</cell></row><row><cell>Baseline</cell><cell>nite-stnd</cell><cell>table</cell><cell>ute</cell><cell>toilet</cell><cell>table</cell></row><row><cell>Ours</cell><cell>table</cell><cell>chair</cell><cell>truck</cell><cell>chair</cell><cell>bed</cell></row><row><cell cols="6">Figure 4: Some exemplar classification results. We show examples on which the outputs of</cell></row><row><cell>the two networks differ.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell></row><row><cell></cell><cell cols="2">Conv. Batch</cell><cell>No</cell><cell cols="2">Rough, Automatic Perfect, Manual</cell></row><row><cell>Method</cell><cell cols="3">Layers Norm. Alignment</cell><cell>Alignment</cell><cell>Alignment</cell></row><row><cell>VoxNet [21] (baseline)</cell><cell>2</cell><cell>×</cell><cell>83</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>2</cell><cell>×</cell><cell>-</cell><cell>88.1</cell><cell>87.5</cell></row><row><cell>ORION (Ours)</cell><cell>2</cell><cell></cell><cell>-</cell><cell>88.6</cell><cell>88.2</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell>-</cell><cell>89.4</cell><cell>89.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Details of the extended architecture introduced in Tables 1 &amp; 2 of the main article.</figDesc><table /><note>† The number of nodes dedicated to the orientation output varies in different experiments.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We acknowledge funding by the ERC Starting Grant VideoLearn.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for RGB-D based object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and Discriminative Voxel Modeling with Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ssd: Smooth signed distance surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Calakli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1993" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of global descriptors for velodyne-based urban object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="667" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07759</idno>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3572</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Alastair Quadros, Calvin Hung, and Bertrand Douillard. Unsupervised Feature Learning for Classification of Outdoor 3D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Field</forename><surname>Mark De Deuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robotics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PointNet: A 3D Convolutional Neural Network for realtime object class recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez-Donoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azorin-Lopez</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2016.7727386</idno>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="1578" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiview random forest of local experts combining RGB and LIDAR data for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Villalonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<idno type="DOI">10.1109/IVS.2015.7225711</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<biblScope unit="page" from="356" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aligning 3D Models to RGB-D Images of Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4731" to="4740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishakh</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05695</idno>
		<title level="m">FusionNet: 3D Object Classification Using Multiple Data Representations</title>
		<imprint>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extended gaussian images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthold Klaus Paul</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1671" to="1686" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">DenseBox: Unifying Landmark Localization with End to End Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>David Blei and Francis Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<ptr target="http://github.com/alecjacobson/gptoolbox" />
		<title level="m">Geometry processing toolbox</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3D scenes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D shape matching with 3D shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Körtgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil-Joo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Novotni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 7th central European seminar on computer graphics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Zeeshan</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02699</idno>
		<title level="m">Deep Supervision with Shape Concepts for Occlusion-Aware 3D Object Parsing</title>
		<imprint>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Algorithms for Local and Global Accessibility Shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/192161.192244</idno>
		<ptr target="http://doi.acm.org/10.1145/192161.192244" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;94</title>
		<meeting>the 21st Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;94<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object detection from large-scale 3d datasets using bottom-up and top-down descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2008</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<title level="m">Deep Learning with Sets and Point Clouds</title>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<idno>978-1-4757-4321-0. Google-Books- ID: 8KgACAAAQBAJ</idno>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning informative point classes for the acquisition of object model maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Control, Automation, Robotics and Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="643" to="650" />
		</imprint>
	</monogr>
	<note>10th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation, 2009. ICRA&apos;09. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">311</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised Generation of a Viewpoint Annotated Car Dataset from Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeepPano: Deep Panoramic Representation for 3-D Shape Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2015.2480802</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1558" to="2361" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02300</idno>
		<title level="m">Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multiview convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<title level="m">Generic Features for Video Analysis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voting for Voting in Online Point Cloud Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generativeadversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SUN3D: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3D Object Recognition in Cluttered Scenes with Local Surface Features: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2014.2316828</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2270" to="2287" />
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
