<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of Assumptions in Bayesian Optimisation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Cowen-Rivers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Lyu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasul</forename><surname>Tutunov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Grosnit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Rhys</forename><surname>Griffiths</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jianye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitham</forename><forename type="middle">Bou</forename><surname>Ammar</surname></persName>
						</author>
						<title level="a" type="main">An Empirical Study of Assumptions in Bayesian Optimisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by the increasing desire to efficiently tune machine learning hyper-parameters, in this work we rigorously analyse conventional and nonconventional assumptions inherent to Bayesian optimisation. Across an extensive set of experiments we conclude that: 1) the majority of hyperparameter tuning tasks exhibit heteroscedasticity and non-stationarity, 2) multi-objective acquisition ensembles with Pareto-front solutions significantly improve queried configurations, and 3) robust acquisition maximisation affords empirical advantages relative to its non-robust counterparts. We hope these findings may serve as guiding principles, both for practitioners and for further research in the field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although achieving significant success in numerous applications <ref type="bibr" target="#b7">(Bobadilla et al., 2013;</ref><ref type="bibr" target="#b44">Litjens et al., 2017;</ref><ref type="bibr" target="#b21">Fatima &amp; Pasha, 2017;</ref><ref type="bibr" target="#b31">Kandasamy et al., 2018;</ref><ref type="bibr" target="#b15">Cowen-Rivers et al., 2020)</ref>, the performance of machine learning models chiefly depends on the correct setting of hyper-parameters. As models grow larger and more complex, efficient and autonomous hyper-parameter tuning algorithms become crucial determinants of performance. To this end, a variety of methods from black-box and multi-fidelity optimisation have been adopted <ref type="bibr" target="#b30">(Kandasamy et al., 2017;</ref><ref type="bibr" target="#b58">Sen et al., 2018</ref>) with varying degrees of success. Techniques such as Bayesian optimisation (BO), for example, enable sample efficiency (in terms of black-box evaluations) at the expense of high computational demands, while "unguided" bandit-based approaches can fail to converge <ref type="bibr" target="#b20">(Falkner et al., 2018)</ref>. Identifying such failure modes, the authors in <ref type="bibr" target="#b20">(Falkner et al., 2018)</ref> built on <ref type="bibr" target="#b42">(Li et al., 2017)</ref> to propose a combination of * Equal contribution 1 Huawei R&amp;D London 2 Work completed during an internship at Huawei R&amp;D London 3 University College London 4 Honorary position at University College London. Correspondence to: Alexander I. Cowen-Rivers &lt;alexander.cowen.rivers@huawei.com&gt;.</p><p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). bandits and BO that achieves the best of both worlds; fast convergence and computational scalability. Though impressive, such successes of BO and alternatives, conceal a set of restrictive modelling and acquisition function assumptions that are hindering the widespread adoption of BO. We begin by describing these assumptions.</p><p>Modelling Assumptions: Critical to BO performance is a set of data modelling assumptions that admit an effective probabilistic model of the true black-box objective (e.g., validation loss in hyper-parameter tuning tasks). This model should not only provide accurate point estimates, but should also maintain calibrated uncertainty estimates to guide exploration of the objective. Amongst many possible surrogates <ref type="bibr" target="#b63">(Springenberg et al., 2016;</ref><ref type="bibr" target="#b29">Hutter et al., 2011)</ref>, Gaussian processes (GPs) are the default choice due to their flexibility and sample efficiency. Growing interest in applications of Bayesian optimisation has catalysed significant engineering feats enhancing the scalability and training efficiency of GP surrogates by exploiting graphical processing units <ref type="bibr" target="#b37">(Knudde et al., 2017;</ref><ref type="bibr" target="#b3">Balandat et al., 2020)</ref>.</p><p>Similar to any other framework, the correct specification of a GP model is dictated by the data modelling assumptions imposed by the user. For instance, a homoscedastic GP suffers when asked to model data with heteroscedastic noise whilst stationary GPs fail to track non-stationary targets. Of course, the aforementioned shortcomings are not unnatural in real-world problems but arise precisely in tasks related to hyper-parameter tuning of machine learning algorithms as depicted in our tests in Section 3.1. Hence, even if one improves computational efficiency, commonlymade assumptions such as homoscedasticity and stationarity may easily hinder any tuning algorithm's performance. Despite the importance of these assumptions in practice, GPs that presume homoscedasticity and stationarity are typically taken at face value and implemented as is.</p><p>Acquisition Function &amp; Optimiser Assumptions: Modelling choices like those mentioned above are not unique to the fitting procedure but rather transcend to other pivotal steps of hyper-parameter tuners. Precisely, given a model that adheres to some (or all) assumptions mentioned above, the second step involves maximising an acquisition function to query novel input locations that are then evalu-arXiv:2012.03826v3 <ref type="bibr">[cs.</ref>LG] 12 Feb 2021 ated. Hence, practitioners introduce additional constraints relating to the category of optimisation variables and the choice of acquisitions. When it comes to variable types, main-stream implementations <ref type="bibr" target="#b37">(Knudde et al., 2017;</ref><ref type="bibr" target="#b3">Balandat et al., 2020)</ref> assume continuous domains and employ first and second-order optimisers e.g., LBFGS <ref type="bibr" target="#b45">(Liu &amp; Nocedal, 1989</ref>) and ADAM <ref type="bibr" target="#b33">(Kingma &amp; Ba, 2015)</ref> to determine queried points. Real-valued configurations cover but a subset of possible machine learning hyper-parameters rendering discrete types, like hidden layer size in deep networks, out of scope. Moreover, from the point of view of acquisition functions, libraries tend to presuppose that one unique acquisition performs best in a given task, limiting benefits that can arise from a combined solution as we demonstrate in Section 5.</p><p>Contributions: Having identified essential modelling choices in BO, our goal in this paper is to provide empirical insight into their effect on experimental performance with the aim of informing the community on best practices for hyper-parameter tuning. We wish for our findings to apply to a broad array of tasks and datasets, be attentive to the effect of random initialisation on algorithmic performance, and naturally, be reproducible. As such, we prefer to build on already established benchmark packages, especially those that enable fast and scalable evaluations sanctioning multiseeding protocols. To that end, we undertake our evaluation in 2140 experiments from 108 real-world problems from the UCI repository <ref type="bibr" target="#b17">(Dua &amp; Graff, 2017)</ref>, which also featured as a testbed in the NeurIPS 2020 black-box optimisation challenge. Our empirical findings point towards the following conclusions:</p><p>1. Hyper-parameter tuning of machine learning tasks exhibit significant levels of heteroscedasticity and nonstationarity;</p><p>2. Applying input-warping and output transformation mitigates these effects giving rise to more well-behaved tuners with higher mean, and median performance across all 108 black-box functions under examination;</p><p>3. Individual acquisition functions tend to conflict in their solution (i.e., an optimum for one can be a bad point for the other and vice versa). Using a multi-objective formulation significantly improves performance;</p><p>4. Targeting robust formulations of acquisitions admit better tuners.</p><p>To further solidify our conclusions, we conducted additional ablation studies, realising a ranked order of importance in significant components. We found that input and output warping (conclusion 2) and multi-objective acquisitions (conclusion 3) led to the most significant improvements followed by the robustness of acquisitions (conclusion 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Standard Design Choices in BO</head><p>As discussed earlier, the problem of hyper-parameter tuning can be framed as an instance of black-box optimisation:</p><p>arg max</p><formula xml:id="formula_0">x∈X f (x),<label>(1)</label></formula><p>with x denoting a configuration choice, X a (potentially) mixed design space, and f (x) a validation accuracy we wish to maximise.</p><p>In this paper, we focus on BO as a solution concept for black-box problems of the form depicted in Equation 1. BO considers a sequential decision approach to the global optimisation of a function f : X → R over a bounded input domain X . At each decision round, i, the algorithm selects a collection of q inputs x 1:q ). The goal is to rapidly approach the maximum x = arg max x∈X f (x). Since both f (·) and x are unknown, solvers need to trade off exploitation and exploration during this search process.</p><p>To achieve this goal, BO algorithms operate in two steps. In the first, a Bayesian model is learned, while in the second an acquisition function determining new query locations is maximised. Next, we survey frequently-made assumptions in mainstream BO implementations and contemplate their implications on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Modelling Assumptions</head><p>When black-boxes are real-valued, Gaussian process regression <ref type="bibr" target="#b55">(Rasmussen &amp; Williams, 2006)</ref> are effective surrogates due to their flexibility and ability to maintain calibrated uncertainty estimates. In established implementations of BO, designers place GP priors on latent functions, f (·), which are fully specified through a mean function, m(x), and a covariance function or kernel k θ (x, x ) with θ representing kernel hyper-parameters. The model specification is completed by defining a likelihood. Here, practitioners typically assume that observations y l adhere to a Gaussian noise model such that y l = f (x l ) + l where l ∼ N (0, σ 2 noise ). This, in turn, generates a Gaussian likelihood of the form y l |x l ∼ N (f l , σ 2 noise ) where we use f l to denote f (x l ) with f (x) ∼ GP(m(x), k θ (x, x )). Additionally, a further design choice commonly made by practitioners is that the GP kernel is stationary, depending only on the norm between x and x , ||x − x ||. From this exposition, we conclude two important modelling assumptions stated as data stationarity and homoscedasticity of the noise distribution. If the true latent process does not adhere to these assumptions, the resultant model will be a poor approximation to the black-box. Realising the potential empirical effects of these modelling choices, we identify the first two questions of this paper: Q.I. Are parameter tuning tasks stationary and homoscedastic ?</p><p>Q.II. How do inexact modelling assumptions effect BO performance ?</p><p>In Section 3.1, we show that even the simplest among machine learning tasks pass tests of heteroscedasticity and nonstationarity and that overlooking such factors (Section 3.2) can quickly deteriorate performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Acquisitions &amp; Optimisation Assumptions</head><p>Acquisition functions trade off exploration and exploitation by utilising statistics from the posterior p θ (f (·)|D) with D denoting data (parameter configurations as inputs and validation accuracy as outputs) collected so far. Under a GP surrogate, such a posterior is Gaussian itself. To simplify the exposition, we defer the exact equations to Appendix C.1 and just note that p(f (x 1:q )|D) = N (µ θ (x 1:q ), Σ θ (x 1:q )). In this paper, we focus on three widely-used myopic acquisition functions which in a reparameterised form can be written as <ref type="bibr" target="#b66">(Wilson et al., 2017)</ref>:</p><formula xml:id="formula_1">Expected improvement (EI): α θ EI (x 1:q |D) = E post. max j∈1:q {ReLU(f (x j ) − f (x + ))} ,</formula><p>where the subscript post. is the predictive posterior of a GP <ref type="bibr" target="#b55">(Rasmussen &amp; Williams, 2006</ref>), x j is the j th vector of x 1:q , and x + is the best performing input in the data so far.</p><p>Probability of improvement (PI):</p><formula xml:id="formula_2">α θ PI (x 1:q |D) = E post. max j∈1:q {1 1{f(x j ) − f (x + )}} ,</formula><p>where 1 1{·} is the left-continuous Heaviside step function.</p><p>Upper confidence bound (UCB):</p><formula xml:id="formula_3">α θ UCB (x j ) = E post. max j∈1:q µ θ (x j ) + βπ /2|γ θ (x j )| ,</formula><p>where µ θ (x j ) is the posterior mean of the predictive distribution and γ θ (x j ) = f (x j ) − µ θ (x j ).</p><p>When it comes to practicality, generic BO implementations make additional assumptions during the acquisition maximisation step. First, it is assumed that one of the aforementioned acquisitions works best for a specific task, and that the GP model is an accurate approximation to the black-box. However, when it comes to real-world applications, both of these assumptions are hard to validate; the best-performing acquisition is challenging to identify up-front and GP models can easily be misspecified. With this in mind, we identify a further question that we wish to answer:</p><p>Q.III. Can acquisition function solutions conflict in hyperparameter tuning tasks ?</p><p>In the next section, we affirm that acquisitions can conflict even on the simplest of machine learning tasks. Moreover, we show that a robust formulation to tackle misspecification of acquisition maximisation can improve overall performance (see Section 4.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Analysis &amp; Question Answering</head><p>Before proceeding to potential improvements to BO, we now detail several tests we conducted to answer the questions (Q.I., Q.II., and Q.III.) posed in the previous section. Our analysis indicates: Experimental Setting: We create a wide range of hyperparameter tasks problems across a variety of classification and regression problems. It contains nine models including but not limited to multi-layer perceptrons, decision trees, random forests, and support-vector machines, six datasets (two regression and four classification) from the UCI repository, and two metrics per dataset such as negative log-likelihood or mean squared error. Each of the models possesses tuneable hyper-parameters, e.g., the regularisation constant of a support-vector machine, or the number of units of a neural network. The goal is to fit those hyperparameters so as to maximise/minimise one of the metrics previously mentioned. Of course, such black-box values are stochastic with randomness originating from various sources, e.g., train-test splits to compute validation losses, and random seeds of optimisation algorithms. Experimentation was facilitated by the Bayesmark packaged.</p><p>As a solver, we wish to adopt BO but note that we also experimented with other approaches such as HyperOpt <ref type="bibr" target="#b4">(Bergstra et al., 2015)</ref>. Our findings in Section 5 demonstrate that a Bayesian optimiser with improved design choices can achieve state-of-the-art results on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Answer A.I.: Simple Tasks are Complex</head><p>To provide answers to Q.I., we run tests for heteroscedasticity and non-stationarity. Each of these tests is run on every task combination (i.e., model, data, metric) with 50 unique hyper-parameters. We repeat each ten folds while fixing the hyper-parameter configuration.</p><p>Heteroscedasticity Tests: To gauge heteroscedasticity, we use Fligner and Killeen <ref type="bibr" target="#b22">(Fligner &amp; Killeen, 1976)</ref> and Lev-ene <ref type="bibr" target="#b41">(Levene, 1960)</ref> tests. It was shown in <ref type="bibr" target="#b14">(Conover et al., 1981)</ref> that both these tests are reliable in terms of robustness to departures from normality <ref type="bibr" target="#b51">(Park &amp; Lindsay, 1999)</ref>. Both test the null hypothesis that the underlying black-box function (e.g., validation accuracy (hyper-parameter i )) is homoscedastic. In all 108 tests, we see a p-value significantly lower than 0.05 in 72 tasks using Levene's test, and in 73 tasks using Fligner and Killeen. Such results (shown in detail in Appendix B) imply that at least 66% of the experimental tasks exhibit heteroscedastic behaviour. Additionally, <ref type="figure" target="#fig_6">Figure 6</ref> illustrates the heteroscedasticity of several tasks showing drastic changes of the observed noise level around the mean validation accuracy (hyper-parameter i ) for varying values of hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Stationarity Assessment:</head><p>To test for non-stationarity we compare the fit of a vanilla Gaussian Process surrogate model equipped with a stationary kernel (Matern32) to a non-stationary kernel (Spectral Mixture). We took 32 unique hyper-parameters and their respective evaluation score for training and 32 for held-out testing. Over 108 tasks, averaged over five random seeds, we see in ≈ 30% of tasks that the non-stationary kernel offers significantly better log probability on the held-out test set vs the stationary kernel, implicitly suggesting that handling non-stationarity is beneficial for the GP surrogate. See <ref type="table" target="#tab_5">Table 4</ref> in the Appendix for detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Answers A.II.: Modelling Assumptions Matter</head><p>In answer to Q.II., we offer a first indication that data modelling assumptions matter through a comparison of approaches affording progressively better-specified models for black-box objectives. We evaluate a simple random search strategy along with four BO methods. These algorithms serve a dual-purpose as the set of baselines against which we compare our solution in section 5. SkOpt (Pedregosa et al., 2011) utilises homoscedastic, non-stationary GPs, HyperOpt <ref type="bibr" target="#b4">(Bergstra et al., 2015)</ref> and BOHB <ref type="bibr" target="#b20">(Falkner et al., 2018)</ref> allow for some relaxation of the assumptions of SkOpt whilst TuRBO <ref type="bibr" target="#b19">(Eriksson et al., 2019b)</ref> employs local modelling capable of mitigating against the effects of heteroscedasticity and non-stationarity.</p><p>We consider 108 task combinations and apply standard batch BO for 16 iterations while generating 8 query points per BO step. We repeat each task across 20 random seeds amounting to 2140 experiments. To gauge performance, we use the normalised task score (as detailed in Appendix 3) to compare across tasks. We hypothesise that as modelling assumptions increase in sophistication, we would improve our modelling capabilities and improve performance, validated by <ref type="figure" target="#fig_1">Figure 1</ref> with TuRBO performing best at around 6% improvement in mean normalised score compared to random search. We attribute this to the fact that TuRBO uses the most advanced modelling assumptions amongst tested algorithms. Having local models appears to reduce the effects of heteroscedasticity and non-stationarity. BOHB tended to underperform in our experiment, which we attribute to the fact that BOHB was designed explicitly for multi-fidelity optimisation in deep networks focusing on computational time reduction without explicit support for the batch BO setting.</p><p>In Section 5, we further highlight the importance of modelling choices by demonstrating that models capable of specifying heteroscedastic noise and non-stationary latent functions can achieve an additional 3% gain in mean and high quantiles and up to 4% increase in low normalised score quantiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Answer A.III.: A No Clear Winner</head><p>To answer Q.III. of whether acquisition functions can conflict in their solutions, we collect 32 samples from each task by evaluating various hyper-parameter configurations across different metrics. We then gather a dataset</p><formula xml:id="formula_4">D = {hyper-param i , y i } 32 i=1 ,</formula><p>where hyper-param i is a vector whose dimensionality depends on the number of hyper-parameters in a given model, and y i corresponds to an evaluation metric, e.g., mean squared error (hyper-param i ). We then fit a GP surrogate model and consider each of the three acquisitions from Section 2.2. We visualise all three acquisition functions per hyperparameter dimension and report partial results in <ref type="figure">Figure 2</ref> with a full set presented in Appendix D. From <ref type="figure">Figure 2</ref>, we realise that in some instances, acquisition functions not only provide different solutions, but those solutions can in fact conflict. To elaborate consider the Wine dataset using an AdaBoost model (i.e., left figure in 2). Here, a PI acquisition function deems </p><note type="other">PI UCB EI Max PI Max UCB Max</note><p>Acquisition Value <ref type="figure">Figure 2</ref>. Examples depicting conflicting acquisitions across varying datasets (Wine, Boston Housing, and Iris) and models (AdaBoost, Multi-Layer perceptron, K-Nearest neighbours, and support vector machines).</p><p>a value of 6 for the learning rate as optimal, while EI considers that same value as an unlikely solution. It is worth noting that a learning rate in an AdaBoost model weighs ensemble estimators' contribution rather than directly dictating the descent direction, thus explaining large domain values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimising Bayesian Optimisation</head><p>Now, we elaborate on more general design choices that prove empirically effective. Some of the forthcoming remedies have been proposed elsewhere in settings beyond BO, while acquisition function robustness is unique to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tackling Heteroscedasticity and Non-Stationarity</head><p>To enable flexible modelling solutions capable of handling heteroscedasticity and non-stationarity, we apply ideas from the warped GP literature <ref type="bibr" target="#b60">(Snelson et al., 2004)</ref> where output transformations allow for more complex noise processes. We found that the Box-Cox <ref type="bibr" target="#b9">(Box &amp; Cox, 1964)</ref> and <ref type="bibr">Yeo-Jonhson (Yeo &amp; Johnson, 2000)</ref> output transformations and a Kumaraswamy <ref type="bibr" target="#b39">(Kumaraswamy, 1980)</ref> input warping offered a balance of simplicity in implementation and empirical performance. In fact, in our ablation study (Section 5), we demonstrate that the addition of these two modelling components resulted in the largest performance gain.</p><p>Output Transformation for Heteroscedasticity: We consider the Box-Cox transformation typically used to map non-Gaussian data closer to "Gaussianity". The class of transforms depend on a tuneable parameter ζ and apply the following map to each of the labels: T ζ (y l ) = y ζ l − 1/ζ for ζ = 0 and T ζ (y l ) = log y l if ζ = 0, where in our case y l denotes a validation accuracy of the l th hyper-parameter configuration. Of course, ζ has to be fit based on the observed data such that the distribution of the transformed labels closely resembles a Gaussian. This is achieved by minimising the negative of the Box-Cox likelihood function:</p><formula xml:id="formula_5">log n l=1 (T ζ (y l ) − T ζ (y)) 2 n n 2 + n l=1 log [T ζ (y l )] (1−ζ) ,</formula><p>where n is the number of datapoints and T ζ (y) is the sample mean of the transformed labels. Of course, Box-Cox transforms only considers strictly positive (or strictly negative) labels y l . To handle a more general setting, we also make use of the Yeo-Johnson transform <ref type="bibr" target="#b67">(Yeo &amp; Johnson, 2000)</ref>. Due to space constraints we defer the details to Appendix C.2.</p><p>An important property we also considered when proposing such transformations is ease of implementation. For both transforms mentioned above, efficient implementations are readily available in various libraries such as Scikit-learn <ref type="bibr" target="#b53">(Pedregosa et al., 2011)</ref> within the PowerTransform package where the Brent optimiser <ref type="bibr" target="#b10">(Brent, 2013)</ref> is used for determining ζ .</p><p>Input Transformations for Non-Stationarity: Having dealt with heteroscedasticity, now we shift our attention to tackle data non-stationarity. As a general solution concept, we consider input warping see <ref type="bibr" target="#b61">(Snoek et al., 2012)</ref>. In our implementation, we relied on a Kumaraswamy input warping, which executes the following for each of the input dimensions:</p><formula xml:id="formula_6">[Kumaraswamy γ (x l )] k = 1−(1 − [x l ] a k k ) b k ∀k ∈ [1 : d],</formula><p>with d being the dimensionality of the decision variable (e.g., number of free hyper-parameters), a k and b k are tuneable warping parameters for each of the dimensions, and γ is a vector concatenating all free parameters, i.e., γ = [a 1:d , b 1:d ] T . Of course, γ is fit based on observed data. Similar to <ref type="bibr" target="#b3">(Balandat et al., 2020)</ref>, we consider γ as part parameters of the marginal likelihood that we optimise while fitting our GP.</p><p>All Modelling Improvements Together: Combining the above considerations of heteroscedasticity and nonstationarity leads us to an improved GP model with more flexible capabilities. The implementation of such a model is relatively simple and involves maximising a new marginal that can be written as:</p><formula xml:id="formula_7">max θ,γ − 1 2 T ζ (y) T (K γ θ + σ noise I) −1 T ζ (y) − 1 2 |K γ θ + σ 2 noise I| − const,</formula><p>where θ are GP hyper-parameters, γ corresponds to nonstationary transforms, and ζ denotes the solution to a Box-Cox likelihood objective. It is worth noting that we used Box-Cox as a running example but as mentioned previously we interchange Box-Cox with Yeo-Johnson transforms based on the properties of the label y l . Moreover, we used K γ θ ∈ R n×n to represent a matrix such that each entry depends on both θ and γ, where k γ θ (x, x ) = k θ (Kumaraswamy γ (x), Kumaraswamy γ (x )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Tackling Acquisition Conflict &amp; Robustness</head><p>Having proposed modifications to the modelling process of BO, we now concentrate on the acquisition maximisation step. In particular, we handle two problems, one related to the assumption of a perfect GP surrogate, with the second centred around conflicting acquisitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">A ROBUST ACQUISITION OBJECTIVE</head><p>As mentioned in Section 2.2, the acquisition maximisation step assumes that adequate surrogates are readily available. Especially during early rounds of training, such a property is hard to validate as data is scarce leading to (sometimes) severe model misspecification that hurts performance. One way to tackle such a problem is to assume a robust formulation <ref type="bibr" target="#b35">(Kirschner et al., 2020;</ref><ref type="bibr" target="#b36">Klein et al., 2017</ref>) that attempts to find the best performing query location under the worstcase GP model, i.e., solving max x min θ α θ (x|D). Granted such a formulation allows for a solution x that is robust to worst-case misspecification in θ, having a max min acquisition is troubling for several reasons. From a conceptual perspective max min formulations are known to lead to very conservative solutions if not correctly constrained or regularised since the optimiser possesses the power to impair the GP fit while updating θ 1 . From an implementation perspective, one faces two further issues. First, no global convergence guarantees are known for the non-convex nonconcave case that we face <ref type="bibr" target="#b43">(Lin et al., 2020)</ref>, and second, ensuring gradients can propagate through the computation graph restrict surrogates and acquisition functions to be within the same programming framework.</p><p>To avoid worst-case solutions and enable independence between acquisition functions and surrogate models, we borrow ideas from domain randomisation <ref type="bibr" target="#b64">(Tobin et al., 2017)</ref> and consider an expected formulation instead: max x α rob. (·) ≡ max x E ∼N (0,σ 2 I) α θ+ (x|D) . Importantly, this problem seeks to find new query locations that perform well on average over a distribution of surrogate 1 Of course, one can argue augmenting the objective with a constraint such that θ updates remain close to θ of the marginal. The value by which such proximity needs to be enforced remains unclear in robust literature to date <ref type="bibr" target="#b1">(Abdullah et al., 2019;</ref><ref type="bibr" target="#b35">Kirschner et al., 2020)</ref>. models rather than assuming a perfect surrogate.</p><p>Though appealing, our formulation still assumes access to the GP's hyper-parameters, complicating implementation and restricting models and optimisers to the same programming paradigm. We wish to enable robustness by only having access to the GP's mean and variance predictions for simplicity. Fortunately, we are able to show that upon a simple acquisition perturbation one can approximate the above α robt (·). Namely, we prove (see Appendix C) that if α θ (x|D) = α θ (x|D) + ησ n with η ∼ N (0, 1), then upon further technical details 2 we have (with high probability):</p><formula xml:id="formula_8">α θ (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D) ≤ ρ,</formula><p>for any arbitrary ρ ∈ (0, 1).</p><p>The bound above enables uncomplicated yet effective, robust implementations. For instance, if we would like to "robustify" a UCB acquisition, we add ησ n to the posterior's mean and follow standard UCB onwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">MULTI-OBJECTIVE ACQUISITIONS</head><p>As a final component of our general framework, we propose the usage of multi-objective acquisitions and seek a Paretofront solution concept. This formulation enables a form of "hedging" between different acquisitions such that any other acquisition does not dominate the solution, thus representing the best solution trade-off <ref type="bibr" target="#b47">(Lyu et al., 2018)</ref>. Formally, we solve:</p><formula xml:id="formula_9">max x ᾱ θ EI (x|D),ᾱ θ PI (x|D),ᾱ θ UCB (x|D) ,<label>(2)</label></formula><p>whereᾱ θ type (x|D) is a robust acquisition as introduced in the previous section and type ∈ {EI, PI, UCB}. We also note that our formulation is meant to reflect the fact of using a robust objective value of α θ (x|D) = α θ (x|D) + η k σ n with η k being a sample from N (0, 1) at each iteration of the evolutionary solver.</p><p>Although solving the problem in Equation 2 is a formidable challenge, we note the existence of many mature multiobjective optimisation algorithms. These range from firstorder <ref type="bibr" target="#b34">(Kingma &amp; Welling, 2014)</ref> to zero-order <ref type="bibr" target="#b46">(Loshchilov &amp; Hutter, 2016;</ref><ref type="bibr" target="#b23">Gabillon et al., 2020)</ref> and evolutionary methods <ref type="bibr" target="#b27">(Hansen, 2016;</ref><ref type="bibr" target="#b16">Deb et al., 2002)</ref>. Due to the discrete nature of hyper-parameters in machine learning tasks, we advocate using evolutionary solvers that naturally handle categorical and integer-valued variables. In our experiments, we employ the non-dominated sorting genetic algorithm II (NSGA-II) that allows for mixed variable crossover and mutation to optimise real and integer input types <ref type="bibr" target="#b16">(Deb et al., 2002)</ref>. Importantly, an available stable implementation of NSGA-II along with other solvers can be found in the latest release of Pymoo <ref type="bibr" target="#b6">(Blank &amp; Deb, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Performance Experimentation</head><p>In this section, we continue our empirical evaluation and validate gains (if any) that arise from the improvements proposed in the previous section. Our setting exactly matches that described in Section 3 in which we consider 108 hyperparameter tuning tasks repeated over 20 random seeds, and utilise normalised score as the evaluation metric.</p><p>We also experiment with a wider range of solvers that either rely on BO-strategies or follow zero-order techniques such as differential evolution or particle swarms. These include pySOT a parallel global optimisation package <ref type="bibr" target="#b18">(Eriksson et al., 2019a)</ref>, OpenTuner a package allowing for ensemble of methods <ref type="bibr" target="#b2">(Ansel et al., 2014)</ref>, and NeverGrad (Rapin &amp; Teytaud, 2018) a gradient-free optimisation toolbox. Additionally, we carried our modelling improvements to TuRBO, augmenting the standard GP with remedies from Section 4.1 effectively producing a new baseline that we entitle TuRBO+. <ref type="figure" target="#fig_3">Figure 3</ref> demonstrates results showing gains from adopting the general framework, which we designate as G-BO in the plots. In <ref type="figure" target="#fig_3">Figure 3</ref> (a), we compare G-BO against other baselines and report up to 8% performance gain compared to a random search strategy. It is also worth noting that TuRBO+ tended to underperform, achieving ca. 4% improvement relative to random search. We believe such a result is related to the interplay between our approach's capabilities to address heteroscedasticity and non-stationarity as well as the size of the trust-regions; an interesting avenue that we plan to explore in future work. Interestingly, NeverGrad, a model-free solution, also underperformed attaining less than 2% gain to random search hinting at the importance of a model-based solution. We also elaborate further results in <ref type="figure">Figure 4</ref> across multiple datasets and models showing that G-BO achieves highest normalised mean scores in 37 out of 54 datasets in total. Further results on all tasks can be found in Appendix A.3.</p><p>Ablation Results: Although previous results demonstrate significant advantages of a holistic BO framework, G-BO introduces multiple ingredients affecting performance. To better understand each addition's contribution, we conducted an ablation study by removing major components and testing those remaining. Specifically, we removed each of the heteroscedastic, non-stationary, robustness, and the multiacquisition parts, ran BO with the remaining ingredients, and reported average normalised scores and % improvement to random search in <ref type="figure" target="#fig_3">Figure 3</ref> (b). First, it is clear that removing any of the components affects performance with some causing about 1% decrease. Second, such a result sheds light on an order of importance between those constituents: Output-Warping ≥ Multi-Objective Acquisitions ≥ Input-Warping ≥ Robustness, where it is be understood that ≥ reflects performance precedence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>We introduce work on the following topics relating to modelling, acquisition and optimisers in Bayesian optimisation:</p><p>Heteroscedasticity with output transforms: Among various approaches to handling heteroscedasticity <ref type="bibr" target="#b32">(Kersting et al., 2007;</ref><ref type="bibr" target="#b40">Lázaro-Gredilla &amp; Titsias, 2011;</ref><ref type="bibr" target="#b38">Kuindersma et al., 2013;</ref><ref type="bibr" target="#b11">Calandra, 2017;</ref><ref type="bibr" target="#b26">Griffiths et al., 2021)</ref>, transforming the output variables is a straightforward option giving rise to warped Gaussian processes <ref type="bibr" target="#b60">(Snelson et al., 2004)</ref>. More recently, output transformations have been extended to compositions of elementary functions <ref type="bibr" target="#b57">(Rios &amp; Tobar, 2019)</ref>  Breast MLP-SGD Normalised Score <ref type="figure">Figure 4</ref>. G-BO against all baselines. Each experiment repeated with 20 random seeds. We averaged each seed over both metrics to examine the 54 distinct black-box functions. G-BO achieves the highest normalised mean score in optimising 68.5% of the black-box functions. We highlight 8 of the 54 tasks above, and show the remaining in the Appendix. 2015; <ref type="bibr" target="#b48">Maronas et al., 2020)</ref>. Output transformations have not featured prominently in the Bayesian optimisation literature, perhaps due to the commonly-held opinion that warped GPs require more data relative to standard GPs in order to function as effective surrogates <ref type="bibr" target="#b49">(Nguyen &amp; Osborne, 2020)</ref>. Rather than introducing additional hyper-parameters to the GP, we enable efficient output warping through methods that only require pre-training.</p><p>Non-stationarity with input warpings: Many surrogate models with input warpings exist for optimising nonstationary black-box objectives <ref type="bibr" target="#b62">(Snoek et al., 2014;</ref><ref type="bibr" target="#b12">Calandra et al., 2016a;</ref><ref type="bibr" target="#b50">Oh et al., 2018</ref>) and have enjoyed particular success in hyperparameter tuning where the natural scale of parameters is often logarithmic. Traditionally, a Beta cumulative distribution function is used. In this paper, we adopt the Kumaraswamy warping which is another instance of the generalised Beta class of distributions which we have observed to achieve superior performance to <ref type="bibr" target="#b62">(Snoek et al., 2014)</ref>; confirming results reported in <ref type="bibr" target="#b3">(Balandat et al., 2020)</ref>.</p><p>Multi-objective acquisition ensembles: Multi-objective acquisition ensembles were first proposed in <ref type="bibr" target="#b47">(Lyu et al., 2018)</ref> and are closely related to portfolios of acquisition functions <ref type="bibr" target="#b28">(Hoffman et al., 2011;</ref><ref type="bibr" target="#b59">Shahriari et al., 2014;</ref><ref type="bibr" target="#b3">Balandat et al., 2020)</ref>. In this form, the optimisation problem involves at least two conflicting and expensive blackbox objectives and as such, solutions are located along the Pareto-efficient frontier. The multi-objective acquisition ensemble employs these ideas to find a Pareto-efficient solution amongst multiple acquisition functions. Although we utilised the multi-objective acquisition ensemble, we note that our framework is solver agnostic in so far as any multi-objective optimiser <ref type="bibr" target="#b0">(Abdolshah et al., 2019</ref>) may be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness of Acquisitions:</head><p>Methods achieving robustness with respect to either surrogates <ref type="bibr" target="#b52">(Park et al., 2020)</ref> or the optimisation process <ref type="bibr" target="#b8">(Bogunovic et al., 2018;</ref><ref type="bibr" target="#b5">Bertsimas et al., 2010)</ref> have been previously proposed. Most relevant to our setting, is the approach of <ref type="bibr" target="#b8">(Bogunovic et al., 2018)</ref> that introduces robustness to BO by solving a max min objective to determine optimal input perturbations. Their method, however, relies on gradient ascent-descent-type algorithms that require real-valued variables and are not guaranteed to converge in the general non-convex, non-concave setting <ref type="bibr" target="#b43">(Lin et al., 2020)</ref>. On the other hand, our solution possesses two advantages: 1) simplicity of implementation as we merely require random perturbations of acquisition functions to guarantee robustness, and 2) support for mixed variable solutions through the use of evolutionary solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion &amp; Future Work</head><p>In this paper, we presented an in-depth empirical study of Bayesian optimisation. We demonstrated that even the simplest among machine learning problems can exhibit heteroscedasticity and non-stationarity. We also reflected on the effects of misspecified models and conflicting acquisition functions. We augmented BO algorithms with various enhancements and revealed that with a revised set of assumptions BO can in fact act as a competitive baseline in hyper-parameter tuning. We hope this paper's findings can guide the community when employing BO in practice.</p><p>In the future, we wish to extend our analysis to highdimensional domains and consider latent space optimisation <ref type="bibr" target="#b65">(Tripp et al., 2020)</ref> in contexts beyond hyper-parameter tun-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Black-Box Functions</head><p>As specified in Section 3, we evaluate black-box optimisation solvers based on their performances on a large set of tasks from Bayesmark package. Each task consists in optimising the hyperparameters of a model to minimise the cross validation loss incurred when this model is applied to perform a regression (reg) or a classification (clf) on a given dataset. Thus, a task is characterised by a model, a dataset and a loss function, or metric, measuring the quality of the regression or classification. In total, 108 distinct tasks can be defined from the valid combination of the nine models specified in <ref type="table">Table 1</ref>, the following six real-world UCI datasets <ref type="bibr" target="#b17">(Dua &amp; Graff, 2017)</ref>, Boston (reg), Breast Cancer (clf), Diabetes (reg), Digits (clf), Iris (clf) and Wine (clf); the following two regression metrics, negative mean-squared error (MSE), negative mean absolute error (MAE), and two classification metrics, negative log-likelihood (NLL) and negative accuracy (ACC). The results reported on Figures 3 and 4 have been obtained by applying each black-box optimisation method using 16 iterations of 8-batch acquisition steps on all of the 108 tasks. In order to provide a reliable evaluation of the different solvers, we repeated each run with 20 random seeds and considered the normalised score given by:</p><formula xml:id="formula_10">Normalised Score = 100 × L − L * L rand − L *<label>(3)</label></formula><p>where L is the best-achieved cross validation loss at the end of the 16 acquisition steps, L * is the estimated optimal loss for the task and L rand is the mean loss (across multiple runs) obtained using random search with the same number of acquisition steps. The normalisation procedure allows to aggregate the scores across tasks although different crossvalidation loss functions were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Black-Box Optimisation Variables</head><p>We provide in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref> the list of the hyperparameters controlling the behaviour of each model along with their optimisation domains, which can differ whether the model is used for a classification or a regression task. The search domain can include a mix of continuous and integer variables (e.g. MLP-SGD set of hyperparameters includes a hidden layer size, which needs to be an integer, and an initial learning rate that can take any values between 10 −5 and 10 −1 ), and its dimensionality -which corresponds to the number of hyperparameters to tune -ranges from 2 to 9. We also specify in the last column of these Tables whether the search domain is modified through a standard transformation ( log or logit) to ease optimisation. <ref type="table">Table 3</ref> synthesises the performances achieved on the 108 tasks by the black-box optimisation solvers considered in our experiments. We note that the distribution of the scores attained by G-BO has the largest mean and the smallest standard-deviation, indicating significant out-performance of this method over alternative solvers. In complement, normalised score distributions obtained on each task by each black-box optimisers are provided on <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Implementation details on BOHB</head><p>BOHB is a scalable hyper-parameter tuning algorithm introduced in <ref type="bibr" target="#b20">(Falkner et al., 2018)</ref> mixing bandits and BO approaches to achieve strong anytime and final performances. Contrary to the other solvers considered in this paper, BOHB is specifically designed to tackle multi-fidelity optimisation and it uses Hyperband <ref type="bibr" target="#b42">(Li et al., 2017)</ref> routine to define the fidelity levels under which points are asynchronously evaluated. The selection of points follows a BO strategy based on Tree Parzen Estimator (TPE) method. Given a set D of already observed data points and a threshold α ∈ R, the TPE models p(x|y) using kernel density estimations of</p><formula xml:id="formula_11">(x) = p(y &lt; α|x, D) g(x) = p(y ≥ α|x, D).</formula><p>In the TPE algorithm, maximising the expected improvement criterion α EI (x) = max(0, α − p(y|x))p(y|x)dy is equivalent to maximising the ratio r(x) = (x) g(x) which is carried out to select a single new candidate point at a time.</p><p>In the absence of a multi-fidelity setup in our experiments, we ran a modified version of the BOHB algorithm implemented in HpBandSter package. We left TPE method for modelling unchanged, but we ignored fidelity level assignment from Hyperband. Moreover, as our experimental setup involves batch-acquisitions, we tested two alternatives to standard BOHB acquisition procedure to support synchronous suggestion of multiple points. In a first approach, we run q independent maximisation processes of r(x) from random starting points and recover a single candidate from each process to form the q-batch suggestion. In a second approach, we obtain one point as a result of a single maximisation of r(x) and we sample q − 1 random points to complete the q-batch suggestion. As the latter method yields better overall performance, the results reported in this work under BOHB label have been obtained using the second approach. <ref type="table">Table 1</ref>. Search spaces for hyperparameter tuning on classification tasks. We specify the type of each hyperparameters (with R for real-valued and Z for integer valued)as well as the search domain. We specify log −U (resp. logit − U) to indicate that a log (resp. logit) transformation is applied to the optimisation domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parameter Type Domain    <ref type="table">Table 1</ref>).</p><formula xml:id="formula_12">kNN n neighbors Z U(1, 25) p Z U(1, 4) Support Vector Machine C R log −U(1, 10 3 ) gamma R log −U(10 −4 , 10 −3 ) tol R log −U(10 −5 , 10 −1 ) Decision Tree max depth Z U(1, 15) min samples split R logit − U(0.01, 0.99) min samples leaf R logit − U(0.01, 0.49) min weight fraction leaf R logit − U(0.01, 0.49) max features R logit − U(0.01, 0.99) min impurity decrease R U(0, 0.5) Random Forest max depth Z U(1, 15) max features R logit − U(0.01, 0.99) min samples split R logit − U(0.01, 0.99) min samples leaf R logit − U(0.01, 0.49) min weight fraction leaf R logit − U(0.01, 0.49) min impurity decrease R U(0, 0.5) MLP-Adam hidden layer sizes Z U(50, 200) alpha R log −U(10 −5 , 10 1 ) batch size Z U(10, 250) learning rate init R log −U(10 −5 , 10 −1 ) tol R log −U(10 −5 , 10 −1 ) validation fraction R logit − U(0.1, 0.9) beta 1 R logit − U(0.5, 0.99) beta 2 R logit − U(0.9, 1 − 10 −6 ) epsilon R log −U(10 −9 , 10 −6 ) MLP-SGD hidden layer sizes Z U(50, 200) alpha R log −U(10 −5 , 10 1 ) batch size Z U(10, 250) learning rate init R log −U(10 −5 , 10 −1 ) power t R logit − U(0.1, 0.9) tol R log −U(10 −5 , 10 −1 ) momentum R logit − U(0.001, 0.999) validation fraction R logit − U(0.1, 0.9) AdaBoost n estimators Z U(10, 100) learning rate R log −U(10 −4 , 10 1 ) Lasso C R log −U(10 −2 , 10 2 ) intercept scaling R log −U(10 −2 , 10 2 ) Linear C R log −U(10 −2 , 10 2 ) intercept scaling R log −U(10 −2 ,<label>10</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Parameter Type Domain</head><p>AdaBoost n estimators Z U(10, 100)  <ref type="table">Table 3</ref>. Mean and n-th percentile normalized scores over 108 black-box functions, each repeated with 20 random seeds. We see significant mean improvements from G-BO compared with all other algorithms.</p><formula xml:id="formula_13">learning rate R log −U(10 −4 , 10 1 ) Lasso alpha R log −U(10 −2 , 10 2 ) fit intercept Z U(0, 1) normalize Z U(0, 1) max iter Z log −U(10, 5000) tol R log −U(10 −5 , 10 −1 ) positive Z U(0, 1) Linear alpha R log −U(10 −2 , 10 2 ) fit intercept Z U(0, 1) normalize Z U(0, 1) max iter Z log −U(10, 5000) tol R log −U(10 −4 , 10 −1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Heteroschedasticity Testing</head><p>GP modelling usually considers a conditional normal distribution of the observations y|· ∼ N (f (·), σ 2 (·)). In most cases GP regression is run assuming σ(·) 2 to be constant, in which case the GP is called homoscedastic. To assess whether this assumption holds for the tasks under examination, we used Levine's test and Fligner and Killeen test.</p><p>To run these tests on a given task, we evaluated k = 50 distinct sets of hyperparameters {x i } 1≤i≤k for n = 10 times and obtained scores {Y ij } 1≤i≤k,1≤j≤n , where Y ij is the j th score observed when evaluating the i th configuration. For i = 1, . . . , k, let σ 2 i denote the observed variance of y|x i , then both Levine's and Fligner and Killeen tests share the same null hypothesis of homoscedasticity:</p><formula xml:id="formula_14">H 0 : σ 2 1 = · · · = σ 2 k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Levine's test</head><p>Levine's test statistic is defined as</p><formula xml:id="formula_15">W = N − k k − 1 · k i=1 n(Z i· −Z ·· ) 2 k i=1 n j=1 (Z ij −Z i· ) 2 where N = k × n, Z ij = |Y ij − 1 n n j=1 Y ij |,Z i· = 1 n n j=1 Z ij andZ ·· = 1 k k i=1Z i· , for all i = 1, . . . , k, j = 1, . . . , n. The Levene test rejects homoscedasticity hypothesis H 0 if W &gt; F α,k−1,N −k</formula><p>where F α,k−1,N −k is the upper critical value at a significance level α of the F distribution with k − 1 and N − k degrees of freedom.</p><p>Fligner and Killen test is an alternative to Levene's test that is particularly robust to outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Fligner &amp; Killen test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation of Fligner and Killen test requires ranking all the absolute values</head><formula xml:id="formula_16">{|Y ij −Ỹ i |} 1≤i≤k,1≤j≤n whereỸ i is the median of {Y ij } 1≤j≤n . Increasing scores a N,r = Φ −1 1+ r N +1 2</formula><p>are associated to each rank r = 1, . . . , N , where N = kn and Φ(·) is the cumulative distribution function for standard Normal variable. We denote the rank score associated to Y ij as r ij .</p><p>The Fligner and Killen test statistic is given by</p><formula xml:id="formula_17">χ 2 o = k i=1 n Ā i −ā 2 V 2 whereĀ i = 1 n n j=1 a N,rij ,ā = 1 N N r=1 a N,r and V 2 = 1 N −1 N r=1 (a N,r −ā) 2 .</formula><p>As χ 0 has asymptotic X 2 distribution with (k − 1) degrees of freedom, therefore the test rejects homoscedasticity hy-</p><formula xml:id="formula_18">pothesis H 0 if χ 0 &gt; X 2 α,k−1 where X 2 α,k−1 is the upper critical value at a significance level α of the X 2 distribution with k − 1 degrees of freedom.</formula><p>The results of Levine test as well as Fligner and Killen test are reported on <ref type="table">Table 10</ref>-15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Mathematical Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. GP posterior</head><p>Under a GP assumption with Gaussian-corrupted observations y = f (x ) + where ∼ N (0, σ 2 ), and given a set D = {x, y} of available data, the joint distribution of D and an arbitrary set of input points x 1:q is given by</p><formula xml:id="formula_19">y f (x 1:q ) θ ∼ N m(x) m(x 1:q ) , K θ + σ 2 I k θ (x 1:q ) k T θ (x 1:q ) k θ (x 1:q , x 1:q ) , where K θ = K θ (x, x) and k θ (x 1:q ) = k θ (x, x 1:q ).</formula><p>From this joint distribution one can derive though marginalisation <ref type="bibr" target="#b55">(Rasmussen &amp; Williams, 2006)</ref> the predictive poste-</p><formula xml:id="formula_20">rior p(f (x 1:q )|D) = N (µ θ (x 1:q ), Σ θ (x 1:q )) with: µ θ (x 1:q ) = m(x 1:q ) + k θ (x 1:q ) (K θ + σ 2 I) −1 (y − m(x)) Σ θ (x 1:q ) = K θ (x 1:q , x 1:q ) − k θ (x 1:q ) (K θ + σ 2 I) −1 k θ (x 1:q )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Yeo-Johnson Transform</head><p>When labels take on arbitrary values, we use a Yeo-Johnson transform instead of Box-Cox. Such a transformation operates as follows:</p><formula xml:id="formula_21">Y.J. ζ (y l ) =          (y l +1) ζ −1 ζ , if ζ = 0, y l ≥ 0 log(y l + 1), if ζ = 0, y l ≥ 0 (1−y l ) 2−ζ −1 ζ−2 if ζ = 2, y l &lt; 0 − log(1 − y l ) if ζ = 2, y l &lt; 0.</formula><p>Analogous to the Box-Cox transform, the Yeo-Johnson's parameter is fit based on observed data solving the following 1-dimensional optimisation problem:</p><formula xml:id="formula_22">max ζ − n 2 log n j=1 (Y.J. ζ (y l ) − Y.J. ζ (y)) 2 n − 1 + (ζ − 1) n i=1 [sign(y l ) log(|y l | + 1)] ,</formula><p>with Y.J. ζ (y) being the sample average computed after applying the Yeo-Johnson transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Robust Acquisition Objectives</head><p>Notice, the robust form of acquisition function given as</p><formula xml:id="formula_23">α rob. (x|D) ≡ E ∼N (0,σ 2 I) α θ+ (x|D)</formula><p>constitutes an intractable integral. Therefore, in order to be maximised during BO execution, it should be replaced with accurate approximation. Our next result establishes such arbitrary accurate approximation with high probability.</p><p>Lemma: Let δ ∈ (0, 1) be a desirable probability threshold, and ρ ∈ (0, 1) be a desirable accuracy parameter. Consider the GP process with mean function m(x) and covariance function k θ (x, x ) such that ∀x, x ∈ X , θ ∈ R p :</p><formula xml:id="formula_24">|k θ (x, x)| ≥ M 0 , |k θ (x, x )| ≤ M 1 , (4) ||∇ θ k θ (x, x )|| 2 ≤ M 2 , |m(x)| ≤ M 4 .</formula><p>Moreover, assume that observations y ∈ D are bounded, i.e. |y| ≤ C and let α θ (x|D) = α θ (x|D) + ησ n with η being standard normal random variable. Then, there are constants c 1 and c 2 , such that choosing σ n ≤ c 1 and σ ≤ c 2 :</p><formula xml:id="formula_25">α θ (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D) ≤ ρ.</formula><p>with probability at least 1 − δ.</p><p>Proof. Without loss of generality we chose UCB acquisition function α(x) θ (x|D) = α UCB (x) θ (x|D) and to avoid technical complications related to multivariate calculus we consider batch size q = 1. In this case, UCB acquisition function can be written as α UCB (x) θ (x|D) = µ θ (x|D) + βπ 2 σ θ (x|D), where µ θ (x|D) and σ θ (x|D) are posterior mean and posterior deviation respectively.</p><p>Consider an Monte-Carlo estimation of α rob. (x|D) ≡ E ∼N (0,σ 2 I) α θ+ (x|D) :</p><formula xml:id="formula_26">α θ (x|D) = 1 N N j=1 α θ+ j (x|D)</formula><p>where j are i.i.d. samples drawn from N (0, σ 2 I). Then, adding and subtractingα θ (x|D) gives:</p><formula xml:id="formula_27">α θ (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D) ≤ α θ (x|D) −α θ (x|D) + α θ (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D) .</formula><p>Using definition ofα θ (x|D) in the above result gives:</p><formula xml:id="formula_28">α θ (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D) ≤ (5) 1 N N j=1 α θ (x|D) − α θ+ j (x|D) + 1 N N j=1 α θ+ j (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D)</formula><p>Let us study separately each term in the above result. Applying Chebyshev inequality for the second term in the above expression, we have that with probability at least</p><formula xml:id="formula_29">p 1 = 1 − 8[E [µ 2 θ+ (x|D)]+ βπ 2 E [σ 2 θ+ (x|D)]] N ρ 2 : 1 N N j=1 α θ+ j (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D) ≤ ρ 2 .<label>(6)</label></formula><p>In order to ensure that p 1 = 1 − δ 2 the number of samples j should be taken:</p><formula xml:id="formula_30">N =     16 E µ 2 θ+ (x|D) + βπ 2 E σ 2 θ+ (x|D) δρ 2     .</formula><p>We will simplify this expression using bounds in 4 later. Now, let us focus on the second term in Inequality 5. To bound it, we are going to establish a bound on |α θ (x|D) − α θ+ j (x|D). For small random perturbation j we have (with probability 1):</p><formula xml:id="formula_31">α θ+ j (x|D) = α θ (x|D) + T j ∇ θ α θ (x|D) + o(|| j ||) = α θ (x|D) + T j ∇ θ µ θ (x|D) + βπ 2 σ θ (x|D) + o(|| j || 2 ).</formula><p>Let us denote</p><formula xml:id="formula_32">h θ (x|D) = ∇ θ µ θ (x|D) + βπ 2 σ θ (x|D)</formula><p>then, using Cauchy-Schwarz inequality we have:</p><formula xml:id="formula_33">α θ+ j (x|D) − α θ (x|D) ≤ || j || 2 ||[h θ (x|D)|| 2 + o(1)]</formula><p>Since j ∼ N (0, 1), then with probability at least 1 − δ 4N :</p><formula xml:id="formula_34">|| j || 2 ≤ 4σ √ p + 2σ log 4N δ</formula><p>Let us assume (and later we will prove the existence of such bound) that ||h θ (x|D)|| 2 ≤ A 1 . Then, with probability at least 1 − δ 4N :</p><formula xml:id="formula_35">α θ+ j (x|D) − α θ (x|D) ≤ 4σ √ p + 2σ log 4N δ [A 1 + o(1)]</formula><p>On the other hand, for α θ (x|D) = α θ (x|D) + ησ η with probability at least 1 − δ 4N we have:</p><formula xml:id="formula_36">α θ (x|D) − α θ (x|D) ≤ Φ −1 1 − δ 8N σ n .</formula><p>where Φ(·) is cumulative distribution function for standard Gaussian variable. Hence, by choosing σ = min 1,</p><formula xml:id="formula_37">Φ −1 (1− δ 8N )σn 4 √ p+2 √ log 4N δ [A1+o(1)]</formula><p>with probability at least 1 − δ 2N we have that both α θ (x|D) and α θ+ j (x|D) belong to the interval centred at α θ (x|D) of size Φ −1 1 − δ 8N σ n . Therefore, with probability at least</p><formula xml:id="formula_38">1 − δ 2N : α θ (x|D) − α θ+ j (x|D) ≤ 2Φ −1 1 − δ 8N σ n Hence, by choosing σ n = ρ 4Φ −1 (1− δ 8N )</formula><p>we arrive:</p><formula xml:id="formula_39">α θ (x|D) − α θ+ j (x|D) ≤ ρ 2</formula><p>and, therefore, for the first term in (5) with probability at least 1 − δ 2 we have:</p><formula xml:id="formula_40">1 N N j=1 α θ (x|D) − α θ+ j (x|D) ≤ ρ 2</formula><p>Combining this result with <ref type="formula" target="#formula_29">(6)</ref> gives, that with probability at least 1 − δ we have:</p><formula xml:id="formula_41">α θ (x|D) − E ∼N (0,σ 2 I) α θ+ (x|D) ≤ ρ</formula><p>upon the following setup:</p><formula xml:id="formula_42">σ n = ρ 4Φ −1 1 − δ 8N ,<label>(7)</label></formula><formula xml:id="formula_43">σ = min        1, ρ 8 2 √ p + log 4N δ [A 1 + o(1)]        , with N = 16 E µ 2 θ+ (x|D) + βπ 2 E σ 2 θ+ (x|D) δρ 2 .</formula><p>Our last step is to prove the existence of constant A 1 such that ||h θ (x)|| 2 ≤ A 1 and also to simplify these expressions by deriving bounds on E [µ θ+ (x|D)] and E σ 2 θ+ (x|D) . This will be provided as a separate Claim: Claim: Let bounds (4) are hold, then there are positive constants A 1 , A 2 and A 3 , such that:</p><formula xml:id="formula_44">||h θ (x)|| 2 ≤ A 1 ,<label>(8)</label></formula><formula xml:id="formula_45">E [µ θ+ (x|D)] ≤ A 2 , E σ 2 θ+ (x|D) ≤ A 3 .</formula><p>Proof. We start with bound on ||h θ (x)|| 2 . Let us denote for simplicity</p><formula xml:id="formula_46">a θ = [k θ (x, x i )] xi∈D , B θ = [k θ (x, x )] x∈D,x ∈D + I −1 , y = [y(x)] x∈D , m D = [m(x)] x∈D , m = m(x), and k θ = k θ (x, x), then µ θ (x|D) = a T θ B θ [y − m D ] + m, σ 2 θ (x|D) = a T θ B θ a θ + k θ<label>Let</label></formula><p>us also denote the size of D as N , then: have:</p><formula xml:id="formula_47">∇ θ µ θ (x|D) = N i=1 N j=1 ∇ θ [[y j − m j ][a θ ] i [B θ ] ij ] = N i=1 N j=1 [[y j − m j ][B θ ] ij ∇ θ [a θ ] i ] + N i=1 N j=1 [[y j − m j ][a θ ] i ∇ θ [[B θ ] ij ]]</formula><p>Consider each term in this expression separately:</p><formula xml:id="formula_48">N i=1 N j=1 [y j − m j ][B θ ] ij ∇ θ [a θ ] i 2 = N i=1 [B θ [y − m D ]] i ∇ θ [a θ ] i 2 ≤ N i=1 ||B θ (i, :)|| 2 ||y − m D || 2 ||∇ θ [a θ ] i || 2 Using |y| ≤ C and |m(x)| ≤ M 4 we have ||y − m D || 2 ≤ (C + M 4 ) √ N and ||∇ θ [a(θ)] i || 2 ≤ M 2 we have: N i=1 N j=1 [y j − m j ][B θ ] ij ∇ θ [a θ ] i 2 ≤ (9) (C + M 4 ) √ N N i=1 ||B(θ)(i, :)|| 2 ||∇ θ [a θ ] i || 2 ≤ (C + M 4 ) √ N ||B θ || F N i=1 ||∇ θ [a θ ] i || 2 ≤ (C + M 4 ) √ N rank(B θ ||B θ || 2 N i=1 ||∇ θ [a θ ] i || 2 ≤ (C + M 4 )N σ 2 n N i=1 ||∇ θ [a(θ)] i || 2 = (C + M 4 )N 2 M 2 σ 2 n</formula><p>Now, let us consider the second term in the expression for the posterior mean:</p><formula xml:id="formula_49">N i=1 N j=1 [y j − m j ][a θ ] i ∇ θ [[B θ ] ij ] = N i=1 [a θ ] i   j=1 [y j − m j ]∇ θ [[B θ ] ij ]  </formula><p>Notice, that the gradient expression above is presented in a form of a vector:</p><formula xml:id="formula_50">∇ θ [[B θ ] ij ] =     ∂ ∂θ1 [K θ + σ n I] −1 ij , . . . ∂ ∂θp [K θ + σ n I] −1 ij     where we use notation K θ = [k θ (x i , x j )] N,N i=1,j=1</formula><p>. For the r th component we have:</p><formula xml:id="formula_51">∂ ∂θ r [K θ + σ n I] −1 ij = (10) − [K θ + σ n I] −1 ∂ ∂θ r [K θ + σ n I] [K θ + σ n I] −1 ij</formula><p>Now we can study the gradient of the second term in the posterior mean expression:</p><formula xml:id="formula_52">N i=1 [a θ ] i   j=1 [y j − m j ]∇ θ [[B θ ] ij ]   2 ≤ N i=1 |[a θ ] i |   N j=1 ||y − m D || 2 ||∇ θ [[B θ ] ij ]|| 2   ≤ (C + M 4 ) √ N M 1 N i=1 N j=1 p r=1 ∂ ∂θ r [K θ + σ n I] −1 ij</formula><p>Using result (10) in the above expression we have:</p><formula xml:id="formula_53">N i=1 [a θ ] i   j=1 [y j − m j ]∇ θ [[B θ ] ij ]   2 ≤ (C + M 4 ) √ N M 1 N i=1 N j=1 p r=1 ∂ ∂θ r [K θ + σ n I] −1 ij ≤ (C + M 4 ) √ N M 1 p r=1 N i=1 N j=1 ∂ ∂θ r [K θ + σ n I] −1 ij ≤ (C + M 4 )N √ N M 1 × p r=1 [K θ + σ n I] −1 ∂ ∂θ r [K θ + σ n I] [K θ + σ n I] −1 F where we used that N i=1 N j=1 |C ij | ≤ N ||C|| F for any arbitrary matrix C ∈ R N ×N . Because ∂ ∂θr [K θ + σ n I] = ∂ ∂θr K θ . Therefore: N i=1 [a θ ] i j=1 [y j − m j ]∇ θ [[B θ ] ij ] 2 (C + M 4 )N √ N M 1 ≤ p r=1 [K θ + σ n I] −1 ∂ ∂θ r K θ [K θ + σ n I] −1 F ≤ √ N p r=1 [K θ + σ n I] −1 ∂ ∂θ r K θ [K θ + σ n I] −1 2</formula><p>Using properties of matrix 2-norm || · || 2 :</p><formula xml:id="formula_54">[K θ + σ n I] −1 2 ≤ 1 σ 2 n Hence, N i=1 [a θ ] i j=1 [y j − m j ]∇ θ [[B θ ] ij ] 2 (C + M 4 )N 2 M 1 ≤ (11) p r=1 [K θ + σ n I] −1 2 ∂ ∂θ r K θ 2 [K θ + σ n I] −1 2 ≤ 1 σ 4 n p r=1 ∂ ∂θ r K θ 2 .</formula><p>Let us study the last term in the above expression. Using c 2 1 + . . . + c 2 R ≤ |c 1 | + . . . + |c R | for any set of real numbers c 1 , . . . , c R ∈ R we have:</p><formula xml:id="formula_55">∂ ∂θ r K θ 2 =    ∂ ∂θr k θ (x 1 , x 1 ), . . . ∂ ∂θr k θ (x 1 , x N ) . . . . . . . . . ∂ ∂θr k θ (x N , x 1 ), . . . ∂ ∂θr k θ (x N , x N )    2 ≤    ∂ ∂θr k θ (x 1 , x 1 ), . . . ∂ ∂θr k θ (x 1 , x N ) . . . . . . . . . ∂ ∂θr k θ (x N , x 1 ), . . . ∂ ∂θr k θ (x N , x N )    F = N i=1 N j=1 ∂ ∂θ r k θ (x i , x j ) 2 ≤ N i=1 N j=1 ∂ ∂θ r k θ (x i , x j )</formula><p>Substituting this expression in <ref type="formula" target="#formula_0">(11)</ref> gives us:</p><formula xml:id="formula_56">N i=1 [a θ ] i j=1 [y j − m j ]∇ θ [[B θ ] ij ] 2 (C + M 4 )N 2 M 1 ≤ (12) 1 σ 4 n d r=1 N i=1 N j=1 ∂ ∂θ r k θ (x i , x j ) ≤ √ p σ 4 n N i=1 N j=1 ||∇ θ k θ (x i , x j )|| 2 ≤ N 2 √ pM 2 σ 4 n .</formula><p>Hence, combining results <ref type="formula">(9)</ref> and <ref type="formula" target="#formula_0">(12)</ref> we have:</p><formula xml:id="formula_57">||∇ θ µ θ (x|D)|| 2 ≤ (C + M 4 )N 2 M 2 σ 2 n 1 + N 2 M 1 √ p σ 2 n<label>(13)</label></formula><p>Now, let us focus on the gradient of the posterior deviation:</p><formula xml:id="formula_58">∇ θ σ θ (x|D) = (14) ∇ θ k θ (x, x) − a T θ [K θ + σ 2 n I] −1 a θ = 1 2σ θ (x|D) ∇ θ k θ (x, x) − a T θ [K θ + σ 2 n I] −1 a θ = 1 2σ θ (x) ∇ θ k θ (x, x) − ∇ θ a T θ [K θ + σ 2 n I] −1 a θ</formula><p>Let us study the second gradient expression. Using or notation we have:</p><formula xml:id="formula_59">a T θ [K θ + σ 2 n I] −1 a θ = a T θ B θ a θ = N i=1 N j=1 [a θ ] i [a θ ] j [B θ ] ij</formula><p>Hence, for the gradient we have:</p><formula xml:id="formula_60">∇ θ a T θ B θ a θ = N i=1 N j=1 ∇ θ [a θ ] i [a θ ] j [B θ ] ij = N i=1 N j=1 ∇ θ [[a θ ] i ] [a θ ] j [B θ ] ij + N i=1 N j=1 ∇ θ [[a θ ] j ] [a θ ] i [B θ ] ij + N i=1 N j=1 ∇ θ [B θ ] ij [a θ ] i [a θ ] j .</formula><p>Hence, for the norm of the above expression we have:</p><formula xml:id="formula_61">∇ θ a T θ B θ a θ 2 = N i=1 N j=1 ∇ θ [a θ ] i [a θ ] j [B θ ] ij = N i=1 N j=1 ∇ θ [B θ ] ij 2 |[a θ ] i [a θ ] j | + N i=1 N j=1 ||∇ θ || 2 [[a θ ] i ] [a θ ] j [B θ ] ij + N i=1 N j=1 ||∇ θ [[a θ ] j ]|| 2 [a θ ] i [B θ ] ij .</formula><p>Let us bound each term in this expression:</p><p>1. The first term:</p><formula xml:id="formula_62">N i=1 N j=1 ∇ θ [B θ ] ij 2 |[a θ ] i [a θ ] j | ≤ N i=1 N j=1 ∇ θ [B θ ] ij 2 ||a θ || 2 ||a θ || 2 ≤ M 2 1 N i=1 N j=1 ∇ θ [B θ ] ij 2 Using previous bound for ∇ θ [B θ ] ij 2 we have: N i=1 N j=1 ∇ θ [B θ ] ij 2 |[a θ ] i [a θ ] j | ≤ M 2 1 N i=1 N j=1 p r=1 ∂ ∂θ r K θ + σ 2 n I −1 ij = N M 2 1 p r=1 [K θ + σ n I] −1 ∂ ∂θ r K θ [K θ + σ n I] −1 F ≤ N 3 2 M 2 1 p r=1 [K θ + σ n I] −1 ∂ ∂θ r K θ [K θ + σ n I] −1 2 Since [K θ + σ n I] −1 2 ≤ 1 σ 2 n we have: N i=1 N j=1 ∇ θ [B θ ] ij 2 |[a θ ] i [a θ ] j | ≤ N √ N M 2 1 σ 4 n p r=1 N i=1 N j=1 ∂ ∂θ r k θ (x i , x j ) . Using p r=1 N i=1 N j=1 ∂ ∂θr k θ (x i , x j ) = √ p N i=1 N j=1 ||∇ θ k θ (x i , x j )|| 2 ≤ N 2 √ pM 2 , we have: N i=1 N j=1 ∇ θ [B θ ] ij 2 |[a θ ] i [a θ ] j | ≤ N 7 2 √ pM 2 1 M 2 σ 4 n 2.</formula><p>The second and the third terms are identical with respect to bounding strategy:</p><formula xml:id="formula_63">N i=1 N j=1 ||∇ θ [[a θ ] i ] || 2 [a θ ] j [B θ ] ij = N i=1 |B θ (i, :)a θ | ||∇ θ [[a θ ] i ]|| 2 ≤ N i=1 ||B θ (i, :)|| 2 ||a θ || 2 ||∇ θ [[a θ ] i ]|| 2 ≤ ||B θ || F ||a θ || 2 N i=1 ||∇ θ [[a θ ] i ]|| 2 Since ||B θ || F ≤ rank(B θ )||B θ || 2 ≤ √ N σ 2 n . Hence, N i=1 N j=1 ||∇ θ [[a θ ] i ] || 2 [a θ ] j [B θ ] ij ≤ N √ N M 1 M 2 σ 2 n</formula><p>Combining these results and using</p><formula xml:id="formula_64">||∇ θ k θ (x, x)|| ≤ M 2 , |σ θ (x|D)| ≥ k θ (x, x) ≥ M 0 , we have: ||∇ θ [σ θ (x|D)]|| 2 ≤ N √ N M 1 M 2 2σ 2 n M 0 N 2 √ pM 1 σ 2 n + 2<label>(15)</label></formula><p>Hence, combining <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(15)</ref> we have:</p><formula xml:id="formula_65">||h θ (x|D)|| 2 ≤ ||∇ θ µ θ (x|D)|| 2 + βπ 2 ||∇ θ [σ θ (x|D)]|| 2 ≤ (C + M 4 )N 2 M 2 σ 2 n 1 + N 2 M 1 √ p σ 2 n + βπ 2 N √ N M 1 M 2 2σ 2 n M 0 N 2 √ pM 1 σ 2 n + 2 A 1 .</formula><p>Now, we are ready to bound the other two terms in the claim:</p><formula xml:id="formula_66">µ 2 θ+ (x|D) ≤ 2 a T θ+ B θ+ [y − m D ] 2 + 2|m| 2 ≤ 2 (C + M 4 ) 2 M 2 1 σ 4 n + 2M 2 4</formula><p>Therefore, for E µ 2 θ+ (x|D) we have:</p><formula xml:id="formula_67">E µ 2 θ+ (x|D) ≤ 2 (C + M 4 ) 2 M 2 1 σ 4 n + 2M 2 4 A 2 .</formula><p>Finally, for the posterior mean:</p><formula xml:id="formula_68">σ 2 θ+ (x|D) ≤ k θ (x, x) + a T θ+ B θ+ a θ+ ≤ M 1 + M 2 1 σ 2 n Therefore, for E σ 2 θ+ (x|D) we have: E σ 2 θ+ (x|D) ≤ M 1 + M 2 1 σ 2 n A 3 .</formula><p>This finishes the proof of the claim.</p><p>Having equipped with these results, we can further simplify the setup expressions <ref type="formula" target="#formula_42">(7)</ref>:</p><formula xml:id="formula_69">σ n = ρ 4Φ −1 1 − δ 8N , σ = min        1, ρ 8 2 √ p + log 4N δ [A 1 + o(1)]        , with N =     16 A 2 + βπ 2 A 3 δρ 2     .</formula><p>This finishes the proof of the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Acquisition function conflicts</head><p>The full set of experiments described in section 3.3 highlighting conflicts of widely used acquisition functions can be visualised on          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Density of normalised scores relative to random search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Analysis of the results on 108 tuning tasks. (a-Left) Normalised score comparison demonstrating that G-BO (i.e., BO with improvements from Section 4) outperforms others. (a-Right) Empirical gain demonstrating that G-BO can arrive at 8% improvement compared to random. (b-Left) Ablation study reflecting the importance of each addition. (b-Right) Gains associated with each step in % improvement compared to random search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Normalised score (see 3) obtained by each black-box optimiser on all tasks. On each graph optimisers are ranked by mean scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figures 7-12. For each plot, respective maximisers of EI, PI and UCB are shown with vertical lines to clearly indicate when optimising different acquisition functions leads to conflicting recommendations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>These figures show for several tasks the evolution of the observed noise level as hyperparameter values vary. Changes in the standard deviation magnitude (represented by the shaded areas) indicate that the noise level depends on the hyperparameter value, in other words they indicate heteroscedasticity. Drastic changes in standard deviation level are notably observed for (Lasso, Iris, NLL) or (Linear, Digits, NLL) tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 7. Conflicting acquisition functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 11. Conflicting acquisition functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Conflicting acquisition functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Models and search spaces for hyperparameter tuning on regression tasks. Models having same search spaces for classification and regression tasks are omitted (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>AlgorithmMean Std Median 40 th Centile 30 th Centile 20 th Centile 5 th Centile</figDesc><table><row><cell>G-BO</cell><cell cols="3">100.12 8.70 100.01</cell><cell>100.00</cell><cell>99.88</cell><cell>98.64</cell><cell>85.71</cell></row><row><cell>PySOt</cell><cell>98.18</cell><cell>9.03</cell><cell>100.00</cell><cell>99.81</cell><cell>98.60</cell><cell>95.36</cell><cell>80.00</cell></row><row><cell>TuRBO</cell><cell cols="2">97.95 10.80</cell><cell>100.00</cell><cell>99.88</cell><cell>98.75</cell><cell>95.26</cell><cell>78.63</cell></row><row><cell>HyperOpt</cell><cell>96.37</cell><cell>8.79</cell><cell>99.31</cell><cell>98.16</cell><cell>95.94</cell><cell>92.38</cell><cell>78.52</cell></row><row><cell>SkOpt</cell><cell cols="2">96.18 11.51</cell><cell>99.78</cell><cell>98.66</cell><cell>96.73</cell><cell>91.62</cell><cell>74.77</cell></row><row><cell>TuRBO+</cell><cell cols="2">95.29 10.93</cell><cell>98.97</cell><cell>97.60</cell><cell>95.27</cell><cell>90.92</cell><cell>74.77</cell></row><row><cell>OpenTuner</cell><cell cols="2">94.32 14.18</cell><cell>98.44</cell><cell>96.93</cell><cell>93.84</cell><cell>89.97</cell><cell>68.96</cell></row><row><cell>NeverGrad</cell><cell cols="2">93.20 17.52</cell><cell>99.65</cell><cell>97.84</cell><cell>94.57</cell><cell>88.28</cell><cell>55.34</cell></row><row><cell>BOHB</cell><cell cols="2">92.03 11.16</cell><cell>96.02</cell><cell>93.55</cell><cell>90.14</cell><cell>85.71</cell><cell>67.82</cell></row><row><cell>Random-Search</cell><cell cols="2">92.00 11.71</cell><cell>96.18</cell><cell>93.55</cell><cell>90.05</cell><cell>85.16</cell><cell>69.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Non-stationarity test on tasks involving Wine dataset. Log-probabilities on the held-out test set comprised of 32 points is reported for GP equipped with Matern-3/2 kerenl (stationary) and Spectral Mixture kernel (non-stationary).</figDesc><table><row><cell cols="2">Dataset Model</cell><cell cols="3">Metrics Nonstationary kernel Stationary kernel</cell></row><row><cell>wine</cell><cell>DT</cell><cell>acc</cell><cell>-4.86e+02</cell><cell>-1.25e+03</cell></row><row><cell></cell><cell cols="2">MLP-adam acc</cell><cell>-3.27e+01</cell><cell>-2.81e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>acc</cell><cell>-2.80e+01</cell><cell>-7.95e+02</cell></row><row><cell></cell><cell>RF</cell><cell>acc</cell><cell>-1.43e+03</cell><cell>-8.26e+02</cell></row><row><cell></cell><cell>ada</cell><cell>acc</cell><cell>2.12e+01</cell><cell>-1.27e+02</cell></row><row><cell></cell><cell>kNN</cell><cell>acc</cell><cell>-1.27e+02</cell><cell>-1.24e+02</cell></row><row><cell></cell><cell>lasso</cell><cell>acc</cell><cell>-1.32e+01</cell><cell>1.02e+02</cell></row><row><cell></cell><cell>linear</cell><cell>acc</cell><cell>5.60e+01</cell><cell>7.30e+01</cell></row><row><cell></cell><cell>DT</cell><cell>nll</cell><cell>-2.39e+03</cell><cell>-5.45e+02</cell></row><row><cell></cell><cell cols="2">MLP-adam nll</cell><cell>-6.37e+02</cell><cell>-1.54e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>nll</cell><cell>-4.81e+02</cell><cell>-1.41e+02</cell></row><row><cell></cell><cell>RF</cell><cell>nll</cell><cell>-1.35e+04</cell><cell>-8.36e+02</cell></row><row><cell></cell><cell>ada</cell><cell>nll</cell><cell>-4.33e+03</cell><cell>-1.85e+03</cell></row><row><cell></cell><cell>kNN</cell><cell>nll</cell><cell>-6.48e+05</cell><cell>-6.54e+05</cell></row><row><cell></cell><cell>lasso</cell><cell>nll</cell><cell>-4.27e+02</cell><cell>-2.96e+02</cell></row><row><cell></cell><cell>linear</cell><cell>nll</cell><cell>-3.95e+00</cell><cell>1.71e+01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Non-stationarity test on tasks involving Breast dataset</figDesc><table><row><cell cols="2">Dataset Model</cell><cell cols="3">Metrics Nonstationary kernel Stationary kernel</cell></row><row><cell>breast</cell><cell>DT</cell><cell>acc</cell><cell>-1.88e+04</cell><cell>-5.26e+02</cell></row><row><cell></cell><cell cols="2">MLP-adam acc</cell><cell>-2.26e+05</cell><cell>-3.04e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>acc</cell><cell>-3.52e+05</cell><cell>-1.84e+02</cell></row><row><cell></cell><cell>RF</cell><cell>acc</cell><cell>-4.27e+04</cell><cell>-7.42e+02</cell></row><row><cell></cell><cell>ada</cell><cell>acc</cell><cell>-3.53e+03</cell><cell>-7.13e+03</cell></row><row><cell></cell><cell>kNN</cell><cell>acc</cell><cell>1.06e+02</cell><cell>1.14e+02</cell></row><row><cell></cell><cell>lasso</cell><cell>acc</cell><cell>-1.32e+01</cell><cell>1.43e+02</cell></row><row><cell></cell><cell>linear</cell><cell>acc</cell><cell>8.39e+01</cell><cell>1.01e+02</cell></row><row><cell></cell><cell>DT</cell><cell>nll</cell><cell>9.20e+00</cell><cell>-9.23e+02</cell></row><row><cell></cell><cell cols="2">MLP-adam nll</cell><cell>-6.45e+02</cell><cell>-1.65e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>nll</cell><cell>-5.71e+02</cell><cell>-1.60e+02</cell></row><row><cell></cell><cell>RF</cell><cell>nll</cell><cell>-3.75e+02</cell><cell>-7.01e+02</cell></row><row><cell></cell><cell>ada</cell><cell>nll</cell><cell>-8.18e+02</cell><cell>-3.05e+03</cell></row><row><cell></cell><cell>kNN</cell><cell>nll</cell><cell>-7.57e+04</cell><cell>-7.58e+04</cell></row><row><cell></cell><cell>lasso</cell><cell>nll</cell><cell>-1.45e+03</cell><cell>2.50e+01</cell></row><row><cell></cell><cell>linear</cell><cell>nll</cell><cell>1.79e+01</cell><cell>2.14e+01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .Table 7 .Table 8 .</head><label>678</label><figDesc>Non-stationarity test on tasks involving Boston dataset Non-stationarity test on tasks involving Diabetes dataset Non-stationarity test on tasks involving Digits dataset</figDesc><table><row><cell cols="2">Dataset Model</cell><cell cols="3">Metrics Nonstationary kernel Stationary kernel</cell></row><row><cell cols="2">boston DT</cell><cell>mae</cell><cell>-1.15e+03</cell><cell>-6.96e+02</cell></row><row><cell></cell><cell cols="2">MLP-adam mae</cell><cell>-2.91e+02</cell><cell>-1.37e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>mae</cell><cell>-2.80e+06</cell><cell>-8.25e+10</cell></row><row><cell></cell><cell>RF</cell><cell>mae</cell><cell>-3.93e+03</cell><cell>-6.09e+03</cell></row><row><cell></cell><cell>SVM</cell><cell>mae</cell><cell>-3.30e+02</cell><cell>-1.47e+03</cell></row><row><cell></cell><cell>ada</cell><cell>mae</cell><cell>-4.84e+04</cell><cell>-9.13e+04</cell></row><row><cell></cell><cell>kNN</cell><cell>mae</cell><cell>-3.69e+04</cell><cell>-3.68e+04</cell></row><row><cell></cell><cell>DT</cell><cell>mse</cell><cell>-1.91e+04</cell><cell>-2.53e+02</cell></row><row><cell></cell><cell cols="2">MLP-adam mse</cell><cell>-4.80e+05</cell><cell>-1.28e+03</cell></row><row><cell></cell><cell>RF</cell><cell>mse</cell><cell>-1.44e+07</cell><cell>-1.88e+04</cell></row><row><cell></cell><cell>SVM</cell><cell>mse</cell><cell>-6.59e+02</cell><cell>-4.53e+03</cell></row><row><cell></cell><cell>kNN</cell><cell>mse</cell><cell>-1.11e+07</cell><cell>-9.94e+06</cell></row><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">Metrics Nonstationary kernel Stationary kernel</cell></row><row><cell cols="2">diabetes DT</cell><cell>mae</cell><cell>-1.58e+02</cell><cell>-3.39e+02</cell></row><row><cell></cell><cell cols="2">MLP-adam mae</cell><cell>-3.05e+04</cell><cell>-4.40e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>mae</cell><cell>-2.01e+05</cell><cell>-1.03e+03</cell></row><row><cell></cell><cell>RF</cell><cell>mae</cell><cell>-2.26e+04</cell><cell>-1.15e+04</cell></row><row><cell></cell><cell>ada</cell><cell>mae</cell><cell>-3.13e+04</cell><cell>-4.65e+04</cell></row><row><cell></cell><cell>kNN</cell><cell>mae</cell><cell>-2.46e+06</cell><cell>-2.34e+06</cell></row><row><cell></cell><cell>DT</cell><cell>mse</cell><cell>-1.51e+04</cell><cell>-1.03e+04</cell></row><row><cell></cell><cell>SVM</cell><cell>mse</cell><cell>-5.14e+02</cell><cell>-2.39e+02</cell></row><row><cell cols="2">Dataset Model</cell><cell cols="3">Metrics Nonstationary kernel Stationary kernel</cell></row><row><cell>digits</cell><cell>DT</cell><cell>acc</cell><cell>-6.75e+02</cell><cell>-1.02e+03</cell></row><row><cell></cell><cell cols="2">MLP-adam acc</cell><cell>-2.38e+02</cell><cell>-3.39e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>acc</cell><cell>-8.02e+04</cell><cell>-2.98e+02</cell></row><row><cell></cell><cell>RF</cell><cell>acc</cell><cell>-4.53e+02</cell><cell>-1.50e+03</cell></row><row><cell></cell><cell>SVM</cell><cell>acc</cell><cell>1.43e+02</cell><cell>-7.92e+00</cell></row><row><cell></cell><cell>ada</cell><cell>acc</cell><cell>-1.17e+11</cell><cell>5.89e-01</cell></row><row><cell></cell><cell>kNN</cell><cell>acc</cell><cell>1.41e+02</cell><cell>1.07e+02</cell></row><row><cell></cell><cell>lasso</cell><cell>acc</cell><cell>-1.11e+01</cell><cell>1.05e+02</cell></row><row><cell></cell><cell>linear</cell><cell>acc</cell><cell>2.68e+01</cell><cell>9.78e+01</cell></row><row><cell></cell><cell>DT</cell><cell>nll</cell><cell>-6.52e+03</cell><cell>-1.55e+03</cell></row><row><cell></cell><cell cols="2">MLP-adam nll</cell><cell>-1.25e+02</cell><cell>-2.33e+02</cell></row><row><cell></cell><cell>MLP-sgd</cell><cell>nll</cell><cell>-2.65e+02</cell><cell>-1.32e+02</cell></row><row><cell></cell><cell>RF</cell><cell>nll</cell><cell>-1.82e+02</cell><cell>-6.96e+02</cell></row><row><cell></cell><cell>ada</cell><cell>nll</cell><cell>-1.87e+05</cell><cell>-8.71e+00</cell></row><row><cell></cell><cell>kNN</cell><cell>nll</cell><cell>-2.08e+03</cell><cell>-2.09e+03</cell></row><row><cell></cell><cell>lasso</cell><cell>nll</cell><cell>-4.82e+01</cell><cell>-4.07e+02</cell></row><row><cell></cell><cell>linear</cell><cell>nll</cell><cell>-3.72e+03</cell><cell>-3.48e+03</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We would like to mention that though a gradient-based algorithm remains intact upon the addition of ησn, in our formulation we use an evolutionary method which utilises acquisition function values. Consequently, the path followed by the optimiser will be altered based on η samples leading to more robust probes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ing, including drug discovery, molecule design <ref type="bibr" target="#b24">(Gómez-Bombarelli et al., 2018;</ref><ref type="bibr" target="#b25">Griffiths &amp; Hernández-Lobato, 2020)</ref> and robotics <ref type="bibr" target="#b13">(Calandra et al., 2016b)</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-objective Bayesian optimisation with preferences over objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdolshah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="12235" to="12245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13196</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein robust reinforcement learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An extensible framework for program autotuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bosboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-M</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opentuner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Parallel Architectures and Compilation</title>
		<meeting>the 23rd International Conference on Parallel Architectures and Compilation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BoTorch: A framework for efficient Monte-Carlo Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bakshy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hyperopt: a Python library for model selection and hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14008</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonconvex robust optimization for problems with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nohadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-objective optimization in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pymoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="89497" to="89509" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Recommender systems survey. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gutiérrez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="109" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarially robust optimization with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bogunovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5765" to="5775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="243" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Algorithms for minimization without derivatives. Courier Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Brent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian modeling for optimization and control in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Darmstadt</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Manifold Gaussian processes for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3338" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian optimization for learning gaits under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seyfarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="23" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparative study of tests for homogeneity of variances, with applications to the outer continental shelf bidding data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Conover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="361" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Cowen-Rivers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Palenicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sootla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bou-Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09436</idno>
		<title level="m">Safe model-based &amp; active reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: NSGA-II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00420</idno>
		<title level="m">An event-driven asynchronous framework for surrogate optimization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable global optimization via local Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poloczek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5496" to="5507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BOHB: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1437" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Survey of machine learning algorithms for disease diagnostic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fatima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Learning Systems and Applications</title>
		<imprint>
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distribution-free twosample tests for scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fligner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Killeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">353</biblScope>
			<biblScope unit="page" from="210" to="213" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Derivative-free &amp; order-robust optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tutunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bou-Ammar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2293" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sánchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constrained Bayesian optimization for automatic chemical design using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-R</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="577" to="586" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Achieving robustness to aleatoric uncertainty with heteroscedastic Bayesian optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-R</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Aldrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garcia-Ortegon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Lalchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07779</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The CMA evolution strategy: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00772</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Portfolio allocation for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning and Intelligent Optimization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-fidelity Bayesian optimisation with continuous approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Póczos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1799" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural architecture search with Bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Most likely heteroscedastic Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributionally robust Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bogunovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RoBo: A flexible and robust Bayesian optimization framework in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 2017 Workshop on Bayesian Optimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GpflowOpt: a Bayesian optimization library using TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Knudde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Herten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dhaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Couckuyt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 2017 Workshop on Bayesian Optimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Variable risk control via stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kuindersma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Grupen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="806" to="825" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A generalized probability density function for double-bounded random processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumaraswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variational heteroscedastic Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Contributions to probability and statistics. Essays in Honor of Harold Hotelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="page" from="278" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyperband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6765" to="6816" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On gradient descent ascent for nonconvex-concave minimax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6083" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">CMA-ES for hyperparameter optimization of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07269</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Batch Bayesian optimization via multi-objective acquisition ensemble for automated analog circuit design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3306" to="3314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maronas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamelijnck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Damoulas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01596</idno>
		<title level="m">Transforming Gaussian processes with normalizing flows</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Knowing the what but not the where in Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7317" to="7326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bayesian optimization with cylindrical kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3868" to="3877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Robust scale estimation and hypothesis testing based on quadratic inference function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Lindsay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Center for Likelihood Studies, Department of Statistics, The Pennsylvania State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Friedersdorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04639</idno>
		<title level="m">Robust Gaussian process regression with a bias model</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Nevergrad -A gradientfree optimization platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<ptr target="https://GitHub.com/FacebookResearch/Nevergrad" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaussian</surname></persName>
		</author>
		<title level="m">Processes for Machine Learning</title>
		<meeting>esses for Machine Learning<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Compositionally-warped Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tobar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="235" to="246" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-fidelity black-box optimization with hierarchical partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shakkottai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4538" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">An entropy search portfolio for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4625</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Warped Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Input warping for Bayesian optimization of non-stationary functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4134" to="4142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sample-efficient optimization in the latent space of deep generative models via weighted retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daxberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The marginal value of adaptive gradient methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4151" to="4161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A new family of power transformations to improve normality or symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-K</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="954" to="959" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
