<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nortumbria Univesity</orgName>
								<address>
									<settlement>Newcastle</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fisher Discriminant</term>
					<term>Hand Gesture Recognition</term>
					<term>Mobile Devices</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gesture recognition is a challenging problem in the field of biometrics. In this paper, we integrate Fisher criterion into Bidirectional Long-Short Term Memory (BLSTM) network and Bidirectional Gated Recurrent Unit (BGRU), thus leading to two new deep models termed as F-BLSTM and F-BGRU. Both Fisher discriminative deep models can effectively classify the gesture based on analyzing the acceleration and angular velocity data of the human gestures. Moreover, we collect a large Mobile Gesture Database (MGD) based on the accelerations and angular velocities containing 5547 sequences of 12 gestures. Extensive experiments are conducted to validate the superior performance of the proposed networks as compared to the state-of-the-art BLSTM and BGRU on MGD database and two benchmark databases (i.e. BUAA mobile gesture and SmartWatch gesture).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Towards natural human-computer interaction, the emergence of smartphones has changed our lives and made our lives more convenient. The interaction recognition systems.</p><p>2. We integrate Fisher criterion into BLSTM network to improve the traditional softmax loss training function. Extensive experiments on our MGD, BUAA Mobile Gesture database, and a public database are conducted to verify the superior performance of the proposed networks.</p><p>The rest of the paper is organized as follows. Section 2 introduces the related works, and Section 3 describes the details of the proposed method. Experiments and results are presented in Section 4. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Gesture recognition has been extensively investigated in the last two decades, and remarkable advances are achieved using inertial sensors in mobile devices <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. For the application of human computer interaction, <ref type="bibr">Rekimoto et</ref> al. <ref type="bibr" target="#b21">[22]</ref> detected the movement of arm using a specific wearable device. But it is difficult to get high precision in practice because of large size of the equipment.</p><p>Fresca et al. <ref type="bibr" target="#b25">[26]</ref> studied and recognized human gesticulation and the manipulation of graspable and movable everyday artifacts as a potentially effective means for the interaction with smart things. Parsani et al. <ref type="bibr" target="#b26">[27]</ref> designed an embedded system which could analyze and recognize smartphone gestures involving a combination of straight line motions in three dimensions. Roy et al. <ref type="bibr" target="#b27">[28]</ref> suggested that the walking direction should be detectable through the accelerometer and get blended into various other motion patterns during the act of walking, including up and down bounce, side-to-side sway, swing of arms or legs, etc.. They also analyzed the human walking dynamics to estimate the dominating forces and used this knowledge to find the heading direction of the pedestrian. For the application of biological monitoring, Park et al. <ref type="bibr" target="#b28">[29]</ref> demonstrated a very promising application to classify and monitor heartbeats, while Nandakumar et al. <ref type="bibr" target="#b29">[30]</ref> monitored sleep apnea using the sensors in smartphones to develop more convenient conditions for gesture recognition <ref type="bibr" target="#b30">[31]</ref>. Besides, there are also some other applications. As reported in <ref type="bibr" target="#b31">[32]</ref>, Agrawal et al.proposed a system called PhonePoint Pen that uses the built-in accelerometer in mobile phones to recognize the human writing. The system, based on Nokia N95 platform, was evaluated through 10 students and 5 hospital patients. Results showed that English characters can be identified with an average accuracy of 91%. The system presented a promising prospect for mobile based gesture recognition.</p><p>Mobile gesture recognition has provided new directions and also delivered compelling performance for the application of machine learning. Hofmann et al. <ref type="bibr" target="#b32">[33]</ref> proposed a recognition scheme based on Hidden Markov Models (HMM) <ref type="bibr" target="#b33">[34]</ref> and used discrete HMM (dHMM) to recognize dynamic gestures.</p><p>The approach essentially divided the input data into different regions and assigned each of them to a corresponding codebook for classifying them with dHMM. The vector codebook was obtained by a clustering method, serving as an unsupervised learning procedure to model the feature vector distribution in the input data space. The experiments were carried out using 500 training gestures with 10 samples per gesture, yielding an accuracy of 95.6% for 100 test gestures. In <ref type="bibr" target="#b23">[24]</ref>, gestures were captured with a small wireless sensor-box that produced three dimensional acceleration signal. Kallio et al.trained the dHMM model by using five states and a codebook size of eight. They measured the recognition accuracy of system using four degrees of complexity. In <ref type="bibr" target="#b34">[35]</ref>, an HMM model was trained with five states, achieving a rate of 96.1% accuracy for classifying 8 gestures. Pylv et al. <ref type="bibr" target="#b35">[36]</ref> proposed a method based on continuous HMM (cHMM), which takes correlated time information into consideration.</p><p>The experiment achieved reliable results, with 96.67% of correct classification on a database of 20 samples for 10 gestures. In <ref type="bibr" target="#b36">[37]</ref>, multi-stream HMM consisting of EMG and ACC streams was utilized as decision fusion method to recognize hand gestures. For a data set of 18 gestures, each trained with 10 repetitions, the average recognition accuracy was about 91.7% in real application.</p><p>Besides HMM, a few other popular techniques have been used in gesture recognition. Akl et al. proposed a gesture recognition system based primarily on a single 3 dimensional accelerometer, by employing DTW <ref type="bibr" target="#b37">[38]</ref>. The system defined a dictionary of 18 gestures and a database of 3700 repetitions from 7 users and got up to 90% classification accuracy in the experiment. David et al. <ref type="bibr" target="#b38">[39]</ref> proposed two approaches including Naive Bayes and DTW for recognizing four gesture types from five different subjects in the experiment. The results revealed Bayesian classification is better than DTW. Wu et al. The gesture recognition model was trained using the SmartWatch Gestures database <ref type="bibr" target="#b43">[44]</ref>. Each gesture sequence contains acceleration data from the 3 dimensional accelerometer. An evaluation of the network size was presented, and the best performance was obtained by using the LSTM layer with the size of 128. Lefebvre <ref type="bibr" target="#b20">[21]</ref> carried out gesture recognition experiments on a database consisted of both accelerometer and gyrometer sensors. The sensor data was captured using an Android Nexus S Samsung device. 22 participants, from 20 to 55 years old, contributed to the database of the 14 symbolic gestures.</p><p>The results showed that gesture recognition utilizing both sensors can achieve better performance than using each individual sensor. Moreover, the BLSTM based method achieved an accuracy of 95.57% on the database of total 1540 gestures. To the best of our knowledge, the BLSTM based method is currently the state-of-the-art baseline and performs better than previous approaches such as cHMM, DTW, FDSVM and LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head><p>In this section, we first describe the network structures of Bidirectional Long-Short Term Memory (BLSTM) and its variant -LSTM with Gate Recurrent Unit (BGRU). Then, we propose to incorporate the Fisher criterion to improve the discriminative power of these deep models, dubbed F-BLSTM and F-BGRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bidirectional LSTM</head><p>We briefly describe the LSTM unit which is the basic building block of the </p><formula xml:id="formula_0">i t = σ (W xi x t + W hi h t−1 + W ci c t−1 + b i ) , f t = σ (W xf x t + W hf h t−1 + W cf c t−1 + b f ) , c t = f t c t−1 + i t tanh (W xc x t + W hc h t−1 + b c ) , o t = σ (W xo x t + W ho h t−1 + W co c t + b o ) , h t = o t tanh (c t ) .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Pooling</head><p>Forward Layer</p><formula xml:id="formula_1">      1 2 , , , N x T T x x T     LSTM Input Sequence LSTM LSTM LSTM Output Layer</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backward Layer</head><p>Problem:</p><formula xml:id="formula_2">      1 2 1 , 1 , , 1 N x x x     s f      1 1 o   2 1 o   1 M o   1 o T   2 o T   M o T LSTM unit 1 o M o 2 o</formula><p>Softmax loss with Fisher criterion The model of BLSTM uses Recurrent Neural Nets (RNNs) made of LSTM units, which have shown the great ability to deal with temporal data in many applications <ref type="bibr" target="#b7">[8]</ref>. We consider the gesture data using 3 dimensional accelerome- </p><formula xml:id="formula_3">G = {G 1 , ..., G T } is a gesture sequence of T size, G t = (x 1 (t) , ..., x N (t))</formula><p>is a vector at timestep t and N denotes the sensor number, and (y 1 , ..., y n ) is the BLSTM output set with n being the number of gestures to be classified.</p><p>The softmax activation function is used for this layer to give network response between 0 and 1. Classically, these outputs can be considered as posterior probabilities of the input sequence belonging to a specific gesture class, and the softmax loss function is presented as follows</p><formula xml:id="formula_4">L s = − 1 m m i=1 log e W T y i Oi+by i n j=1 e W T j Oi+bj ,<label>(2)</label></formula><p>where O i = (o 1 , ..., o M ) denotes the ith output belonging to the y i th class. W j denotes the jth column of the weights W in the last layer and b is the bias term.</p><p>m is the size of mini-batch and n is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Fisher Discriminant Learning</head><p>To further enhance the performance of BLSTM, we incorporate the Fisher criterion into the softmax loss function, which is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. First, the input layer consists of the concatenation of 3 dimensional accelerometer and 3 dimensional gyrometer signals synchronized in time (i.e. N = 6). The sensor data is normalized between 0 and 1 according to the maximum value that sensors can provide. In order to minimize the intra-class variations and maximize the inter-class variations of gesture data, we propose a new Fisher criterion based on Fisher Linear Discrimination as follows:</p><formula xml:id="formula_5">L f = 1 m m i=1 O i − µ yi 2 2 − δ n (n − 1) n j=1,k=1 µ j − µ k 2 2 (3)</formula><p>where µ yi is the y i th class mean of output vectors, and δ is the discriminative factor. As updating the mean vector µ yi when learning BLSTM, the Fisher criterion utilizes the whole training set and mean vectors of each class in every iteration. We propose to augment the loss in Eq. (2) with the additional Fisher criterion term in Eq. (3) as follows:</p><formula xml:id="formula_6">L = L s + θL f<label>(4)</label></formula><p>where θ and δ are bounded in [0, 1], and these two parameters are used for balancing three parts of the loss function. In forward and backward processes, </p><p>Then, we update the parameter W , mean vector µ j and BLSTM parameter H f in the e + 1 iteration by the following formulas until the converge stopping criterion.</p><formula xml:id="formula_8">W e+1 = W e − λ e · ∂L e f ∂W e , µ e+1 j = µ e j − α · ∆µ e j , H e+1 f = H e f − λ e m i ∂L e ∂O e i · ∂O e i ∂H e f .<label>(6)</label></formula><p>With proper scalar parameters θ, δ and α, the discriminative power of F-LSTM can be significantly enhanced for hand gesture recognition. This network is learned using classical online backpropagation through time with momentum. For classifying a new gesture sequence, we use a majority voting rule over the outputs along the sequence (i.e. keeping only the most probable class argmax i∈ <ref type="bibr">[1,n]</ref> O i ) to determine the final gesture class. A detailed parameter analysis of θ, δ and α is presented in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bidirectional GRU and F-BGRU</head><p>To further enhance the performance of network, a variant of BLSTM termed Bidirectional Gated Recurrent Unit (BGRU) was proposed in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> to make each recurrent unit to adaptively capture dependence of different time scales.</p><p>Similarly to the BLSTM unit, the BGRU has the activation h t , candidate activationh t , update gate z t and reset gate r t units to modulate the flow of information in unit without some separate memory cells, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>The flows in BGRU are summarized as follows:</p><formula xml:id="formula_9">z t = σ (W z x t + W zf h t + b z ) , r t = σ (W r x t + W rf h t + b r ) , h t = tanh (W x t + U (r t h t−1 ) + b h ) , h t = (1 − z t ) h t−1 + z tht .<label>(7)</label></formula><p>where the activation h t at time t is a linear interpolation between previous activation h t−1 and the candidate activationh t , the candidate activationh t is computed same as traditional recurrent unit. The update gate z t decides the number of units to update its activation, and so as the reset gate r t .</p><p>It is easy to notice that the BGRU unit also controls the flow of information like the BLSTM unit, but without having to use a memory unit. Similar to F-BLSTM, we also apply the Fisher discriminative function for BGRU and learn a new variant named F-BGRU model to recognize hand gestures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hardware Device</head><p>Our mobile hand gesture database is collected using a Huawei device with</p><p>Android system, which has a 3 dimensional accelerometer and a 3 dimensional gyrometer. According to <ref type="bibr" target="#b20">[21]</ref>, we collect the data of both accelerometer and gyrometer, and record each gesture by pressing, holding and releasing the "Sensor"</p><p>button on the touch screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Gesture Dictionary</head><p>As shown in <ref type="figure" target="#fig_6">Fig. 4</ref>, the gesture dictionary consists of two categories including  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Database Collection</head><p>The database named MGD consists of 12 gestures performed by 32 participants (23 males and 9 females) with about fifteen times per gesture. Therefore, there are a total of 5547 gesture sequences. The sampling time of accelerometer and gyrometer sensors is 5ms corresponding to a frequency of 200Hz. To the best of our knowledge, it is the largest database so far for mobile based gesture recognition, which is of benefit to the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In this section, we present the details of the implementation of our experiments. We use Tensorflow toolbox as the deep learning platform and an NVIDIA sequences. Specifically, we first normalize a signal x n i (t) by</p><formula xml:id="formula_10">x n i (t) = x i (t) − min T t=1 x i (t) max T t=1 x i (t) − min T t=1 x i (t)</formula><p>, ∀i ∈ {1, ..., 6} .</p><p>Then, we use cubic spline interpolation to normalize the length of a sequence to a fixed size (we set this size as 1000 in our experiments). <ref type="figure">Fig. 6</ref> shows the preprocessed accelerometer and gyrometer data, where the sequence has been filtered and normalized.</p><p>(a) Preprocessed Accelerometer Data (b) Preprocessed Gyrometer Data <ref type="figure">Figure 6</ref>: The preprocessed accelerometer and gyrometer data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Study of Fisher Criterion Parameters</head><p>In our model, the hyperparameter θ impacts the Fisher criterion, α controls the update rate of mean µ in F-BLSTM, and δ adjusts the relationship of intra-class distance and inter-class distance between features. These parameters would affect the performance of gesture recognition. In order to configure an optimal parameter setting, we conduct parameter tuning experiments for the F-BLSTM model as follows.</p><p>Experiment 1. We fix α to 0.5, δ to 0.01 and vary θ from 0 to 1 to investigate the effect of parameter θ. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the classification accuracy on testing set. The result shows that the model trained with only softmax loss has poor performance, which certificates the necessity of introducing Fisher criterion. Experiment 2. We fix α to 0.5, θ to 0.1 and vary δ from 1e-5 to 0.1 to verify that the inter-class distances joining promote the classification ability. As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, δ balances the relationship of intraclass distance and inter-class distance. We can set δ to an appropriate value to make the classification better according to different circumstances.</p><p>Experiment 3. We fix θ to 0.1, δ to 0.01 and vary α from 0 to 1 to train different models. The classification accuracy of these models on our gesture database are illustrated in <ref type="figure" target="#fig_10">Fig. 9</ref>. We find that the classification performance of our model remains relatively stable across a wide range of α, but a moderate value of α has a better performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis of Model Effect</head><p>As shown in the analysis mentioned above, the F-BLSTM and FBGRU achieves a better discriminant ability than the baseline BLSTM and BGRU.</p><p>This section further discusses the better feature disctribution has been achieved.</p><p>By the study in last section, we set θ to 0.1, δ to 0.01 and α to 0.5 in the F-BLSTM model, and set the parameters to 0.3, 0.01, 0.5 in the F-BGRU model,</p><p>respectively. <ref type="figure" target="#fig_11">Fig. 10</ref> shows the feature visualizations on MGD database. In <ref type="figure" target="#fig_11">Fig. 10(a)</ref> and <ref type="figure" target="#fig_11">Fig. 10(b)</ref>, the BLSTM and BGRU features of 12 classes are visualized by t-SNE <ref type="bibr" target="#b47">[48]</ref> with the parameters of initial dimension 100 and perplexity dimension 30, while the F-BLSTM and F-BGRU features are illustrated in <ref type="figure" target="#fig_11">Fig. 10</ref>(c) and <ref type="figure" target="#fig_11">Fig. 10(d)</ref>, respectively. Clearly, the fisher discriminant learning features are more discriminative than the original baseline features, especially the F-BGRU feature in <ref type="figure" target="#fig_11">Fig. 10(d)</ref> can be better discriminated than the BLSTM feature in <ref type="figure" target="#fig_11">Fig. 10(a)</ref>. As another verification, the quantitative evaluation is performed based on three databases in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-Arts</head><p>Experiment on MGD Database. For the proposed database, we select 3500 sequences for training our model and 2047 sequences for testing. After preprocessing, the length of each data sequence is set to 1000, thus each input sample (3-axis accelerometer and gyrometer signals) is a matrix of 1000 × 6. Here, we train the network using adaptive moment estimation, with the learning rate as 0.002 and the batch size as 200. For the F-BLSTM model, we set θ to 0.1, δ to 0.01 and α to 0.5. We complete the training of BLSTM and F-BLSTM models at 1.5K iterations. The parameters of F-BGRU model are set to 0.3, 0.01, 0.5 respectively. The training of BGRU and F-BGRU is completed at 1.2K iterations. We report the performance of different methods on the testing set based on the average over 5 runs. The class-wise classification accuracy comparison of different mothods is presented in <ref type="table" target="#tab_2">Table.</ref> 1. It is clear that by incorporating the Fisher criterion to the base models (BLSTM and BGRU), the recognition performance can be improved. <ref type="figure" target="#fig_13">Fig. 11</ref> also shows the behaviors of        <ref type="figure" target="#fig_3">Fig. 13</ref>. Different from the 6-dimensional sequences of previous two databases, each sequence in this dataset only contains acceleration data from the 3-axis accelerometer of a first generation Sony SmartWatch. Furthermore, due to the lower sampling frequency, we set the length of each gesture sequence preprocessed to 50. We randomly select 2400 sequences as training set and the rest 800 sequences as testing set. The parameters of Fisher criterion follow the same setting in previous experiment. Adaptive moment estimation is used to train the network, and the initial learning rate λ is set to 0.0001.</p><p>The batch size is 1000. Training for BLSTM and F-BLSTM is stopped at 1.4K iterations, and BGRU and F-BGRU at 2K iterations. <ref type="figure" target="#fig_6">Fig. 14</ref> shows the training error and testing error during the training process. Like <ref type="figure" target="#fig_13">Fig. 11</ref> and <ref type="figure" target="#fig_2">Fig. 12</ref>, dotted lines denote training errors, and solid lines denote testing errors. <ref type="table" target="#tab_2">Table. 3</ref> lists the classification results for different gestures. Notice that our proposed models perform considerably better than the baselines across the 20 gestures.</p><p>Based on the experimental evaluations, we can observe that LSTM models with Fisher criterion consistently outperform the standard LSTMs on three different databases, validating the advantage of the proposed Fisher criterion. Furthermore, with even less data, the proposed Fisher criterion helps the LSTMs obtain a better result. This proves that it has a significant effect on a small database, showing the Fisher criterion has a widespread application scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have collected a large gesture database, namely MGD, for the application of mobile based gesture recognition. We proposed a novel Fisher criterion for F-BLSTM network to effectively classify the mobile hand gestures. Based on F-BLSTM, we also extended the F-BLSTM to a variant F-BGRU. By conducting numerous experiments, we proved that the mobile gesture recognition based on F-BLSTM networks which is supervised by the  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[40] employed multi-class Support Vector Machine (SVM) for user-independent gesture recognition and demonstrated that SVM significantly outperformed other methods including DTW, Naive Bayes and HMM. In [41], Wang et al. combined LCS and SVM to perform the classification task and achieved the classification accuracy of 93%. Another line of research focuses on the feature extraction and selection. For example, the principle component analysis was used for feature selection and dimensionality reduction in gesture classification [42]. In [43], the hybrid features combined short-time energy with Fast Fourier Transform, denoting the fusion of time-domain features and frequency-domain features, were presented for recognizing seventeen complex gestures on cell phone. An average recognition accuracy of 89.89% was obtained using multi-class SVM. Driven by the tremendous success of deep learning, the research paradigm has been shifted from traditional approaches to deep learning methods for mobile gesture recognition [20, 21]. Shin et al. [20] developed a dynamic hand gesture recognition technique using recurrent neural network (RNN) algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>proposed F-BLSTM model. The neurons of LSTM contain a constant memory cell name, which has a state c t at time t. A LSTM neuron unit is presented in detail at the bottom of Fig. 2. Each LSTM unit for reading or modifying is controlled through a sigmoidal input gate i t , a forget gate f t and an output gate o t . At each time step t, LSTM unit receives inputs from two external sources at each of the three gates. The current frame x t and previous hidden states h t−1 are two sources, and the cell state c t−1 in the cell block is an internal source of each gate. The gates are passed through the tanh non-linearity and activated by logistic function. After multiplying the cell state by the forget gate f t , the final output of the LSTM unit h t is computed by multiplying the activation o t of the output gates with updated cell state. Denoting all W * are diagonal matrices, the updates in a layer of LSTM units are summarized as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of F-BLSTM. We intuitively change the loss function of BLSTM, and the resulting algorithm does not affect the training convergence and the model size, but leading to a performance improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ter and 3</head><label>3</label><figDesc>dimensional gyrometer synchronized input vectors through sampling timestep. As shown in Fig. 2, the forward and backward LSTM hidden layers are fully connected to the input layer and consist of multiple LSTM neurons each with full recurrent connections. Several experiments have been conducted with different hidden neuron sizes and 128 neurons yield the best results. The output layer has a size equivalent to the number of neuron to classify (i.e. M = 128).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>BGRU unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Arabic numerals<ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6)</ref> and English capital letters (A, B, C, D, E, F). Furthermore, the stroke order of gestures is set in advance to ensure the consistency of gestures drawed by left or right hand of each participant. We directly collect all accelerometer and gyrometer data, then transfer the data from the cellphone memory to the computer for gesture recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Examples of hand gestures in MGD database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 .Figure 5 :</head><label>55</label><figDesc>GTX 1070 GPU to run the experiments. In order to validate the effectiveness of our proposed Fisher criterion in LSTM for modeling temporal sequences, we compare our methods, F-BLSTM and F-BGRU, with the state-of-the-art baselines (BLSTM and BGRU<ref type="bibr" target="#b45">[46]</ref>) on three benchmarks including our proposed database (MGD), and two previous databases: the BUAA Mobile Gesture database<ref type="bibr" target="#b46">[47]</ref> and the SmartWatch Gestures database<ref type="bibr" target="#b45">[46]</ref>. We comprehensivelyevaluate the performance of the proposed model under different parameter settings of δ, α and θ in Sec. 4.3, and provide extensive experimental comparison in Sec. 4.Data preprocessing. The main objective for data preprocessing is to facilitate gesture recognition. In real world applications, the sensor data often contain a lot of noise due to complex environmental conditions and hardware limitations. Therefore, we first carry out a filtering process to suppress noise (i.e. data smoothing). We experimented with Average Filter, Median Filter and Butterworth Filter, and selected the Average Filter in terms of its good performance and computational efficiency.Fig. 5 shows the original accelerometer and gyrometer data and the processed data using the Average Filter. The gesture execution speed of different participants may vary considerably, which leads to different signal lengths due to a fixed sampling frequency (200HZ) of accelerometer and gyrometer in the mobile phone. For example, gestures completed relatively faster will have fewer sampling points. Also, the signal strength of gesture sequences may vary. To cope with signal strength and speed variations, we apply amplitude and sequence normalization to the original signal (a) Original Accelerometer Data (b) Original Gyrometer Data (c) Filtered Accelerometer Data (d) Filtered Gyrometer Data The original accelerometer and gyrometer data vs. the processed data by Moving Average Filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Influence of parameter θ on recognition accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Influence of parameter δ on recognition accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Influence of parameter α on recognition accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Feature visualization of 12 classes of MGD database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>F</head><label></label><figDesc>-BLSTM and F-BGRU models. Dotted lines denote training errors, and solid lines denote testing errors for different methods. As can be observed from the figure, our proposed softmax function with Fisher criterion effectively speeds up the convergence of training and achieves a smaller error rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Training on MGD database. Dotted lines denote training errors, and solid lines denote testing errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Training on BUAA Mobile Gesture Database. Dotted lines denote training errors, and solid lines denote testing errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Examples of hand gestures in SmartWatch Database.mobile applications using gestures. Eight different users performed twenty repetitions of twenty different gestures for a total of 3200 sequences. The gestures are depicted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Training on SmartWatch Gesture Database. Dotted lines denote training errors, and solid lines denote testing errors.Fisher criterion has an outstanding performance. An appropriate values of Fisher criterion parameters contribute to the better results. versity, Beijing, China, in 2009, the M.S. degree in electrical engineering from Mississippi State University, Starkville, MS, USA, in 2012, and the Ph.D. degree from the University of Texas at Dallas, Richardson, TX, USA, in 2016. He is currently a Postdoctoral Fellow with the Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA. His current research interests include compressed sensing, signal and image processing, pattern recognition, and computer vision. He has published over 40 papers in refereed journals and conferences in the above areas. Jungong Han is currently a Senior Lecturer with the Department of Computer Science and Digital Technologies at Northumbria University, Newcastle, UK. Previously, he was a Senior Scientist (2012-2015) with Civolution Technology (a combining synergy of Philips Content Identification and Thomson STS), a Research Staff (2010-2012) with the Centre for Mathematics and Computer Science (CWI), and a Senior Researcher (2005-2010) with the Technical University of Eindhoven (TU/e) in Netherlands. Dr. Hans research interests include Multimedia Content Identification, Multi-Sensor Data Fusion, Computer Vision and Multimedia Security. He is an Associate Editor of Elsevier Neurocomputing (IF 2.4) and an Editorial Board Member of Springer Multimedia Tools and Applications (IF 1.4). He has been (lead) Guest Editor for five international journals, such as IEEE-T-SMCB, IEEE-T-NNLS. Dr. Han is the recipient of the UK Mobility Award Grant from the UK Royal Society in 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The average accuracy(%) of BLSTM, BGRU and our proposed F-BLSTM, F-BGRU on MGD database.</figDesc><table><row><cell>Method</cell><cell cols="4">BLSTM F-BLSTM BGRU F-BGRU</cell></row><row><cell>Gesture</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>97.41</cell><cell>97.85</cell><cell>97.09</cell><cell>98.09</cell></row><row><cell>B</cell><cell>94.17</cell><cell>96.50</cell><cell>97.24</cell><cell>98.78</cell></row><row><cell>C</cell><cell>98.95</cell><cell>99.40</cell><cell>99.85</cell><cell>100.00</cell></row><row><cell>D</cell><cell>96.88</cell><cell>99.04</cell><cell>98.02</cell><cell>98.87</cell></row><row><cell>E</cell><cell>96.88</cell><cell>97.40</cell><cell>98.48</cell><cell>98.61</cell></row><row><cell>F</cell><cell>96.86</cell><cell>98.59</cell><cell>97.62</cell><cell>99.54</cell></row><row><cell>1</cell><cell>93.80</cell><cell>95.33.82</cell><cell>96.62</cell><cell>98.53</cell></row><row><cell>2</cell><cell>98.60</cell><cell>98.82</cell><cell>99.03</cell><cell>99.35</cell></row><row><cell>3</cell><cell>96.69</cell><cell>97.56</cell><cell>98.29</cell><cell>99.42</cell></row><row><cell>4</cell><cell>98.77</cell><cell>98.97</cell><cell>99.28</cell><cell>99.29</cell></row><row><cell>5</cell><cell>96.55</cell><cell>98.16</cell><cell>99.77</cell><cell>100.00</cell></row><row><cell>6</cell><cell>99.10</cell><cell>99.32</cell><cell>99.77</cell><cell>99.61</cell></row><row><cell>Overall</cell><cell>97.05</cell><cell>98.04</cell><cell>98.38</cell><cell>99.15</cell></row><row><cell cols="5">Experiment on BUAA Mobile Gesture Database [47]. This database</cell></row><row><cell cols="5">has 1120 samples for gestures A, B, C, D, 1, 2, 3, 4. Each sample includes three-</cell></row><row><cell cols="5">dimensional acceleration and angular velocity of the mobile phone. The training</cell></row><row><cell cols="5">and testing sets are divided randomly into 70% and 30%, respectively. We</cell></row><row><cell cols="5">conduct the experiments using the same setting for F-BLSTM and F-BGRU. We</cell></row><row><cell cols="5">set θ to 0.1, δ to 0.03 and α to 0.5. Model training is finished at 400 iterations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table . 2</head><label>.</label><figDesc>shows that LSTMs with Fisher criterion still have better results than baselines on a smaller dataset. Similarly, the models converge faster and yield lower classification error rates with the Fisher criterion as shown inFig. 12.</figDesc><table><row><cell>Experiment on SmartWatch Gesture Database [44]. The database</cell></row><row><cell>has been used to evaluate gesture recognition algorithms for interacting with</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The average accuracy(%) of BLSTM, BGRU and our proposed F-BLSTM, F-BGRU on BUAA mobile gesture database.</figDesc><table><row><cell>Method</cell><cell cols="4">BLSTM F-BLSTM BGRU F-BGRU</cell></row><row><cell>Gesture</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>100.00</cell><cell>99.17</cell><cell>98.34</cell><cell>99.58</cell></row><row><cell>B</cell><cell>97.29</cell><cell>98.92</cell><cell>97.84</cell><cell>98.37</cell></row><row><cell>C</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>D</cell><cell>99.26</cell><cell>97.42</cell><cell>96.77</cell><cell>99.35</cell></row><row><cell>1</cell><cell>97.87</cell><cell>99.57</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>2</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>3</cell><cell>97.06</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>4</cell><cell>95.83</cell><cell>97.50</cell><cell>97.08</cell><cell>97.08</cell></row><row><cell>Overall</cell><cell>98.44</cell><cell>99.06</cell><cell>98.75</cell><cell>99.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The average accuracy(%) of BLSTM, BGRU and our proposed F-BLSTM, F-BGRU on SmartWatch gesture database.</figDesc><table><row><cell>Method</cell><cell cols="4">BLSTM F-BLSTM BGRU F-BGRU</cell></row><row><cell>Gesture</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>94.58</cell><cell>97.91</cell><cell>97.08</cell><cell>97.50</cell></row><row><cell>2</cell><cell>95.00</cell><cell>97.22</cell><cell>95.56</cell><cell>95.56</cell></row><row><cell>3</cell><cell>86.90</cell><cell>87.59</cell><cell>93.10</cell><cell>93.10</cell></row><row><cell>4</cell><cell>95.91</cell><cell>97.27</cell><cell>97.27</cell><cell>97.73</cell></row><row><cell>5</cell><cell>96.88</cell><cell>98.13</cell><cell>96.88</cell><cell>98.13</cell></row><row><cell>6</cell><cell>93.33</cell><cell>94.07</cell><cell>96.30</cell><cell>100.00</cell></row><row><cell>7</cell><cell>96.44</cell><cell>96.89</cell><cell>98.22</cell><cell>99.56</cell></row><row><cell>8</cell><cell>97.62</cell><cell>98.57</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>9</cell><cell>93.49</cell><cell>96.74</cell><cell>96.74</cell><cell>97.67</cell></row><row><cell>10</cell><cell>94.84</cell><cell>98.06</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>11</cell><cell>89.76</cell><cell>94.15</cell><cell>94.15</cell><cell>95.12</cell></row><row><cell>12</cell><cell>92.89</cell><cell>92.44</cell><cell>96.00</cell><cell>97.33</cell></row><row><cell>13</cell><cell>90.42</cell><cell>95.00</cell><cell>94.17</cell><cell>95.42</cell></row><row><cell>14</cell><cell>94.88</cell><cell>96.30</cell><cell>96.30</cell><cell>97.21</cell></row><row><cell>15</cell><cell>95.14</cell><cell>95.14</cell><cell>100.00</cell><cell>97.84</cell></row><row><cell>16</cell><cell>92.20</cell><cell>89.27</cell><cell>93.17</cell><cell>93.17</cell></row><row><cell>17</cell><cell>96.52</cell><cell>95.65</cell><cell>99.13</cell><cell>100.00</cell></row><row><cell>18</cell><cell>96.22</cell><cell>97.30</cell><cell>96.76</cell><cell>95.68</cell></row><row><cell>19</cell><cell>94.29</cell><cell>94.76</cell><cell>94.76</cell><cell>96.67</cell></row><row><cell>20</cell><cell>97.21</cell><cell>98.60</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>Overall</cell><cell>94.30</cell><cell>95.65</cell><cell>96.80</cell><cell>97.40</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work was supported in part by the Natural Science </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Campbell, A survey of mobile phone sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename></persName>
		</author>
		<idno type="DOI">10.1109/MCOM.2010.5560598</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="140" to="150" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beatbox music phone: gesture-based interactive mobile phone using a tri-axis accelerometer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C J Y D K S K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonchul</forename><surname>Bang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIT.2005.1600617</idno>
		<idno>102doi:10. 1109/ICIT.2005.1600617</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Industrial Technology</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hand gesture recognition of a mobile device user</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mantyla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mantyjarvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Seppanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tuulari</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2000.869596</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wickramasuriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="DOI">10.1109/PERCOM.2009.4912759</idno>
		<title level="m">uwave: Accelerometer-based personalized gesture recognition and its applications, ieee international conference on pervasive computing and communications</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the use of ensemble of classifiers for accelerometer-based activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Catal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tufekci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pirmit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kocabag</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2015.01.025</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1018" to="1022" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2011.5947611</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International speech Communication Association</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298935</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">3156</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2048" to="2057" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond short snippets: deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299101</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">4694</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.110</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">961</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.516</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4772</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hierarchical deep temporal model for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.217</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298714</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">1110</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.460</idno>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page">4041</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.198</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="914" to="927" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_50</idno>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">816</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic hand gesture recognition for wearable devices with low complexity recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCAS.2016.7539037</idno>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Circuits and Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">2274</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berlemont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40728-4_48</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unobtrusive wearable interaction devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gesturepad</forename></persName>
		</author>
		<idno type="DOI">10.1109/ISWC.2001.962092</idno>
	</analytic>
	<monogr>
		<title level="m">Fifth International Symposium on Wearable Computers</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Signal processing of the accelerometer for gesture awareness on handheld devices, Robot and Human Interactive Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROMAN.2003.1251823</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online gesture recognition system for mobile interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kallio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mantyjarvi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSMC.2003.1244189</idno>
	</analytic>
	<monogr>
		<title level="j">Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2070" to="2076" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A tutorial on human activity recognition using body-worn inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Blanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1145/2499621</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gestural interaction in the pervasive computing landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferscha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Resmerita</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00502-006-0413-4</idno>
	</analytic>
	<monogr>
		<title level="j">Elektrotechnik Und Informationstechnik</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A Single Accelerometer based Wireless Embedded System for Predefined Dynamic Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-81-8489-203-1_18</idno>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<pubPlace>India</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">I am a smartphone and I can tell my user&apos;s walking direction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.1145/2594368.2594392</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="329" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Intelligent classification of heartbeats for automated real-time ecg monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1089/tmj.2014.0033</idno>
	</analytic>
	<monogr>
		<title level="j">Telemedicine journal and e-health : the official journal of the American Telemedicine Association</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">10691077</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gollakota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2867070.2867078</idno>
	</analytic>
	<monogr>
		<title level="m">Contactless sleep apnea detection on smartphones</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive cross-device gait recognition using a mobile accelerometer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.3745/JIPS.2013.9.2.333</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="348" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using mobile phones to write in air</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Constandache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Caves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deruyter</surname></persName>
		</author>
		<idno type="DOI">10.1145/1999995.1999998</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mobile Systems, Applications, and Services</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Velocity profile based recognition of dynamic gestures with discrete hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hommel</surname></persName>
		</author>
		<idno type="DOI">10.1007/BFb0052991</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0959-440X(96)80056-X</idno>
	</analytic>
	<monogr>
		<title level="j">Current opinion in structural biology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="361" to="365" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerometer-based gesture control for a design environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Korpipaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mantyjarvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kallio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Savino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marca</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00779-005-0033-8</idno>
		<idno>doi:10.1007/ s00779-005-0033-8</idno>
	</analytic>
	<monogr>
		<title level="j">Personal and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="285" to="299" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Accelerometer based gesture recognition using continuous hmms, iberian conference on pattern recognition and image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pylvanainen</surname></persName>
		</author>
		<idno type="DOI">10.1007/11492429_77</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">639</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hand gesture recognition and virtual game control based on 3d accelerometer and EMG sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/1502650.1502708</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="401" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accelerometer-based gesture recognition via dynamictime warping, affinity propagation, &amp; compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valaee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2010.5495895</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">2270</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accelerometer-based hand gesture recognition using feature weighted naïve bayesian classifiers and dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coskun</surname></persName>
		</author>
		<idno type="DOI">10.1145/2451176.2451211</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion</title>
		<meeting>the Companion Publication of the 2013 International Conference on Intelligent User Interfaces Companion</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="83" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gesture recognition with a 3-d accelerometer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-02830-4_4</idno>
	</analytic>
	<monogr>
		<title level="j">Ubiquitous Intelligence and Computing</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Integrating LCS and SVM for 3d handwriting recognition on handheld devices using accelerometers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Communications and Information Technology</title>
		<meeting>the 3rd International Conference on Communications and Information Technology</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="195" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accelerometer-based gesture classification using principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Marasovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoftCOM 2011, 19th International Conference on Software, Telecommunications and Computer Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accelerometer based gesture recognition using fusion features and SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.4304/jsw.6.6.1042-1049</idno>
	</analytic>
	<monogr>
		<title level="j">JSW</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1042" to="1049" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Personalizing a smartwatch-based gesture interface with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2530" to="2534" />
		</imprint>
	</monogr>
	<note>22nd European Signal Processing Conference (EUSIPCO</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<imprint>
			<date type="published" when="1259" />
			<biblScope unit="page">1409</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Eprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Gesture recognition benchmark based on mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46654-5_48</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dimensionality reduction: A comparative review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J V D</forename><surname>Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Biography Chunyu Xie received the B.S. degree and is a master in automation from</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">His current research interests include signal and image processing, pattern recognition and computer vision</title>
		<imprint/>
		<respStmt>
			<orgName>Beihang University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">D. degrees in Computer Science from the School of Electronic, Electrical and Communication Engineering at the University of</title>
	</analytic>
	<monogr>
		<title level="m">respectively. She is currently a research assistant with China University of Mining &amp; Technology</title>
		<editor>M.S. and Ph.</editor>
		<meeting><address><addrLine>Tianjin, China; Beijing, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Ce Li received the B.E. degree in Computer Science from Tianjin University</orgName>
		</respStmt>
	</monogr>
	<note>Her current interests include computer vision, video analysis and machine learning. She was supported by the Natural Science Foundation of China for Youth</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">degrees in Computer Science from Harbin Institue of the Technology</title>
		<editor>Baochang Zhang received the B.S., M.S. and Ph.D.</editor>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Harbin, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
