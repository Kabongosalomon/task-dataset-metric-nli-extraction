<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asynchronous Temporal Fields for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Asynchronous Temporal Fields for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/gsig/temporal-fields/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Actions are more than just movements and trajectories: we cook to eat and we hold a cup to drink from it. A thorough understanding of videos requires going beyond appearance modeling and necessitates reasoning about the sequence of activities, as well as the higher-level constructs such as intentions. But how do we model and reason about these? We propose a fully-connected temporal CRF model for reasoning over various aspects of activities that includes objects, actions, and intentions, where the potentials are predicted by a deep network. End-to-end training of such structured models is a challenging endeavor: For inference and learning we need to construct mini-batches consisting of whole videos, leading to mini-batches with only a few videos. This causes high-correlation between data points leading to breakdown of the backprop algorithm. To address this challenge, we present an asynchronous variational inference method that allows efficient end-to-end training. Our method achieves a classification mAP of 22.4% on the Charades [43] benchmark, outperforming the state-of-the-art (17.2% mAP), and offers equal gains on the task of temporal localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the video shown in <ref type="figure" target="#fig_1">Figure 1</ref>: A man walks through a doorway, stands at a table, holds a cup, pours something into it, drinks it, puts the cup on the table, and finally walks away. Despite depicting a simple activity, the video involves a rich interplay of a sequence of actions with underlying goals and intentions. For example, the man stands at the table 'to take a cup', he holds the cup 'to drink from it', etc. Thorough understanding of videos requires us to model such interplay between activities as well as to reason over extensive time scales and multiple aspects of actions (objects, scenes, etc).</p><p>Most contemporary deep learning based methods have treated the problem of video understanding as that of only appearance and motion (trajectory) modeling <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b6">7</ref>, * Work was done while Gunnar was an intern at AI2.   <ref type="bibr" target="#b27">28]</ref>. While this has fostered interesting progress in this domain, these methods still struggle to outperform models based on hand-crafted features, such as Dense Trajectories <ref type="bibr" target="#b56">[57]</ref>. Why such a disconnect? We argue that video understanding requires going beyond appearance modeling, and necessitates reasoning about the activity sequence as well as higher-level constructs such as intentions. The recent emergence of large-scale datasets containing rich sequences of realistic activities <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b60">61]</ref> comes at a perfect time facilitating us to explore such complex reasoning. But what is the right way to model and reason about temporal relations and goal-driven behaviour? Over the last couple of decades, graphical models such as Conditional Random Fields (CRFs) have been the prime vehicles for structured reasoning. Therefore, one possible alternative is to use ConvNet-based approaches <ref type="bibr" target="#b19">[20]</ref> to provide features for a CRF training algorithm. Alternatively, it has been shown that integrating CRFs with ConvNet architectures and training them in an end-to-end manner provides substantial improvements in tasks such as segmentation and situation recognition <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Inspired by these advances, we present a deep-structured model that can reason temporally about multiple aspects of activities. For each frame, our model infers the activity cate-gory, object, action, progress, and scene using a CRF, where the potentials are predicted by a jointly end-to-end trained ConvNet over all predictions in all frames. This CRF has a latent node for the intent of the actor in the video and pairwise relationships between all individual frame predictions.</p><p>While our model is intuitive, training it in an end-to-end manner is a non-trivial task. Particularly, end-to-end learning requires computing likelihoods for individual frames and doing joint inference about all connected frames with a CRF training algorithm. This is in stark contrast with the standard stochastic gradient descent (SGD) training algorithm (backprop) for deep networks, where we require mini-batches with a large number of independent and uncorrelated samples, not just a few whole videos. In order to handle this effectively: <ref type="bibr" target="#b0">(1)</ref> we relax the Markov assumption and choose a fully-connected temporal model, such that each frame's prediction is influenced by all other frames, and (2) we propose an asynchronous method for training fully-connected structured models for videos. Specifically, this structure allows for an implementation where the influence (messages) from other frames are approximated by emphasizing influence from frames computed in recent iterations. They are more accurate, and show advantage over being limited to only neighboring frames. In addition to being more suitable for stochastic training, fullyconnected models have shown increased performance on various tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>In summary, our key contributions are: (a) a deep CRF based model for structured understanding and comprehensive reasoning of videos in terms of multiple aspects, such as action sequences, objects, and even intentions; (b) an asynchronous training framework for expressive temporal CRFs that is suitable for end-to-end training of deep networks; and, (c) substantial improvements over state-of-theart, increasing performance from 17.2% mAP to 22.4% mAP on the challenging Charades <ref type="bibr" target="#b42">[43]</ref> benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Understanding activities and actions has an extensive history <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref>. Interestingly, analyzing actions by their appearance has gone through multiple iterations. Early success was with handcrafted representations such as Space Time Interest Points (STIP) <ref type="bibr" target="#b22">[23]</ref>, 3D Histogram of Gradient (HOG3D) <ref type="bibr" target="#b16">[17]</ref>, Histogram of Optical Flow (HOF) <ref type="bibr" target="#b23">[24]</ref>, and Motion Boundary Histogram <ref type="bibr" target="#b1">[2]</ref>. These methods capture and analyze local properties of the visual-temporal datastream. In the past years, the most prominent hand-crafted representations have been from the family of trajectory based approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref>, where the Improved Dense Trajectories (IDT) <ref type="bibr" target="#b56">[57]</ref> representation is in fact on par with state-of-the-art on multiple recent datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Recently there has been a push towards mid-level rep-resentations of video <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>, that capture beyond local properties. However, these approaches still used handcrafted features. With the advent of deep learning, learning representations from data has been extensively studied <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref>. Of these, one of the most popular frameworks has been the approach of Simonyan et al. <ref type="bibr" target="#b44">[45]</ref>, who introduced the idea of training separate color and optical flow networks to capture local properties of the video. Many of those approaches were designed for short clips of individual activities and hence do not generalize well to realistic sequences of activities. Capturing the whole information of the video in terms of temporal evolution of the video stream has been the focus of some recent approaches <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b30">31]</ref>. Moving towards more expressive deep networks such as LSTM has become a popular method for encoding such temporal information <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b64">65]</ref>. Interestingly, while those models move towards more complete understanding of the full video stream, they have yet to significantly outperform local methods <ref type="bibr" target="#b44">[45]</ref> on standard benchmarks.</p><p>A different direction in understanding comes from reasoning about the complete video stream in a complementary direction -Structure. Understanding activities in a human-centric fashion encodes our particular experiences with the visual world. Understanding activities with emphasis on objects has been a particularly fruitful direction <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">55]</ref>. In a similar vein, some works have also tried modeling activities as transformations <ref type="bibr" target="#b58">[59]</ref> or state changes <ref type="bibr" target="#b4">[5]</ref>. Recently, there has been significant progress in modelling the complete human-centric aspect, where image recognition is phrased in terms of objects and their roles <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b9">10]</ref>. Moving beyond appearance and reasoning about the state of agents in the images requires understanding human intentions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>. This ability to understand people in terms of beliefs and intents has been traditionally studied in psychology as the Theory of mind <ref type="bibr" target="#b33">[34]</ref>. How to exactly model structure of the visual and temporal world has been the pursuit of numerous fields. Of particular interest is work that combines the representative power of deep networks with structured modelling. Training such models is often cumbersome due to the differences in jointly training deep networks (stochastic sampling) and sequential models (consecutive samples) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b66">67]</ref>. In this work, we focus on fully-connected random fields, that have been popular in image segmentation <ref type="bibr" target="#b18">[19]</ref>, where image filtering was used for efficient message passing, and later extended to use CNN potentials <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Given a video with multiple activities, our goal is to understand the video in terms of activities. Understanding activities requires reasoning about objects being interacted Hallway Scene <ref type="figure">Figure 2</ref>. An overview of our structured model. The semantic part captures object, action, etc. at each frame, and temporal aspects captures those over time. On the left side, we show how for each timepoint in the video, a Two-Stream Network predicts the potentials. Our model jointly reasons about multiple aspects of activities in all video frames. The Intent captures groups of activities of the person throughout the whole sequence of activities, and fine-grained temporal reasoning is through fully-connected temporal connections.</p><p>with, the place where the interaction is happening, what happened before and what happens after this current action and even the intent of the actor in the video. We incorporate all these by formulating a deep Conditional Random Field (CRF) over different aspects of the activity over time. That is, a video can be interpreted as a graphical model, where the components of the activity in each frame are nodes in the graph, and the model potentials are the edges in the graph.</p><p>In particular, we create a CRF which predicts activity, object, etc., for every frame in the video. For reasoning about time, we create a fully-connected temporal CRF, referred as Asynchronous Temporal Field in the text. That is, unlike a linear-chain CRF for temporal modelling (the discriminative counterpart to Hidden Markov Models), each node depends on the state of every other node in the graph. We incorporate intention as another latent variable which is connected to all the action nodes. This is an unobserved variable that influences the sequence of activities. This variable is the common underlying factor that guides and better explains the sequence of actions an agent takes. Analysis of what structure this latent variable learns is presented in the experiments. Our model has three advantages: (1) it addresses the problem of long-term interactions; (2) it incorporates reasoning about multiple parts of the activity, such as objects and intent; and (3) more interestingly, as we will see, it allows for efficient end-to-end training in an asynchronous stochastic fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>In this work we encode multiple components of an activity. Each video with T frames is represented as {X 1 , . . . , X T , I} where X t is a set of frame-level random variables for time step t and I is an unobserved random variable that represent global intent in the entire video. We</p><formula xml:id="formula_0">can further write X t = {C t , O t , A t , P t , S t },</formula><p>where C is the activity category (e.g., 'drinking from cup'), O corresponds to the object (e.g., 'cup'), A represents the action (e.g., 'drink'), P represents the progress of the activity {start, middle, end}, and S represents the scene (e.g. 'Dining Room'). For clarity in the following derivation we will refer to all the associated variables of X t as a single random variable X t . A more detailed description of the CRF is presented in the appendix.</p><p>Mathematically we consider a random field {X, I} over all the random variables in our model ({X 1 , . . . , X T , I}). Given an input video V ={V 1 , . . . , V T }, where V t is a video frame, our goal is to estimate the maximum a posteriori labeling of the random field by marginalizing over the intent I. This can be written as:</p><formula xml:id="formula_1">x * = arg max x I P (x, I|V ).<label>(1)</label></formula><p>For clarity in notation, we will drop the conditioning on V and write P (X, I). We can define P (X, I) using Gibbs distribution as: P (X, I)= 1 Z(V) exp (−E(x, I)) where E(x, I) is the Gibbs energy over x. In our CRF, we model all unary and pairwise cliques between all frames {X 1 , . . . , X T } and the intent I. The Gibbs energy is:</p><formula xml:id="formula_2">E(x, I) = i φ X (x i ) Semantic + i φ XI (x i , I) + i,j i =j φ XX (x i , x j ) Temporal ,<label>(2)</label></formula><p>where φ XX (x i , x j ) is the potential between frame i and frame j, and φ XI (x i , I) is the potential between frame i and the intent. For notational clarity φ X (x i ) incorporates all unary and pairwise potentials for C t , O t , A t , P t , S t . The model is best understood in terms of two aspects: Semantic  <ref type="figure">Figure 3</ref>. Illustration of the learning algorithm, and the message passing structure. Each timepoint that has been processed has a message (Blue highlights messages that have recently been computed). The loss receives a combination of those messages, uses those to construct new messages, and updates the network. aspect, which incorporates the local variables in each frame (C t , O t , A t , P t , S t ); and Temporal aspect, which incorporates interactions among frames and the intent I. This is visualized in <ref type="figure">Figure 2</ref>. We will now explain the semantic, and temporal potentials. Semantic aspect The frame potential φ X (x i ) incorporates the interplay between activity category, object, action, progress and scene, and could be written explicitly as φ X (C t , O t , A t , P t , S t ).</p><p>In practice this potential is composed of unary, pairwise, and tertiary potentials directly predicted by a CNN. We found predicting only the following terms to be sufficient without introducing too many additional parameters:</p><formula xml:id="formula_3">φ X (C t , O t , A t , P t , S t )=φ(O t , P t )+φ(A t , P t )+φ(O t , S t )+ φ(C t , O t , A t , P t )</formula><p>where we only model the assignments seen in the training set, and assume others are not possible. Temporal aspect The temporal aspect of the model is both in terms of the frame-intent potentials φ XI (x i , I) and frame-frame potentials φ XX (x i , x j ). The frame-intent potentials are predicted with a CNN from video frames (pixels and motion). The pairwise potentials φ XX (x i , x j ) for two time points i and j in our model have the form:</p><formula xml:id="formula_4">φ XX (x i , x j ) = µ(x i , x j ) m w (m) k (m) (v i , v j ),<label>(3)</label></formula><p>where µ models the asymmetric affinity between frames, w are kernel weights, and each k (m) is a Gaussian kernel that depends on the videoframes v i and v j . In this work we use a single kernel that prioritises short-term interactions:</p><formula xml:id="formula_5">k(v i , v j ) = exp − (j − i) 2 2σ 2<label>(4)</label></formula><p>The parameters of the general asymmetric compatibility function µ(x i , x j ) are learned from the data, and σ is a hyper-parameter chosen by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference</head><p>While it is possible to enumerate all variable configurations in a single frame, doing so for multiple frames and their interactions is intractable. Our algorithm uses a structured variational approximation to approximate the full probability distribution. In particular, we use a mean-field approximation to make inference and learning tractable. With this approximation, we can do inference by keeping track of message between frames, and asynchronously train one frame at a time (in a mini-batch fashion).</p><p>More formally, instead of computing the exact distribution P (X, I) presented above, the structured variational approximation finds the distribution Q(X, I) among a given family of distributions that best fits the exact distribution in terms of KL-divergence. By choosing a family of tractable distributions, it is possible to make inference involving the ideal distribution tractable. Here we use Q(X, I) = Q I (I) i Q i (x i ), the structured mean-field approximation. Minimizing the KL-divergence between those two distributions yields the following iterative update equation:</p><formula xml:id="formula_6">Q i (x i ) ∝ exp φ X (x i ) + E U ∼Q I [φ XI (x i , U )] + j&gt;i E U j ∼Q j [φ XX (x i , U j )] + j&lt;i E U j ∼Q j [φ XX (U j , x i )] (5) Q I (I) ∝ exp j E U j ∼Q j [φ XI (U j , I)]<label>(6)</label></formula><p>where Q i is marginal distribution with respect to each of the frames, and Q I is the marginal with respect to the intent. An algorithmic implementation of this equation is as presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Inference for Asynchronous Temporal Fields</head><formula xml:id="formula_7">1: Initialize Q Uniform distribution 2: while not converged do 3:</formula><p>Visit frame i 4:</p><p>Get</p><formula xml:id="formula_8">j&gt;i E U j ∼Q j [φ XX (x i , U j )] 5: Get j&lt;i E U j ∼Q j [φ XX (U j , x i )] 6: Get j E U j ∼Q j [φ XI (U j , I)] 7:</formula><p>while not converged do 8:</p><p>Update Q i and Q I using Eq. 6 9:</p><p>Send</p><formula xml:id="formula_9">E U ∼Q i [φ XX (x, U )] 10: Send E U ∼Q i [φ XX (U, x)] 11: Send E U ∼Q i [φ XI (U, I)]</formula><p>Here 'Get' and 'Send' refer to the message server, and f (x) is a message used later by frames in the same video. The term message server is used for a central process that  <ref type="figure">Figure 4</ref>. Evolution of prediction with increasing messages passes. The first row shows the initial prediction for the category tidying with a broom without any message passing, where darker colors correspond to higher likelihood, blue is then an increase in likelihood, and brown decrease. In the first message pass, the confidence of high predictions gets spread around, and eventually increases the confidence of the whole prediction.</p><p>distributes them accordingly when requested. In practice, this could be implemented in a multi-machine setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning</head><p>Training a deep CRF model requires calculating derivatives of the objective in terms of each of the potentials in the model, which in turn requires inference of P (X, I|V ). The network is trained to maximize the log-likelihood of the data l(X) = log I P (x, I|V ). The goal is to update the parameters of the model, for which we need gradients with respect to the parameters. Similar to SGD, we find the gradient with respect to one part of the parameters at a time, specifically with respect to one potential in one frame. That is, φ i X (x) instead of φ X (x). The partial derivatives of this loss with respect to each of the potentials are as follows:</p><formula xml:id="formula_10">∂l(X) ∂φ i X (x) = 1 x=x − Q i (x)<label>(7)</label></formula><p>∂l(X)</p><formula xml:id="formula_11">∂φ i XI (x,Î) = exp j φ XI (x j ,Î) I exp j φ XI (x j , I) 1 x=x − Q i (x)Q I (Î) (8) ∂l(X) ∂µ i (a, b) = j&gt;i 1x=ak(v i , v j ) − Q i (x) j&gt;i Q I (b)k(v i , v j ) + j&lt;i 1 x=b k(v j , v i ) − Q i (x) j&lt;i Q I (a)k(v i , v j ) (9) where φ i X (x) and φ i XI (x,Î)</formula><p>is the frame and frame-intent potentials of frame i, and we usex to distinguish between the labels and variables the derivative is taken with respect to. µ i (a, b) are the parameters of the asymmetric affinity kernel with respect to frame i, and 1 x=x is a indicator variable that has the value one if the ground truth label corresponds to the variable. Complete derivation is presented in the appendix. These gradients are used to update the underlying CNN model. These update equations lead to the learning procedure presented in Algorithm 2. <ref type="figure">Figure 3</ref> graphically illustrates the learning procedure. Since the videos are repeatedly visited throughout the training process, we do not have to run multiple message passes Algorithm 2 Learning for Asynchronous Temporal Fields 1: Given videos V 2: while not converged do 3:</p><p>for each example in mini-batch do 4:</p><p>Sample frame v ∈ V ⊆ V 5:</p><p>Get incoming messages 6:</p><p>Update Q i and Q I 7:</p><p>Find gradients with Eq. 7-9 8:</p><p>Backprop gradients through CNN 9:</p><p>Send outgoing messages to calculate each partial gradient. This shares ideas with contrastive divergence <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>. Given a single video at test time, we visualize in <ref type="figure">Figure 4</ref> how the predictions changes as the distribution converges with multiple messages passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message Passing</head><p>The key thing to note is all the incoming messages are of the form</p><formula xml:id="formula_12">M (z)= j f j (z) where f j is some function from node j; for e.g., M (z) = j E Uj ∼Qj [φ XI (U j , z)] = j f j (z)</formula><p>from Algorithm 1. We use the following approximation during training:</p><formula xml:id="formula_13">M (z)≈ h j d j j d j f J(j) (z),<label>(10)</label></formula><p>where d ∈ [0, 1] is a discount factor, h is a hyperparameter, and J(·) is an ordering of the messages in that video based on the iteration in which the message was computed. The messages are a weighted combination of stored messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results and Analysis</head><p>We analyzed the efficacy of our model on the challenging tasks of video activity classification and temporal localization. In addition, we investigated the different parts of the model, and will demonstrate how they operate together. Dataset Recent years have witnessed an emergence of large-scale datasets containing sequences of common daily activities <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b60">61]</ref>. For our evaluation, we chose the Charades dataset <ref type="bibr" target="#b42">[43]</ref>. This dataset is a challenging benchmark containing 9,848 videos across 157 action classes with 66,500 annotated activities, including nouns (objects), verbs (actions), and scenes. A unique feature of this dataset is the presence of complex co-occurrences of realistic humangenerated activities making it a perfect test-bed for our analysis. We evaluate video classification using the evaluation criteria and code from <ref type="bibr" target="#b42">[43]</ref>. Temporal localization is evaluated in terms of per-frame classification using the provided temporal annotations. Implementation details We use a VGG16 network <ref type="bibr" target="#b45">[46]</ref> with additional layers to predict the model potentials <ref type="figure" target="#fig_3">(Figure 5</ref>). We train both a network on RGB frames, and stacks of optical flow images, following the two-stream architecture <ref type="bibr" target="#b44">[45]</ref>. The main challenge in training the network is the increase in the output layer size. For the larger potentials, we used the following structure to go from fc7 to φ XI : Linear layer (4096 to 100), ReLU, Dropout, Linear layer (100 to the potential values). The input to the RGB network is an image of size 224×224×3 where we crop random location, size, and aspect ratio. We use data augmentation with color jitter and PCA lighting noise. The RGB network was pretrained on ImageNet. The input to the Flow network is a stack of 10 consecutive optical flow frames at 24 FPS starting with the current frame. Since each optical flow has two channels, the input size is 224×224×20 as in <ref type="bibr" target="#b44">[45]</ref>. The Flow network was pretrained on UCF101 <ref type="bibr" target="#b47">[48]</ref> as in Sigurdsson et al. <ref type="bibr" target="#b42">[43]</ref>, and random cropped in the same way as RGB.</p><p>We follow the training setup in Charades <ref type="bibr" target="#b42">[43]</ref> and consider a frame to have one activity label at a time. Even so, our method is still able to reason about other activities in the video. Convergence of the model is evaluated using the approximate distribution Q i (X) at each frame. The Charades dataset has the property that scenes were chosen at random for each sequence of activities. For this reason, we found reasoning about scenes to reduce the performance, and the weight of that term was lowered in the model.</p><p>To obtain annotations for action progress p t , we split each activity annotation into three equally sized parts. All layers of the network are trained with a batch size of 240 and a learning rate of 10 −3 (RGB), 10 −5 (Flow). Learning rate was reduced by a factor of 10 every 30k iterations for RGB, and every 140k iterations for Flow. The value of the message decay parameter d was set to d = 0.9, and the standard deviation σ in (4) was set to 6.25 sec (150 frames).</p><p>For testing, we sampled 25 equally spaced frames from the video and synchronously pass messages between the frames until convergence (10 message passes). The predictions of the RGB and Flow networks are combined in a probabilistic fashion by multiplying their probabilistic predictions for each class. More implementation details may be found in the appendix. The networks were implemented in Torch, and the code is available on project page. Diverse batches As highlighted in Section 1, the standard  <ref type="figure">Figure 6</ref>. Convergence of our method compared to other methods that capture temporal structure. Our asynchronous training method contains more diverse batches, has faster and more stable convergence, and reaches higher accuracy on the test set.</p><p>way of sampling batches for temporal models results in high correlation between data points leading to a breakdown of the SGD. To understand the importance of having many diverse examples from multiple videos, we compare the convergence of our method to two alternatives using homogeneous batches: CNN+LSTM from Ng et al. <ref type="bibr" target="#b65">[66]</ref>, and a synchronous version of our method, where each batch contains full videos (only three videos fit into each mini-batch).</p><p>We do synchronous message passing until convergence before calculating gradients for backprop. <ref type="figure">Figure 6</ref> shows that our asynchronous training method, containing more diverse training batches, has faster and more stable convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Classification</head><p>Given a video, the task here is to verify whether it contains one or several of the 157 activity categories. Classification accuracy is measured with the standard mean average precision (mAP) criterion, where a prediction is given for each video. This task has been shown to be highly challenging, with the state-of-the-art non-ensemble methods reaching an mAP of only 17.2%, particularly as each video in this dataset has a sequence of multiple fine-grained activities with a real-world long-tailed activity distribution.</p><p>We trained our models using the provided training split following the procedure outlined in Section 3. To make predictions for the whole video, we marginalize out everything except the activity category for 25 equidistant frames in the video. The score for each activity category is the maximum across all frames following the setup from <ref type="bibr" target="#b42">[43]</ref>. In our analysis, we include the provided non-ensemble baselines from <ref type="bibr" target="#b42">[43]</ref> as well as the following additional baselines:</p><p>Two-Stream++. We reimplemented the network described in <ref type="bibr" target="#b42">[43]</ref>, which follows Simonyan et al. <ref type="bibr" target="#b45">[46]</ref>, with the same parameters. We added data augmentation and finetuned all layers of the network. The performance of only the RGB stream is included (RGB++). We also consider Two-Stream Extended which is the same network, but the Flow network was trained for 25 times more iterations than the RGB network (two weeks of computation on a Titan X  <ref type="figure">Figure 7</ref>. The classes with the highest positive and negative difference between our method and Two-Stream (no structure). Our method does better on many classes, without doing much worse on any. In particular, activities that have temporal structure, such as Opening/Closing a refrigerator have significantly higher performance, since our model can reason jointly about those. GPU). Combined with the augmentation, we found this to non-trivially increase the accuracy. Two-Stream+LSTM. We followed the method outlined in <ref type="bibr" target="#b65">[66]</ref> to jointly train a LSTM on top of the two-stream network. We trained both an RGB and an Optical Flow network using the same setup from <ref type="bibr" target="#b42">[43]</ref>. The trained networks from Two-Stream++ were used to initialize the models. <ref type="table" target="#tab_2">Table 1</ref> displays the accuracy obtained by our method along with the baselines. Our proposed approach obtains an mAP of 22.4% substantially outperforming the Twostream Extended baseline at 18.6% mAP, and the IDT baseline at 17.2%. Our method reasons over significantly larger timescales and multiple aspects of the activities. To ascertain this, we highlight in <ref type="figure">Figure 7</ref>, the activity classes with the highest positive and negative difference between our method and the Two-Stream network. It is interesting to note that two of those activities are opening and closing a refrigerator, that arguably have a significant causal structure (an open refrigerator was opened at some point), which our model harnesses to significantly increase the accuracy.</p><p>Ablation studies To study the contribution of different model parts, we also train ablated versions of our model separately choosing the best hyperparameters for each version. In addition to our model with only RGB or Flow, we also consider dropping φ XX (i.e., no sequential informa-  tion), φ XI (i.e., no intent), both (i.e., only semantic information), and further dropping φ X (i.e., dropping all structure). <ref type="figure">Figure 8</ref> shows that semantic reasoning improves over the baseline. Further, while both φ XI and φ XX capture temporal information, they are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temporal Localization</head><p>To measure the ability of the methods to temporally localize and understand when exactly activities happen, we adapt the benchmark of <ref type="bibr" target="#b42">[43]</ref> to evaluate with the same mAP metric but on individual frames. That is, instead of having a single prediction per video, evaluation is now split into 25 equidistant timepoints having zero or more activities, and the models make a prediction for each of those * . We find this way of evaluating localization robust to annotation ambiguity, and informative for challenging datasets. All hyperparameters were kept equal between localization and classification experiments. All baselines are run on 75 frames across the video, and then every third frame selected for a total of 25 frames. We also considered methods with postprocessing where the model predictions for the 75 frames are averaged across 30 frames to obtain more spatial consistency, and then 25 frames selected as before. <ref type="table" target="#tab_4">Table 2</ref> shows that our method outperforms the alternatives, including the LSTM model which has been shown to be a powerful temporal modeling tool, but challenging to train on top of a two-stream network due to correlations between consecutive samples. These results demonstrate the our method is tractable way of training end-to-end structured models to understand activities. Interestingly, our method still benefits from adding post-processing, significantly more than the LSTM baseline, likely since our method is reasoning on larger time-scales. This suggests <ref type="figure">Figure 9</ref>. Model predictions for a sample video. We see the interplay between categories, objects and actions over time. For example, model becomes confident about the action sit early, which aids the understanding of Sitting in a chair once the chair becomes visible, and helps predicting Reading a book. Darker colors represent higher likelihood, and we average predictions to correspond to each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster 1</head><p>Cluster 2 Cluster 3 <ref type="figure" target="#fig_1">Figure 10</ref>. To visualize the learned intent, we cluster videos based on intent. In Cluster 1, the model captures the intent of get up from lying down. In Cluster 2, folding clothes is followed by putting them away, and Cluster 3 shows cleaning with a broom/vacuum/towel, followed by picking up things.</p><p>that our model could further benefit from joint training with additional kernels in the temporal term.</p><p>Qualitative visualization A key advantage of our model is the structured understanding of videos in terms of multiple aspects, such as action sequences, objects, and even intentions. To visualize this, we display predictions over time in <ref type="figure">Figure 9</ref> for the three most confident activity categories, two most confident actions, and the most confident object. More examples are presented in the Appendix.</p><p>Interpretation of Intent In our model, the intent I is a continuous distribution over the latent variables. To get an insight into how our model learns the intent, we ran a simple experiment that clustered videos in the dataset that have the most similar inferred intent distributions. The first cluster in <ref type="figure" target="#fig_1">Figure 10</ref> shows the model captures the simple intent that the person intends to get up from lying down. In the videos, these actions are 10-20 seconds apart, demonstrating that the intent helps reason over large time scales. In order to further analyze the 'intent' variable, we plot the t-SNE embedding of the intent variable for the videos in the test set. We see that there is clear clustering of similar videos in <ref type="figure" target="#fig_1">Fig. 11a</ref>. We also annotated 10 types of intent (100 videos total). More details are presented in the Appendix. We observe that the intent representation preserves some of the intent types in <ref type="figure" target="#fig_1">Fig. 11b</ref>. Quantitatively, even without mitigating outliers, the average distance (in 10 −3 ) between pairs of videos within an intent type was 6.02 compared to 7.25 (σ=1.06) for any points, and the difference is significant for 5 of 10 intent types (p=0.1). This tentatively suggest that the intent captures interesting structure in the data, and we hope this will encourage future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a deep-structured model using a fully-connected temporal CRF that not only models semantic aspects of activities but also reasons about longterm temporal relations. We also presented an asynchronous stochastic inference algorithm that circumvents a key bottleneck in the large-scale end-to-end model learning. Using our proposed method, we have demonstrated impressive activity classification and temporal localization results on a challenging dataset of realistic activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Description of the CRF</head><p>We create a CRF which predicts activity, object, etc., for every frame in the video. For reasoning about time, we create a fully-connected temporal CRF, referred to as Asynchronous Temporal Field in the text. That is, unlike a linear-chain CRF for temporal modelling (the discriminative counterpart to Hidden Markov Models), each node depends on the state of every other node in the graph. We incorporate intention as another latent variable which is connected to all the action nodes.</p><p>In this work we encode multiple components of an activity. Each video with T frames is represented as {X 1 , . . . , X T , I} where X t is a set of frame-level random variables for time step t and I is a random variable that represent global intent in the entire video. As discussed in the paper, for clarity of derivation X t includes all frame level variables (C t , O t , A t , P t , S t )</p><p>Mathematically we consider a random field {X, I} over all the random variables in our model ({X 1 , . . . , X T , I}). We now list the complete description of the CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF Variables:</head><p>• Random field {X, I} = {X 1 , . . . , X T , I} CRF Potentials:</p><formula xml:id="formula_14">• Frame X t = {C t , O t , A t , P t , S t }, X t ∈ X , X = C×O×A×P×S</formula><formula xml:id="formula_15">• φ X : X → R, equivalently: φ X : C×O×A×P×S → R • φ X decomposes as follows: φ X (C t , O t , A t , P t , S t )=φ(O t , P t )+φ(A t , P t )+φ(O t , S t )+φ(C t , O t , A t , P t ) -φ(O t , P t ) : O×P → R -φ(A t , P t ) : A×P → R -φ(O t , S t ) : O×S → R -φ(C t , O t , A t , P t ) : B → R, here B is all configurations of C t , O t ,</formula><p>A t , P t that exist in the training data.</p><p>• φ XI : X ×I → R (specifically we parametrize this as φ XI : O×I → R)</p><p>• φ XX : X ×X → R (specifically we parametrize this as φ XI : O×O → R) <ref type="figure" target="#fig_1">Figure 12</ref>. The model captures interactions between all frames Xt and the intent I, that is, a fully-connected model. Here shown for T = 5. We visualize some of the potentials of the model, and where they fit into the graph. All φ i XI share the same parameters, but we calculate the gradients with respect for each of them separately below. For efficient inference, we use a mean-field approximation presented below. A mean-field approximation is a simpler distribution that is fit to the original distribution when needed.</p><p>The complete distribution of the model is:</p><formula xml:id="formula_16">P (X, I) = 1 Z exp    i φ i X (x i ) + i φ i XI (x i , I) + i j =i φ i XX (x i , x j )   <label>(11)</label></formula><p>where φ XX (x i , x j ) is the potential between frame i and frame j, and φ XI (x i , I) is the potential between frame i and the intent. For notational clarity φ X (x i ) incorporates all potentials for C t , O t , A t , P t , S t . The model is presented in <ref type="figure" target="#fig_1">Figure 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Derivation of the Update Equations</head><p>Given an input video V ={V 1 , . . . , V T }, our goal is to estimate the maximum a posteriori labeling of the random field by marginalizing over the intent I, I P (X, I|V ) as discussed in the paper. In the following derivations we omit the conditioning on V and write P (X, I) and φ(X, I).</p><p>Before we present the update equations and gradients, we define the following messages which will be used in the final version of the following equations for clarity in their presentation. Messages are a term used for cached computations sent between different functions in a dynamic programming fashion. In the following derivations, X * is used to explicitly denote the ground truth used for training. Plain X is used to refer to the variable.</p><p>Outgoing Messages (Messages that are calculated from a single frame)</p><formula xml:id="formula_17">FA j (x j ) = E U ∼Qj [µ(x j , U )] (12) FB j (x j ) = E U ∼Qj [µ(U, x j )] (13) H j (I) = E U ∼Qj [φ XI (U, I)] (14) H * j (I) = φ XI (x * j , I)<label>(15)</label></formula><formula xml:id="formula_18">K j (x j ) = Q j (x j ) (16) K * j (x j ) = 1 xj =x * j<label>(17)</label></formula><p>Incoming Messages (Messages that are calculated from messages from multiple frames and used for the computation of a single frame)</p><formula xml:id="formula_19">FA i (x i ) = j&gt;i E Uj ∼Qj [µ(x i , U j )]K(v i , v j ) = j&gt;i FA j (x i )K(v i , v j ) (18) FB i (x i ) = j&lt;i E Uj ∼Qj [µ(U j , x i )]K(v j , v i ) = j&lt;i FB j (x i )K(v j , v i )<label>(19)</label></formula><formula xml:id="formula_20">H i (I) = j =i E Uj ∼Qj [φ XI (U j , I)] = j =i H j (I)<label>(20)</label></formula><formula xml:id="formula_21">H * i (I) = j =i φ XI (x * j , I) = j =i H * j (I)<label>(21)</label></formula><formula xml:id="formula_22">KA i (x i ) = j&gt;i Q j (x j )K(x i , x j ) = j&gt;i K j (x i )<label>(22)</label></formula><formula xml:id="formula_23">KA * i (x i ) = j&gt;i 1 xj =x * j K(x i , x * j ) = j&gt;i K * j (x i )<label>(23)</label></formula><formula xml:id="formula_24">KB i (x i ) = j&lt;i Q j (x j )K(x j , x i ) = j&lt;i K j (x i )<label>(24)</label></formula><formula xml:id="formula_25">KB * i (x i ) = j&lt;i 1 xj =x * j K(x * j , x i ) = j&lt;i K * j (x i )<label>(25)</label></formula><p>Instead of computing the exact distribution P (X, I) presented above, the structured variational approximation finds the distribution Q(X, I) among a given family of distributions that best fits the exact distribution in terms of KL-divergence. By choosing a family of tractable distributions, it is possible to make inference involving the ideal distribution tractable. Here we use Q(X, I) = Q I (I) i Q i (x i ), the structured mean-field approximation. More details on mean-field approximation are presented section 11.5 generic update equation for Q (Equation 11.54 in <ref type="bibr" target="#b17">[18]</ref>) is:</p><formula xml:id="formula_26">Q(x i ) ∝ exp E X−i∼Q [log P (x i |X −i )]<label>(26)</label></formula><p>where X −i refers to all variables except x i . Using Eq. 11 along with Eq. 26 we get the following update equations:</p><formula xml:id="formula_27">Q i (x i ) ∝ exp φ X (x i ) + E U ∼Q I [φ XI (x i , U )] + j&gt;i E Uj ∼Qj [φ XX (x i , U j )] + j&lt;i E Uj ∼Qj [φ XX (U j , x i )] ∝ exp φ X (x i ) + E U ∼Q I [φ XI (x i , U )] + FA i (x i ) + FB i (x i )<label>(27)</label></formula><formula xml:id="formula_28">Q I (I) ∝ exp j E Uj ∼Qj [φ XI (U j , I)]<label>(28)</label></formula><p>∝ exp H i (I) + H i (I) (Here i refers to the frame of interest, but any choice of i holds)</p><p>where Q i is marginal distribution with respect to each of the frames, and Q I is the marginal with respect to the intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Details of the learning algorithm</head><p>Training a deep CRF model requires calculating derivatives of the objective in terms of each of the potentials in the model, which in turn requires inference of P (X, I|V ). The network is trained to maximize the log-likelihood of the data:</p><formula xml:id="formula_30">l(X * ) = log I P (X * , I|V ) (30) = log IP (X * , I|V ) Z(V ) (31) = log IP (X * , I|V ) − log Z(V )<label>(32)</label></formula><formula xml:id="formula_31">Z(V ) = I XP (X, I|V )<label>(33)</label></formula><p>where we explicitly write out the partition function Z(V), andP () is the unnormalized version of P (). Again, we use X * to explicitly refer to the ground truth labels. As before, V is omitted from the following derivations. The goal is to update the parameters of the model, for which we need gradients with respect to the parameters. Similar to SGD, we find the gradient with respect to one part of the parameters at a time, specifically with respect to one potential in one frame. That is, φ i X (x) instead of φ X (x). The partial derivatives of this loss with respect to each of the potentials are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Updating the frame potential φ X</head><p>The frame potential φ X (x i ) incorporates the interplay between activity category, object, action, progress and scene, and could be written explicitly as φ X (C t , O t , A t , P t , S t ). In practice this potential is composed of unary, pairwise, and tertiary potentials directly predicted by a CNN. We found predicting only the following terms to be sufficient without introducing too many additional parameters:</p><formula xml:id="formula_32">φ X (C t , O t , A t , P t , S t )=φ(O t , P t )+φ(A t , P t )+φ(O t , S t ) + φ(C t , O t , A t , P t )</formula><p>where we only model the assignments seen in the training set, and assume others are not possible.</p><p>Let us first derive the update equation for φ X as a whole, and then demonstrate how to update each of the individual potentials. In the following derivation, we simply take the partial derivative where appropriate and iteratively use the chain rule.</p><p>∂l(X * )</p><formula xml:id="formula_33">∂φî X (x) = 1 IP (X * , I) IP (X * , I) ∂ i φ i X (x * i ) ∂φî X (x) − ∂ log Z ∂φî X (x)<label>(34)</label></formula><p>= 1x =x * − 1 Z X I ∂P (X, I) ∂φî X (x) (Denominator and numerator cancel)</p><formula xml:id="formula_34">(35) = 1x =x * − 1 Z X I 1x =xP (X, I) (36) = 1x =x * − X I 1x =x P (X, I)<label>(37)</label></formula><p>≈ 1x =x * − X I 1x =x Q(X, I) (Using the mean-field) (38)</p><formula xml:id="formula_35">= 1x =x * − X I 1x =x Q I (I) i Q i (x i ) (39) = 1x =x * − Qî(x) (Since xi Q i (x i ) = 1)<label>(40)</label></formula><p>where we use X * to refer to the ground truth labels, andX to refer to the variables we are taking the partial derivative with respect to. We note that</p><formula xml:id="formula_36">∂( i φ i X (x * i )) ∂φî X (x)</formula><p>= 1x =x * . Intuitively this implies the partial gradient is the difference between the ground truth and the model prediction. This equation is easily extended to update each of the individual potentials as follows:</p><formula xml:id="formula_37">∂l(X * ) ∂φî(Ô t ,P t ) = 1 (Ôt,Pt)=(O * t ,P * t ) − Ct At St Qî(X * t ) (41) ∂l(X * ) ∂φî(Â t ,P t ) = 1 (Ât,Pt)=(A * t ,P * t ) − Ct Ot St Qî(X * t )<label>(42)</label></formula><p>∂l(X * )</p><formula xml:id="formula_38">∂φî(Ô t ,Ŝ t ) = 1 (Ôt,Ŝt)=(O * t ,S * t ) − Ct At Pt Qî(X * t )<label>(43)</label></formula><p>∂l(X * )</p><formula xml:id="formula_39">∂φî(Ĉ t ,Ô t ,Â t ,P t ) = 1 (Ĉt,Ôt,Ât,Pt)=(C * t ,O * t ,A * t ,P * t ) − St Qî(X * t )<label>(44)</label></formula><p>where we marginalize out the variables that are not a part of each potential. Again, X t incorporates all the frame variables {C t , O t , A t , P t , S t }. These partial derivatives are passed down the CNN (backprop) to update the parameters of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Updating the frame-intent potential φ XI</head><p>Similarly to φ X we proceed as follows:</p><formula xml:id="formula_40">∂l(X * ) ∂φî XI (x,Î) = 1 IP (X * , I) IP (X * , I)1x =x * 1Î =I − ∂ log Z ∂φî XI (x,Î)<label>(45)</label></formula><p>=P (X * ,Î)</p><formula xml:id="formula_41">IP (X * , I) 1x =x * − ∂ log Z ∂φî XI (x,Î) (46) = exp i φ i XI (x * i ,Î) I exp i φ i XI (x * i , I) 1x =x * − ∂ log Z ∂φî XI (x,Î) (Terms without I cancel) (47) = exp i φ i XI (x * i ,Î) I exp i φ i XI (x * i , I) 1x =x * − 1 Z X I ∂P (X, I) ∂φî XI (x,Î) (48) = exp i φ i XI (x * i ,Î) I exp i φ i XI (x * i , I) 1x =x * − 1 Z X IP (X, I)1x =x 1Î =I (49) = exp i φ i XI (x * i ,Î) I exp i φ i XI (x * i , I) 1x =x * − X I P (X, I)1x =x 1Î =I (50) ≈ exp i φ i XI (x * i ,Î) I exp i φ i XI (x * i , I) 1x =x * − X I Q(X, I)1x =x 1Î =I (Mean-field approximation) (51) = exp i φ XI (x * i ,Î) I exp i φ XI (x * i , I) 1x =x * − Qî(x)Q I (Î) (52) = exp H * i (Î) + H * i (Î) I exp {H * i (I) + H * i (I)} 1x =x * − Qî(x)Q I (Î)<label>(53)</label></formula><p>This equation can be interpreted in that it captures the difference between the distribution of the intent given the ground truth, and the predicted distribution of the intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Updating the frame-frame potential φ XX</head><p>The pairwise potentials φ XX (x i , x j ) for two time points i and j in our model have the form:</p><formula xml:id="formula_42">φ XX (x i , x j ) = µ(x i , x j ) m w (m) k (m) (v i , v j ) (54) = µ(x i , x j )k(v i , v j )<label>(55)</label></formula><p>where µ models the asymmetric affinity between frames, w are kernel weights, and each k (m) is a Gaussian kernel that depends on the videoframes v i and v j which are omitted from this notation for convenience, but the probability and the potentials are conditioned on V. In this work we use a single kernel that prioritises short-term interactions:</p><formula xml:id="formula_43">k(v i , v j ) = exp − (j − i) 2 2σ 2<label>(56)</label></formula><p>The parameters of the general asymmetric compatibility function µ(x i , x j ) are learned from the data, and σ is a hyperparameter chosen by cross-validation. The parameters of µ are learned as follows, and this could be extended to a more general form of φ XX :</p><formula xml:id="formula_44">∂l(X * ) ∂µî(x,b) = 1 IP (X * , I) IP (X * , I) ∂ ∂µî(x,b)   j&gt;î φ i XX (x * i , x * j ) + j&lt;î φ i XX (x * j , x * i )   − ∂ log Z ∂µî(x,b) (57) = j&gt;î 1x =x * 1b =x * j k(vî, v j ) + j&lt;î 1x =x * 1b =x * j k(v j , vî) − 1 Z X I ∂P (X, I) ∂µî(x,b) (58) = j&gt;î 1x =x * 1b =x * j k(vî, v j ) + j&lt;î 1x =x * 1b =x * j k(v j , vî) − 1 Z X IP (X, I) i   j&gt;i 1x =x 1b =xj k(v i , v j ) + j&lt;i 1x =x 1b =xj k(v j , v i )   (59) = j&gt;î 1x =x * 1b =x * j k(vî, v j ) + j&lt;î 1x =x * 1b =x * j k(v j , vî) − X I Q I (I) i Q i (x i ) i   j&gt;i 1x =x 1b =xj k(v i , v j ) + j&lt;i 1x =x 1b =xj k(v j , v i )   (Mean-field) (60) ∂l(X * ) ∂µî(a, b) = j&gt;î 1 a=x * i 1 b=x * j k(vî, v j ) − Qî(a) j&gt;î Q j (b)k(vî, v j ) + j&lt;î 1 b=x * i 1 a=x * j k(v j , vî) − Qî(b) j&lt;î Q j (a)k(v j , vî)<label>(61)</label></formula><formula xml:id="formula_45">= 1 a=x * i KA * i (b) − Qî(a)KAî(b) + 1 b=x * i KB * i (a) − Qî(b)KBî(a)<label>(62)</label></formula><p>This update equation consists of two symmetric parts, one for influence from frames before, and one for influence from frames after. Intuitively, this captures the difference in the true affinity between frame i and all frames j on the one hand, and on the other hand the predicted affinity, where the affinity is weighted by the kernel. for each example in mini-batch do <ref type="bibr">4:</ref> Sample frame v ∈ V ⊆ V that has index i 5:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Additional implementation details</head><p>Calculate messages with Eq. 18-25, approximated by Eq. 9 (from paper) <ref type="bibr">6:</ref> Alternate updating Q i and Q I until convergence <ref type="bibr">7:</ref> Find gradients with Eqs. 40,53,62 <ref type="bibr">8:</ref> Backprop gradients through CNN <ref type="bibr">9:</ref> Store computations of Eq. 12-17 for later use <ref type="bibr">10:</ref> Update CNN using accumulated gradients hardware and model. Our learning rate schedule was chosen by finding the largest learning rate that did not cause divergence, and then making sure the learning rate was decayed by a factor of 100 over the course of training. Investigations into training these kinds of models faster are likely to yield substantial benefits. Training Deep Models with Latent Variables One of the pursuits of this work was introducing latent variables into a deep framework, the intent. The gradient for the frame-intent potential, contains predictions of the model on both sides, which is a common problem in deep reinforcement learning, where a variety of tricks such as target fixing, double Q-learning, and gradient clipping, are used to combat the instability caused by this. In this work we found that simply severing the dependency of the frame-intent variable on the input data got rid of the instability, and still gave acceptable performance on the RGB stream, however we found that this did not give good performance on the Flow stream.</p><p>In order to train the network with the frame-intent potential depending on the input data, we experimented with a variety of techniques from the reinforcement learning literature. Only two methods were found to help: Alternating target and prediction networks, and regularization. For alternating target and prediction networks, the network predicts two frameintent potentials, and then the network randomly chooses which to use as the target, and which to use as the source, and backprop only through one of them. For regularization, we enforce the frame-intent potential to be close to zero, similar to weight decay (set to 4 · 10 −4 ). Regularization was found to be give slightly better performance, and easy to implement/tune, and was used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Details about intent analysis</head><p>To analyze the learned intent variable, we defined 10 types of intent: getting something to eat, clean the living space, getting dressed, getting something from storage, get informed, get out of bed, leave the house, photograph something, relaxing, working. To identify videos corresponding to the intent, we used keyword related to the intent (such as closet and clothes for getting dressed) and manually verified that the content of the video matched the intent. The analysis demonstrates that the latent intent variables captures non-trivial structure of the label space, but precisely understanding goal-oriented behavior compared to simple activity analysis remains important future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Additional Visualizations of Output Predictions</head><p>Due to space constraints in the full paper, we present here additional visualizations from the model. In <ref type="figure" target="#fig_1">Figure 13</ref> we present in the same way as <ref type="figure">Figure 9</ref> (from the paper). That is, we present the 3 most confident categories, 2 most confident actions, and 1 most confident object. For example, in the first row we can see that once the light turns on in the room and the couch becomes visible the category Sitting on a sofa/couch fires, which in turn increases the likelihood of sitting in the next few frames. Furthermore, in <ref type="figure" target="#fig_1">Figure 14</ref> we present similar visualizations, but only the 6 most confident categories, to further understand the interplay between the activity categories. In the first row, we can see a video of a person walking towards the camera, and we can see how one after the other the model recognizes cup, phone, and sandwich, and reasons about these connected activities. Finally, in <ref type="figure" target="#fig_1">Figure 15</ref> we present a breakdown of the mean average precision (mAP) by our model for each class of the dataset, sorted by the mAP of our model.   <ref type="figure" target="#fig_1">Figure 15</ref>. mAP for our model for all classes, sorted by mAP. The column on the right is the continuation of the left column.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Time</head><label></label><figDesc>Holding a cupPouring into a cup Drinking from a cup Intent: Getting something to drink</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Understanding human activities in videos requires jointly reasoning about multiple aspects of activities, such as 'what is happening', 'how', and 'why'. In this paper, we present an end-toend deep structured model over time trained in a stochastic fashion. The model captures rich semantic aspects of activities, including Intent (why), Category (what), Object (how). The figure shows video frames and annotations used in training from the Charades [43] dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>keeps track of what node in what video sent what message, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The VGG-16 variant predicts the potentials for both RGB and Flow. The network predicts the values of all potentials except one (in this figure we group the frame potentials φX into one layer for clarity). The model is trained end-to-end by passing gradients from the Asynchronous Temporal Field through the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11</head><label>11</label><figDesc>. t-SNE visualization for the learned intent. Each point corresponds to a video. In a) it is colored based on its activity shared by the most of the 10 nearest neighbors (each video has multiple actions). In b) videos with 6 annotated intent types are emphasized with larger points colored by the type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>-</head><label></label><figDesc>Category C t ∈ C, C = {1, 2, ..., 157} (For each category in the dataset) -Object O t ∈ O, O = {1, 2, ..., 38} (Includes "No object") -Action A t ∈ A, A = {1, 2, ..., 33} -Progress P t ∈ P, P = {1, 2, 3} (Before, Middle, End) -Scene S t ∈ S, S = {1, 2, ..., 15} • Intent I ∈ I, I = {1, 2, ..., N I } (N I = 30 in this work)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A</head><label></label><figDesc>more detailed algorithmic description of the model is presented in Algorithm 3. More details can be found on the project page https://github.com/gsig/temporal-fields/. Training time Training the models in this paper took a while: The RGB stream of the Two-Stream model converged after only 0.2 epochs (20% of the total data, randomly selected) of the training data, but training the Flow stream needed 4.0 epochs to reach the best performance. Our model needed 0.7 epochs for the RGB stream and 8.3 epochs for the Flow stream. Each 0.1 epoch is approximately 1450 batches of size 256 (all labelled frames at 8 FPS), and takes between 3-8 hours depending on Algorithm 3 Learning for Asynchronous Temporal Fields (Detailed) 1: Given videos V 2: while not converged do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 .</head><label>13</label><figDesc>Visualizations of the model predictions for the 3 most confident categories, 2 most confident actions, and 1 most confident object. Darker colors indicate higher likelihood.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Visualizations of the model predictions for the 6 most confident categories. Darker colors indicate higher likelihood. 0 m AP Holding a box Lying on the floor Sm iling a t a book Ta king a bla nke t from s om e whe re Tidying up a bla nke t/s Tidying up a c los e t/c a bine t Putting a c up/g la s s /bottle s om e whe re Snug g ling with a pillow Som e one is la ug hing Som e one is running s om e whe re Ta king a broom from s om e whe re Tidying s om e c lothe s Putting a dis h/e s s om e whe re Wa tc hing s om e thing /s om e one /the m s e lve s in a m irror Ta king a dis h/e s from s om e whe re Putting s om e thing on a s he lf Holding a towe l/s Ta king a c up/g la s s /bottle from s om e whe re Clos ing a c los e t/c a bine t Som e one is holding a pa pe r/note book Holding a ba g Wa s hing s om e c lothe s Throwing c lothe s s om e whe re Putting s om e food s om e whe re Clos ing a book Ope ning a book Putting a bla nke t s om e whe re Wa s h a dis h/dis he s Ta king food from s om e whe re Tidying up a ta ble Working on pa pe r/note book Som e one is undre s s ing Holding a bla nke t Som e one is dre s s ing Pla ying with a phone /c a m e ra Som e one is e a ting s om e thing Holding a va c uum Som e one is s m iling Som e one is a wa ke ning s om e whe re Som e one is s ta nding up from s om e whe re Holding a la ptop Wa tc hing /Re a ding /Looking a t a book Snug g ling with a bla nke t Holding a phone /c a m e ra Som e one is a wa ke ning in be d Putting a broom s om e whe re Holding a dis h Ope ning a c los e t/c a bine t Putting s om e thing on a ta ble Som e one is g oing from s ta nding to s itting Holding s om e food Clos ing a door Putting c lothe s s om e whe re Ta king s om e c lothe s from s om e whe re Working a t a ta ble Holding s om e c lothe s Sitting in a be d Holding a book Holding a c up/g la s s /bottle of s om e thing Wa tc hing te le vis ion Drinking from a c up/g la s s /bottle Sitting on the floor Wa s hing a window Wa tc hing a la ptop or s om e thing on a la ptop Working /Pla ying on a la ptop Sitting on s ofa /c ouc h Ope ning a door Lying on a s ofa /c ouc h Tidying s om e thing on the floor Lying on a be d Gra s ping onto a doorknob Som e one is c ooking s om e thing Sitting in a c ha ir Clos ing a re frig e ra tor Holding a broom Sitting a t a ta ble Ope ning a re frig e ra tor Tidying up with a broom Wa lking throug h a doorwa y 60 30 0 m AP Throwing a box s om e whe re Sta nding on a c ha ir Throwing a ba g s om e whe re Throwing a book s om e whe re Throwing food s om e whe re Fixing a lig ht Clos ing a window Re a c hing for a nd g ra bbing a pic ture Throwing a pillow s om e whe re Putting a pic ture s om e whe re La ug hing a t a pic ture Ta king a va c uum from s om e whe re La ug hing a t te le vis ion Clos ing a la ptop Fixing a door Ope ning a window Putting a phone /c a m e ra s om e whe re Putting the ir pa pe r/note book s om e whe re Throwing s hoe s s om e whe re Putting a s a ndwic h s om e whe re Holding a pic ture Ta king /c ons um ing s om e m e dic ine Ta king a ba g from s om e whe re Throwing a towe l/s s om e whe re Holding s om e m e dic ine Sm iling in a m irror Turning off a lig ht Ta king a s a ndwic h from s om e whe re Pouring s om e thing into a c up/g la s s /bottle Ta king s om e thing from a box Putting a book s om e whe re Holding a m irror Putting a ba g s om e whe re Ta king pa pe r/note book from s om e whe re Ta king a book from s om e whe re Ma king a s a ndwic h Putting a box s om e whe re Wa tc hing /looking a t a pic ture Ta king off s om e s hoe s Wa s hing a ta ble Sitting on a ta ble Putting on s hoe /s hoe s Tidying up a towe l/s Ta king a la ptop from s om e whe re Putting s hoe s s om e whe re Ta king a phone /c a m e ra from s om e whe re Holding a s hoe /s hoe s Ta king a pillow from s om e whe re Ta king s hoe s from s om e whe re Ope ning a box Ope ning a la ptop Throwing a broom s om e whe re Putting a pillow s om e whe re Fixing the ir ha ir Ta king a pic ture of s om e thing Holding a s a ndwic h Ea ting a s a ndwic h Som e one is s ne e zing Fixing a va c uum Ta king a box from s om e whe re Ta king a towe l/s from s om e whe re Clos ing a box Ta lking on a phone /c a m e ra Wa s hing a c up/g la s s /bottle Wa s hing a m irror Fixing a doorknob Putting a towe l/s s om e whe re Throwing a bla nke t s om e whe re Wa s hing the ir ha nds Holding a pillow Tidying a s he lf or s om e thing on a s he lfPutting a la ptop s om e whe re Turning on a lig ht Wa s hing s om e thing with a towe l Ope ning a ba g Putting g roc e rie s s om e whe re Throwing s om e thing on the floor Wa tc hing /Looking outs ide of a window Holding a box<ref type="bibr" target="#b29">30</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Video classification results on Charades<ref type="bibr" target="#b42">[43]</ref>. The left shows the published baselines from<ref type="bibr" target="#b42">[43]</ref> and the right show additional new baselines. Our proposed approach outperforms all competing methods on this dataset.</figDesc><table><row><cell>Approach</cell><cell>mAP</cell><cell>Approach</cell><cell>mAP</cell></row><row><cell>Random [43]</cell><cell>5.9</cell><cell>RGB++</cell><cell>15.6</cell></row><row><cell>C3D [54]</cell><cell>10.9</cell><cell>Two-Stream++</cell><cell>16.8</cell></row><row><cell>AlexNet [20]</cell><cell>11.3</cell><cell>Two-Stream+LSTM</cell><cell>17.8</cell></row><row><cell>IDT [57]</cell><cell>17.2</cell><cell>Two-Stream Extended</cell><cell>18.6</cell></row><row><cell>Two-Stream [44]</cell><cell>14.3</cell><cell>Ours (RGB Only)</cell><cell>18.3</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>22.4</cell></row><row><cell cols="2">Washing a window Holding a broom Closing a refrigerator Putting broom somewhere Opening a refrigerator Tidying up with a broom Lying on a bed Taking a broom Washing a mirror Drinking from a cup</cell><cell>Closing a window Fixing a light Someone is smiling Working at a table Washing a cup Smiling in a mirror Throwing shoes Turning off a light Lying on the floor Wash dishes</cell><cell></cell></row><row><cell></cell><cell>mAP Difference -7.0 +34.1</cell><cell cols="2">mAP Difference -7.0 +34.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Temporal localization results (mAP %) on the Charades<ref type="bibr" target="#b42">[43]</ref> dataset. Our proposed method outperforms the LSTM model, and is also more tractable to train at a large-scale.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was partly supported by ONR MURI N00014-16-1-2007, ONR N00014-13-1-0720, NSF IIS-1338054, NSF-1652052, NRI-1637479, Intel via the Intel Science and Technology Center for Visual Cloud Systems, Allen Distinguished Investigator Award, gifts from Google, and the Allen Institute for Artificial Intelligence. The authors would like to thank Mark Yatskar for lending his expertise on deep CRFs, and Olga Russakovsky, Christoph Dann, and the anonymous reviewers for their invaluable suggestions and advice.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>This appendix contains the following additional content:</p><p>1. Description of the CRF. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Deep Structured Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML, 2015. * equal contribution</title>
		<meeting>ICML, 2015. * equal contribution</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sympathy for the details: Dense trajectories and hybrid classification architectures for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lpez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://www.thumos.info/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Observing humanobject interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visual semantic role labeling. CoRR, /abs/1505.04474</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing complex events using large margin joint low-level event model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representing videos using mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical mid-level action elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What, where and who? classifying events by scene and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trajectons: Action recognition through the motion analysis of tracked features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<title level="m">Playing atari with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Inferring the why in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5472</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A survey on vision-based human action recognition. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="976" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Does the chimpanzee have a theory of mind?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Premack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="515" to="526" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of interactions between humans and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Script data for attribute-based recognition of composite activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical recognition of human activities interacting with objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning visual storylines with skipping recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, /abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6034</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical sequence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>1502.04681</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Active: Activity concept transitions in video event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Predicting motivations of actions by leveraging text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Actions˜transforma-tions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A survey of visionbased methods for action representation, segmentation and recognition. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="224" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Towards weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05197</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06984</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Instance-level segmentation with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
