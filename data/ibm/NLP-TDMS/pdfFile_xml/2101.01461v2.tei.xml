<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointCutMix: Regularization Strategy for Point Cloud Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlai</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyujie</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ouyang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmei</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">PointCutMix: Regularization Strategy for Point Cloud Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As 3D point cloud analysis has received increasing attention, the insufficient scale of point cloud datasets and the weak generalization ability of networks become prominent. In this paper, we propose a simple and effective augmentation method for the point cloud data, named PointCutMix, to alleviate those problems. It finds the optimal assignment between two point clouds and generates new training data by replacing the points in one sample with their optimal assigned pairs. Two replacement strategies are proposed to adapt to the accuracy or robustness requirement for different tasks, one of which is to randomly select all replacing points while the other one is to select k nearest neighbors of a single random point. Both strategies consistently and significantly improve the performance of various models on point cloud classification problems. By introducing the saliency maps to guide the selection of replacing points, the performance further improves. Moreover, PointCutMix is validated to enhance the model robustness against the point attack. It is worth noting that when using as a defense method, our method outperforms the state-of-the-art defense algorithms. The code is available at: https://github.com/ cuge1995/PointCutMix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rapid development of autonomous driving and robotics industries, making machines understand the real three-dimensional world has become a guarantee for safe and efficient task execution <ref type="bibr" target="#b26">(Guo et al., 2020)</ref>. As a commonly used format for 3D data representation that can be directly obtained by Light Detection And Ranging <ref type="bibr">(Li-DAR)</ref> sensors, the point cloud has been widely applied in many computer vision fields <ref type="bibr" target="#b14">(Lang et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2019;</ref><ref type="bibr" target="#b24">Rao et al., 2020)</ref>, such as 3D object detection <ref type="bibr" target="#b26">(Shi et al., 2020;</ref><ref type="bibr" target="#b1">Bhattacharyya &amp; Czarnecki, 2020)</ref>, point cloud segmentation <ref type="bibr" target="#b20">(Liu et al., 2020b)</ref>, and point cloud classification <ref type="bibr" target="#b23">(Qi et al., 2017b;</ref><ref type="bibr" target="#b19">Liu et al., 2019c;</ref><ref type="bibr" target="#b34">Wang et al., 2019)</ref>. Following the pioneering work of PointNet <ref type="bibr" target="#b22">(Qi et al., 2017a)</ref>, a series of deep-learning-based methods brought the performance of these tasks to a higher level. However, due to the complexity and costs of fine-grained 3d point cloud annotations <ref type="bibr" target="#b38">(Xu &amp; Lee, 2020)</ref>, the scale of existing point cloud datasets is much smaller than that of the image datasets , resulting in the overfitting and poor generalization of these methods <ref type="bibr" target="#b12">(Jing &amp; Tian, 2020)</ref>. Although researchers have explored several data augmentation techniques for point cloud analysis, such as rotation, scaling, and jittering <ref type="bibr" target="#b39">(Yan et al., 2020;</ref><ref type="bibr" target="#b20">Liu et al., 2020b)</ref>, these kinds of augmentations ignore the shape complexity of the samples , thus lead to insufficient training.</p><p>Over the past few years, mixed sample data augmentation (MSDA) for images has attached increasing interest which aims to create new training data by mixing the original training samples according to some rules <ref type="bibr" target="#b10">(Harris et al., 2020;</ref><ref type="bibr" target="#b7">Guo et al., 2019)</ref>. There are two mainstream methods in MSDA. The first one is MixUp <ref type="bibr" target="#b44">(Zhang et al., 2018)</ref>, which interpolates between training samples by performing weighting on the whole image and its label. Another method is CutMix <ref type="bibr" target="#b43">(Yun et al., 2019)</ref>. It inserts a rectangle region from one image into another and then performs weighting on the image and its label by the ratio of the region size. In comparison, CutMix achieves better results across various models and datasets in image classification, weakly supervised object localization, and transfer learning to object detection.</p><p>In this paper, inspired by the success of MSDA in the image domain, we propose an MSDA strategy to the point cloud data, named PointCutMix. To adapt to its unordered feature, we first calculate the optimal assignment of two point clouds refer to MSN <ref type="bibr" target="#b17">(Liu et al., 2020a)</ref>. Then, two PointCutMix methods that replace the points in one sample with their optimal assigned pairs in another sample are formulated, i.e., arXiv:2101.01461v2 [cs.CV] 5 Feb 2021 PointCutMix-R and PointCutMix-K. The former randomly selects all replacing points while the latter selects k nearest neighbors of one random chosen point. Experimental results demonstrate that both methods achieve consistent and significant improvements in object-level classification task on ModelNet40 <ref type="bibr" target="#b35">(Wu et al., 2015)</ref> and ModelNet10 <ref type="bibr" target="#b35">(Wu et al., 2015)</ref> datasets. We also at the first time exploit PointCut-Mix for point-wise classification, i.e., point cloud segmentation task. It is observed that PointCutMix can evidently improve the recognition accuracy for the uncommon categories. Inspired by the successful use of attention maps in CutMix <ref type="bibr" target="#b32">(Walawalkar et al., 2020)</ref>, we further introduce the saliency maps to guide the selection of replacing points which achieves better results. Additionally, we validate that PointCutMix can enhance the robustness of different models under the point cloud attack. When using as a defense method, under the point dropping attack <ref type="bibr" target="#b47">(Zheng et al., 2019)</ref>, our PointCutMix-ModelNet40 pre-trained models surpass the state-of-the-art defense algorithm IF-Defense  by a large margin without using any transformation on the adversarial point clouds. We also perform defense to other point cloud attacks and achieve promising results. Extensive experiments verify the effectiveness of our method. We believe this simple regularization strategy could be applied to various tasks and help future research in the 3D computer vision community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning on point cloud. PointNet <ref type="bibr" target="#b22">(Qi et al., 2017a)</ref> is the first work that processes the point cloud using deep neural networks where the shared pointwise multi-layer perceptions (MLPs) followed by the max-pooling operation are used for point cloud learning. After that, the recent works focus on efficiently capturing local features <ref type="bibr" target="#b23">(Qi et al., 2017b;</ref><ref type="bibr" target="#b39">Yan et al., 2020;</ref><ref type="bibr" target="#b45">Zhao et al., 2019;</ref><ref type="bibr" target="#b41">Yang et al., 2019b)</ref> and investigating convolutional kernels for 3D point clouds. <ref type="bibr" target="#b19">Liu et al. (Liu et al., 2019c)</ref> proposed RS-CNN which implemented the convolution using an MLP in the local subset of points. DensePoint ) defined a single-layer perceptron with a nonlinear activator as convolution. In KPConv <ref type="bibr" target="#b29">(Thomas et al., 2019)</ref>, by using a set of learnable kernel points, the rigid and deformable Kernel point convolution operators were proposed. Some other researchers have explored graph-based networks, where each point in a point cloud is considered as a vertex of a graph. DGCNN <ref type="bibr" target="#b34">(Wang et al., 2019)</ref> constructed a graph in the feature space and MLP is used for each edge. To simplify the process of points agglomeration, the Dynamic Points Agglomeration Module based on graph convolution was proposed by <ref type="bibr" target="#b16">Liu et al. (Liu et al., 2019a)</ref>. In RGCNN <ref type="bibr" target="#b28">(Te et al., 2018)</ref>, a graph was constructed by connecting all points with each other in the point cloud. To utilize the local structural information, LocalSpecGCN <ref type="bibr" target="#b33">(Wang et al., 2018)</ref> used the spectral convolution network to a local graph.</p><p>Mixed sample data augmentation. Mixed Sample Data Augmentation (MSDA) is a strategy that produces new training data by mixing samples according to some rules <ref type="bibr" target="#b10">(Harris et al., 2020)</ref>. Training with the mixed data, the model would learn multiple features in a balanced way <ref type="bibr" target="#b27">(Taghanaki et al., 2020)</ref> and achieve better performance. Therefore, MSDA has become the mainstream data augmentation approach and dominated modern image classification for years <ref type="bibr" target="#b10">(Harris et al., 2020;</ref><ref type="bibr" target="#b7">Guo et al., 2019;</ref><ref type="bibr" target="#b44">Zhang et al., 2018;</ref><ref type="bibr" target="#b43">Yun et al., 2019;</ref><ref type="bibr" target="#b31">Verma et al., 2019)</ref>. Among them, MixUp <ref type="bibr" target="#b44">(Zhang et al., 2018)</ref> and Cutmix <ref type="bibr" target="#b43">(Yun et al., 2019)</ref> are two classical methods that have been widely used in various computer vision research <ref type="bibr" target="#b11">(He et al., 2019)</ref> and competition <ref type="bibr" target="#b4">(Dolhansky et al., 2020)</ref>. MixUp <ref type="bibr" target="#b44">(Zhang et al., 2018)</ref> interpolates the training samples by performing weighting on the whole image and its label. CutMix <ref type="bibr" target="#b43">(Yun et al., 2019)</ref> inserts a rectangle region from one image into another one and then performing weighting on the image and its label by the ratio of the region size. The experiment results show that CutMix has better performance improvement across different datasets and networks. Our work can be viewed as an extension of CutMix <ref type="bibr" target="#b43">(Yun et al., 2019)</ref> for the point cloud.</p><p>Data augmentation on point cloud. Although random rotation, jittering, and scaling are commonly used in point cloud learning <ref type="bibr" target="#b22">(Qi et al., 2017a;</ref>, the data augmentation for point clouds has obviously not been studied systematically compared to the image domain. Recently, PointAugment  and PointMixup  were proposed for point cloud data augmentation. PointAugment is the first auto-augmentation framework for the point cloud which optimizes the augmentor and classifier networks jointly. However, the additional augmentor network and the complicated adversarial training process makes it less practical. PointMixup extents Mixup <ref type="bibr" target="#b44">(Zhang et al., 2018)</ref> to point cloud by interpolation between point cloud samples. However, for point cloud networks like Point-Net++ <ref type="bibr" target="#b23">(Qi et al., 2017b)</ref> and RS-CNN <ref type="bibr" target="#b19">(Liu et al., 2019c)</ref> that local features are important for point cloud learning, this approach is easy to fall into locally ambiguous and unnatural. In this paper, we proposed PointCutMix to naturally combine two point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PointCutMix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem setting</head><p>The goal of a standard point cloud classification task is to learn a function f : x → [0, 1] C that maps a point cloud to a one-hot class label for a total of C classes. Here x ∈ R N ×d represents a set of 3D points {P i |i = 1, ..., N } which either sampled from a shape or pre-segmented from a scene point cloud. N is the point number and each point P i is a vector with d channels. In this paper, we simplicit use the 3d coordinate features. So d = 3 and P i ∈ R 3 . The optimal parameters θ of function f can be learned by minimizing the loss as</p><formula xml:id="formula_0">θ * = arg min θ x∈D L D (f (x), y)<label>(1)</label></formula><p>where f (x) is the network output, y is the ground truth with respect to x, D is the training set, and L represents the training loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimal assignment of point clouds.</head><p>To perform MSDA, it requires a one-to-one correspondence between the minimal unit of two samples. For image, this unit is pixel while for point cloud data, that is a single point.</p><p>In the image domain, the pixels are arranged in a grid form. By merely resizing or cropping two images to the same size, it is natural to make them correspond according to their coordinate. However, the point clouds are permutationinvariant and orderless. It is essential to define the one-toone correspondence between points based on rules other than position.</p><p>Following the method in PointMixup  and MSN <ref type="bibr" target="#b17">(Liu et al., 2020a)</ref>, we define the optimal assignment φ * between two point clouds x 1 , x 2 as the optimal assignment of Earth Mover's Distance (EMD) <ref type="bibr" target="#b25">(Rubner et al., 2000)</ref> function. The EMD calculates the minimum total displacement required for matching each point in x 1 to the corresponding point in x 2 . We define the assignment function in the EMD as:</p><formula xml:id="formula_1">φ * = arg min φ∈Φ i x 1,i − x 2,φ(i) 2 (2) where Φ = {{1, . . . , N } → {1, .</formula><p>. . , N }} give one-to-one correspondences between the two point clouds. After given the optimal assignment φ * , the EMD is then defined as:</p><formula xml:id="formula_2">EMD = 1 N i x 1,i − x 2,φ * (i) 2<label>(3)</label></formula><p>where φ * (i) denotes the index of optimal assignment point of x 1,i in x 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Algorithm</head><p>The key idea of PointCutMix is to create a new training point cloud (x,ỹ) given two distinct training point clouds (x 1 , y 1 ) and (x 2 , y 2 ). Here, x is the training point cloud and y is the corresponding label. After obtaining the optimal assignment φ * between two samples, we definex 2,i = x 2,φ * (i) and the combining operation as</p><formula xml:id="formula_3">x = B · x 1 + (I N − B) ·x 2 y = λy 1 + (1 − λ)y 2 (4) where B = diag{b 1 , b 2 , · · · , b N } and b i ∈ {0, 1}</formula><p>indicates which sample the point belongs to. When b i = 1, the i th point is chosen from x 1 , otherwise it will replaced by the optimal assigned point in x 2 . I N is an identity matrix. λ ∈ [0, 1] is the PointCutMix ratio, sampled from the beta distribution Beta(β, β), which means n = λ × N points will be kept and the rest points will be replaced.</p><p>To perform cutting and pasting in the point cloud, we propose two replacement methods to construct the diagonal matrix B. The first method, abbreviated as PointCutMix-R, is to randomly sample n points from x 1 as a subset x s 1 . Those points are marked 1 in B, indicating that they will not be replaced. The rest points are marked 0. In addition, to retain the local characteristics of the point cloud, we come up with the second method, noted as PointCutMix-K, which randomly sample one central point p from x 1 , and then finding its n − 1 nearest neighbors. We combine p and its nearest neighbors to form x s 1 and marked those points as 1 in B. Similarly, the rest points are marked 0 and be replaced. In <ref type="figure" target="#fig_0">Figure 1</ref>, we show the visualization of some mixed samples of PointCutMix-R and PointCutMix-K. It can be seen that the samples produced by PointCutMix-R look like two objects cross together while the mixed data from PointCutMix-K are the obvious combination of two object parts.</p><p>We also introduce a hyperparameter ρ ∈ [0, 1] to indicate the probability of each point cloud to be augmented during the training. When ρ = 0, PointCutMix will not be used which is equivalent to the baseline model. On the contrary, ρ = 1 means all point clouds will be augmented. Therefore, the training loss can be denoted as</p><formula xml:id="formula_4">x∈D (1 − 1 ρ )L D (f (x), y) + 1 ρ L D (f (x),ỹ)<label>(5)</label></formula><p>where 1 ρ = 1 with a probability ρ, otherwise it equals to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analysis</head><p>The difference of replacement methods. In <ref type="figure" target="#fig_1">Figure 2</ref>, we list the visualization of mixed samples between a plane and a chair produced by PointCutMix-R (top row) and PointCutMix-K (bottom row) under different replacement ratios λ. The ratios from left to right are 0, 0.2, 0.4, 0.6, 0.8, <ref type="figure">Figure 3</ref>. Saliency maps of different point clouds. Points with higher values are colored as red and the color of irrelevant points is closer to blue. and 1.0. We can see that the samples in the top row are a little messy and like two objects fuse. Especially when λ close to 0 or 1, such as λ = 0.2 or λ = 0.8, only one of the objects can be easily recognized. Those hard to distinguished points actually perform like a kind of noise. We infer that this characteristic will impair the performance for learning classification task, but can improve the robustness of the model. This assumption has been verified in the experiment in Section 4. On the contrary, the mixed samples from PointCutMix-K are relatively regular, like a natural combination of two object parts. Since at least a part of each object can be identified, it provides more features for learning the classification task.</p><p>Is attention works for PointCutMix? Inspired by the successful use of attention maps to guide the cutting and pasting region of an object in CutMix <ref type="bibr" target="#b32">(Walawalkar et al., 2020)</ref>, we speculate that the selection of central point p in PointCutMix-K can also be guided rather than random. So we try to obtain the contribution of each point to the classification result with saliency map <ref type="bibr" target="#b47">(Zheng et al., 2019)</ref>. Then the point with greater contribution has a higher probability to be selected as the central point. Through the visualization of the saliency maps shown in <ref type="figure">Figure 3</ref>, we find that the points with high contributions (in red color) are not scattered uniformly. For example, they gather more in the lampshades, the seat of the chair, and the fuselage of the airplane. In the next section, we will examine whether this strategy improves the accuracy of the model.</p><p>Extending to point cloud segmentation. So far, we have elaborated on how to apply our method to the object-level classification. Natural intuition is to extend to the pointwise classification problem, i.e., point cloud segmentation. However, we find that the existing augmentation methods are limited by their fusion strategies, thus fail to complete this task. For example, when applying PointMixup to fuse a new point cloud, the semantic information of each point has lost, making it hard to get the semantic labels for new data. On the contrary, since our method is simply cutting and pasting points, the semantic information can be persisted. So in this paper, we at the first time perform augmentation to point cloud segmentation task. Specifically, we mix two point clouds using the same method mentioned before. The point-wise labels are replaced along with the points. For datasets that also contain object-level labels, the object-level annotation is fused referred to Section 3.3. In <ref type="figure" target="#fig_2">Figure 4</ref>, we show some mixed samples for the point cloud segmentation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments to verify the effectiveness of PointCutMix. At first, we find the optimal hyperparameters through several comparative experiments. Then we assess our method from two aspects, one of which is to evaluate how much it improves the accuracy of object-level point cloud classification and point-wise segmentation while the other one is to evaluate the generalization ability and robustness of the model trained with augmented data provided by PointCutMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets. We evaluate PointCutMix on two object-level point cloud classification datasets and a point-wise segmentation dataset, i.e., ModelNet40 <ref type="bibr" target="#b35">(Wu et al., 2015)</ref>, Mod-elNet10 <ref type="bibr" target="#b35">(Wu et al., 2015)</ref>, and ShapeNet Parts <ref type="bibr" target="#b42">(Yi et al., 2016)</ref>. Networks. Since PointCutMix is a general data augmentation method, it is agnostic to the network architecture that is employed. Therefore, we select four popular networks in 3D computer vision area <ref type="bibr" target="#b46">Zhao et al., 2020)</ref> for evaluation, i.e., PointNet <ref type="bibr" target="#b22">(Qi et al., 2017a)</ref>, Point-Net++ <ref type="bibr" target="#b23">(Qi et al., 2017b)</ref>, RS-CNN <ref type="bibr" target="#b19">(Liu et al., 2019c)</ref>, and DGCNN <ref type="bibr" target="#b34">(Wang et al., 2019)</ref>. As mentioned in Section 2, PointNet only uses global information while other three models take the local information into account.</p><p>Implementation details. Our work is implemented using PyTorch <ref type="bibr" target="#b21">(Paszke et al., 2017)</ref> on NVIDIA GeForce GTX 2080Ti GPU. All networks take 1024 points as input and are trained for 300 epochs with a batch size of 16. For PointNet, PointNet++, and RS-CNN, we use the Adam <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014)</ref> optimizer with an initial learning rate of 0.001 and a decay rate of 0.5 every 20 epochs, which is the same configuration as the original released paper and code. We train DGCNN with SGD optimizer with an initial learning rate of 0.1. The minimum learning rate is 0.001 and the momentum of SGD is 0.9. The cosine annealing strategy is used to decay the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparative experiments</head><p>We perform the comparative experiments in ModelNet40 dataset using the experimental settings described in implementation details.</p><p>Influence of ρ. We first compare the performance of PointCutMix-K on four representative models under different values of ρ to figure out whether our method is useful and how much of the data need to be augmented during the training. The results are illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. For point-Net++, RS-CNN, and DGCNN, we observe that even with only 25% of the samples are augmented (ρ = 0.25), the accuracy is greatly improved, which proves that our method is very essential and effective. Under different values of ρ, there is no much difference in accuracy for three models. However, PointNet performs in a completely different way. It is improved when ρ is small, but the accuracy is signif- icantly dropped when ρ reaches 1. We speculate that this is because the coordinate feature of a single point has no actual information. The object-level classification must rely on the learning of the relationship between points. However, PointNet lacks the ability to learn local features since it performs MLP for all points in the object together, which makes it difficult to distinguish the replaced region.</p><p>In the following experiments, although each model reaches its optimal performance with different values of ρ, we choose ρ = 1.0 for the object-level point cloud classification task in order to make a fair comparison to other augmentation methods. Here we do not report PointNet since our method is not suitable for it. While for the point cloud segmentation task, we select ρ = 0.5 for better performance.</p><p>Influence of β. Next, we investigate the influence of β, i.e., whether there is a difference in choosing the different number of replaced points at augmentation. From the results in <ref type="figure" target="#fig_3">Figure 5</ref>, it can be seen that the difference of accuracy under various values of β for PointNet++ and DGCNN is very small, but RSCNN prefers a small value of β. To use the same hyperparameter for all models and simplify the selecting process of α, we select the <ref type="figure" target="#fig_0">Beta(1, 1)</ref>, i.e., the uniform distribution in the subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Point cloud classification</head><p>After determining the hyperparameters, we conduct point cloud classification experiments on ModelNet40 and Mod-elNet10 to evaluate various data augmentation methods, including conventional data augmentation (baseline) <ref type="bibr" target="#b23">(Qi et al., 2017b)</ref>, PointMixup , PointAugment , PointCutMix-R, and PointCutMix-K. In addition, to verify the influence of attention maps mentioned in Section 3.4, we introduce the saliency map to guide the selection of central point p. This strategy is named PointCutMix-S. The results of baseline models refer to PointAugment. The models trained with PointCutMix methods are implemented with the settings in our implementation details. The saliency maps are produced by corresponding pre-trained baseline models during the training. The results of PointMixup and PointAugment refer to their original papers.</p><p>From <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table">Table 2</ref>, we observe that our methods consistently outperform PointMixup and have comparative results to PointAugment. This is a very impressive result because PointCutMix is much simpler than the existing methods. PointMixup needs to pre-align the point clouds of the training and test sets in the horizontal facing direction. But our method does not rely on any pre-process for the input point clouds. PointAugment uses an additional network for data augmentation. It requires much more memory cost, which is not practical in real applications. In comparison, PointCutMix uses little computing resources and time but still achieves better performance.</p><p>PointCutMix-R occasionally has better results than PointCutMix-K on ModelNet10. However, in most cases across two datasets, PointCutMix-K performs better. The results also show that the saliency maps have limited help for the performance. Considering the addition calculation time and memory consumption used for generating the saliency maps during training, we hold that PointCutMix-K is a more versatile and efficient strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Point cloud segmentation</head><p>To explore the extensibility of our method, we at the first time apply augmentation to the point cloud segmentation task. Here we train the baseline model and PointCutMix-S for 251 epochs with a batch size of 16. We use Adam <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014)</ref> optimizer with an initial learning rate of 0.001 and a decay rate of 0.5 every 20 epochs. In <ref type="table" target="#tab_2">Table 3</ref>, we report the part-average Intersection-over-Union results. It shows that PointCutMix makes an improvement of 0.5% over the PointNet++ baseline. Although the improvement is not as significant as that for the object-level classification task, there is a special finding that the accuracy gains mainly come from the uncommon categories. Specifically, the ShapeNet Parts dataset <ref type="bibr" target="#b42">(Yi et al., 2016)</ref> has an uneven distribution of training data, where the table has 5271 samples but the bag, cap, and rocket have only 76, 55, and 66 samples respectively. Training with our PointCutMix augmentation method, over 6.1 pIoU improvement is made for the rocket part-segmentation.</p><p>We infer the reason is that through the fusion of training samples in PointCutMix, the frequency of occurrence of uncommon categories is greatly increased. This also enlightens us that by carefully adjusting the ratio of selecting different categories of samples for augmentation, the unbalanced distribution problem might be effectively alleviated, which is worth exploring in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Robustness test</head><p>After verifying the accuracy improvement of PointCutMix on the point cloud classification, we then use the adversarial  <ref type="bibr" target="#b5">(Dong et al., 2018;</ref><ref type="bibr" target="#b0">Akhtar &amp; Mian, 2018)</ref>. Recently, point perturbation attack <ref type="bibr" target="#b37">(Xiang et al., 2019)</ref>, kNN attack <ref type="bibr" target="#b30">(Tsai et al., 2020)</ref>, and point dropping attack <ref type="bibr" target="#b47">(Zheng et al., 2019)</ref> are proposed for 3D point cloud. In this paper, our method is trained after the normalization of point clouds. Since the point perturbation attack and the kNN attack don't perform normalization of point clouds during the attack and the generated point clouds may not center within a unit sphere, we only consider the point dropping attack in our robustness test.</p><p>We report the recognition accuracy after the point dropping attack on the test set of ModelNet40 in <ref type="table" target="#tab_3">Table 4</ref>, where the results of baseline models refer to IF-Defense . It is observed that the baseline model dramatically degrade. But all models trained with PointCutMix-R and PointCutMix-K still have more than 80% accuracy. It verifies that our method can significantly improve the robustness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Point cloud defense</head><p>Motivated by the impressive performance under point drop attack, we consider applying our method to the point cloud defense. We surprisingly find that using the pre-trained models trained with PointCutMix augmentation as defense methods outperforms the state-of-the-art defense algorithm IF-Defense  by a large margin. Specifically, we first generate adversarial point clouds by point dropping attack on the pre-trained baseline model provided by . We then compare the classifiers trained using PointCutMix augmentation method on these generated adversarial point clouds to several recent developed defense methods, i.e., SRS , SOR (Zhou et al., <ref type="table">Table 5</ref>. Classification accuracy of various defense methods on ModelNet40 under point dropping attack <ref type="bibr" target="#b47">(Zheng et al., 2019)</ref>, kNN attack <ref type="bibr" target="#b30">(Tsai et al., 2020)</ref> and point perturbation attack <ref type="bibr" target="#b37">(Xiang et al., 2019)</ref>. Drop 200 and Drop 100 denote the dropping points is 200 and 100 respectively. * denotes that results are reported in IF-Defense . We report the best result of three IF-Defense methods. The best and second-place results for each row are emphasized as blue and bold.  . From the results listed in <ref type="table">Table 5</ref>, we observe that PointCutMix-R and PointCutMix-K consistently surpass all defense methods under two point dropping attacks. The improvement of recognition accuracy can reach up to 15% in a certain case, which fully proves the scalability and effectiveness of our method. It is worth noting that unlike previous defense methods that need to alter the adversarial point clouds which might cause information loss, our method just uses very limited computing power to classify the adversarial point clouds, which is a more natural defense approach.</p><p>Moreover, to verify the generalization of PointCutMix in defense, we also test with the kNN attack <ref type="bibr" target="#b30">(Tsai et al., 2020)</ref> and the point perturbation attack <ref type="bibr" target="#b37">(Xiang et al., 2019)</ref>. We first perform normalization on the generated adversarial point clouds to limit all points into a unit sphere and then test it with deep point cloud classification networks trained using PointCutMix-K and PointCutMix-R. The kNN attack smoothes the attack by using a k-Nearest Neighbor loss, which is hard to defense by the simple method such as statistical outlier removal . On the contrary, for the point perturbation attack <ref type="bibr" target="#b37">(Xiang et al., 2019)</ref> where the attacked point clouds are messy, it can be easily defended by simple random sampling and statistical outlier removal . As shown in <ref type="table">Table 5</ref>, although these two attacks are very unfavorable for our models that are trained with normalized point clouds, PointCutMix-R still achieves second place and has a very close performance to the state-of-the-art defense method in all cases. We can also find that PointCutMix-R constantly surpasses PointCutMix-K in the defense of two attacks, proving the assumption in Section 3.4 that models trained with PointCutMix-R achieve better robustness.</p><p>From the above analysis, we can conclude that PointCutMix has strong generalization ability across various point cloud attack algorithms and the defense approach is very simple and computing cost-effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose PointCutMix, a regularization strategy for point cloud classification. We conduct extensive experiments to verify the effectiveness of our method. For the object-level point cloud classification problem, the results show that PointCutMix evidently improves the performance of networks that learned with local features. While for the point-wise segmentation task, PointCutMix alleviates the unbalanced distribution problem and enhances the performance of uncommon categories. We also validate that PointCutMix significantly enhances the robustness of the model. By applying our method as a defense method, it outperforms the SOTA defense algorithm. We hope this simple regularization strategy could be applied to more tasks and help future researches.</p><p>In the future, we plan to extend our work to 3D object detection <ref type="bibr" target="#b26">(Shi et al., 2020)</ref>. However, due to the point cloud is different from images, there are still some challenges. For example, in 3D object detection, the point cloud of KITTI <ref type="bibr" target="#b6">(Geiger et al., 2012)</ref> and ModelNet are very different, thus it is hard to directly use the pre-trained model of the classification network in the 3D detection task. Moreover, we also plan to apply our PointCutMix-R and PointCutMix-K to defense methods to recently proposed attacks AdvPC <ref type="bibr" target="#b9">(Hamdi et al., 2020)</ref> and LG-GAN .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Some mixed samples produced by PointCutMix-R (top row) and PointCutMix-K (bottom row). The data generated by PointCutMix-R looks like two objects cross each other while the samples from PointCutMix-K are the obvious combination of two object parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The visualization of the mixed samples between a plane and a chair under different replacement ratio λ. The samples in the first and second row are generated by PointCutMix-R and PointCutMix-K respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Mixed samples of point cloud segmentation problem using PointCutMix-K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Performance of various models with PointCutMix-K under different value of ρ and β. In the upper plot, β = 1. In the lower plot, ρ = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>ModelNet40 classification results. PointMixup-U and PointMixup-A represent the results on unaligned and pre-aligned ModelNet40 with input mixup.</figDesc><table><row><cell>Method</cell><cell cols="3">PointNet++ RS-CNN DGCNN</cell></row><row><cell>baseline</cell><cell>90.7</cell><cell>91.7</cell><cell>92.3</cell></row><row><cell>PointMixup-U</cell><cell>91.7</cell><cell>-</cell><cell>-</cell></row><row><cell>PointMixup-A</cell><cell>92.7</cell><cell>-</cell><cell>92.9</cell></row><row><cell>PointAugment</cell><cell>92.9</cell><cell>92.7</cell><cell>93.4</cell></row><row><cell>PointCutMix-R</cell><cell>92.8</cell><cell>91.9</cell><cell>92.8</cell></row><row><cell>PointCutMix-K</cell><cell>93.4</cell><cell>92.5</cell><cell>93.1</cell></row><row><cell>PointCutMix-S</cell><cell>93.4</cell><cell>92.7</cell><cell>93.2</cell></row><row><cell cols="3">Table 2. ModelNet10 classification results.</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Pointnet++ RS-CNN DGCNN</cell></row><row><cell>baseline</cell><cell>93.3</cell><cell>94.2</cell><cell>94.8</cell></row><row><cell>PointAugment</cell><cell>95.8</cell><cell>96.0</cell><cell>96.7</cell></row><row><cell>PointCutMix-R</cell><cell>96.3</cell><cell>95.7</cell><cell>95.2</cell></row><row><cell>PointCutMix-K</cell><cell>95.7</cell><cell>95.6</cell><cell>95.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison on the ShaperNet part segmentation dataset. pIoU means part-average Intersection-over-Union. We perform the experiment using the settings described in Section 4.4.</figDesc><table><row><cell>Method</cell><cell>pIoU</cell><cell>air-plane</cell><cell>bag cap car chair</cell><cell>ear-phone</cell><cell>guitar knife lamp laptop</cell><cell>motor-bike</cell><cell>mug pistol rocket</cell><cell>skate-board</cell><cell>table</cell></row><row><cell cols="10">PointNet++ 85.0 82.2 81.7 81.5 77.7 90.1 76.7 90.9 87.3 83.8 95.2 69.9 94.2 82.6 56.2 76.6 82.8</cell></row><row><cell cols="10">+PointCutMix 85.5 82.6 85.9 83.7 78.3 90.7 72.5 90.9 87.7 84.3 95.3 70.7 95.1 82.4 62.3 74.9 83.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Classification accuracy of ModelNet40 under point dropping attack<ref type="bibr" target="#b47">(Zheng et al., 2019)</ref> , the dropping points is 200.</figDesc><table><row><cell>Model</cell><cell cols="3">Baseline PointCutMix-R PointCutMix-K</cell></row><row><cell cols="2">PointNet++ 68.96</cell><cell>86.18</cell><cell>87.97</cell></row><row><cell>RS-CNN</cell><cell>56.97</cell><cell>82.50</cell><cell>83.10</cell></row><row><cell>DGCNN</cell><cell>55.06</cell><cell>81.16</cell><cell>85.86</cell></row><row><cell cols="4">attack to investigate whether this regularization strategy can</cell></row><row><cell cols="4">enhance the robustness of the model. As we know, deep neu-</cell></row><row><cell cols="4">ral networks are vulnerable to adversarial examples, which</cell></row><row><cell cols="3">have been extensively studied in 2D images</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Ieee Access</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14410" to="14430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deformable pv-rcnn: Improving 3d object detection with learned deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08766</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointmixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="330" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07397</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">The deepfake detection challenge dataset. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Advpc: Transferable adversarial perturbations on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-B</forename><forename type="middle">J</forename><surname>Fmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<title level="m">Enhancing mixed sample data augmentation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointaugment: an auto-augmentation framework for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6378" to="6387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic points agglomeration for hierarchical point sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7546" to="7555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11596" to="11603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densepoint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="326" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<title level="m">Deep hierarchical feature learning on point sets in a metric space</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5376" to="5385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Taghanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Custis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointmask</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04525</idno>
		<title level="m">Towards interpretable and bias-resilient point cloud processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularized graph cnn for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rgcnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust adversarial objects against deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="954" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">If-defense: 3d adversarial point cloud defense via implicit function based restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05272</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating 3d adversarial point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9136" to="9144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic point cloud segmentation: Towards 10x fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13706" to="13715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointasnl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10899</idno>
		<title level="m">Adversarial attack and defense on point sets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3323" to="3332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointweb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On isometry robustness of deep 3d point cloud models under adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1201" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointcloud saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1598" to="1606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dup-net: Denoiser and upsampler network for 3d adversarial point clouds defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Lg-gan: Label guided adversarial network for flexible targeted attack of point cloud based deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10356" to="10365" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
