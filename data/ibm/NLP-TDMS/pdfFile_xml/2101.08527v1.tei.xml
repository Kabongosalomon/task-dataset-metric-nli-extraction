<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PROGRESSIVE CO-ATTENTION NETWORK FOR FINE-GRAINED VISUAL CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">PROGRESSIVE CO-ATTENTION NETWORK FOR FINE-GRAINED VISUAL CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fine-grained visual classification</term>
					<term>chan- nel interaction</term>
					<term>attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual classification aims to recognize images belonging to multiple sub-categories within a same category. It is a challenging task due to the inherently subtle variations among highly-confused categories. Most existing methods only take individual image as input, which may limit the ability of models to recognize contrastive clues from different images. In this paper, we propose an effective method called progressive co-attention network (PCA-Net) to tackle this problem. Specifically, we calculate the channel-wise similarity by interacting the feature channels within same-category images to capture the common discriminative features. Considering that complementary imformation is also crucial for recognition, we erase the prominent areas enhanced by the channel interaction to force the network to focus on other discriminative regions. The proposed model can be trained in an end-to-end manner, and only requires image-level label supervision. It has achieved competitive results on three fine-grained visual classification benchmark datasets: CUB-200-2011, Stanford Cars, and FGVC Aircraft.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the past few years, deep convolutional neural networks (CNNs) have achieved remarkable success in general image classification tasks. However, recognizing fine-grained object categories (e.g., bird species <ref type="bibr" target="#b0">[1]</ref>, car <ref type="bibr" target="#b1">[2]</ref> and aircraft <ref type="bibr" target="#b2">[3]</ref> models) is still a challenging task due to high intra-class variances and low inter-class variances, which attracts extensive research attention.</p><p>In order to alleviate the problem of high intra-class variances, a number of fine-grained frameworks have been proposed to find the discriminative semantic parts. Specifically, a localization sub-network is designed to locate these key parts, and then a classification sub-network follows for identification <ref type="bibr" target="#b3">[4]</ref>. However, the training procedures of these methods are sophisticated, requiring multiple alternations or cascaded stages due to the complex architecture designs <ref type="bibr" target="#b4">[5]</ref>. Others directly learn a more discriminative feature representation by developing powerful deep models <ref type="bibr" target="#b3">[4]</ref>. Among them, the most representative method is bilinear CNNs <ref type="bibr" target="#b5">[6]</ref>. However, it does CNN CNN <ref type="figure">Fig. 1</ref>. The motivation of the proposed method. Most previous methods only took individual image as input, and the relationship between images was not explored. In our work, we input a pair of same-category images and model the channel interaction between them to capture their common features. not have an effective design for fine-grained visual classification in terms of the relationship between categories and the number of key parts. Meanwhile, most of the methods mentioned above only take individual image as input, which may limit their ability to identify contrastive clues from different images for fine-grained visual classification. Generally, humans often recognize fine-grained objects by comparing image pair, instead of checking single image alone <ref type="bibr" target="#b6">[7]</ref>. As shown in <ref type="figure">Figure 1</ref>, by comparing the pair of same-category images, we can easily capture their common and prominent features, such as head and wings. In this way, we can learn the discriminative features precisely.</p><p>Inspired by this observation, we propose a co-attention module (CA-Module) to model the channel interaction between a pair of same-category images. By capturing the contrastive features between channels, the model can better learn the commonality of same-category images, thereby force the network to focus on the common discriminative features. However, only focusing on common features of samecategory images will cause the network to ignore complementary features that are essential for highly-confused categories. In order to tackle this problem, we propose an attention erase module (AE-Module) to learn complementary features by erasing the most prominent area found in CA-Module. With the combination of these two modules, the proposed method can capture more relevant areas, make the prediction of network more comprehensive, and improve the classification performance. The main contributions of this paper are summarized as follows:</p><p>• We propose an effective method named progressive co-attention network (PCA-Net), which utilizes CA-Module to focus on the common features in the samecategory images and AE-Module to distract attention in subtle areas to capture complementary information.</p><p>• We conduct extensive experiments on three popular fine-grained visual classification benchmark datasets, CUB-200-2011, Stanford Cars, and FGVC Aircraft, where our PCA-Net obtains competitive results.</p><p>• Compared with existing methods, the proposed PCA-Net does not need extra part/object annotation and introduces no computational overhead during inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Many early works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> not only used image-level labels, but also added auxiliary information such as bounding boxes or part annotations. These models obtained decent results, but the training cost was very expensive due to the large labor overhead, which limited its application in actual scenarios <ref type="bibr" target="#b9">[10]</ref>. In order to increase the usability of fine-grained visual classification in real scenarios, more recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> only use image-level class labels to tackle this challenging task. Many existing fine-grained models used weak supervision for localization, then followed by a classification sub-network for recognition. Such as STN <ref type="bibr" target="#b14">[15]</ref>, RA-CNN <ref type="bibr" target="#b10">[11]</ref>, NTS-Net <ref type="bibr" target="#b11">[12]</ref>. They first detected the local areas, and then cropped the detected areas on the original image to learn the key parts. However, these methods often required additional local positioning modules, which led to excessive calculations. Therefore, Sun et al. <ref type="bibr" target="#b4">[5]</ref> proposed MAMC, which adopted a structure that could directly extract key regionals. It was equivalent to applying a soft map to feature maps for positioning, which greatly reduced the computational cost. These methods were limited by a fixed number of components and the lack of surrounding environment due to hard cropping. Therefore, Ding et al. <ref type="bibr" target="#b12">[13]</ref> proposed a selective sparse sampling network to capture fine-grained clues by learning a set of sparse attention evidences without cropping, which could preserve the surrounding environment of the areas.</p><p>Learning representative features is essential for finegrained visual classification. Therefore, many works attempt to adopt end-to-end feature coding to directly learn more discriminative representations by developing powerful deep models <ref type="bibr" target="#b3">[4]</ref>. Lin et al. <ref type="bibr" target="#b5">[6]</ref> proposed a bilinear pooling model, which used two deep convolutional networks to fuse output features, so that it could encode high-order statistics of convolutional activation. However, the extremely high dimensionality of bilinear features made it limited in large-scale practical applications. Gao et al. <ref type="bibr" target="#b15">[16]</ref> proposed a compact bilinear model. It could improve the original bilinear model into a compact structure, which reduced the feature dimension by two orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head><p>In this section, we present the proposed progressive coattention network (PCA-Net) for fine-grained visual classification, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. To calculate the channelwise similarity for same-category images, a co-attention module (CA-Module) is designed to model the interaction between channels within a pair of same-category images. We generate new interacted feature maps where the response values of similar parts increase, and the response values of other parts decrease. For highly-confused categories, it is crucial to considering the complementary information hidden in subtle areas. Thus an attention erase module (AE-Module) is designed to erase the prominent areas found in CA-Module to force the network to focus on complementary features.</p><p>Compared with existing methods, the proposed method can be trained in an end-to-end manner and introduces no computational overhead at inference time. We use ResNet-50 <ref type="bibr" target="#b16">[17]</ref> and Resnet-101 <ref type="bibr" target="#b16">[17]</ref> for illustration and experiments, and the proposed method is also applicable to other convolutional network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Co-attention module</head><p>Each channel of feature maps can be considered as the representation of a certain feature, and its value is the response to the current feature strength. Convolutional layer is to model the interaction between channels within an image to generate more abundant features. Gao et al. <ref type="bibr" target="#b17">[18]</ref> proposed a selfchannel interaction (SCI) module to model the interplay between different channels within an image more effectively. Since the intra-class variances of fine-grained images are very high, it is important to compare a pair of same-category images to obtain common features. Hence, we propose a coattention module (CA-Module) to model the interaction between channels within two same-category images, forcing the network to focus on the common areas.</p><p>Specifically, given an image pair, the two images are first processed by a convolutional network to generate a pair of feature maps F 1 , F 2 ∈ R c×h×w . The c, h and w indicate channel numbers, height, and width respectively. We reshape the feature maps F 1 , F 2 to F 1 , F 2 ∈ R c×l , l = h × w. Then calculate the channel-wise similarity by performing a bilinear operation on F 1 and F T 2 to obtain a bilinear matrix F 1 F T 2 . Then we add a minus sign to it and get the weight matrix through a softmax function:</p><formula xml:id="formula_0">W ij = exp(−F 1 F T 2 ij ) c k=1 exp(−F 1 F T 2 ik ) .<label>(1)</label></formula><p>We weight the weight matrix W to the original feature maps. Then the interacted feature maps can be obtained as:</p><formula xml:id="formula_1">F W 1 = W ij F 1 ∈ R c×l , F W 2 = W ij F 2 ∈ R c×l .<label>(2)</label></formula><p>Moreover, we use a cross-entropy loss for classification based on the predictions that are generated by the features F W .</p><p>After weighting the feature maps, the channels corresponding to similar features are enhanced to find the commonality of this pair of images. The response values of similar parts increase, and the response values of other parts decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention erase module</head><p>The CA-Module aims to focus on the common features of same-category images, but it ignores other slight clues containing complementary information. Because there are subtle differences between highly-confused categories in finegrained images, it is necessary to pay attention to subtle features. Hence, we propose an attention erase module (AE-Module) to capture the subtle complementary features by erasing the prominent areas in the image.</p><p>We perform global average pooling on the feature maps weighted by CA-Module and rank them in descending order. Then we select the feature maps corresponding to the maximum value as the attention map, and up-sample it to the original image size:</p><formula xml:id="formula_2">A i,j = max m [F W (m, i, j)].<label>(3)</label></formula><p>We obtain a drop mask M by setting the elements A(x, y) larger than threshold θ to 0, and set other elements to 1:</p><formula xml:id="formula_3">M (x, y) = 0, if A(x, y) &gt; θ 1, else .<label>(4)</label></formula><p>Overlay the drop mask on the original image to obtain a new image with partial areas erased:</p><formula xml:id="formula_4">I e (x, y) = I(x, y) ⊗ M (x, y),<label>(5)</label></formula><p>where ⊗ denotes element-wise multiplication, I denotes the original image, I e denotes the erased image. As the prominent areas of the image are erased, the attention is distracted and the network is forced to learn discriminative information from other areas. It can also reduce the dependence of network on training samples, prevent overfitting, and further improve the robustness of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fine-grained Feature Learning</head><p>In order to make the learned features more aggregated in embedding space, we introduce a center loss <ref type="bibr" target="#b24">[25]</ref>. For each category, a feature vector is calculated as the center of the corresponding category, and it will be continuously updated during training phase. By penalizing the deviation between the bilinear feature vector of each sample and the center of the corresponding category, the samples belonging to the same category are grouped together as many as possible, which enhances the discrimination of the learned features.</p><p>Benefiting from global and local informative features of an image, we define the feature representations for each image:</p><formula xml:id="formula_5">F J = {F O , F W , F E },</formula><p>where F O denotes the feature maps extracted from the original image, F W denotes the weighted feature maps of the original image, and F E denotes the feature maps extracted from the erased image. These features are then fed to a fully-connection layer with a softmax function for the final classification.</p><p>During training phase, the whole model is optimized by losses defined as</p><formula xml:id="formula_6">L = i∈I L cls (Y i , Y * ) + λL cen (Y i , Y c ),<label>(6)</label></formula><p>where L cls denotes the cross-entropy loss, L cen denotes the center loss and λ denotes the weight of L cen . Y i is the predicted bilinear label vector based on features F O , F W and F E . Y * is the ground-truth label vector and Y c is the learned center vector of category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND DISCUSSIONS</head><p>We evaluate the proposed approach on three publicly competitive fine-grained visual classification datasets, including CUB-200-2011 <ref type="bibr" target="#b0">[1]</ref>, Stanford Cars <ref type="bibr" target="#b1">[2]</ref>, and FGVC Aircraft <ref type="bibr" target="#b2">[3]</ref>. The detailed statistics with category numbers and the standard training/test splits are summarized in <ref type="table" target="#tab_0">Table 1</ref>. We employ top-1 accuracy as evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>In all experiments, we use ResNet-50 <ref type="bibr" target="#b16">[17]</ref> and ResNet-101 <ref type="bibr" target="#b16">[17]</ref> as our base networks. We remove the last pooling layer and fully-connected layer, and add a billinear pooling to generate a more powerful feature representation. Then send the feature representation to a new fully-connected layer. The convolutional layers are pretrained on ImageNet <ref type="bibr" target="#b25">[26]</ref> and the fully connected layer is randomly initialized. The input image size of all experiments is 448 × 448, which is the same as the most state-of-the-art fine-grained classification approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. In order to improve the generalization ability of the model, we implement data augmentation including random cropping and random horizontal flipping during training phase. Only center cropping is involved in inference phase. All experimental models are trained for 180 epochs, using SGD as optimizer. The initial learning rate is set to 0.01, which annealed by 0.9 every 2 epoches. We use a batch size of 32, and the last 16 samples belong to the same category as the first 16 samples. The <ref type="figure">Fig. 3</ref>. The first line is the original images, the second line is the features learned by the base model, and the third line is the features learned by the proposed method. It can be observed that the proposed method has learned more feature information than the base model. weight decay is set to 10 −5 and the momentum is set to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental results</head><p>The proposed method is compared with several representative methods without extra annotations on three fine-grained benchmark datasets of CUB-200-2011, Stanford Cars, and FGVC Aircraft, as shown in <ref type="table" target="#tab_1">Table 2</ref>. The proposed method performs competitive results compared with the state of-theart methods. Specifically, the top-1 accuracy of the proposed method on CUB-200-2011 dataset is 88.9%, which is better than 88.6% of API-Net <ref type="bibr" target="#b6">[7]</ref>. API-Net performs well because it also takes a pair of images as input, and learns a mutual feature vector by modeling the interaction between them to capture the semantic difference in the input pair. Our method is to capture the semantically similarity of a pair of same-category images, and disperse the network's attention to each discriminative area of the image.</p><p>On the other two datasets, our method also performs well, which achieves the result with 94.6% top-1 accuracy for Stanford Cars dataset and 92.8% top-1 accuracy for FGVC-Aircraft dataset. However, our method does not obtain the best results on these two datasets. Since cars and aircrafts are rigid objects, the intra-class variances of them are not assignificant as birds. And our method works better for objects that greatly vary within the category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>In order to verify the effectiveness of each component in the model, we conduct ablation studies on CUB-200-2011 dataset using ResNet-50 with some of components removed for better understanding the behavior of the model, as shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>It can be observed that CA-Module generates weighted feature maps through modeling the channel-wise interaction of same-category images, so that the network can learn their common features. AE-Module erases the most prominent area based on the weighted feature maps, making the informative areas learned by the network diversified. The center loss makes the extracted features more discriminative by constraining the distance between the feature vector and the center vector of the corresponding category. Each component of the model contributes to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualizations</head><p>In order to further evaluate the effectiveness of our method, we apply Grad-CAM <ref type="bibr" target="#b26">[27]</ref> to visualize the images of the CUB-200-2011 dataset. Grad-CAM is formed by weighted summation of feature maps, which can show the importance of each area to its classification. We compare the visualization results of our method with the base model (ResNet-50), as shown in <ref type="figure">Figure 3</ref>. It can be observed that the base model only learns the most prominent area of the image, such as the bird's beak. Our method can learn more abundant and discriminative features, including wings and claws. This is because that our method can distribute attention to each area to make the prediction more comprehensive, which can not only focus on the salient features, but also capture the subtle and fine-grained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>We propose an effective fine-grained visual classification method, namely progressive co-attention network. Among them, the co-attention module learns discriminative features by comparing same-category images, and the attention erase module learns subtle complementary features of images by erasing the most prominent area. We have conducted experiments on CUB-200-2011, Stanford Cars, and FGVC Aircraft datasets, which are superior to most existing methods. The proposed method still has the possibility of improved, such as exploring the association between heterogeneous images, combining with target detection, further mining image features, etc., which needs further research in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of the progressive co-attention network (PCA-Net). The CA-Module can model the channel-wise interaction within a pair of same-category images to generate weighted feature maps, which can force the model to focus on the prominent areas with common characteristics. To capture other complementary features, an AE-Module is proposed to erase the images based on the weighted feature maps, which can force the model to distract attention to other areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of benchmark datasets</figDesc><table><row><cell>Datesets</cell><cell cols="3">Class Training Test</cell></row><row><cell>CUB-200-2011</cell><cell>200</cell><cell>5997</cell><cell>5794</cell></row><row><cell>Stanford Cars</cell><cell>196</cell><cell>8144</cell><cell>8041</cell></row><row><cell>FGVC Aircraft</cell><cell>100</cell><cell>6667</cell><cell>3333</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparative experiment between recent methods with our method on CUB-200-2011, Stanford Cars, and FGVC Aircraft. The best result is colored in red, and the second best result is colored in blue.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">Input size CUB-200-2011 (%) Stanford Cars (%) FGVC-Aircraft (%)</cell></row><row><cell>FT VGG-19 [19]</cell><cell>VGG-19</cell><cell>448 × 448</cell><cell>77.8</cell><cell>84.9</cell><cell>84.8</cell></row><row><cell>RA-CNN [11]</cell><cell>VGG-19</cell><cell>448 × 448</cell><cell>85.3</cell><cell>92.5</cell><cell>−</cell></row><row><cell>MA-CNN [20]</cell><cell>VGG-19</cell><cell>448 × 448</cell><cell>86.5</cell><cell>92.8</cell><cell>89.9</cell></row><row><cell cols="3">FT ResNet-50 [19] ResNet-50 448 × 448</cell><cell>84.1</cell><cell>91.7</cell><cell>88.5</cell></row><row><cell>RAM [21]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>86.0</cell><cell>93.1</cell><cell>-</cell></row><row><cell>DFL-CNN [19]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>87.4</cell><cell>93.1</cell><cell>91.7</cell></row><row><cell>NTS-Net [12]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>87.5</cell><cell>93.9</cell><cell>91.4</cell></row><row><cell>MC-Loss [10]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>87.3</cell><cell>93.7</cell><cell>92.6</cell></row><row><cell>TASN [22]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>87.9</cell><cell>93.8</cell><cell>-</cell></row><row><cell>Cross-X [23]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>87.7</cell><cell>94.6</cell><cell>92.6</cell></row><row><cell>ACNet [24]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>88.1</cell><cell>94.6</cell><cell>92.4</cell></row><row><cell>MAMC [5]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>86.2</cell><cell>92.8</cell><cell>-</cell></row><row><cell>MAMC [5]</cell><cell cols="2">ResNet-101 448 × 448</cell><cell>86.5</cell><cell>93.0</cell><cell>-</cell></row><row><cell>CIN [18]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>87.5</cell><cell>94.1</cell><cell>92.6</cell></row><row><cell>CIN [18]</cell><cell cols="2">ResNet-101 448 × 448</cell><cell>88.1</cell><cell>94.5</cell><cell>92.8</cell></row><row><cell>API-Net [7]</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>87.7</cell><cell>94.8</cell><cell>93.0</cell></row><row><cell>API-Net [7]</cell><cell cols="2">ResNet-101 448 × 448</cell><cell>88.6</cell><cell>94.9</cell><cell>93.4</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50 448 × 448</cell><cell>88.3</cell><cell>94.3</cell><cell>92.4</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-101 448 × 448</cell><cell>88.9</cell><cell>94.6</cell><cell>92.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation</figDesc><table><row><cell>studies</cell></row><row><cell>CA-Module AE-Module center loss accuracy(%)</cell></row><row><cell>86.8</cell></row><row><cell>86.5</cell></row><row><cell>87.9</cell></row><row><cell>87.5</cell></row><row><cell>88.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning for fine-grained image analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03069</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to navigate for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Channel Interaction Networks for Fine-Grained Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crossx learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11378</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
