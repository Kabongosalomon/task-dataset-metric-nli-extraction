<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<postCode>SY23 3FL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* This work is supported by The National Key Research and Develop-ment Program of China (No. 2017YFA0700800), the National Natural Science Foundation of China (No.61925107, No.U1936202) and Beijing Academy of Artificial Intelligence (BAAI). Xiaohan Ding is funded by the Baidu Scholarship Program 2019. This work is done during Xiaohan Ding&apos;s internship at MEGVII. † Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose RepMLP, a multi-layer-perceptron-style neural network building block for image recognition, which is composed of a series of fully-connected (FC) layers. Compared to convolutional layers, FC layers are more efficient, better at modeling the long-range dependencies and positional patterns, but worse at capturing the local structures, hence usually less favored for image recognition. We propose a structural re-parameterization technique that adds local prior into an FC to make it powerful for image recognition. Specifically, we construct convolutional layers inside a RepMLP during training and merge them into the FC for inference. On CIFAR, a simple pure-MLP model shows performance very close to CNN. By inserting RepMLP in traditional CNN, we improve ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and 2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight that combining the global representational capacity and positional perception of FC with the local prior of convolution can improve the performance of neural network with faster speed on both the tasks with translation invariance (e.g., semantic segmentation) and those with aligned images and positional patterns (e.g., face recognition). The code and models are available at https://github.com/DingXiaoH/RepMLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The locality of images (i.e., a pixel is more related to its neighbors than the distant pixels) makes Convolutional Neural Network (ConvNet) successful in image recognition, as a conv layer only processes a local neighborhood. In this paper, we refer to this inductive bias as the local prior.</p><p>On top of that, we also desire the ability to capture the long-range dependencies, which is referred to as the global capacity in this paper. Traditional ConvNets model the long-range dependencies by the large receptive fields formed by deep stacks of conv layers <ref type="bibr" target="#b24">[25]</ref>. However, repeating local operations is computationally inefficient and may cause optimization difficulties. Some prior works enhance the global capacity with self-attention-based modules <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>, which has no local prior. For example, ViT <ref type="bibr" target="#b10">[11]</ref> is a pure-Transformer model without convolution, which feeds images into the Transformers as a sequence. Due to the lack of local prior as an important inductive bias, ViT needs an enormous amount of training data (3 × 10 8 images in JFT-300M) to converge.</p><p>On the other hand, some images have intrinsic positional prior, which cannot be effectively utilized by a conv layer because it shares parameters among different positions. For example, when someone tries to unlock a cellphone via face recognition, the photo of the face is very likely to be centered and aligned so that the eyes appear at the top and the nose shows at the middle. We refer to the ability to utilize such positional prior as the positional perception.</p><p>This paper revisits fully-connected (FC) layers to provide traditional ConvNet with global capacity and positional perception. We directly use an FC as the transformation between feature maps to replace conv in some cases. By flattening a feature map, feeding it through FC, and reshaping back, we can enjoy the positional perception (because its parameters are position-related) and global capac-ity (because every output point is related to every input point). Such an operation is efficient in terms of both the actual speed and theoretical FLOPs, as shown in <ref type="table">Table.</ref> 4. For the application scenarios where the primary concerns are the accuracy and throughput but not the number of parameters, one may prefer FC-based models to traditional Con-vNets. For example, the GPU inference serves usually have tens of GBs of memory, so that the memory occupied by the parameters is minor compared to that consumed by the computations and internal feature maps.</p><p>However, an FC has no local prior because the spatial information is lost. In this paper, we propose to incorporate local prior into FC with a structural re-parameterization technique. Specifically, we construct conv and batch normalization (BN) <ref type="bibr" target="#b14">[15]</ref> layers parallel to the FC during training, then merge the trained parameters into the FC to reduce the number of parameters and latency for inference. Based on that, we propose a re-parameterized multilayer perceptron (RepMLP). As shown in <ref type="figure">Fig. 1</ref>, the training-time RepMLP has FC, conv, and BN layers but can be equivalently converted into an inference-time block with only three FC layers. The meaning of structural reparameterization is that the training-time model has a set of parameters while the inference-time model has another set, and we parameterize the latter with the parameters transformed from the former. Note that we do not derive the parameters before each inference. Instead, we convert it once for all, and then the training-time model can be discarded.</p><p>Compared to conv, RepMLP runs faster under the same number of parameters and has global capacity and positional perception. Compared to a self-attention module <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11]</ref>, it is simpler and can utilize the locality of images. As shown in our experiments <ref type="table" target="#tab_5">(Table. 4</ref>, 5, 6), RepMLP outperforms the traditional ConvNets in a variety of vision tasks, including 1) general classification (ImageNet [8]), 2) task with positional prior (face recognition) and 3) task with translation invariance (semantic segmentation).</p><p>Our contributions are summarized as follows.</p><p>• We propose to utilize the global capacity and positional perception of FC and equip it with local prior for image recognition. • We propose a simple, platform-agnostic and differentiable algorithm to merge the parallel conv and BN into FC for the local prior without any inference-time costs. • We propose RepMLP, an efficient building block, and show its effectiveness on multiple vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Designs for Global Capacity</head><p>Non-local Network <ref type="bibr" target="#b24">[25]</ref> proposed to model the longrange dependencies via the self-attention mechanism. For each query position, the non-local module first computes the pairwise relations between the query position and all positions to form an attention map and then aggregates the features of all the positions by a weighted sum with the weights defined by the attention map. Then the aggregated features are added to the features of each query position.</p><p>GCNet <ref type="bibr" target="#b0">[1]</ref> created a simplified network based on a query-independent formulation, which maintains the accuracy of Non-local Network with less computation. The input to a GC block goes through a global attention pooling, feature transform (a 1 × 1 conv), and feature aggregation.</p><p>Compared to these works, RepMLP is simpler as it uses no self-attention and contains only three FC layers. As will be shown in <ref type="table">Table.</ref> 4, RepMLP improves the performance of ResNet-50 more than Non-local module and GC block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Structural Re-parameterization</head><p>In this paper, structural re-parameterization refers to constructing the conv and BN layers parallel to an FC for training and then merging the parameters into the FC for inference. The following two prior works can also be categorized into structural re-parameterization.</p><p>Asymmetric Convolution Block (ACB) <ref type="bibr" target="#b8">[9]</ref> is a replacement for regular conv layers, which uses horizontal (e.g., 1 × 3) and vertical (3 × 1) conv to strengthen the "skeleton" of a square (3 × 3) conv. Reasonable performance improvements are reported on several ConvNet benchmarks.</p><p>RepVGG <ref type="bibr" target="#b9">[10]</ref> is a VGG-like architecture, as its body uses only 3 × 3 conv and ReLU for inference. Such an inference-time architecture is converted from a trainingtime architecture with identity and 1 × 1 branches.</p><p>RepMLP is more related to ACB since they are both neural network building blocks, but our contributions are not about making convolutions stronger but making MLP powerful for image recognition as a replacement for regular conv. Besides, the training-time convolutions inside RepMLP may be enhanced by ACB, RepVGG block, or other forms of convolution for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RepMLP</head><p>A training-time RepMLP is composed of three parts termed as Global Perceptron, Partition Perceptron and Local Perceptron <ref type="figure">(Fig. 1</ref>). In this section, we introduce our formulation, describe every component, and show how to convert a training-time RepMLP into three FC layers for inference, where the key is a simple, platform-agnostic and differentiable method for merging a conv into an FC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>In this paper, a feature map is denoted by a tensor M ∈ R N ×C×H×W , where N is the batch size, C is the number of channels, H and W are the height and width, respectively. We use F and W for the kernel of conv and FC, respectively. For the simplicity and ease of re-implementation, we  = 2 (i.e., a channel is split into four partitions) for the better readability. We assume h, w &gt; 7 so that the Local Perceptron has conv branches of kernel size 1, 3, 5, 7. The shapes of parameter tensors are shown alongside FC and conv layers. Via structural re-parameterization, the training-time block with conv and BN layers is equivalently converted into a three-FC block, which is saved and used for inference.</p><p>use the same data format as PyTorch <ref type="bibr" target="#b19">[20]</ref> and formulate the transformations in a pseudo-code style. For example, the data flow through a K × K conv is formulated as</p><formula xml:id="formula_0">M (out) = CONV(M (in) , F, p) ,<label>(1)</label></formula><p>where M (out) ∈ R N ×O×H ×W is the output feature map, O is the number of output channels, p is the number of pixels to pad, F ∈ R O×C×K×K is the conv kernel (we temporarily assume the conv is dense, i.e., the number of groups is 1). From now on, we assume H = H, W = W for the simplicity (i.e., the stride is 1 and p = K 2 ). For an FC, let P and Q be the input and output dimensions, V (in) ∈ R N ×P and V (out) ∈ R N ×Q be the input and output, respectively, the kernel is W ∈ R Q×P and the matrix multiplication (MMUL) is formulated as</p><formula xml:id="formula_1">V (out) = MMUL(V (in) , W) = V (in) · W .<label>(2)</label></formula><p>We now focus on an FC that takes M (in) as input and outputs M (out) . We assume the FC does not change the resolution, i.e., H = H, W = W . We use RS (short for "reshape") as the function that only changes the shape specification of tensors but not the order of data in memory, which is cost-free. The input is first flattened into N vectors of length CHW , which is</p><formula xml:id="formula_2">V (in) = RS(M (in) , (N, CHW )), multiplied by the kernel W(OHW, CHW ), then the output V (out) (N, OHW ) is re- shaped back into M (out) (N, O, H, W ).</formula><p>For the better readability, we omit the RS if there is no ambiguity,</p><formula xml:id="formula_3">M (out) = MMUL(M (in) , W) .<label>(3)</label></formula><p>Such an FC cannot take advantage of the locality of images as it computes each output point according to every input point, unaware of the positional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Components of RepMLP</head><p>We do not use FC in the above-mentioned manner because of not only the lack of local prior but also the huge number of parameters, which is COH 2 W 2 . With the common settings, e.g., H = W = 28, C = O = 128 on Im-ageNet, this single FC would have 10G parameters, which is clearly unacceptable. To reduce the parameters, we propose Global Perceptron and Partition Perceptron to model the inter-and intra-partition dependencies separately.</p><p>Global Perceptron splits up the feature map so that different partitions can share parameters. For example, an (N, C, 14, 14) input can be split into (4N, C, 7, 7), and we refer to every 7 × 7 block as a partition. We use an efficient implementation for such splitting with a single operation of memory re-arrangement. Let h and w be the desired height and width of every partition (we assume H, W are divisible by h, w respectively, otherwise we can simply pad the input), the input M ∈ R N ×C×H×W is first reshaped into (N, C, H h , h, W w , w). Note that this operation is cost-free as it does not move data in memory. Then we re-arrange the order of axes as (N, H h , W w , C, h, w), which moves the data in memory efficiently. For example, it requires only one function call (permute) in PyTorch. Then the (N, H h , W w , C, h, w) tensor is reshaped (which is costfree again) as ( N HW hw , C, h, w) (noted as a partition map in <ref type="figure">Fig. 1</ref>). In this way, the number of parameters required is reduced from COH 2 W 2 to COh 2 w 2 .</p><p>However, splitting breaks the correlations among different partitions of the same channel. In other words, the model will view the partitions separately, totally unaware that they were positioned side by side. To add correlations onto each partition, Global Perceptron 1) uses average pooling to obtain a pixel for each partition, 2) feeds it though BN and a two-layer MLP, then 3) reshapes and adds it onto the partition map. This addition can be efficiently implemented with automatic broadcasting (i.e., implicitly replicating ( N HW hw , C, 1, 1) into ( N HW hw , C, h, w)) so that every pixel is related to the other partitions. Then the partition map is fed into Partition Perceptron and Local Perceptron. Note that if H = h, W = w, we directly feed the input feature map into Partition Perceptron and Local Perceptron without splitting, hence there will be no Global Perceptron.</p><p>Partition Perceptron has an FC and a BN layer, which takes the partition map. The output ( N HW hw , O, h, w) is reshaped, re-arranged and reshaped in the inverse order as before into (N, O, H, W ). We further reduce the parameters of FC3 inspired by groupwise conv <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>. With g as the number of groups, we formulate the groupwise conv as</p><formula xml:id="formula_4">M (out) = gCONV(M (in) , F, g, p) , F ∈ R O× C g ×K×K . (4)</formula><p>Similarly, the kernel of groupwise FC is W ∈ R Q× P g , which has g× fewer parameters. Though groupwise FC is not directly supported by some computing frameworks like PyTorch, it can be alternatively implemented by a groupwise 1 × 1 conv. The implementation is composed of three steps: 1) reshaping V (in) as a "feature map" with spatial size of 1 × 1; 2) performing 1 × 1 conv with g groups; 3) reshaping the output "feature map" into V (out) . We formulate the groupwise matrix multiplication (gMMUL) as</p><formula xml:id="formula_5">M = RS(V (in) , (N, P, 1, 1)), F = RS(W, (Q, P g , 1, 1) , gMMUL(V (in) , W, g) = RS(gCONV(M , F , g, 0), (N, Q)) .</formula><p>(5) Local Perceptron feeds the partition map through several conv layers. A BN follows every conv, as inspired by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. <ref type="figure">Fig. 1</ref> shows an example of h, w &gt; 7 and K = 1, 3, 5, 7. Theoretically, the only constraint on the kernel size K is K ≤ h, w (because it does not make sense to use kernels larger than the resolution), but we only use odd kernel sizes as a common practice in ConvNet. We use K × K just for the simplicity of notation and a non-square conv (e.g., 1 × 3 or 3 × 5) also works. The padding of conv should be configured to maintain the resolution (e.g., p = 0, 1, 2, 3 for K = 1, 3, 5, 7, respectively), and the number of groups g should be the same as the Partition Perceptron. The outputs of all the conv branches and Partition Perceptron are added up as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A Simple, Platform-agnostic, Differentiable Algorithm for Merging Conv into FC</head><p>Before converting a RepMLP into three FC layers, we first show how to merge a conv into FC. With the FC kernel W (1) (Ohw, Chw), conv kernel F(O, C, K, K) (K ≤ h, w) and padding p, we desire to construct W so that</p><formula xml:id="formula_6">MMUL(M (in) , W ) = MMUL(M (in) , W (1) ) + CONV(M (in) , F, p) .<label>(6)</label></formula><p>We note that for any kernel W (2) of the same shape as W <ref type="bibr" target="#b0">(1)</ref> , the additivity of MMUL ensures that</p><formula xml:id="formula_7">MMUL(M (in) , W (1) ) + MMUL(M (in) , W (2) ) = MMUL(M (in) , W (1) + W (2) ) ,<label>(7)</label></formula><p>so we can merge F into W <ref type="bibr" target="#b0">(1)</ref> as long as we manage to construct W (F,p) of the same shape as W (1) which satisfies</p><formula xml:id="formula_8">MMUL(M (in) , W (F,p) ) = CONV(M (in) , F, p) . (8)</formula><p>Obviously, W (F,p) must exist, since a conv can be viewed as a sparse FC that shares parameters among spatial positions, which is exactly the source of its translation invariance, but it is not obvious to construct it with given F and p. As modern computing platforms use different algorithms of convolution (e.g., im2col- <ref type="bibr" target="#b1">[2]</ref>, Winograd- <ref type="bibr" target="#b16">[17]</ref>, FFT- <ref type="bibr" target="#b17">[18]</ref>, MEC- <ref type="bibr" target="#b3">[4]</ref>, and sliding-window-based) and the memory allocation of data and implementations of padding may be different, a means for constructing the matrix on a specific platform may not work on another platform. In this paper, we propose a simple and platform-agnostic solution.</p><p>As discussed above, for any input M (in) and conv kernel F, padding p, there exists an FC kernel W (F,p) such that <ref type="figure">W (F,p)</ref> ) .</p><formula xml:id="formula_9">M (out) = CONV(M (in) , F, p) = MMUL(M (in) ,</formula><p>(9) With the formulation used before (Eq. 2), we have</p><formula xml:id="formula_10">V (out) = V (in) · W (F,p) .<label>(10)</label></formula><p>We insert an identity matrix I (Chw, Chw) and use the associative law</p><formula xml:id="formula_11">V (out) = V (in) · (I · W (F,p) ) .<label>(11)</label></formula><p>We note that because W (F,p) is constructed with F, I · W (F,p) is a convolution with F on a feature map M (I) which is reshaped from I. With explicit RS, we have</p><formula xml:id="formula_12">M (I) = RS(I, (Chw, C, h, w)) ,<label>(12)</label></formula><formula xml:id="formula_13">I · W (F,p) = CONV(M (I) , F, p) ,<label>(13)</label></formula><formula xml:id="formula_14">V (out) = V (in) · RS(I · W (F,p) , (Chw, Ohw)) .<label>(14)</label></formula><p>Comparing Eq. 10 with Eq. 13, 14, we have W (F,p) = RS(CONV(M (I) , F, p), (Chw, Ohw)) . <ref type="bibr" target="#b14">(15)</ref> Which is exactly the expression we desire for constructing W (F,p) with F, p. In short, the equivalently FC kernel of a conv kernel is the result of convolution on an identity matrix with proper reshaping. Better still, the conversion is efficient and differentiable, so one may derive the FC kernel during training and use it in the objective function (e.g., for penalty-based pruning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>). The expression and code for the groupwise case are derived in a similar way and provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Converting RepMLP into Three FC Layers</head><p>To use the theory presented above, we need to first eliminate the BN layers by equivalently fusing them into the preceding conv layers and FC3. Let F ∈ R O× C g ×K×K be the conv kernel, µ, σ, γ, β ∈ R O be the accumulated mean, standard deviation and learned scaling factor and bias of the following BN, we construct the kernel F and bias b as</p><formula xml:id="formula_15">F i,:,:,: = γ i σ i F i,:,:,: , b i = − µ i γ i σ i + β i .<label>(16)</label></formula><p>Then it is easy to verify the equivalence:  where the left side is the original computation flow of a conv-BN, and the right is the constructed conv with bias.</p><formula xml:id="formula_16">γ i σ i (CONV(M, F, p) :,i,:,: − µ i ) + β i = CONV(M, F , p) :,i,:,: + b i , ∀1 ≤ i ≤ O ,<label>(17)</label></formula><p>The 1D BN and FC3 of Partition Perceptron are fused in a similar way intoŴ ∈ R Ohw× Chw g ,b ∈ R Ohw . Then we convert every conv via Eq. 15 and add the resultant matrix ontoŴ. The biases of conv are simply replicated by hw times (because all the points on the same channel share a bias value) and added ontob. Finally, we obtain a single FC kernel and a single bias vector, which will be used to parameterize the inference-time FC3.</p><p>The BN in Global Perceptron is also removed because the removal is equivalent to applying an affine transformation before FC1, which can be absorbed by FC1 as two sequential MMULs can be merged into one. The formulas and code are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pure MLP and Ablation Studies</head><p>We first verify the effectiveness of RepMLP by testing a pure MLP model on CIFAR-10. More precisely, since an FC is equivalent to a 1 × 1 conv, by "pure MLP" we means no usage of conv kernels bigger than 1 × 1. We interleave RepMLP and regular FC (1 × 1 conv) to construct three stages and downsample by max pooling, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, and construct a ConvNet counterpart for comparison by replacing the RepMLPs with 3 × 3 conv. For the comparable FLOPs, the channels of the three stages are 16,32,64 for the pure MLP and 32,64,128 for the ConvNet, so the latter is named Wide ConvNet. We adopt the standard data augmentation <ref type="bibr" target="#b13">[14]</ref>: padding to 40 × 40, random cropping and left-right flipping. The models are trained with a batch size of 128 and a cosine learning rate annealing from 0.2 Then we conduct a series of ablation studies. A) We also report the FLOPs of the MLP before the conversion, which still contains conv and BN layers. The FLOPs is much higher though the extra parameters are marginal, which shows the significance of structural re-parameterization. B) "w/o Local" is a variant with no Local Perceptron, and the accuracy is 8.5% lower, which shows the significance of local prior. C) "w/o Global" removes FC1 and FC2 and directly feed the partition map into Local Perceptron and Partition Perceptron. D) "FC3 as conv9" replaces FC3 with a conv (K = 9 and p = 4, so that its receptive field is larger than FC3) followed by BN to compare the representational capacity of FC3 to a regular conv. Though the comparison is biased towards conv because its receptive field is larger, its accuracy is 3.5% lower, which validates that FC is more powerful than conv since a conv is a degraded FC. E) "RepMLP as conv9" directly replaces the RepMLP with a 9 × 9 conv and BN. Compared to D, its accuracy is lower as it has no Global Perceptrons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">RepMLP-ResNet for ImageNet Classification</head><p>We take ResNet-50 <ref type="bibr" target="#b13">[14]</ref> (the torchvision version <ref type="bibr" target="#b20">[21]</ref>) as the base architecture to evaluate RepMLP as a building block in traditional ConvNet. For the fair comparison, all the models are trained with identical settings in 100 epochs: global batch size of 256 on 8 GPUs, weight decay of 10 −4 , momentum of 0.9, and cosine learning rate annealing from 0.1 to 0. We use mixup <ref type="bibr" target="#b26">[27]</ref> and a data augmentation pipeline of Autoaugment <ref type="bibr" target="#b6">[7]</ref>, random cropping and flipping. All the models are evaluated with single central crop and the speed is tested on the same 1080Ti GPU with a batch size of 128 and measured in examples/second. For the fair comparison, the RepMLPs are converted and all the original conv-BN structures of every model are also converted into conv layers with bias for the speed tests.</p><p>As a common practice, we refer to the four residual stages of ResNet-50 as c2, c3, c4, c5, respectively. With  224 × 224 input, the output resolutions of the four stages are 56, 28, 14, 7, and the 3 × 3 conv layers in the four stages have C = O = 64, 128, 256, 512, respectively. To replace the big 3 × 3 conv layers with RepMLP, we use h = w = 7 and three conv branches in the Local Perceptron with K = 1, 3, 5. We perform r× channel reduction before RepMLP and r× channel expansion afterwards via 3 × 3 conv to further reduce the channels of RepMLP. The whole block is termed as RepMLP Bottleneck <ref type="figure" target="#fig_2">(Fig. 3)</ref>. For a specific stage, we replace all the stride-1 bottlenecks with RepMLP Bottlenecks and keep the original stride-2 (i.e., the first) bottleneck. We begin by using RepMLP in c4 only and varying the hyper-parameters r and g to test how they influence the accuracy, speed, and number of parameters <ref type="table">(Table.</ref> 2). Notably, with violent 8× reduction (so that the input and output channels of RepMLP is 256/8 = 32), RepMLP-Res50 has fewer parameters and run 10% faster than ResNet-50. The comparison between the first two rows suggest that the current groupwise 1 × 1 conv is inefficient, as the parameters increase by 59% but the speed decreases by only 0.7%. Further optimizations on groupwise 1 × 1 conv may make RepMLP more efficient. In the following experiments, we use r = 2 or 4 and g = 4 or 8 for the better trade-off.</p><p>We continue to test RepMLP in different stages. Specifically, we set g = 8 and r = 2, 2, 4, 4 for c2,c3,c4,c5, respectively, for the reasonable model sizes. <ref type="table">Table.</ref> 3 shows that replacing the original bottlenecks with RepMLP Bottlenecks causes very minor slowdown, and the accuracy is significantly improved. Using RepMLP only on c4 brings only 5M more parameters but 0.94% higher accuracy, and using RepMLP in c3 and c4 offers the best trade-off. It also suggests that RepMLP should be combined with traditional conv for the best performance, as using it in all the four stages delivers lower accuracy than c2+c3+c4 and c3+c4. We use RepMLP in c3+c4 in the following experiments. The comparisons to the larger traditional ConvNets with higher input resolution <ref type="table">(Table.</ref> 4) further justifies the effectiveness of RepMLP and offers some interesting discoveries. When trained and tested with 320 × 320 inputs, we use RepMLP with h = w = 10 and the Local Perceptron has four branches with K = 1, 3, 5, 7. We also vary the number of groups to generate three models with different sizes. For example, g8/16 means that g = 8 for c3 and 16 for c4. As two classic models for modeling the long-range dependencies, we construct the Non-local <ref type="bibr" target="#b24">[25]</ref> and GC <ref type="bibr" target="#b0">[1]</ref> counterparts following the instructions in the original papers, and the models are trained with the identical settings. We also present the well-known EfficientNet <ref type="bibr" target="#b21">[22]</ref> series as a strong baseline trained with the identical settings again. We have the following observations. 1) Compared to the traditional ConvNets with comparable numbers of parameters, the FLOPs of RepMLP-Res50 is much lower and the speed is faster. For example, compared to ResNet-101 with 224 × 224 inputs, RepMLP-Res50 has only 50% FLOPs and 4M fewer parameters, runs 50% faster, but their accuracies are the same. With 320 × 320 inputs, RepMLP-Res50 outperforms in accuracy, speed, and FLOPs by a large margin. Additionally, the improvements of ResNet-50 should not be simply attributed to the increased depth because it is still shallower than ResNet-101. 2) Increasing the parameters of RepMLPs causes very minor slowdown. From RepMLP-Res50-g8/16 to RepMLP-Res50-g4/8, the parameters increase by 47%, but the FLOPs increases by only 3.6% and the speed is lowered by only 2.2%. This property is particularly useful for high-throughput inference on large-scale servers, where the throughput and accuracy are our major concerns while the model size is not. 3) Compared to Nonlocal and GC, the speed of RepMLP-Res50 is almost the same, but the accuracy is around 1% higher. 4) Compared to EfficientNets, which are actually not efficient on GPU, RepMLP-Res50 outperforms in both the speed and accuracy.</p><p>We visualize the weights of FC3 in <ref type="figure" target="#fig_3">Fig. 4</ref>, where the sampled output point <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b5">6)</ref> is marked by a dashed square. , which is (64, 10, 10, 8, 10, 10), then sample the weights related to the first input channel and the (6,6) point (marked by a dashed square) on the first output channel, which isW 0,6,6,0,:,: . Then we take the absolute value, normalize by the minimum of the whole matrix, and take the natural logarithm for the better readability. A point with darker color indicates the FC considers the corresponding position on the input channel more related to the output point at <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b5">6)</ref>.</p><p>The original FC3 has no local prior as the marked point and the neighborhood have no larger values than the others. But after merging the Local Perceptron, the resultant FC3 kernel has larger values around the marked point, suggests that the model focuses more on the neighborhood, which is expected. Besides, the global capacity is not lost because some points (marked by red rectangles) outside the largest conv kernel (7 × 7 in this case, marked by a blue square) still have larger values than the points inside.</p><p>We also present another design of bottleneck (RepMLP Light Block) in the Appendix, which uses no 3 × 3 conv but only 1 × 1 for 8× channel reduction/expansion. Compared to the original ResNet-50, it achieves comparable accuracy (77.14% vs. 77.19%) with 30% lower FLOPs and 55% faster speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face Recognition</head><p>Unlike conv, FC is not translation-invariant, making RepMLP particularly effective for images with positional prior, i.e., human faces. The dataset we use for training is MS1M-V2, a large-scale face dataset with 5.8M images from 85k celebrities. It is a semi-automatic refined version of the MS-Celeb-1M dataset <ref type="bibr" target="#b11">[12]</ref> which consists of 1M photos from 100k identities and has many noisy images and wrong ID labels. We use MegaFace <ref type="bibr" target="#b15">[16]</ref> for evaluation, which includes 1M images of 60k identities as the gallery set and 100k images of 530 identities from FaceScrub as the probe set. It is also a refined version by manual clearing. We use 96 × 96 inputs for both training and evaluation. Apart from MobileFaceNet <ref type="bibr" target="#b2">[3]</ref> as a well-known baseline, which was originally designed for low-power devices, we also use a customized ResNet (referred to as FaceRes-Net in this paper) as a stronger baseline. Compared to a regular ResNet-50, the numbers of blocks in c2,c3,c4,c5 are reduced from 3,4,6,3 to 3,2,2,2, the widths are reduced from 256,512,1024,2048 to 128,256,512,1024, and the channels of 3 × 3 are increased from 64,128,256,512 to 128,256,512,1024. In other words, the 1 × 1 conv layers in residual blocks do not reduce or expand the channels. Because the input resolution is 96 × 96, the spatial sizes of c2,c3,c4,c5 are 24,12,6,3, respectively. For the RepMLP counterpart, we modify FaceResNet by replacing the stride-1 bottlenecks of c2,c3,c4 (i.e., the last two bottlenecks of c2 and the last blocks of c3,c4) by RepMLP Bottlenecks with h = w = 6, r = 2, g = 4.</p><p>For training, we use a batch size of 512, momentum of 0.9, AM-Softmax loss <ref type="bibr" target="#b23">[24]</ref>, and weight decay following <ref type="bibr" target="#b2">[3]</ref>. All the models are trained for 420k iterations with a learning rate beginning with 0.1 and divided by 10 at 252k, 364k and 406k iterations. For evaluation, we report the top-1 accuracy on MegaFace. <ref type="table" target="#tab_6">Table. 5</ref> shows that FaceRes-Net delivers higher accuracy than MobileFaceNet but runs slower, while RepMLP-FaceRes outperforms in both accuracy and speed. Compared to MobileFaceNet, RepMLP-FaceRes shows 4.91% higher accuracy and runs 8% faster (though it has 2.5× FLOPs), which is obviously a better fit for the high-power devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semantic Segmentation</head><p>Semantic segmentation is a representative task with translation invariance, as a car may occur at the left or right. We verify the generalization performance of ImageNetpretrained RepMLP-Res50 on Cityscapes <ref type="bibr" target="#b5">[6]</ref>, which contains 5K finely annotated images and 19 categories. We use the RepMLP-Res50-g4/8 and the original ResNet-50 pretrained with 320 × 320 on ImageNet as the backbones. For the better reproducibility, we simply adopt the official implementation and default configurations <ref type="bibr" target="#b27">[28]</ref> of PSPNet <ref type="bibr" target="#b28">[29]</ref> framework: poly learning rate policy with base of 0.01 and power of 0.9, weight decay of 10 −4 and a global batch size of 16 on 8 GPUs for 200 epochs. Following PSPNet-50, we use dilated conv in c5 of both models and c4 of the original ResNet-50. We do not use dilated conv in c4 of RepMLP-Res50-g4/8 because its receptive field is already large. Since the resolution of c3 and c4 becomes 90 × 90, the Global Perceptron will have 81 partitions of each channel hence more parameters in FC1 and FC2. We address this problem by reducing the output dimensions of the FC1 and the input dimensions of FC2 by 4× for c3 and 8× for c4. FC1 are FC2 are initialized randomly, and all the other parameters are inherited from the ImageNet-pretrained model. <ref type="table">Table.</ref> 6 shows that the PSPNet with RepMLP-Res50-g4/8 outperforms the Res-50 backbone by 2.21% in mIoU. Though it has more parameters, the FLOPs is lower and the speed is faster. Of note is that our PSPNet baseline is lower than the reported PSPNet-50 because the latter was customized for semantic segmentation (added two more layers before the max pooling) but ours is not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>An FC has stronger representational capacity than a conv, as the latter can be viewed as a sparse FC with shared parameters. However, an FC has no local prior, which makes it less favored for image recognition. In this paper, we have proposed RepMLP, which utilizes the global capacity and positional perception of FC and incorporates the local prior into FC by re-parameterizing convolutions into it via a simple and platform-agnostic algorithm. From the theoretical side, viewing conv as a degraded case of FC opens up a new perspective, which may deepen our understanding of the traditional ConvNets. It should not be left unmentioned that RepMLP is designed for the application scenarios where the major concerns are the inference throughput and accuracy, less concerning the number of parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Figure 1 :</head><label>31</label><figDesc>Sketch of a RepMLP. Here N, C, H, W are the batch size, number of input channels, height and width, h, w, g, p, O are the desired partition height and width, number of groups, padding, and output channels, respectively. The input feature map is split into a set of partitions, and the Global Perceptron adds the correlations among partitions onto each partition. Then the Local Perceptron captures the local patterns with several conv layers, and the Partition Perceptron models the long-range dependencies. This sketch assumes N = C = 1,H = W , H w = W w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The pure MLP model and the convolutional counterpart. The stage1 and stage3 are displayed in detail. Taking stage1 for example, 32 × 32 is the resolution, C = 16 is the number of output channels (except the last layer). Left: FC(32,16) is the kernel size, suggesting that this FC (equivalent to a 1 × 1 conv) projects 16 channels into 32 channels; all the RepMLPs are configured with g = 2, h = w = 8. Right: the convolutional counterpart uses 3 × 3 conv. A BN follows every conv and a ReLU follows every RepMLP or conv-BN sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Sketch of a RepMLP Bottleneck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>FC weights sampled from the FC3 of the first RepMLP in c3 of RepMLP-Res50-g8/8. The left is the original training-time FC3 and the right is the inference-time FC3 merged with Local Perceptron. Specifically, we reshape the kernel of FC3 intoW(O, h, w, C g , h, w)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>+ conv + BN FC1</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>average pool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ReLU</cell><cell>FC2</cell><cell>average pool</cell><cell>FC1</cell><cell>ReLU</cell><cell>FC2</cell></row><row><cell>( , , , )</cell><cell cols="3">( , , ℎ</cell><cell>, )</cell><cell>( ℎ</cell><cell>, ℎ</cell><cell>)</cell><cell>( ℎ</cell><cell>, ℎ</cell><cell>)</cell><cell>( , , , )</cell></row><row><cell>( ℎ</cell><cell cols="2">, , ℎ, )</cell><cell cols="2">Partition Map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">( ℎ</cell><cell>, , 1,1)</cell><cell>Global Perceptron</cell><cell>+</cell></row><row><cell></cell><cell>( ℎ</cell><cell cols="3">, ℎ )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Structural</cell></row><row><cell>FC3</cell><cell>( ℎ ,</cell><cell cols="2">ℎ )</cell><cell>( , , 1,1)</cell><cell cols="3">( , , 3,3)</cell><cell cols="2">( , , 5,5)</cell><cell>Re-param</cell><cell>FC3</cell></row><row><cell></cell><cell>( ℎ</cell><cell cols="3">, ℎ )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Parameterized layers</cell></row><row><cell></cell><cell>BN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(FC, conv, BN)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN</cell><cell></cell><cell>BN</cell><cell></cell><cell></cell><cell>BN</cell></row><row><cell cols="5">Partition Perceptron</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Local Perceptron</cell><cell>Feature maps</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( , , , )</cell><cell>Features as vectors</cell></row><row><cell>( , , , )</cell><cell></cell><cell></cell><cell></cell><cell cols="6">Training-time RepMLP</cell><cell>Inference-time RepMLP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Top-1 accuracy, FLOPs and parameters of pure MLP and ConvNet on CIFAR-10.</figDesc><table><row><cell>Model</cell><cell cols="3">Acc FLOPs (M) Params (M)</cell></row><row><cell>Pure MLP</cell><cell>91.11</cell><cell>52.8</cell><cell>22.41</cell></row><row><cell cols="2">A) before conversion 91.11</cell><cell>118.9</cell><cell>22.91</cell></row><row><cell>B) w/o Local</cell><cell>82.64</cell><cell>52.8</cell><cell>22.41</cell></row><row><cell>C) w/o Global</cell><cell>89.61</cell><cell>52.5</cell><cell>22.08</cell></row><row><cell>D) FC3 as conv9</cell><cell>87.64</cell><cell>66.2</cell><cell>0.81</cell></row><row><cell cols="2">E) RepMLP as conv9 87.29</cell><cell>65.8</cell><cell>0.48</cell></row><row><cell>Wide ConvNet</cell><cell>91.99</cell><cell>65.1</cell><cell>0.50</cell></row><row><cell cols="4">to 0 in 100 epochs. As shown in Table. 1, the pure MLP</cell></row><row><cell cols="4">model reaches 91.11% accuracy with only 52.8M FLOPs.</cell></row><row><cell cols="4">Not surprisingly, the pure MLP model does not outperform</cell></row><row><cell cols="4">the Wide ConvNet, motivating us to combine RepMLP and</cell></row><row><cell>traditional ConvNet.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results with 224 × 224 input and different r, g in c4 only. The speed is in examples/second.</figDesc><table><row><cell cols="4">r g Top-1 acc Speed Params (M)</cell></row><row><cell>RepMLP-Res50 4 8</cell><cell>78.13</cell><cell>671</cell><cell>30.87</cell></row><row><cell>RepMLP-Res50 4 2</cell><cell>78.22</cell><cell>666</cell><cell>49.31</cell></row><row><cell>RepMLP-Res50 8 8</cell><cell>77.79</cell><cell>759</cell><cell>25.02</cell></row><row><cell>RepMLP-Res50 2 8</cell><cell>78.60</cell><cell>639</cell><cell>52.77</cell></row><row><cell>Original Res50 --</cell><cell>77.19</cell><cell>689</cell><cell>25.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Using RepMLP in different stages of ResNet-50 with 224 × 224 input. The speed is in examples/second.</figDesc><table><row><cell></cell><cell cols="3">c2 c3 c4 c5 Top-1 acc Speed Params (M)</cell></row><row><cell>RepMLP-Res50</cell><cell>78.32</cell><cell>574</cell><cell>74.46</cell></row><row><cell>RepMLP-Res50</cell><cell>78.60</cell><cell>575</cell><cell>66.97</cell></row><row><cell>RepMLP-Res50</cell><cell>78.30</cell><cell>632</cell><cell>48.35</cell></row><row><cell>RepMLP-Res50</cell><cell>78.55</cell><cell>636</cell><cell>40.87</cell></row><row><cell>RepMLP-Res50</cell><cell>78.09</cell><cell>644</cell><cell>35.52</cell></row><row><cell>RepMLP-Res50</cell><cell>78.13</cell><cell>671</cell><cell>30.87</cell></row><row><cell>Original Res50</cell><cell>77.19</cell><cell>689</cell><cell>25.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with traditional ConvNets on ImageNet all trained with the identical settings. The speed is tested on the same 1080Ti with a batch size of 128. The input resolutions of the EfficientNets are different because they are fixed as the structural hyper-parameters.</figDesc><table><row><cell>Model</cell><cell cols="5">Input resolution Top-1 acc Speed (examples/s) FLOPs (M) Params (M)</cell></row><row><cell>ResNet-50</cell><cell>224</cell><cell>77.19</cell><cell>689</cell><cell>4089</cell><cell>25.53</cell></row><row><cell>ResNet-101</cell><cell>224</cell><cell>78.55</cell><cell>421</cell><cell>7801</cell><cell>44.49</cell></row><row><cell>RepMLP-Res50</cell><cell>224</cell><cell>78.55</cell><cell>636</cell><cell>3890</cell><cell>40.87</cell></row><row><cell>ResNet-50</cell><cell>320</cell><cell>78.03</cell><cell>344</cell><cell>8343</cell><cell>25.53</cell></row><row><cell>ResNet-101</cell><cell>320</cell><cell>79.40</cell><cell>213</cell><cell>15919</cell><cell>44.49</cell></row><row><cell>RepMLP-Res50-g8/16</cell><cell>320</cell><cell>79.76</cell><cell>312</cell><cell>8057</cell><cell>59.22</cell></row><row><cell>RepMLP-Res50-g8/8</cell><cell>320</cell><cell>79.84</cell><cell>311</cell><cell>8108</cell><cell>72.02</cell></row><row><cell>RepMLP-Res50-g4/8</cell><cell>320</cell><cell>80.07</cell><cell>305</cell><cell>8354</cell><cell>87.38</cell></row><row><cell>NL-Res50</cell><cell>320</cell><cell>78.95</cell><cell>316</cell><cell>9182</cell><cell>27.63</cell></row><row><cell>GC-Res50</cell><cell>320</cell><cell>78.93</cell><cell>312</cell><cell>8351</cell><cell>28.05</cell></row><row><cell>EfficientNet-B1</cell><cell>240</cell><cell>75.76</cell><cell>512</cell><cell>686</cell><cell>7.76</cell></row><row><cell>EfficientNet-B2</cell><cell>260</cell><cell>76.46</cell><cell>396</cell><cell>993</cell><cell>9.07</cell></row><row><cell>EfficientNet-B3</cell><cell>300</cell><cell>78.17</cell><cell>228</cell><cell>1827</cell><cell>12.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results of face recognition on MS1M-V2 and MegaFace. The speed (examples/second) is tested with a batch size of 512 and input 96×96 on the same 1080Ti GPU .</figDesc><table><row><cell>Model</cell><cell cols="3">Acc Speed FLOPs (M) Params (M)</cell></row><row><cell>MobileFaceNet</cell><cell>90.99 5002</cell><cell>162</cell><cell>0.98</cell></row><row><cell>FaceResNet</cell><cell>92.97 3811</cell><cell>1050</cell><cell>40.35</cell></row><row><cell cols="2">RepMLP-FaceRes 95.90 5425</cell><cell>406</cell><cell>28.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Semantic segmentation on Cityscapes [6] tested on the validation subset. The speed (examples/second) is tested with a batch size of 16 and input 713×713 on the same 1080Ti GPU.</figDesc><table><row><cell>Backbone</cell><cell cols="3">mIoU Speed FLOPs (M) Params (M)</cell></row><row><cell cols="2">RepMLP-Res50 76.58 10.43</cell><cell>342,696</cell><cell>175.41</cell></row><row><cell>ResNet-50</cell><cell>74.27 10.22</cell><cell>350,004</cell><cell>46.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: RepMLP-ResNet for High Speed</head><p>The RepMLP Bottleneck presented in the paper is designed to improve the accuracy. Here we present another means of using RepMLP in ResNet for the higher speed. Specifically, we build a RepMLP Light Block <ref type="figure">(Fig. 5</ref>) with no 3 × 3 conv but drastic 8× channel reduction/expansion via 1 × 1 conv before and after RepMLP. Same as the 78.55%-accuracy RepMLP-ResNet50 reported in the paper, we use h = w = 7, g = 8 and three conv branches in the Local Perceptron with K = 1, 3, 5. The speed is tested in the same way as all the models reported in the paper. Table. 7 shows that the ResNet with RepMLP Light Block achieves almost the same accuracy as the original ResNet-50 with 30% lower FLOPs and 55% faster speed.</p><p>Of note is that RepMLP is a building block that can be combined with numerous other structures in various ways. We only present two means for using RepMLP in ResNet, which may not be the optimal. We will make the code and models publicly available to encourage further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Converting Groupwise Conv into FC</head><p>The groupwise case of converting conv into FC is a bit more complicated, which can be derived by first splitting the input into g parallel groups and then converting every group separately. The PyTorch code is shown in Alg. 1 and the submitted repmlp.py contains an executable example to verify the equivalence. It is easy to verify that with g = 1 the code exactly implements Eq. 15 in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Absorbing BN into FC1</head><p>The BN in Global Perceptron applies a linear scaling and a bias adding to the input. After the matrix multiplication by the FC1 kernel, the added bias is projected and then added onto the bias of FC1. Therefore, the removal of this BN can be offset by scaling the kernel of FC1 and changing the bias of FC1. The code is shown in Alg. 2 and the submitted repmlp.py contains an executable example to verify the equivalence.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High performance convolutional neural networks for document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidd</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="428" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mec: memory-efficient convolution for deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Brand</surname></persName>
		</author>
		<idno>PMLR, 2017. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1911" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03697</idno>
		<title level="m">Repvgg: Making vgg-style convnets great again</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4013" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5851</idno>
		<title level="m">Fast training of convolutional networks through ffts</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Torchvision Official Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Official pspnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<idno>2020. 8</idno>
		<ptr target="https://github.com/hszhao/semseg" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ResNet-50 with different blocks in c3 and c4. The speed is in examples/second</title>
	</analytic>
	<monogr>
		<title level="m">Block Top-1 acc Speed FLOPs (M) Params (M)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
