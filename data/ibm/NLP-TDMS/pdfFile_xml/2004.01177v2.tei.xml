<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tracking Objects as Points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tracking Objects as Points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-object tracking</term>
					<term>Conditioned detection</term>
					<term>3D object tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking has traditionally been the art of following interest points through space and time. This changed with the rise of powerful deep networks. Nowadays, tracking is dominated by pipelines that perform object detection followed by temporal association, also known as tracking-by-detection. We present a simultaneous detection and tracking algorithm that is simpler, faster, and more accurate than the state of the art. Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That's it. CenterTrack is simple, online (no peeking into the future), and real-time. It achieves 67.8% MOTA on the MOT17 challenge at 22 FPS and 89.4% MOTA on the KITTI tracking benchmark at 15 FPS, setting a new state of the art on both datasets. CenterTrack is easily extended to monocular 3D tracking by regressing additional 3D attributes. Using monocular video input, it achieves 28.3% AMOTA@0.2 on the newly released nuScenes 3D tracking benchmark, substantially outperforming the monocular baseline on this benchmark while running at 28 FPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In early computer vision, tracking was commonly phrased as following interest points through space and time <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>. Early trackers were simple, fast, and reasonably robust. However, they were liable to fail in the absence of strong low-level cues such as corners and intensity peaks. With the advent of high-performing object detection models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>, a powerful alternative emerged: tracking-by-detection (or more precisely, tracking-after-detection) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>. These models rely on a given accurate recognition to identify objects and then link them up through time in a separate stage. Tracking-bydetection leverages the power of deep-learning-based object detectors and is currently the dominant tracking paradigm. Yet the best-performing object trackers are not without drawbacks. Many rely on slow and complex association strategies to link detected boxes through time <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. Recent work on simultaneous detection and tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> has made progress in alleviating some of this complexity. Here, we show how combining ideas from point-based tracking and simultaneous detection and tracking further simplifies tracking.</p><p>We present a point-based framework for joint detection and tracking, referred to as CenterTrack. Each object is represented by a single point at the center of its bounding box. This center point is then tracked through time <ref type="figure">(Figure 1</ref>). Specifically, we adopt the arXiv:2004.01177v2 [cs.CV] 21 Aug 2020 <ref type="figure">Fig. 1</ref>: We track objects by tracking their centers. We learn a 2D offset between two adjacent frames and associate them based on center distance. recent CenterNet detector to localize object centers <ref type="bibr" target="#b55">[56]</ref>. We condition the detector on two consecutive frames, as well as a heatmap of prior tracklets, represented as points. We train the detector to also output an offset vector from the current object center to its center in the previous frame. We learn this offset as an attribute of the center point at little additional computational cost. A greedy matching, based solely on the distance between this predicted offset and the detected center point in the previous frame, suffices for object association. The tracker is end-to-end trainable and differentiable.</p><p>Tracking objects as points simplifies two key components of the tracking pipeline. First, it simplifies tracking-conditioned detection. If each object in past frames is represented by a single point, a constellation of objects can be represented by a heatmap of points <ref type="bibr" target="#b3">[4]</ref>. Our tracking-conditioned detector directly ingests this heatmap and reasons about all objects jointly when associating them across frames. Second, point-based tracking simplifies object association across time. A simple displacement prediction, akin to sparse optical flow, allows objects in different frames to be linked. This displacement prediction is conditioned on prior detections. It learns to jointly detect objects in the current frame and associate them to prior detections.</p><p>While the overall idea is simple, subtle details matter in making this work. Tracked objects in consecutive frames are highly correlated. With the previous-frame heatmap given as input, CenterTrack could easily learn to repeat the predictions from the preceding frame, and thus refuse to track without incurring a large training error. We prevent this through an aggressive data-augmentation scheme during training. In fact, our data augmentation is aggressive enough for the model to learn to track objects from static images. That is, CenterTrack can be successfully trained on static image datasets (with "hallucinated" motion), with no real video input.</p><p>CenterTrack is purely local. It only associates objects in adjacent frames, without reinitializing lost long-range tracks. It trades the ability to reconnect long-range tracks for simplicity, speed, and high accuracy in the local regime. Our experiments indicate that this trade-off is well worth it. CenterTrack outperforms complex trackingby-detection strategies on the MOT <ref type="bibr" target="#b27">[28]</ref> and KITTI <ref type="bibr" target="#b11">[12]</ref> tracking benchmarks. We further apply the approach to monocular 3D object tracking on the nuScenes dataset <ref type="bibr" target="#b2">[3]</ref>. Our monocular tracker achieves 28.3% AMOTA@0.2, outperforming the monocular baseline by a factor of 3, while running at 22 FPS. It can be trained on labelled video sequences, if available, or on static images with data augmentation. Code is available at https://github.com/xingyizhou/CenterTrack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Tracking-by-detection. Most modern trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">58]</ref> follow the tracking-by-detection paradigm. An off-the-shelf object detector <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51]</ref> first finds all objects in each individual frame. Tracking is then a problem of bounding box association. SORT <ref type="bibr" target="#b1">[2]</ref> tracks bounding boxes using a Kalman filter and associates each bounding box with its highest overlapping detection in the current frame using bipartite matching. DeepSORT <ref type="bibr" target="#b46">[47]</ref> augments the overlap-based association cost in SORT with appearance features from a deep network. More recent approaches focus on increasing the robustness of object association. Tang et al. <ref type="bibr" target="#b40">[41]</ref> leverage person-reidentification features and human pose features. Xu et al. <ref type="bibr" target="#b49">[50]</ref> take advantage of the spatial locations over time. BeyondPixel <ref type="bibr" target="#b34">[35]</ref> uses additional 3D shape information to track vehicles.</p><p>These methods have two drawbacks. First, the data association discards image appearance features <ref type="bibr" target="#b1">[2]</ref> or requires a computationally expensive feature extractor <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>. Second, detection is separated from tracking. In our approach, association is almost free. Association is learned jointly with detection. Also, our detector takes the previous tracking results as an input, and can learn to recover missing or occluded objects from this additional cue. Joint detection and tracking. A recent trend in multi-object tracking is to convert existing detectors into trackers and combine both tasks in the same framework. Feichtenhofer et al. <ref type="bibr" target="#b7">[8]</ref> use a siamese network with the current and past frame as input and predict inter-frame offsets between bounding boxes. Integrated detection <ref type="bibr" target="#b54">[55]</ref> uses tracked bounding boxes as additional region proposals to enhance detection, followed by bipartite-matching-based bounding-box association. Tracktor <ref type="bibr" target="#b0">[1]</ref> removes the box association by directly propagating identities of region proposals using bounding box regression. In video object detection, Kang et al. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> feed stacked consecutive frames into the network and do detection for a whole video segment. And Zhu et al. <ref type="bibr" target="#b58">[59]</ref> use flow to warp intermediate features from previous frames to accelerate inference.</p><p>Our method belongs to this category. The difference is that all of these works adopt the FasterRCNN framework <ref type="bibr" target="#b30">[31]</ref>, where the tracked boxes are used as region proposals. This assumes that bounding boxes have a large overlap between frames, which is not true in low-framerate regimes. As a consequence, Tracktor <ref type="bibr" target="#b0">[1]</ref> requires a motion model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> for low-framerate sequences. Our approach instead provides the tracked predictions as an additional point-based heatmap input to the network. The network is then able to reason about and match objects anywhere in its receptive field even if the boxes have no overlap at all. Motion prediction. Motion prediction is another important component in a tracking system. Early approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47]</ref> used Kalman filters to model object velocities. Held et al. <ref type="bibr" target="#b12">[13]</ref> use a regression network to predict four scalars for bounding box offset between frames for single-object tracking. Xiao et al. <ref type="bibr" target="#b48">[49]</ref> utilize an optical flow estimation network to update joint locations in human pose tracking. Voigtlaender et al. <ref type="bibr" target="#b44">[45]</ref> learn a high-dimensional embedding vector for object identities for simultaneous object tracking and segmentation. Our center offset is analogous to sparse optical flow, but is learned together with the detection network and does not require dense supervision. Heatmap-conditioned keypoint estimation. Feeding the model predictions as an additional input to a model works across a wide range of vision tasks <ref type="bibr" target="#b43">[44]</ref>, especially for keypoint estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>. Auto-context <ref type="bibr" target="#b43">[44]</ref> feeds the mask prediction back into the network. Iterative-Error-Feedback (IEF) <ref type="bibr" target="#b3">[4]</ref> takes another step by rendering predicted keypoint coordinates into heatmaps. PoseFix <ref type="bibr" target="#b28">[29]</ref> generates heatmaps that simulate test errors for human pose refinement.</p><p>Our tracking-conditioned detection framework is inspired by these works. A rendered heatmap of prior keypoints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref> is especially appealing in tracking for two reasons. First, the information in the previous frame is freely available and does not slow down the detector. Second, conditional tracking can reason about occluded objects that may no longer be visible in the current frame. The tracker can simply learn to keep those detections from the prior frame around. 3D object detection and tracking. 3D trackers replace the object detection component in standard tracking systems with 3D detection from monocular images <ref type="bibr" target="#b29">[30]</ref> or 3D point clouds <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b56">57]</ref>. Tracking then uses an off-the-shelf identity association model. For example, 3DT <ref type="bibr" target="#b13">[14]</ref> detects 2D bounding boxes, estimates 3D motion, and uses depth and order cues for matching. AB3D <ref type="bibr" target="#b45">[46]</ref> achieves state-of-the-art performance by combining a Kalman filter with accurate 3D detections <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Our method, CenterTrack, builds on the CenterNet detector <ref type="bibr" target="#b55">[56]</ref>. CenterNet takes a single image I ∈ R W ×H×3 as input and produces a set of detections {(p i , s i )} N −1 i=0 for each class c ∈ {0, . . . , C − 1}. CenterNet identifies each object through its center point p ∈ R 2 and then regresses to a height and width s ∈ R 2 of the object's bounding box. Specifically, it produces a low-resolution heatmapŶ ∈ [0, 1] W R × H R ×C and a size map S ∈ R W R × H R ×2 with a downsampling factor R = 4. Each local maximump ∈ R 2 (also called peak, whose response is the strongest in a 3 × 3 neighborhood) in the heatmap Y corresponds to a center of a detected object with confidenceŵ =Ŷp and object sizê s =Ŝp.</p><p>Given an image with a set of annotated objects {p 0 , p 1 , . . .}, CenterNet uses a training objective based on the focal loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>:</p><formula xml:id="formula_0">L k = 1 N xyc    (1 −Ŷ xyc ) α log(Ŷ xyc ) if Y xyc = 1 (1 − Y xyc ) β (Ŷ xyc ) α log(1 −Ŷ xyc ) otherwise ,<label>(1)</label></formula><p>where Y ∈ [0, 1] W R × H R ×C is a ground-truth heatmap corresponding to the annotated objects. N is the number of objects, and α = 2 and β = 4 are hyperparameters of the focal loss. For each center p of class c, we render a Gaussian-shaped peak into Y :,:,c using a rendering function Y = R({p 0 , p 1 , . . .}) <ref type="bibr" target="#b21">[22]</ref>. Formally, the rendering function at position q ∈ R 2 is defined as</p><formula xml:id="formula_1">R q ({p 0 , p 1 , . . .}) = max i exp − (p i − q) 2 2σ 2 i .</formula><p>The Gaussian kernel σ i is a function of the object size <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure">Fig. 2</ref>: Illustration of our framework. The network takes the current frame, the previous frame, and a heatmap rendered from tracked object centers as inputs, and produces a center detection heatmap for the current frame, the bounding box size map, and an offset map. At test time, object sizes and offsets are extracted from peaks in the heatmap.</p><formula xml:id="formula_2">Image I (t) Image I (t−1) Tracks T (t−1) CenterTrack DetectionsŶ (t) SizeŜ (t) OffsetÔ (t)</formula><p>The size prediction is only supervised at the center locations. Let s i be the bounding box size of the i-th object at location p i . Size prediction is learned by regression</p><formula xml:id="formula_3">L size = 1 N N i=1 |Ŝ pi − s i |.<label>(2)</label></formula><p>CenterNet further regresses to a refined center local location using an analogous L1 loss L loc . The overall loss of CenterNet is a weighted sum of all three loss terms: focal loss, size, and local location regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tracking objects as points</head><p>We approach tracking from a local perspective. When an object leaves the frame or is occluded and reappears, it is assigned a new identity. We thus treat tracking as the problem of propagating detection identities across consecutive frames, without reestablishing associations across temporal gaps.</p><p>At time t, we are given an image of the current frame I (t) ∈ R W ×H×3 and the previous frame I (t−1) ∈ R W ×H×3 , as well as the tracked objects in the previous frame</p><formula xml:id="formula_4">T (t−1) = {b (t−1) 0 , b (t−1) 1</formula><p>, . . .} i . Each object b = (p, s, w, id) is described by its center location p ∈ R 2 , size s ∈ R 2 , detection confidence w ∈ [0, 1], and unique identity id ∈ I. Our aim is to detect and track objects</p><formula xml:id="formula_5">T (t) = {b (t) 0 , b (t) 1</formula><p>, . . .} in the current frame t, and assign objects that appear in both frames a consistent id.</p><p>There are two main challenges here. The first is finding all objects in every frameincluding occluded ones. The second challenge is associating these objects through time. We address both via a single deep network, trained end-to-end. Section 4.1 describes a tracking-conditioned detector that leverages tracked detections from the previous frame to improve detection in the current frame. Section 4.2 then presents a simple offset prediction scheme that is able to link detections through time. Finally, Sections 4.3 and 4.4 show how to train this detector from video or static image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tracking-conditioned detection</head><p>As an object detector, CenterNet already infers most of the required information for tracking: object locationsp, their sizeŝ =Ŝp, and a confidence measureŵ =Ŷp.</p><p>However, it is unable to find objects that are not directly visible in the current frame, and the detected objects may not be temporally coherent. One natural way to increase temporal coherence is to provide the detector with additional image inputs from past frames. In CenterTrack, we provide the detection network with two frames as input: the current frame I (t) and the prior frame I (t−1) . This allows the network to estimate the change in the scene and potentially recover occluded objects at time t from visual evidence at time t − 1.</p><p>CenterTrack also takes prior detections {p</p><formula xml:id="formula_6">(t−1) 0 , p (t−1) 1</formula><p>, . . .} as additional input. How should these detections be represented in a form that is easily provided to a network? The point-based nature of our tracklets is helpful here. Since each detected object is represented by a single point, we can conveniently render all detections in a class-agnostic single-channel heatmap</p><formula xml:id="formula_7">H (t−1) = R({p (t−1) 0 , p (t−1) 1</formula><p>, . . .}), using the same Gaussian render function as in the training of point-based detectors. To reduce the propagation of false positive detections, we only render objects with a confidence score greater than a threshold τ . The architecture of CenterTrack is essentially identical to CenterNet, with four additional input channels. (See <ref type="figure">Figure 2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.)</head><p>Tracking-conditioned detection provides a temporally coherent set of detected objects. However, it does not link these detections across time. In the next section, we show how to add one additional output to point-based detection to track objects through space and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Association through offsets</head><p>To associate detections through time, CenterTrack predicts a 2D displacement as two additional output channelsD (t) ∈ R W R × H R ×2 . For each detected object at locationp (t) , the displacementd (t) =D (t) p (t) captures the difference in location of the object in the current framep (t) and the previous framep (t−1) :d (t) =p (t) −p (t−1) . We learn this displacement using the same regression objective as size or location refinement:</p><formula xml:id="formula_8">L of f = 1 N N i=1 D p (t) i − (p (t−1) i − p (t) i ) ,<label>(3)</label></formula><p>where p</p><formula xml:id="formula_9">(t−1) i and p (t)</formula><p>i are tracked ground-truth objects. <ref type="figure">Figure 2</ref> shows an example of this offset prediction.</p><p>With a sufficiently good offset prediction, a simple greedy matching algorithm can associate objects across time. For each detection at positionp, we greedily associate it with the closest unmatched prior detection at positionp −Dp, in descending order of confidenceŵ. If there is no unmatched prior detection within a radius κ, we spawn a new tracklet. We define κ as the geometric mean of the width and height of the predicted bounding box for each tracklet. A precise description of this greedy matching algorithm is provided in supplementary material. The simplicity of this greedy matching algorithm again highlights the advantages of tracking objects as points. A simple displacement prediction is sufficient to link objects across time. There is no need for a complicated distance metric or graph matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training on video data</head><p>CenterTrack is first and foremost an object detector, and trained as such. The architectural changed from CenterNet to CenterTrack are minor: four additional input channels and two output channels. This allows us to fine-tune CenterTrack directly from a pretrained CenterNet detector <ref type="bibr" target="#b55">[56]</ref>. We copy all weights related to the current detection pipeline. All weights corresponding to additional inputs or outputs are initialized randomly. We follow the CenterNet training protocol and train all predictions as multi-task learning. We use the same training objective with the addition of offset regression L of f .</p><p>The main challenge in training CenterTrack comes in producing a realistic tracklet heatmap H (t−1) . At inference time, this tracklet heatmap can contain an arbitrary number of missing tracklets, wrongly localized objects, or even false positives. These errors are not present in ground-truth tracklets {p</p><formula xml:id="formula_10">(t−1) 0 , p (t−1) 1 , . . .} provided during training.</formula><p>We instead simulate this test-time error during training. Specifically, we simulate three types of error. First, we locally jitter each tracklet p (t−1) from the prior frame by adding Gaussian noise to each center. That is, we render</p><formula xml:id="formula_11">p i = (x i + r × λ jt × w i , y i + r × λ jt × h i ),</formula><p>where r is sampled from a Gaussian distribution. We use λ jt = 0.05 in all experiments. Second, we randomly add false positives near ground-truth object locations by rendering a spurious noisy peak p i with probability λ f p . Third, we simulate false negatives by randomly removing detections with probability λ f n . λ f p and λ f n are set according to the statistics of our baseline model. These three augmentations are sufficient to train a robust tracking-conditioned object detector.</p><p>In practice, I (t−1) does not need to be the immediately preceding frame from time t − 1. It can be a different frame from the same video sequence. In our experiments, we randomly sample frames near t to avoid overfitting to the framerate. Specifically, we sample from all frames k where |k − t| &lt; M f , where M f = 3 is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training on static image data</head><p>Without labeled video data, CenterTrack does not have access to a prior frame I (t−1) or tracked detections {p</p><formula xml:id="formula_12">(t−1) 0 , p (t−1) 1</formula><p>, . . .}. However, we can simulate tracking on standard detection benchmarks, given only single images I (t) and detections {p</p><formula xml:id="formula_13">(t) 0 , p (t) 1 , . . .}.</formula><p>The idea is simple: we simulate the previous frame by randomly scaling and translating the current frame. As our experiments will demonstrate, this is surprisingly effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">End-to-end 3D object tracking</head><p>To perform monocular 3D tracking, we adopt the monocular 3D detection form of Cen-terNet <ref type="bibr" target="#b55">[56]</ref>. Specifically, we train output heads to predict object depth, rotation (encoded as an 8-dimensional vector <ref type="bibr" target="#b13">[14]</ref>), and 3D extent. Since the projection of the center of the 3D bounding box may not align with the center of the object's 2D bounding box (due to perspective projection), we also predict a 2D-to-3D center offset. Further details are provided in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate 2D multi-object tracking on the MOT17 <ref type="bibr" target="#b27">[28]</ref> and KITTI <ref type="bibr" target="#b11">[12]</ref> tracking benchmarks. We also evaluate monocular 3D tracking on the nuScenes dataset <ref type="bibr" target="#b2">[3]</ref>. Experiments on MOT16 can be found in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and evaluation metrics</head><p>MOT. MOT17 contains 7 training sequences and 7 test sequences <ref type="bibr" target="#b27">[28]</ref>, The videos were captured by stationary cameras mounted in high-density scenes with heavy occlusion. Only pedestrians are annotated and evaluated. The video framerate is 25-30 FPS. The MOT dataset does not provide an official validation split. For ablation experiments, we split each training sequence into two halves, and use the first half frames for training and the second for validation. Our main results are reported on the test set. KITTI. The KITTI tracking benchmark consists of 21 training sequences and 29 test sequences <ref type="bibr" target="#b11">[12]</ref>. They are collected by a camera mounted on a car moving through traffic. The dataset provides 2D bounding box annotations for cars, pedestrians, and cyclists, but only cars are evaluated. Videos are captured at 10 FPS and contain large inter-frame motions. KITTI does not provide detections, and all entries use private detection. We again split all training sequences into halves for training and validation. nuScenes. nuScenes is a newly released large-scale driving dataset with 7 object classes annotated for tracking <ref type="bibr" target="#b2">[3]</ref>. It contains 700 training sequences, 150 validation sequences, and 150 test sequences. Each sequence contains roughly 40 frames at 2 FPS with 6 slightly overlapping images in a panoramic 360 • view, resulting in 168k training, 36k validation, and 36k test images. The videos are sampled at 12 FPS, but frames are only annotated and evaluated at 2 FPS. All baselines and CenterTrack only use keyframes for training and evaluation. Due to the low framerate, the inter-frame motion is significant. Evaluation metrics. We use the official evaluation metrics in each dataset. The common metric is multi-object tracking accuracy <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>:</p><formula xml:id="formula_14">M OT A = 1− t (F Pt+F Nt+IDSWt) t GTt ,</formula><p>where GT t , F P t , F N t , and IDSW t are the number of ground-truth bounding boxes, false positives, false negatives, and identity switches in frame t, respectively. MOTA does not rank tracklets according to confidence and is sensitive to the task-dependent output threshold θ <ref type="bibr" target="#b45">[46]</ref>. The thresholds we use are listed in Section 5.2. The interplay between output threshold and true positive criteria matters. For 2D tracking <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>, &gt; 0.5 bounding box IoU is a the true positive. For 3D tracking <ref type="bibr" target="#b2">[3]</ref>, bounding box center distance &lt; 2m on the ground plane is the criterion for a true positive. When objects are successfully detected, but not tracked, they are identified as an identity switch (IDSW). The IDF1 metric measures the minimal cost change from predicted ids to the correct ids. In our ablation studies, we report false positve rate (FP) t F Pt t GTt , false negative rate (FN) t F Nt t GTt , and identity switches (IDSW) t IDSWt t GTt separately. In comparisons with other methods, we report the absolute numbers following the dataset convention <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>. We also report the Most Tracked ratio (MT) for the ratio of most tracked (&gt; 80% time) objects and Most Lost ratio (ML) for most lost (&lt; 20% time) objects <ref type="bibr" target="#b39">[40]</ref>.</p><p>nuScenes adopts a more robust metric, AMOTA, which is a weighted average of MOTA across different output thresholds. Specifically,</p><formula xml:id="formula_15">AM OT A = 1 n − 1 r∈{ 1 n−1 , 2 n−1 ,··· ,1} M OT Ar M OT Ar = max(0, 1 − α IDSWr + F Pr + F Nr − (1 − r) × P r × P )</formula><p>where r is a fixed recall threshold, P = t GT t is the total number of annotated objects among all frames, and F P r = t F P r,t is the total number of false positive samples only considering the top confident samples that achieve the recall threshold r. The hyperparameters n = 40 and α = 0.2 (AMOTA@0.2), or α = 1 (AMOTA@1) are set by the benchmark organizers. The overall AMOTA is the average AMOTA among all 7 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation details</head><p>Our implementation is based on CenterNet <ref type="bibr" target="#b55">[56]</ref>. We use DLA <ref type="bibr" target="#b52">[53]</ref> as the network backbone, optimized with Adam <ref type="bibr" target="#b19">[20]</ref> with learning rate 1.25e − 4 and batchsize 32. Data augmentations include random horizontal flipping, random resized cropping, and color jittering. For all experiments, we train the networks for 70 epochs. The learning rate is dropped by a factor of 10 at the 60th epoch. We test the runtime on a machine with an Intel Core i7-8086K CPU and a Titan Xp GPU. The runtimes depend on the number of objects for rendering and the input resolution in each dataset.</p><p>The MOT dataset <ref type="bibr" target="#b27">[28]</ref> annotates each pedestrian as an amodal bounding box. That is, the bounding box always covers the whole body even when part of the object is out of the frame. In contrast, CenterNet <ref type="bibr" target="#b55">[56]</ref> requires the center of each inferred bounding box to be within the frame. To handle this, we separately predict the visible and amodal bounding boxes <ref type="bibr" target="#b41">[42]</ref>. Further details on this can be found in the supplement. We follow prior works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref> to pretrain on external data. We train our network on the CrowdHuman <ref type="bibr" target="#b33">[34]</ref> dataset, using the static image training described in Section 4.4. Details on the CrowdHuman dataset and ablations of pretraining are in the supplement.</p><p>The default input resolution for MOT images is 1920 × 1080. We resize and pad the images to 960 × 544. We use random false positive ratio λ f p = 0.1 and random false negative ratio λ f n = 0.4. We only output tracklets that have a confidence of θ = 0.4 or higher, and set the heatmap rendering threshold to τ = 0.5. A controlled study of these hyperparameters is in the supplement.</p><p>For KITTI <ref type="bibr" target="#b11">[12]</ref>, we keep the original input resolution 1280 × 384 in training and testing. The hyperparameters are set at λ f p = 0.1 and λ f n = 0.2, with output threshold θ = 0.4 and rendering threshold τ = 0.4. We fine-tune our KITTI model from a nuScenes tracking model.</p><p>For nuScenes <ref type="bibr" target="#b2">[3]</ref>, we use input resolution 800 × 448. We set λ f p = 0.1 and λ f n = 0.4, and use output threshold θ = 0.1 and rendering threshold τ = 0.1. We first train our nuScenes model for 140 epochs for just 3D detection <ref type="bibr" target="#b55">[56]</ref> and then fine-tune for 70 epochs for 3D tracking. Note that nuScenes evaluation is done per 360 panorama, not per image. We naively fuse all outputs from the 6 cameras together, without handling duplicate detections at the intersection of views <ref type="bibr" target="#b37">[38]</ref>. Track rebirth. Following common practice <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b54">55]</ref>, we keep unmatched tracks "inactive" until they remain undetected for K consecutive frames. Inactive tracks can be matched to detections and regain their ID, but not appear in the prior heatmap or output. The tracker stays online. Rebirth only matters for the MOT test set, where we use K = 32. For all other experiments, we found rebirth not to be required (K = 0).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Public detection</head><p>The MOT17 challenge only supports public detection. That is, participants are asked to use the provided detections. Public detection is meant to test a tracker's ability to associate objects, irrespective of its ability to detect objects. Our method operates in the private detection mode by default. For the MOT challenge we created a public-detection version of CenterTrack that uses the externally provided (public) detections and is thus fairly compared to other participants in the challenge. This shows that the advantages of CenterTrack are not due to the accuracy of the detections but are due to the tracking framework itself. Note that refining and rescoring the given bounding boxes is allowed and is commonly used by participants in the challenge <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>. Following Tracktor <ref type="bibr" target="#b0">[1]</ref>, we keep the bounding boxes that are close to an existing bounding box in the previous frame. We only initialize a new trajectory if it is near a public detection. All bounding boxes in our results are either near a public detection in the current frame or near a tracked box in the previous frame. The algorithm's diagram of this public-detection configuration can be found in the supplement. We use this public-detection configuration of CenterTrack for MOT17 test set evaluation and use the private-detection setting in our ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main results</head><p>All three datasets -MOT17 <ref type="bibr" target="#b27">[28]</ref>, KITTI <ref type="bibr" target="#b11">[12]</ref>, and nuScenes <ref type="bibr" target="#b2">[3]</ref> -host test servers with hidden annotations and leaderboards. We compare to all published results on these leaderboards. The numbers were accessed on Mar. 5th, 2020. We retrain CenterTrack on the full training set with the same hyperparameters in the ablation experiments. <ref type="table" target="#tab_1">Table 1</ref> lists the results on the MOT17 challenge. We use our public configuration in Section 5.3 and do not pretrain on CrowdHuman <ref type="bibr" target="#b33">[34]</ref>. CenterTrack significantly outperforms the prior state of the art even when restricted to the public-detection configuration. For example CenterTrack improves MOTA by 5 points (an 8.6% relative improvement) over Tracktor v2 <ref type="bibr" target="#b0">[1]</ref>.</p><p>The public detection setting ensures that all methods build on the same underlying detector. Our gains come from two sources. Firstly, the heatmap input makes our tracker better preserve tracklets from the previous frame, which results in a much lower rate of   For IDF1 and id-switch, our local model is not as strong as offline methods such as LSST17 <ref type="bibr" target="#b9">[10]</ref>, but is better than other online methods <ref type="bibr" target="#b0">[1]</ref>. We believe that there is an exciting avenue for future work in combining local trackers (such as our work) with stronger offline long-range models (such as SORT <ref type="bibr" target="#b1">[2]</ref>, LMP <ref type="bibr" target="#b40">[41]</ref>, and other ReID-based trackers <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref>).</p><p>On KITTI <ref type="bibr" target="#b11">[12]</ref>, we submitted our best-performing model with flip testing <ref type="bibr" target="#b55">[56]</ref>. The model runs at 82ms and yields 89.44% MOTA, outperforming all published work (Table 2). Note that our model without flip testing runs at 45ms with 88.7% MOTA on the validation set (vs. 89.63% with flip testing on the validation set). We avoid submitting to the test server multiple times following their test policy. The results again indicate that CenterTrack performs competitively with more complex methods.</p><p>On nuScenes <ref type="bibr" target="#b2">[3]</ref>, our monocular tracking method achieves an AMOTA@0.2 of 28.3% and an AMOTA@1 of 4.6%, outperforming the monocular baseline <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46]</ref> by a large margin. There are two main reasons. Firstly, we use a stronger and faster 3D detector <ref type="bibr" target="#b55">[56]</ref> (see the 3D detector comparison in the supplementary). More importantly, as shown in <ref type="table" target="#tab_8">Table 6</ref>, the Kalman-filter-based 3D tracking baseline relies on hand-crafted motion rules <ref type="bibr" target="#b45">[46]</ref>, which are less effective in low-framerate regimes. Our method learns object motion from data and is much more stable at low framerates.  <ref type="table">Table 4</ref>: Ablation study on MOT17, KITTI, and nuScenes. All results are on validation sets (Section 5.1). For each dataset, we report the corresponding official metrics. ↑ indicates that higher is better, ↓ indicates that lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation studies</head><p>We first ablate our two main technical contributions: tracking-conditioned detection (Section 4.1) and offset prediction (Section 4.2) on all three datasets. Specifically, we compare our full framework with three baselines. Detection only runs a CenterNet detector at each individual frame and associates their identity only based on 2D center distance. This model does not use video data, but still uses two input images. Without offset uses just tracking-conditioned prediction with a predicted offset of zero. Every object is again associated to its closest object in the previous frame. Without heatmap predicts the center offset between frames and uses the updated center distance as the association metric, but the prior heatmap is not provided. The offsetbased greedy association is used. <ref type="table">Table 4</ref> shows the results. On all datasets, our full CenterTrack model performs significantly better than the baselines. Tracking-conditioned detection yields ∼ 2% MOTA improvement on MOT and ∼ 3% MOTA improvement on KITTI, with or without offset prediction. It produces more false positives but fewer false negatives. This is because with the heatmap prior, the network tends to predict more objects around the previous peaks, which are sometimes misleading. The merits of the heatmap outweigh the limitations and improve MOTA overall. Using the prior heatmap also significantly reduces IDSW on both datasets, indicating that the heatmap stabilizes detection.</p><p>Tracking offset prediction gives a huge boost on nuScenes and reduces IDSW consistently in MOT and KITTI. The effectiveness of the tracking offset appears to be related to the video framerate. When the framerate is high, motion between frames is small, and a zero offset is often a reasonable starting point for association. When framerate is low, as in the nuScenes dataset, motion between frames is large and static object association is considerably less effective. Our offset prediction scheme helps deal with such large inter-frame motion. Next, we ablate other components on MOT17.</p><p>Training with noisy heatmap. The 2nd row in <ref type="table" target="#tab_6">Table 5</ref> shows the importance of injecting noise into heatmaps during training (Section 4.3). Without noise injection, the model fails to generalize and yields dramatically lower accuracy. In particular, this model has a large false negative rate. One reason is that in the first frame, the input heatmap is empty. This model had a hard time discovering new objects that were not indicated in the prior heatmap. Training on static images. We train a version of our model on static images only, as described in Section 4.4. The results are shown in   Matching algorithm. We use a simple greedy matching algorithm based on the detection score, while most other trackers use the Hungarian algorithm. We show the performance of CenterTrack with Hungarian matching in the 4th row of <ref type="table" target="#tab_6">Table 5</ref>. It does not improve performance. We choose greedy matching for simplicity. Track rebirth. We show CenterTrack with track rebirth (K=32) in the last row of <ref type="table" target="#tab_6">Table 5</ref>. While the MOTA performance keeps similar, it significantly increases IDF1 and reduces ID switch. We use this setting for our MOT test set submission. For other datasets and evaluation metrics no rebirth was required (K = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Comparison to alternative motion models</head><p>Our offset prediction is able to estimate object motion, but also performs a simple association, as current objects are linked to prior detections, which CenterTrack receives as one of its inputs. To verify the effectiveness of our learned association, we replace our offset prediction with three alternative motion models: No motion. We set the offset to zeros. It is copied from <ref type="table">Table 4</ref> for reference only. Kalman filter. The Kalman filter predicts each object's future state through an explicit motion model estimated from its history. It is the most widely used motion model in traditional real-time trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. We use the popular public implementation from SORT <ref type="bibr" target="#b1">[2]</ref>. Optical flow. As an alternative motion model, we use FlowNet2 <ref type="bibr" target="#b14">[15]</ref>. The model was trained to estimate dense pixel motion for all objects in a scene. We run the strongest officially released FlowNet2 model (∼ 150ms / image pair), and replace our learned offset with the predicted optical flow at each predicted object center. The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. All models use the exact same detector. On the high-framerate MOT17 dataset, any motion model suffices, and even no motion model at all performs competitively. On KITTI and nuScenes, where the intra-frame motions are non-trivial, the hand-crafted motion rule of the Kalman filter performs significantly worse, and even the performance of optical flow degrades. This emphasizes that our offset model does more than just motion estimation. CenterTrack is conditioned on prior detections and can learn to snap offset predictions to exactly those prior detections. Our training procedure strongly encourages this through heavy data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented an end-to-end simultaneous object detection and tracking framework. Our method takes two frames and a prior heatmap as input, and produces detection and tracking offsets for the current frame. Our tracker is purely local and associates objects greedily through time. It runs online (no knowledge of future frames) and in real time, and sets a new state of the art on the challenging MOT17, KITTI, and nuScenes 3D tracking benchmarks. Acknowledgements. This work has been supported in part by the National Science Foundation under grant IIS-1845485.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Tracking algorithms A.1 Private tracking</head><p>We adopt a simple greedy id association algorithm based on the center distance, shown in Algorithm 1. We use the same algorithm for both 2D tracking and 3D tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Public tracking</head><p>For public tracking, we follow Tractor <ref type="bibr" target="#b0">[1]</ref> to extend a private tracking algorithm to public detection. The id association is exactly the same as private detection (Line 1 to Line 14). The difference lies in how a track can be created. In public detection, we only initialize a track if it is near a provided bounding box (Line 17 to Line 21).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results on MOT16</head><p>MOT16 shares the same training and testing sequences with MOT17, but officially supports private detection. As is shown in <ref type="table">Table 7</ref>, we rank 2nd among all published entries. We remark that all other entries use a heavy detector trained on private data <ref type="bibr" target="#b51">[52]</ref> and many rely on slow matching schemes <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b51">52]</ref>  <ref type="table">Table 7</ref>: Evaluation on the MOT16 test sets (private detection). We compare to all published on the leaderboard. The runtime is calculated from the HZ column on the leaderboard. +D means detection time, which is usually &gt; 100ms <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Private Detection</head><p>Input :</p><formula xml:id="formula_16">T (t−1) = {(p, s, id) (t−1) j } M j=1</formula><p>: Tracked objects in the previous frame, with center p, size s = (w, h).</p><formula xml:id="formula_17">B (t) = {(p,d) (t) i } N i=1</formula><p>: Heatmap peaks with offsetd in the current frame, sorted in desending confidence. Output:</p><formula xml:id="formula_18">T (t) = {(p, s, id) (t) i } N i=1</formula><p>: Tracked objects in the current frame.</p><p>1 // Initialization: T (t) and S are initialized as empty lists.</p><formula xml:id="formula_19">2 T (t) ← ∅ 3 S ← ∅ // Set of matched tracks 4 W ← Cost(B (t) , T (t−1) )// Wij = ||pi (t) −di (t)</formula><p>, p    </p><formula xml:id="formula_20">T (t) ← T (t) ∪ (p (t) i ,ŝ (t) i , id (t−1) j<label>)</label></formula><formula xml:id="formula_21">T (t) ← T (t) ∪(p (t) i ,ŝ (t) i , N ewId)</formula><formula xml:id="formula_22">B (t) = {(p,d) (t) i } N i=1</formula><p>: Heatmap peaks with offsetd in the current frame, sorted in desending confidence.</p><formula xml:id="formula_23">D (t) = {(p, s) (t) k } K k=1 : Public detections. Output: T (t) = {(p, s, id) (t) i } N i =1 :</formula><p>Tracked objects in the current frame. 1 // Initialization: T (t) and S are initialized as empty lists.   </p><formula xml:id="formula_24">2 T (t) ← ∅ 3 S ← ∅ // Set of matched tracks 4 W ← Cost(B (t) , T (t−1) )// Wij = ||pi (t) −di (t) , p (t−1) j ||2 5 W ← Cost(B (t) , D (t) )// W ik = ||pi (t) , p (t) k<label>||2</label></formula><formula xml:id="formula_25">T (t) ← T (t) ∪ (p (t) i ,ŝ (t) i , id (t−1) j<label>)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C 3D detection</head><p>We follow CenterNet <ref type="bibr" target="#b55">[56]</ref> to regress to object depthD ∈ R</p><formula xml:id="formula_26">W R × H R , 3d extentΓ ∈ R W R × H R ×3 , orientation (encoded as an 8-dimension vector)Â ∈ R W R × H R ×8 .</formula><p>The training loss for these are identical to CenterNet <ref type="bibr" target="#b55">[56]</ref>. Since the 2D bounding box center does not align with the projected 3D bounding box center due to perspective projection, we in addition regress to an offset from the 2D center to the projected 3D bounding box centerF ∈ R W R × H R ×2 . We use L1Loss:</p><formula xml:id="formula_27">L of f 3d = 1 N N k=1 |f k − f k |,<label>(4)</label></formula><p>where f k ∈ R 2 is the ground truth offset of object k, andf k =F p k is the value inF at location p k . We show the 3D detection performance of CenterNet <ref type="bibr" target="#b55">[56]</ref> with the offset prediction in <ref type="table" target="#tab_15">Table 8</ref> for reference. The 3D detection performance is on-par with Mappilary <ref type="bibr" target="#b37">[38]</ref> and PointPillars <ref type="bibr" target="#b20">[21]</ref>, but far below the LiDAR based state-of-the-art Megvii <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Amodal bounding box regression</head><p>CenterNet <ref type="bibr" target="#b55">[56]</ref> requires the bounding box center to be within the image. While in MOT <ref type="bibr" target="#b27">[28]</ref>, the center of the annotated bounding box (Amodal bounding box) can be outside of the image. To accommodate this case, We extend the 2-channel bounding box size head in CenterNet to a 4-channel headÂ ∈ R W R × H R ×4 for the distance to the top-, left-, bottom-, right-bounding box border. Note that we still detect the in-frame bounding box center and regress to the in-frame bounding box size. With this 4-dimensional bounding box formulation, the output bounding box is not necessarily centered on the detected center. The training loss for the 4-dimensional bounding box formulation is L1Loss:</p><formula xml:id="formula_28">L amodal size = 1 N N i=1 |Â pi − a i |<label>(5)</label></formula><p>where a i ∈ R 4 is the ground truth border distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E CrowdHuman dataset</head><p>CrowdHuman <ref type="bibr" target="#b33">[34]</ref>contains 15k training images with common pose annotations. The dataset is featured of high density and large occlusion. Both visible bounding box and the Amodal bounding box are annotated. We use the Amodal bounding box annotation in our experiments to align with MOT <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Pretraining experiments</head><p>For pretraining on CrowdHuman <ref type="bibr" target="#b33">[34]</ref>, we use input resolution 512 × 512, false positive ratio λ f p = 0.1, false negative ratio λ f n = 0.4, random scaling ratio 0.05, and random translation ratio 0.05. The training follows Section.4.4 of the main paper. As shown in <ref type="table">Table 9</ref>, the model trained on CrowdHuman achieves a decent 52.2 MOTA in MOT dataset, without seeing any MOT data. Without CrowdHuman <ref type="bibr" target="#b33">[34]</ref> pretraining, our performance drops to 60.7% MOTA on the validation set. Pretraining help improve detection quality by decreasing the false negatives. Note that most entries on MOT challenges use external data for pretraining, and some of them use private data <ref type="bibr" target="#b51">[52]</ref>. For reference, we also show our public detection results without pretraining in <ref type="table">Table 9</ref>, last row. This model corresponds to the entry we submitted to MOT17 public detection challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional experiments on KITTI</head><p>In <ref type="table" target="#tab_1">Table 10</ref>, we show results of the same additional experiments (Section. 5.5 of the main paper) on KITTI dataset <ref type="bibr" target="#b11">[12]</ref>. The conclusions are the same as on MOT <ref type="bibr" target="#b27">[28]</ref>. Training on static images now performs slightly worse than training on video, mostly due to that KITTI has larger inter-frame motion than MOT. Training without random heatmap noise is much worse than the full model, with a high false-negative rate. And using the Hungarian algorithm works the same as using a greedy matching. Our model without nuScenes <ref type="bibr" target="#b2">[3]</ref> achieves 84.5% MOTA on the validation set, this is on-par with other state-of-the-art trackers on KITTI <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref> with a heavy detector <ref type="bibr" target="#b29">[30]</ref>.  <ref type="table" target="#tab_1">Table 11</ref>: Experiments with different output thresholds (θ) and rendering thresholds (τ ) on the MOT <ref type="bibr" target="#b27">[28]</ref> validation set. We search θ and τ locally in a step of 0.1.</p><formula xml:id="formula_29">MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDSW ↓</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Output and rendering threshold</head><p>As the tracking evaluation metric (MOTA) does not consider the confidence of predictions, picking an output threshold is essential in all tracking algorithms (see discussion in AB3D <ref type="bibr" target="#b45">[46]</ref>). In our case, we also need a threshold to render predictions to the prior heatmap. We search the optimal thresholds on MOT <ref type="bibr" target="#b27">[28]</ref> in <ref type="table" target="#tab_1">Table 11</ref>. Basically, increasing both thresholds results in fewer outputs, thus increases the false negatives while decreases the false positives. We find a good balance at θ = 0.4 and τ = 0.5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results on MOT (1st row), KITTI (2nd row), and nuScenes (3rd and 4th rows). Each row shows three consecutive frames. We show the predicted tracking offset in arrow. Tracks are coded by color. Best viewed on the screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Return: T (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Time(ms) MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDSW ↓</figDesc><table><row><cell cols="2">Tracktor17 [1] 666+D</cell><cell>53.5</cell><cell>52.3 19.5 36.6 12201 248047 2072</cell></row><row><cell>LSST17 [10]</cell><cell>666+D</cell><cell>54.7</cell><cell>62.3 20.4 40.1 26091 228434 1243</cell></row><row><cell cols="2">Tracktor v2 [1] 666+D</cell><cell>56.5</cell><cell>55.1 21.1 35.3 8866 235449 3763</cell></row><row><cell>GMOT</cell><cell>167+D</cell><cell>55.4</cell><cell>57.9 22.7 34.7 20608 229511 1403</cell></row><row><cell>Ours (Public)</cell><cell>57+D</cell><cell>61.5</cell><cell>59.6 26.4 31.9 14076 200672 2583</cell></row><row><cell>Ours (Private)</cell><cell>57</cell><cell>67.8</cell><cell>64.7 34.6 24.6 18498 160332 3039</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on the MOT17 test sets (top: public detection; bottom: private detection). We compare to published entries on the leaderboard. The runtime is calculated from the HZ column on the leaderboard. +D means detection time, which is usually &gt; 100ms<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Time(ms) MOTA ↑ MOTP ↑ MT ↑ ML ↓ IDSW ↓ FRAG ↓</figDesc><table><row><cell>AB3D [46]</cell><cell>4+D</cell><cell>83.84</cell><cell>85.24 66.92 11.38</cell><cell>9</cell><cell>224</cell></row><row><cell cols="2">BeyondPixel [35] 300+D</cell><cell>84.24</cell><cell>85.73 73.23 2.77</cell><cell>468</cell><cell>944</cell></row><row><cell>3DT [14]</cell><cell>30+D</cell><cell>84.52</cell><cell>85.64 73.38 2.77</cell><cell>377</cell><cell>847</cell></row><row><cell>mmMOT [54]</cell><cell>10+D</cell><cell>84.77</cell><cell>85.21 73.23 2.77</cell><cell>284</cell><cell>753</cell></row><row><cell cols="2">MOTSFusion [27] 440+D</cell><cell>84.83</cell><cell>85.21 3.08 2.77</cell><cell>275</cell><cell>759</cell></row><row><cell>MASS [18]</cell><cell>10+D</cell><cell>85.04</cell><cell>85.53 74.31 2.77</cell><cell>301</cell><cell>744</cell></row><row><cell>Ours</cell><cell>82</cell><cell>89.44</cell><cell>85.05 82.31 2.31</cell><cell>116</cell><cell>334</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on the KITTI test set. We compare to all published entries on the leaderboard. Runtimes are from the leaderboard. +D means detection time.</figDesc><table><row><cell></cell><cell cols="4">Time(ms) AMOTA@0.2 ↑ AMOTA@1 ↑ AMOTP ↓</cell></row><row><cell>Mapillary [38]+AB3D [46]</cell><cell>-</cell><cell>6.9</cell><cell>1.8</cell><cell>1.8</cell></row><row><cell>Ours</cell><cell>45</cell><cell>27.8</cell><cell>4.6</cell><cell>1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on the nuScenes test set. We compare to the official monocular 3D tracking baseline, which applies a state-of-the-art 3D tracker<ref type="bibr" target="#b45">[46]</ref>. We list the average AMOTA@0.2, AMOTA@1, and AMOTP over all 7 categories.</figDesc><table /><note>false negatives. And second, our simple learned offset is effective. (See Section 5.6 for more analysis.) For reference, we also included a private detection version, where Cen- terTrack simultaneously detects and tracks objects (Table 1, bottom). It further improves the MOTA to 67.3%, and runs at 17 FPS end-to-end (including detection).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>4.5% 28.4% 1.3% 87.1 5.4% 5.8% 1.6% 17.8 3.6 w/o heatmap 63.9 3.5% 30.3% 2.3% 85.4 4.3% 9.8% 0.4%</figDesc><table><row><cell></cell><cell cols="2">MOT17</cell><cell>KITTI</cell><cell>nuScenes</cell></row><row><cell></cell><cell>MOTA↑ FP↓</cell><cell cols="3">FN↓ IDSW↓ MOTA↑ FP↓ FN↓ IDSW↓ AMOTA@0.2↑AMOTA@1↑</cell></row><row><cell cols="4">detection only 63.6 3.5% 30.3% 2.5 % 84.3 4.3% 9.8% 1.5%</cell><cell>18.1</cell><cell>3.4</cell></row><row><cell>w/o offset</cell><cell cols="4">65.8 26.5</cell><cell>5.9</cell></row><row><cell>Ours</cell><cell cols="3">66.1 4.5% 28.4% 1.0% 88.7 5.4% 5.8% 0.1%</cell><cell>28.3</cell><cell>6.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 (</head><label>5</label><figDesc>3rd row, 'Static image'). As MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDSW ↓ Ours 66.1 64.2 41.3 21.2 4.5% 28.4% 1.0% w.o. noisy hm 34.4 46.2 26.3 42.2 7.3% 57.4% 0.9% Static image 66.1 65.4 41.6 19.2 5.4% 27.5% 1.0% w. Hungarian 66.1 61.0 40.7 20.9 4.5% 28.3% 1.0% w. rebirth 66.2 69.4 39.5 22.1 3.9% 29.5% 0.4%</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Additional experiments on the MOT17 validation set. From top to bottom: our model, our model trained without simulating heatmap noise, our model trained on static images only, our model with Hungarian matching, and our model with track rebirth.</figDesc><table><row><cell></cell><cell cols="2">MOT17</cell><cell>KITTI</cell><cell>nuScenes</cell></row><row><cell></cell><cell>MOTA↑ FP↓</cell><cell cols="3">FN↓ IDSW↓ MOTA↑ FP↓ FN↓ IDSW↓ AMOTA@0.2↑AMOTA@1↑</cell></row><row><cell>no motion</cell><cell cols="3">65.8 4.5% 28.4% 1.3% 87.1 5.4% 5.8% 1.6%</cell><cell>17.8</cell><cell>3.6</cell></row><row><cell cols="4">Kalman filter 66.1 4.5% 28.4% 1.0% 87.9 5.4% 5.8% 0.9%</cell><cell>18.3</cell><cell>3.8</cell></row><row><cell cols="4">optical flow 66.1 4.5% 28.4% 1.0% 88.4 5.4% 5.8% 0.4%</cell><cell>26.6</cell><cell>6.2</cell></row><row><cell>ours</cell><cell cols="3">66.1 4.5% 28.4% 1.0% 88.7 5.4% 5.8% 0.1%</cell><cell>28.3</cell><cell>6.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparing different motion models on MOT17, KITTI, and nuScenes. All results are on validation sets (Section 5.1). All experiments on the same dataset are from the same model.</figDesc><table /><note>reported in this table, training on static images gives the same performance as training on videos on the MOT dataset. Separately, we observed that training on static images is less effective on nuScenes, where framerate is low.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>. For example, LMP p [41] computes person-reidentification features for all pairs of bounding boxes using a Siamese network, requiring O(n 2 ) forward passes through a deep network. In contrast, CenterTrack involves a single pass through a network and operates online at 17 FPS. Time(ms) MOTA ↑ IDF1 ↑ FP ↓ FN ↓ IDSW ↓</figDesc><table><row><cell>SORT [2]</cell><cell>36+D</cell><cell>60.4</cell><cell>56.1 11183 59867 1135</cell></row><row><cell cols="2">DeepSORT [47] 59+D</cell><cell>61.4</cell><cell>62.2 12852 56668 781</cell></row><row><cell>POI [52]</cell><cell>100+D</cell><cell>66.1</cell><cell>65.1 5061 55914 805</cell></row><row><cell>KNDT [52]</cell><cell cols="2">1428+D 68.2</cell><cell>60.0 11479 45605 933</cell></row><row><cell>LMP p [41]</cell><cell cols="2">2000+D 71.0</cell><cell>70.1 7880 44564 434</cell></row><row><cell>Ours (Private)</cell><cell>57</cell><cell>69.6</cell><cell>60.7 10458 42805 2124</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Algorithm 2 :</head><label>2</label><figDesc>Public DetectionInput : T (t−1) = {(p, s, id) Tracked objects in the previous frame, with center p, size s = (w, h).</figDesc><table><row><cell>(t−1) j</cell><cell>} M j=1 :</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Modality mAP ↑ mATE ↓ mASE ↓ mAOE ↓ mAVE ↓ mAAE ↓ NDS ↑</figDesc><table><row><cell>Megvii [57]</cell><cell>LiDAR 52.8 0.300 0.247</cell><cell>0.379</cell><cell>0.245</cell><cell>0.140 63.3</cell></row><row><cell cols="2">PointPillars [21] LiDAR 30.5 0.517 0.290</cell><cell>0.500</cell><cell>0.316</cell><cell>0.368 45.3</cell></row><row><cell cols="2">Mappilary [38] Camera 30.4 0.738 0.263</cell><cell>0.546</cell><cell>1.</cell><cell>0.134 38.4</cell></row><row><cell cols="2">CenterNet [56] Camera 33.8 0.658 0.255</cell><cell>0.629</cell><cell>1.</cell><cell>0.141 40.1</cell></row><row><cell></cell><cell>14</cell><cell cols="3">S ← S ∪ {j} // Mark track j as</cell></row><row><cell></cell><cell></cell><cell cols="2">matched</cell><cell></cell></row><row><cell></cell><cell>15</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell></cell><cell>16</cell><cell>else</cell><cell></cell><cell></cell></row><row><cell></cell><cell>17 18</cell><cell cols="3">k ← arg min K k=1 ← W ik κ ← min( ŵiĥi, √ w k h k )</cell></row><row><cell></cell><cell>19</cell><cell cols="2">if W ik &lt; κ then</cell><cell></cell></row><row><cell></cell><cell>20</cell><cell cols="3">// Create a new track.</cell></row><row><cell></cell><cell>21</cell><cell cols="2">T (t) ←</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">T (t) ∪ (p (t) i ,ŝ (t) i , N ewId)</cell></row><row><cell></cell><cell>22</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell></cell><cell>23</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell></cell><cell>24 end</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">25 Return: T (t)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>3D detection results on nuScenes test set. We show 3D bounding box mAP, mean translation error (mATE), mean size error (mASE), mean orientation error (mAOE), mean velocity error (mATE), mean attributes error (mAAE), and their weighted (with weight 5 on mAP and 1 on others) average NDS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Ours 66.1 64.2 41.3 21.2 4.5% 28.4% 1.0% only CrowdH. 52.2 53.8 33.6 25.1 6.7% 39.7% 1.4% scratch 60.7 62.8 33.0 22.4 4.0% 34.2% 1.0% scratch-Pub. 57.4 59.6 31.1 27.1 2.1% 39.6% 1.0% Table 9: Additional experiments on the MOT17 validation set. From top to bottom: our full model, the model trained only on CrowdHuman dataset, our model trained from scratch, and the public detection mode of our model trained from scratch. MOTA ↑ MOTP ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDSW ↓ Ours 88.7 86.7 90.3 2.1 5.4% 5.8% 0.1% Static image 86.8 86.5 88.5 2.2 4.8% 7.9% 0.4% w.o. noisy hm 80.1 85.3 76.2 7.6 3.8% 16.1% 0.1% Hungarian 88.7 86.7 90.3 2.1 5.4% 5.8% 0.1% scratch 84.5 83.2 83.4 2.8 5.7% 9.6% 0.3% Table 10: Additional experiments on the KITTI validation set. From top to bottom: our full model, the public-detection configuration of our model, our model trained on static images only, our model trained without simulating heatmap noise, our model with the Hungarian algorithm used for matching, and our model trained from scratch. θ τ MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDSW ↓</figDesc><table><row><cell>0.4 0.4 62.6</cell><cell>64.9 44.0 18.9 10.3% 26.4% 0.7%</cell></row><row><cell>0.4 0.6 65.5</cell><cell>63.2 38.6 22.4 2.5% 30.5% 1.5%</cell></row><row><cell>0.4 0.5 66.1</cell><cell>64.2 41.3 21.2 4.5% 28.4% 1.0%</cell></row><row><cell>0.3 0.5 66.2</cell><cell>64.3 43.1 19.2 5.7% 26.9% 1.2%</cell></row><row><cell>0.5 0.5 65.2</cell><cell>62.1 39.8 23.0 3.7% 30.2% 0.9%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuScenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multiple target tracking in world coordinate with single, minimally calibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Parametric image alignment using enhanced correlation coefficient maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Psarakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Recurrent autoregressive networks for online multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06129</idno>
		<title level="m">Multi-object tracking with multiple cues and switcher-aware classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to refine human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Joint monocular 3D detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">T-cnn: Tubelets with convolutional neural networks for object detection from videos. Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiple object tracking with attention to appearance, structure, motion and size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tracking the trackers: an analysis of the state of the art in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02781</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICME</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00130</idno>
		<title level="m">Track to reconstruct and reconstruct to track</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A benchmark for multiobject tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The CLEAR 2006 evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Soundararajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CLEAR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Detection and tracking of point features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU- CS-91-132</idno>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B G</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03961</idno>
		<title level="m">A baseline for 3d multi-object tracking</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Spatial-temporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Robust multi-modality multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11167</idno>
		<title level="m">Integrated object detection and tracking with tracklet-conditioned detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09492</idno>
		<title level="m">Class-balanced grouping and sampling for point cloud 3D object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
