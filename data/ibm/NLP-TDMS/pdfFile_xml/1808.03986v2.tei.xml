<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Differential Network for Visual Question Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badri</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Kumar</surname></persName>
							<email>sandepkr@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
							<email>vinodkk@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
							<email>vinaypn@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Differential Network for Visual Question Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To understand the progress towards multimedia vision and language understanding, a visual Turing test was proposed by <ref type="bibr">(Geman et al., 2015)</ref> that was aimed at visual question answering <ref type="bibr" target="#b0">(Antol et al., 2015)</ref>. Visual <ref type="bibr">Dialog (Das et al., 2017</ref>) is a natural extension for VQA. Current dialog systems as evaluated in <ref type="bibr">(Chattopadhyay et al., 2017)</ref> show that when trained between bots, AI-AI dialog systems show improvement, but that does not translate to actual improvement for Human-AI dialog. This is because, the questions generated by bots are not natural (human-like) and therefore does not translate to improved human dialog. Therefore it is imperative that improvement in the quality of questions will enable dialog agents to perform well in human interactions. Further, <ref type="bibr">(Ganju et al., 2017)</ref> show that unanswered questions can be used for improving VQA, Image captioning and Object Classification. An interesting line of work in this respect is the work of <ref type="bibr">(Mostafazadeh et al., 2016)</ref>. Here the au-thors have proposed the challenging task of generating natural questions for an image. One aspect that is central to a question is the context that is relevant to generate it. However, this context changes for every image. As can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>, an image with a person on a skateboard would result in questions related to the event. Whereas for a little girl, the questions could be related to age rather than the action. How can one have widely varying context provided for generating questions? To solve this problem, we use the context obtained by considering exemplars, specifically we use the difference between relevant and irrelevant exemplars. We consider different contexts in the form of Location, Caption, and Part of Speech tags. Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding. This embedding is used by a question decoder to decode the appropriate question. As discussed further, we observe this implicit differential context to perform better than an explicit keyword based context. The difference between the two approaches is illustrated in <ref type="figure">Figure 2</ref>. This also allows for better optimization as we can backpropagate through the whole network. We provide detailed empirical evidence to support our hypothesis. As seen in <ref type="figure" target="#fig_0">Figure 1</ref> our method generates natural questions and improves over the state-ofthe-art techniques for this problem. <ref type="figure">Figure 2</ref>: Here we provide intuition for using implicit embeddings instead of explicit ones. As explained in section 1, the question obtained by the implicit embeddings are natural and holistic than the explicit ones.</p><p>To summarize, we propose a multimodal differential network to solve the task of visual question generation. Our contributions are: (1) A method to incorporate exemplars to learn differential embeddings that captures the subtle differences between supporting and contrasting examples and aid in generating natural questions. (2) We provide Multimodal differential embeddings, as image or text alone does not capture the whole context and we show that these embeddings outperform the ablations which incorporate cues such as only image, or tags or place information. (3) We provide a thorough comparison of the proposed network against state-of-the-art benchmarks along with a user study and statistical significance test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generating a natural and engaging question is an interesting and challenging task for a smart robot (like chat-bot). It is a step towards having a natural visual dialog instead of the widely prevalent visual question answering bots. Further, having the ability to ask natural questions based on different contexts is also useful for artificial agents that can interact with visually impaired people. While the task of generating question automatically is well studied in NLP community, it has been relatively less studied for image-related natural questions. This is still a difficult task <ref type="bibr">(Mostafazadeh et al., 2016)</ref> that has gained recent interest in the community.</p><p>Recently there have been many deep learning based approaches as well for solving the textbased question generation task such as <ref type="bibr">(Du et al., 2017)</ref>. Further, <ref type="bibr">(Serban et al., 2016)</ref> have proposed a method to generate a factoid based question based on triplet set {subject, relation and ob-ject} to capture the structural representation of text and the corresponding generated question.</p><p>These methods, however, were limited to textbased question generation. There has been extensive work done in the Vision and Language domain for solving image captioning, paragraph generation, Visual Question Answering (VQA) and Visual Dialog. <ref type="bibr">(Barnard et al., 2003;</ref><ref type="bibr">Farhadi et al., 2010;</ref><ref type="bibr">Kulkarni et al., 2011)</ref> proposed conventional machine learning methods for image <ref type="bibr">description. (Socher et al., 2014;</ref><ref type="bibr">Vinyals et al., 2015;</ref><ref type="bibr">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr">Xu et al., 2015;</ref><ref type="bibr">Fang et al., 2015;</ref><ref type="bibr">Chen and Lawrence Zitnick, 2015;</ref><ref type="bibr">Johnson et al., 2016;</ref><ref type="bibr">Yan et al., 2016)</ref> have generated descriptive sentences from images with the help of Deep Networks. There have been many works for solving Visual Dialog <ref type="bibr">(Chappell et al., 2004;</ref><ref type="bibr">Das et al., 2016</ref><ref type="bibr">Das et al., , 2017</ref><ref type="bibr">De Vries et al., 2017;</ref><ref type="bibr">Strub et al., 2017)</ref>. A variety of methods have been proposed by <ref type="bibr">(Malinowski and Fritz, 2014;</ref><ref type="bibr">Lin et al., 2014;</ref><ref type="bibr" target="#b0">Antol et al., 2015;</ref><ref type="bibr">Ren et al., 2015;</ref><ref type="bibr">Ma et al., 2016;</ref><ref type="bibr">Noh et al., 2016)</ref> for solving VQA task including attention-based methods <ref type="bibr">(Zhu et al., 2016;</ref><ref type="bibr">Fukui et al., 2016;</ref><ref type="bibr">Gao et al., 2015;</ref><ref type="bibr">Xu and Saenko, 2016;</ref><ref type="bibr">Lu et al., 2016;</ref><ref type="bibr">Shih et al., 2016;</ref><ref type="bibr">Patro and Namboodiri, 2018)</ref>. However, Visual Question Generation (VQG) is a separate task which is of interest in its own right and has not been so well explored <ref type="bibr">(Mostafazadeh et al., 2016)</ref>. This is a vision based novel task aimed at generating natural and engaging question for an image. <ref type="bibr">(Yang et al., 2015)</ref> proposed a method for continuously generating questions from an image and subsequently answering those questions. The works closely related to ours are that of <ref type="bibr">(Mostafazadeh et al., 2016)</ref> and <ref type="bibr">(Jain et al., 2017)</ref>. In the former work, the authors used an encoder-decoder based framework whereas in the latter work, the authors extend it by using a variational autoencoder based sequential routine to ob-tain natural questions by performing sampling of the latent variable. <ref type="figure">Figure 3</ref>: An illustrative example shows the validity of our obtained exemplars with the help of an object classification network, RESNET-101. We see that the probability scores of target and supporting exemplar image are similar. That is not the case with the contrasting exemplar. The corresponding generated questions when considering the individual images are also shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we clarify the basis for our approach of using exemplars for question generation. To use exemplars for our method, we need to ensure that our exemplars can provide context and that our method generates valid exemplars.</p><p>We first analyze whether the exemplars are valid or not. We illustrate this in figure 3. We used a pre-trained <ref type="bibr">RESNET-101 (He et al., 2016)</ref> object classification network on the target, supporting and contrasting images. We observed that the supporting image and target image have quite similar probability scores. The contrasting exemplar image, on the other hand, has completely different probability scores.</p><p>Exemplars aim to provide appropriate context. To better understand the context, we experimented by analysing the questions generated through an exemplar. We observed that indeed a supporting exemplar could identify relevant tags (cows in <ref type="figure">Figure 3</ref>) for generating questions. We improve use of exemplars by using a triplet network. This network ensures that the joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption and vice-versa. We empirically evaluated whether an explicit approach that uses the differential set of tags as a one-hot encoding improves the question generation, or the implicit embedding obtained based on the triplet network. We observed that the implicit multimodal differential network empirically provided better context for generating questions. Our understanding of this phenomenon is that both target and supporting exemplars generate similar questions whereas contrasting exemplars generate very different questions from the target question. The triplet network that enhances the joint embedding thus aids to improve the generation of target question. These are observed to be better than the explicitly obtained context tags as can be seen in <ref type="figure">Figure 2</ref>. We now explain our method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>The task in visual question generation (VQG) is to generate a natural language questionQ, for an image I. We consider a set of pre-generated context C from image I. We maximize the conditional probability of generated question given image and context as follows:</p><formula xml:id="formula_0">θ = arg max θ (I,C,Q) log P (Q| I, C, θ)<label>(1)</label></formula><p>where θ is a vector for all possible parameters of our model. Q is the ground truth question. The log probability for the question is calculated by using joint probability over {q 0 , q 1 , ....., q N } with the help of chain rule. For a particular question, the above term is obtained as:</p><formula xml:id="formula_1">log P (Q|I, C) = N t=0 log P (q t |I, C, q 0 , .., q t−1 )</formula><p>where N is length of the sequence, and q t is the t th word of the question. We have removed θ for simplicity. Our method is based on a sequence to sequence network <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr">Vinyals et al., 2015;</ref><ref type="bibr">Bahdanau et al., 2014)</ref>. The sequence to sequence network has a text sequence as input and output. In our method, we take an image as input and generate a natural question as output. The architecture for our model is shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Our model contains three main modules, (a) Representation Module that extracts multimodal features (b) Mixture Module that fuses the multimodal representation and (c) Decoder that generates question using an LSTM-based language model. During inference, we sample a question word q i from the softmax distribution and continue sampling until the end token or maximum length for the question is reached. We experimented with both sampling and argmax and found out that argmax works better. This result is provided in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Differential Network</head><p>The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Finding Exemplars</head><p>We used an efficient KNN-based approach (k-d tree) with Euclidean metric to obtain the exemplars. This is obtained through a coarse quantization of nearest neighbors of the training examples into 50 clusters, and selecting the nearest as supporting and farthest as the contrasting exemplars. We experimented with ITML based metric learning <ref type="bibr">(Davis et al., 2007)</ref> for image features. Surprisingly, the KNN-based approach outperforms the latter one. We also tried random exemplars and different number of exemplars and found that k = 5 works best. We provide these results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Representation Module</head><p>We use a triplet network <ref type="bibr">(Frome et al., 2007;</ref><ref type="bibr">Hoffer and Ailon, 2015)</ref> in our representation module. We refereed a similar kind of work done in (Patro and Namboodiri, 2018) for building our triplet network. The triplet network consists of three sub-parts: target, supporting, and contrasting networks. All three networks share the same parameters. Given an image x i we obtain an embedding g i using a CNN parameterized by a function G(x i , W c ) where W c are the weights for the CNN. The caption C i results in a caption embedding f i through an LSTM parameterized by a function F (C i , W l ) where W l are the weights for the LSTM. This is shown in part 1 of </p><formula xml:id="formula_2">g i = G(x i , W c ) = CN N (x i ) f i = F (C i , W l ) = LST M (C i )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Mixture Module</head><p>The Mixture module brings the image and caption embeddings to a joint feature embedding space. The input to the module is the embeddings obtained from the representation module. We have evaluated four different approaches for fusion viz., joint, element-wise addition, hadamard and attention method. Each of these variants receives image features g i &amp; the caption embedding f i , and outputs a fixed dimensional feature vector s i . The Joint method concatenates g i &amp; f i and maps them to a fixed length feature vector s i as follows:</p><formula xml:id="formula_3">s i = W T j * tanh(W ij g i (W cj f i + b j )) (3)</formula><p>where g i is the 4096-dimensional convolutional feature from the FC7 layer of pretrained <ref type="bibr">VGG-19 Net (Simonyan and Zisserman, 2014</ref>  <ref type="bibr">et al., 2015)</ref> is to obtain context vectors that bring the supporting exemplar embeddings closer to the target embedding and vice-versa. This is obtained as follows:</p><formula xml:id="formula_4">D(t(s i ), t(s + i )) + α &lt; D(t(s i ), t(s − i )) ∀(t(s i ), t(s + i ), t(s − i )) ∈ M,<label>(4)</label></formula><p>where D(t(s i ), t(s j )) = ||t(s i ) − t(s j )|| 2 2 is the euclidean distance between two embeddings t(s i ) and t(s j ). M is the training dataset that contains all set of possible triplets. T (s i , s + i , s − i ) is the triplet loss function. This is decomposed into two terms, one that brings the supporting sample closer and one that pushes the contrasting sample further. This is given by</p><formula xml:id="formula_5">T (s i , s + i , s − i ) = max(0, D + + α − D − ) (5) Here D + , D − represent</formula><p>the euclidean distance between the target and supporting sample, and target and opposing sample respectively. The parameter α(= 0.2) controls the separation margin between these and is obtained through validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoder: Question Generator</head><p>The role of decoder is to predict the probability for a question, given s i . RNN provides a nice way to perform conditioning on previous state value using a fixed length hidden vector. The conditional probability of a question token at particular time step q t is modeled using an LSTM as used in machine translation <ref type="bibr">(Sutskever et al., 2014)</ref>. At time step t, the conditional probability is denoted by P (q t |I, C, q 0 , ...q t−1 ) = P (q t |I, C, h t ), where h t is the hidden state of the LSTM cell at time step t, which is conditioned on all the previously generated words {q 0 , q 1 , ...q N −1 }. The word with maximum probability in the probability distribution of the LSTM cell at step k is fed as an input to the LSTM cell at step k + 1 as shown in part 3 of <ref type="bibr">Figure 4</ref>. At t = −1, we are feeding the output of the mixture module to LSTM.Q = {q 0 ,q 1 , ...q N −1 } are the predicted question tokens for the input image I. Here, we are usingq 0 andq N −1 as the special token START and STOP respectively. The softmax probability for the predicted question token at different time steps is given by the following equations where LSTM refers to the standard LSTM cell equations:</p><formula xml:id="formula_6">x −1 = S i = Mixture Module(g i , f i ) h 0 = LSTM(x −1 ) x t = W e * q t , ∀t ∈ {0, 1, 2, ...N − 1} h t+1 = LSTM(x t , h t ), ∀t ∈ {0, 1, 2, ...N − 1} o t+1 = W o * h t+1 y t+1 = P (q t+1 |I, C, h t ) = Softmax(o t+1 ) Loss t+1 = loss(ŷ t+1 , y t+1 )</formula><p>Whereŷ t+1 is the probability distribution over all question tokens. loss is cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cost function</head><p>Our objective is to minimize the total loss, that is the sum of cross entropy loss and triplet loss over all training examples. The total loss is:</p><formula xml:id="formula_7">L = 1 M M i=1 (L cross + γL triplet )<label>(6)</label></formula><p>where M is the total number of samples,γ is a constant, which controls both the loss. L triplet is the triplet loss function 5. L cross is the cross entropy loss between the predicted and ground truth questions and is given by:</p><formula xml:id="formula_8">L cross = −1 N N t=1 y t logP (q t |I i , C i ,q 0 , ..q t−1 )</formula><p>where, N is the total number of question tokens, y t is the ground truth label. The code for MDN-VQG model is provided 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Variations of Proposed Method</head><p>While, we advocate the use of multimodal differential network for generating embeddings that can be used by the decoder for generating questions, we also evaluate several variants of this architecture. These are as follows:</p><p>Tag Net: In this variant, we consider extracting the part-of-speech (POS) tags for the words present in the caption and obtaining a Tag embedding by considering different methods of combining the one-hot vectors. Further details and experimental results are present in the supplementary. This Tag embedding is then combined with the image embedding and provided to the decoder network.</p><p>Place Net: In this variant we explore obtaining embeddings based on the visual scene understanding. This is obtained using a pre-trained <ref type="bibr">PlaceCNN (Zhou et al., 2017)</ref> that is trained to classify 365 different types of scene categories. We then combine the activation map for the input image and the VGG-19 based place embedding to obtain the joint embedding used by the decoder.</p><p>Differential Image Network: Instead of using multimodal differential network for generating embeddings, we also evaluate differential image network for the same. In this case, the embedding does not include the caption but is based only on the image feature. We also experimented with using multiple exemplars and random exemplars. Further details, pseudocode and results regarding these are present in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Dataset</head><p>We conduct our experiments on Visual Question Generation (VQG) dataset <ref type="bibr">(Mostafazadeh et al., 2016)</ref>, which contains human annotated questions based on images of MS-COCO dataset. This dataset was developed for generating natural and engaging questions based on common sense reasoning. We use VQG-COCO dataset for our experiments which contains a total of 2500 training images, 1250 validation images, and 1250 testing images. Each image in the dataset contains five natural questions and five ground truth captions. It is worth noting that the work of (Jain et al., 2017) also used the questions from VQA dataset <ref type="bibr" target="#b0">(Antol et al., 2015)</ref> for training purpose, whereas the work by (Mostafazadeh et al., 2016) uses only the VQG-COCO dataset. VQA-1.0 dataset is also built on images from MS-COCO dataset. It contains a total of 82783 images for training, 40504 for validation and 81434 for testing. Each image is associated with 3 questions. We used pretrained caption generation model <ref type="bibr">(Karpathy and Fei-Fei, 2015)</ref> to extract captions for VQA dataset as the human annotated captions are not there in the dataset. We also get good results on the VQA dataset (as shown in <ref type="table" target="#tab_3">Table 2</ref>) which shows that our method doesn't necessitate the presence of ground truth captions. We train our model separately for VQG-COCO and VQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Inference</head><p>We made use of the 1250 validation images to tune the hyperparameters and are providing the results on test set of VQG-COCO dataset. During inference, We use the Representation module to find the embeddings for the image and ground truth caption without using the supporting and contrasting exemplars. The mixture module provides the joint representation of the target image and ground truth caption. Finally, the decoder takes in the joint features and generates the question. We also experimented with the captions generated by an Image-Captioning network (Karpathy and Fei-Fei, 2015) for VQG-COCO dataset and the result for that and training details are present in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our proposed MDN method in the following ways: First, we evaluate it against other variants described in section 4.4 and 4.1.3. Second, we further compare our network with stateof-the-art methods for VQA 1.0 and VQG-COCO dataset. We perform a user study to gauge human opinion on naturalness of the generated question and analyze the word statistics in <ref type="figure">Figure 6</ref>. This is an important test as humans are the best  <ref type="figure">Figure 6</ref>: Sunburst plot for VQG-COCO: The i th ring captures the frequency distribution over words for the i th word of the generated question. The angle subtended at the center is proportional to the frequency of the word. While some words have high frequency, the outer rings illustrate a fine blend of words. We have restricted the plot to 5 rings for easy readability. Best viewed in color.</p><p>deciders of naturalness. We further consider the statistical significance for the various ablations as well as the state-of-the-art models. The quantitative evaluation is conducted using standard metrics like BLEU <ref type="bibr">(Papineni et al., 2002)</ref>, ME-TEOR (Banerjee and Lavie, 2005), ROUGE <ref type="bibr">(Lin, 2004)</ref>, <ref type="bibr">CIDEr (Vedantam et al., 2015)</ref>. Although these metrics have not been shown to correlate with 'naturalness' of the question these still provide a reasonable quantitative measure for comparison. Here we only provide the BLEU1 scores, but the remaining BLEU-n metric scores are present in the supplementary. We observe that the proposed MDN provides improved embeddings to the decoder. We believe that these embeddings capture instance specific differential information that helps in guiding the question generation. Details regarding the metrics are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Analysis</head><p>We considered different variations of our method mentioned in section 4.4 and the various ways to obtain the joint multimodal embedding as described in section 4.1.3. The results for the VQG-COCO test set are given in table 1. In this table, every block provides the results for one of the variations of obtaining the embeddings and different ways of combining them. We observe that the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline and State-of-the-Art</head><p>The comparison of our method with various baselines and state-of-the-art methods is provided in   module mentioned in section 4.1.3 and also against the state-of-the-art methods. The Critical Difference (CD) for Nemenyi <ref type="bibr">(Fišer et al., 2016)</ref> test depends upon the given α (confidence level, which is 0.05 in our case) for average ranks and N (number of tested datasets). If the difference in the rank of the two methods lies within CD, then they are not significantly different and vice-versa. <ref type="figure" target="#fig_5">Figure 7</ref> visualizes the post-hoc analysis using the CD diagram. From the figure, it is clear that MDN-Joint works best and is statistically significantly different from the state-of-the-art methods.  <ref type="bibr">, 2017)</ref>. The colored lines between the two models represents that these models are not significantly different from each other. Here every question has different number of responses and hence the threshold which is the half of total responses for each question is varying. This plot is only for 50 of the 100 questions involved in the survey. See section 5.4 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Perceptual Realism</head><p>A human is the best judge of naturalness of any question, We evaluated our proposed MDN method using a 'Naturalness' Turing test (Zhang et al., 2016) on 175 people. People were shown an image with 2 questions just as in <ref type="figure" target="#fig_0">figure 1</ref> and were asked to rate the naturalness of both the questions on a scale of 1 to 5 where 1 means 'Least Natural' and 5 is the 'Most Natural'. We provided 175 people with 100 such images from the VQG-COCO validation dataset which has 1250 images. <ref type="figure" target="#fig_6">Figure 8</ref> indicates the number of people who were fooled (rated the generated question more or equal to the ground truth question). For the 100 images, on an average 59.7% people were fooled in this experiment and this shows that our model is able to generate natural questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have proposed a novel method for generating natural questions for an image. The approach relies on obtaining multimodal differential embeddings from image and its caption. We also provide ablation analysis and a detailed comparison with state-of-the-art methods, perform a user study to evaluate the naturalness of our generated questions and also ensure that the results are statistically significant. In future, we would like to analyse means of obtaining composite embeddings. We also aim to consider the generalisation of this approach to other vision and language tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>Section B will provide details about training configuration for MDN, Section C will explain the various Proposed Methods and we also provide a discussion in section D regarding some important questions related to our method. We report BLEU1, BLEU2, BLEU3, BLEU4, METEOR, ROUGE and CIDER metric scores for VQG-COCO dataset. We present different experiments with Tag Net in which we explore the performance of various tags (Noun, Verb, and Question tags) and different ways of combining them to get the context vectors.</p><p>Algorithm 1 Multimodal Differential Network 1: procedure MDN(x i ) 2: Finding Exemplars:</p><p>3:  </p><formula xml:id="formula_9">x + i , x − i := KD − T ree(x i ) 4: c i , c + i , c − i :=Extract caption(x i , x + i , x − i ) 5: Compute Triplet Embedding: 6: g i , g + i , g − i := T riplet CN N (x i , x + i , x − i ) 7: f i , f + i , f − i :=T riplet LST M (c i , c + i , c − i ) 8: Compute</formula><formula xml:id="formula_10">Ajnt = tanh(W ij G img (W cj F cap +b j )) 27: S emb = tanh(W A A jnt + b A ),</formula><formula xml:id="formula_11">h att = tanh(W I G img + (W C F cap + b c )) 31: P att = Softmax(W P h att + b P ) 32: V att = i P att (i)G img (i)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>33:</head><p>A att = V att + f i 34:</p><p>S emb = tanh(W A A att + b A ) 35: Return S emb 36: end procedure     <ref type="table">Table 6</ref>: VQG-COCO-dataset, Analysis of different number of Exemplars for addition model, hadamard model and joint model, R is random exemplar. All these experiment are for the differential image network. k=5 performs the best and hence we use this value for the results in main paper.  <ref type="bibr">, 2017)</ref>. Also the colored lines between two models represent that those models are not significantly different from each other.</p><p>testing images. Each image in the dataset contains 5 natural questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training Configuration</head><p>We have used RMSPROP optimizer to update the model parameter and configured hyperparameter values to be as follows: learning rate = 0.0004, batch size = 200, α = 0.99, = 1e − 8 to train the classification network . In order to train a triplet model, we have used RM-SPROP to optimize the triplet model model parameter and configure hyper-parameter values to be: learning rate = 0.001, batch size = 200, α = 0.9, = 1e − 8. We also used learning rate decay to decrease the learning rate on every epoch by a factor given by:</p><formula xml:id="formula_12">Decay f actor = exp log(0.1) a * b</formula><p>where values of a=1500 and b=1250 are set empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Analysis of Model</head><p>While, we advocate the use of multimodal differential network (MDN) for generating embeddings that can be used by the decoder for generating questions, we also evaluate several variants of this architecture namely (a) Differential Image Network, (b) Tag net and (c) Place net. These are described in detail as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Differential Image Network</head><p>For obtaining the exemplar image based context embedding, we propose a triplet network consist of three network, one is target net, supporting net and opposing net. All these three networks designed with convolution neural network and shared the same parameters. The weights of this network are learnt through end-to-end learning using a triplet loss. The aim is to obtain latent weight vectors that bring the supporting exemplar close to the target image and enhances the difference between opposing examples. More formally, given an image x i we obtain an embedding g i using a CNN that we parameterize through a function G(x i , W c ) where W c are the weights of the CNN. This is illustrated in figure 11.   <ref type="table">Table 7</ref>: Full State-of-the-Art comparison on VQG-COCO Dataset. The first block consists of the state-of-the-art results, second block refers to the baselines mentioned in State-of-the-art section of main paper and the third block provides the results for the best method for different ablations of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Tag net</head><p>The tag net consists of two parts Context Extractor &amp; Tag Embedding Net. This is illustrated in <ref type="figure" target="#fig_0">figure 12</ref>.</p><p>Extract Context: The first step is to extract the caption of the image using NeuralTalk2 <ref type="bibr">(Karpathy et al., 2014)</ref> model. We find the part-ofspeech(POS) tag present in the caption. POS taggers have been developed for two well known corpuses, the Brown Corpus and the Penn Treebanks. For our work, we are using the Brown Corpus tags. The tags are clustered into three category namely Noun tag, Verb tag and Question tags (What, Where, . . . ). Noun tag consists of all the noun &amp; pronouns present in the caption sentence and similarly, verb tag consists of verb &amp; adverbs present in the caption sentence. The question tags consists of the 7-well know question words i.e., why, how, what, when, where, who and which. Each tag token is represented as a one-hot vector of the dimension of vocabulary size. For generalization, we have considered 5 tokens from each category of the Tags.</p><p>Tag Embedding Net: The embedding network consists of word embedding followed by temporal convolutions neural network followed by maxpooling network. In the first step, sparse high dimensional one-hot vector is transformed to dense low dimension vector using word embedding. After this, we apply temporal convolution on the word embedding vector. The uni-gram, bi-gram and tri-gram feature are computed by applying convolution filter of size 1, 2 and 3 respectability. Finally, we applied max-pooling on this to get a vector representation of the tags as shown figure 12. We concatenated all the tag words fol-lowed by fully connected layer to get feature dimension of 512. We also explored joint networks based on concatenation of all the tags, on elementwise addition and element-wise multiplication of the tag vectors. However, we observed that convolution over max pooling and joint concatenation gives better performance based on CIDer score.</p><formula xml:id="formula_13">F C = Tag CNN(C t )</formula><p>Where, T CNN is Temporally Convolution Neural Network applied on word embedding vector with kernel size three. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Place net</head><p>Visual object and scene recognition plays a crucial role in the image. Here, places in the image are labeled with scene semantic categories <ref type="bibr">(Zhou et al., 2017)</ref>, comprise of large and diverse type of environment in the world, such as (amusement park, tower, swimming pool, shoe shop, cafeteria, rain-forest, conference center, fish pond, etc.). So we have used different type of scene semantic categories present in the image as a place based context to generate natural question. A place365 is a convolution neural network is modeled to classify 365 types of scene categories, which is trained on the place2 dataset consist of 1.8 million of scene images. We have used a pre-trained VGG16-places365 network to obtain place based context embedding feature for various type scene categories present in the image. The context feature F C is obtained by:</p><formula xml:id="formula_14">F C = w * p conv(I) + b</formula><p>Where, p conv is Place365 CNN. We have extracted conv5 features of dimension 14x14x512 for attention model and FC8 features of dimension 365 for joint, addition and hadamard model of places365. Finally, we use a linear transformation to obtain a 512 dimensional vector.</p><p>We explored using the CONV5 having feature dimension 14x14 512, FC7 having 4096 and FC8 having feature dimension of 365 of places365.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Sampling Exemplar: KNN vs ITML</head><p>Our method is aimed at using efficient exemplarbased retrieval techniques. We have experimented with various exemplar methods, such as ITML <ref type="bibr">(Davis et al., 2007)</ref> based metric learning for image features and KNN based approaches. We observed KNN based approach (K-D tree) with Euclidean metric is a efficient method for finding exemplars. Also we observed that ITML is computationally expensive and also depends on the training procedure. <ref type="table">The table provides</ref>   <ref type="table">Table 8</ref>: VQG-COCO-dataset, Analysis of different methods of finding Exemplars for hadamard model. ITML vs KNN based methods. We see that both give more or less similar results but since ITML is computationally expensive and the dataset size is also small, it is not that efficient for our use. All these experiment are for the differential image network for K=2 only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Question Generation approaches: Sampling vs Argmax</head><p>We obtained the decoding using standard practice followed in the literature <ref type="bibr">(Sutskever et al., 2014)</ref>. This method selects the argmax sentence. Also, we evaluated our method by sampling from the probability distributions and provide the results for our proposed MDN-Joint method for VQG dataset as follows:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 How are exemplars improving Embedding</head><p>In Multimodel differential network, we use exemplars and train them using a triplet loss. It is known that using a triplet network, we can learn a representation that accentuates how the image is closer to a supporting exemplar as against the opposing exemplar (Hoffer and <ref type="bibr">Ailon, 2015;</ref><ref type="bibr">Frome et al., 2007)</ref>. The Joint embedding is obtained between the image and language representations. Therefore the improved representation helps in obtaining an improved context vector. Further we show that this also results in improving VQG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Are exemplars required?</head><p>We had similar concerns and validated this point by using random exemplars for the nearest neighbor for MDN. (k=R in table 6) In this case the method is similar to the baseline. This suggests that with random exemplar, the model learns to ignore the cue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Are captions necessary for our method?</head><p>This is not actually necessary. In our method, we have used an existing image captioning method <ref type="bibr">(Karpathy and Fei-Fei, 2015)</ref> to generate captions for images that did not have them. For VQG dataset, captions were available and we have used that, but, for VQA dataset captions were not available and we have generated captions while training. We provide detailed evidence with respect to caption-question pairs to ensure that we are generating novel questions. While the caption generates scene description, our proposed method generates semantically meaningful and novel questions. Examples for <ref type="figure" target="#fig_0">Figure 1</ref> of main paper: First Image:-Caption-A young man skateboarding around little cones. Our Question-Is this a skateboard competition? Second Image:-Caption-A small child is standing on a pair of skis. Our Question:-How old is that little girl? D.6 Intuition behind Triplet Network:</p><p>The intuition behind use of triplet networks is clear through this paper <ref type="bibr">(Frome et al., 2007)</ref> that first advocated its use. The main idea is that when we learn distance functions that are close for similar and far from dissimilar representations, it is not clear that close and far are with respect to what measure. By incorporating a triplet we learn distance functions that learn that A is more similar to B as compared to C. Learning such measures allows us to bring target image-caption joint embeddings that are closer to supporting exemplars as compared to contrasting exemplars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Analysis of Network E.1 Analysis of Tag Context</head><p>Tag is language based context. These tags are extracted from caption, except question-tags which is fixed as the 7 'Wh words' (What, Why, Where, Who, When, Which and How). We have experimented with Noun tag, Verb tag and 'Wh-word' tag as shown in tables. Also, we have experimented in each tag by varying the number of tags from 1 to 7. We combined different tags using 1Dconvolution, concatenation, and addition of all the tags and observed that the concatenation mechanism gives better results. As we can see in the table 4 that taking Nouns, Verbs and Wh-Words as context, we achieve significant improvement in the BLEU, METEOR and CIDEr scores from the basic models which only takes the image and the caption respectively. Taking Nouns generated from the captions and questions of the corresponding training example as context, we achieve an increase of 1.6% in Bleu Score and 2% in METEOR and 34.4% in CIDEr Score from the basic Image model. Similarly taking Verbs as context gives us an increase of 1.3% in Bleu Score and 2.1% in METEOR and 33.5% in CIDEr Score from the basic Image model. And the best result comes when we take 3 Wh-Words as context and apply the Hadamard Model with concatenating the 3 WH-words. Also in <ref type="table" target="#tab_9">Table 5</ref> we have shown the results when we take more than one words as context. Here we show that for 3 words i.e 3 nouns, 3 verbs and 3 Wh-words, the Concatenation model performs the best. In this table the conv model is using 1D convolution to combine the tags and the joint model combine all the tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Analysis of Context: Exemplars</head><p>In Multimodel Differential Network and Differential Image Network, we use exemplar images(target, supporting and opposing image) to obtain the differential context. We have performed the experiment based on the single exemplar(K=1), which is one supporting and one opposing image along with target image, based on two exemplar(K=2), i.e. two supporting and two opposing image along with single target image. similarly we have performed experiment for K=3 and K=4 as shown in table-6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Mixture Module: Other Variations</head><p>Hadamard method uses element-wise multiplication whereas Addition method uses element-wise addition in place of the concatenation operator of the Joint method. The Hadamard method finds a correlation between image feature and caption feature vector while the Addition method learns a resultant vector. In the attention method, the output S i is the weighted average of attention probability vector P att and convolutional features G img . The attention probability vector weights the contribution of each convolutional feature based on the caption vector. This attention method is similar to work stack attention method <ref type="bibr">(Yang et al., 2016)</ref>. The attention mechanism is given by:</p><formula xml:id="formula_15">h att = tanh(W I G img ⊕ (W C F cap + b c )) P att = Softmax(W T P h att + b P ) V att = i P att (i)G img (i) A att = V att + f i s i = tanh(W A A att + b A )<label>(7)</label></formula><p>where G img is the 14x14x512-dimensional convolution feature map from the fifth convolution layer of VGG-19 Net of image X i and f i is the caption context vector. The attention probability vector P att is a 196-dimensional vector. W I , W C , W P are the weights and b c , b A , b c is the bias for different layers. We evaluate the different approaches and provide results for the same. Here ⊕ represents element-wise addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Evaluation Metrics</head><p>Our task is similar to encoder -decoder framework of machine translation. we have used same evaluation metric is used in machine translation. <ref type="bibr">BLEU(Papineni et al., 2002)</ref> is the first metric to find the correlation between generated question with ground truth question. BLEU score is used to measure the precision value, i.e That is how much words in the predicted question is appeared in reference question. BLEU-n score measures the n-gram precision for counting cooccurrence on reference sentences. we have evaluated BLEU score from n is 1 to 4. The mechanism of <ref type="bibr">ROUGE-n(Lin, 2004)</ref> score is similar to BLEU-n,where as, it measures recall value instead of precision value in BLEU. That is how much words in the reference question is appeared in predicted question.Another version ROUGE metric is ROUGE-L, which measures longest common sub-sequence present in the generated question. METEOR(Banerjee and Lavie, 2005) score is another useful evaluation metric to calculate the similarity between generated question with reference one by considering synonyms, stemming and paraphrases. the output of the METEOR score measure the word matches between predicted question and reference question. In VQG, it compute the word match score between predicted question with five reference question. CIDer <ref type="bibr">(Vedantam et al., 2015)</ref> score is a consensus based evaluation metric. It measure human-likeness, that is the sentence is written by human or not. The consensus is measured, how often n-grams in the predicted question are appeared in the reference question. If the n-grams in the predicted question sentence is appeared more frequently in reference question then question is less informative and have low CIDer score. We provide our results using all these metrics and compare it with existing baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Can you guess which among the given questions is human annotated and which is machine generated? 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>This is an overview of our Multimodal Differential Network for Visual Question Generation. It consists of a Representation Module which extracts multimodal features, a Mixture Module that fuses the multimodal representation and a Decoder that generates question using an LSTM based language model. In this figure, we have shown the Joint Mixture Module. We train our network with a Cross-Entropy and Triplet Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4. Similarly we obtain image embeddings g s &amp; g c and caption embeddings f s &amp; f c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>These are some examples from the VQG-COCO dataset which provide a comparison between our generated questions and human annotated questions. (a) is the human annotated question for all the images. More qualitative results are present in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The mean rank of all the models on the basis of METEOR score are plotted on the x-axis. Here Joint refers to our MDN-Joint model and others are the different variations described in section 4.1.3 and Natural (Mostafazadeh et al., 2016), Creative (Jain et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Perceptual Realism Plot for human survey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Triplet Fusion Embedding : 9:s i = T riplet F usion(g i , f i , Joint) 10: s + i = T riplet F usion(g s , f s , Joint) 11: s − i = T riplet F usion(g c , f c , Joint) 12: Compute Triplet Loss: 13: Loss T riplet = triplet loss(s i , s + i , s − i ) 14: ComputeDecode Question Sentence: 15:ŷ = Generating LST M (s i , h i , c i ) 16: loss = Cross Entropy(y,ŷ) 17: end procedure 18: ------------------19: procedure TRIPLET FUSION(g i ,f i , f lag) 20: g i :Image feature,14x14x512 21: f i : Caption feature,1x512 22: Match Dimension: 23: G img = reshape(g i ),196x512 24: F caps = clone(f i ) 196x512 25: If flag==Joint Fusion: 26:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>These are some more examples from the VQG-COCO dataset which provide a comparison between the questions generated by our model and human annotated questions. (b) is the human annotated question for the first row-fourth column, &amp; fifth column image and (a) for the rest of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>The mean rank of all the models on the basis of BLEU score are plotted on the x-axis. Here Joint refers to our MDN-Joint model and others are the different variations of our model and Natural-(Mostafazadeh et al., 2016), Creative-(Jain et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Differential Image Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Illustration of Tag Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). W ij , W cj , W j are the weights and b j is the bias for different layers. is the concatenation operator.</figDesc><table><row><cell>Similarly, We obtain context vectors s + i &amp; s − i</cell></row><row><cell>for the supporting and contrasting exemplars. De-</cell></row><row><cell>tails for other fusion methods are present in sup-</cell></row><row><cell>plementary.The aim of the triplet network (Schroff</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Joint Method (JM) of combining the embeddings works the best in all cases except the Tag Embeddings. Among the ablations, the proposed MDN method works way better than the other variants in terms of BLEU, METEOR and ROUGE metrics by achieving an improvement of 6%, 12% and 18% in the scores respectively over the best other variant.</figDesc><table><row><cell>Emb.</cell><cell>Method</cell><cell>BLEU1</cell><cell>METEOR</cell><cell>ROUGE</cell><cell>CIDEr</cell></row><row><cell>Tag</cell><cell>AtM</cell><cell>22.4</cell><cell>8.6</cell><cell>22.5</cell><cell>20.8</cell></row><row><cell>Tag</cell><cell>HM</cell><cell>24.4</cell><cell>10.8</cell><cell>24.3</cell><cell>55.0</cell></row><row><cell>Tag</cell><cell>AM</cell><cell>24.4</cell><cell>10.6</cell><cell>23.9</cell><cell>49.4</cell></row><row><cell>Tag</cell><cell>JM</cell><cell>22.2</cell><cell>10.5</cell><cell>22.8</cell><cell>50.1</cell></row><row><cell>PlaceCNN</cell><cell>AtM</cell><cell>24.4</cell><cell>10.3</cell><cell>24.0</cell><cell>51.8</cell></row><row><cell>PlaceCNN</cell><cell>HM</cell><cell>24.0</cell><cell>10.4</cell><cell>24.3</cell><cell>49.8</cell></row><row><cell>PlaceCNN</cell><cell>AM</cell><cell>24.1</cell><cell>10.6</cell><cell>24.3</cell><cell>51.5</cell></row><row><cell>PlaceCNN</cell><cell>JM</cell><cell>25.7</cell><cell>10.8</cell><cell>24.5</cell><cell>56.1</cell></row><row><cell>Diff. Img</cell><cell>AtM</cell><cell>20.5</cell><cell>8.5</cell><cell>24.4</cell><cell>19.2</cell></row><row><cell>Diff. Img</cell><cell>HM</cell><cell>23.6</cell><cell>8.6</cell><cell>22.3</cell><cell>22.0</cell></row><row><cell>Diff. Img</cell><cell>AM</cell><cell>20.6</cell><cell>8.5</cell><cell>24.4</cell><cell>19.2</cell></row><row><cell>Diff. Img</cell><cell>JM</cell><cell>30.4</cell><cell>11.7</cell><cell>22.3</cell><cell>22.8</cell></row><row><cell>MDN</cell><cell>AtM</cell><cell>22.4</cell><cell>8.8</cell><cell>24.6</cell><cell>22.4</cell></row><row><cell>MDN</cell><cell>HM</cell><cell>26.6</cell><cell>12.8</cell><cell>30.1</cell><cell>31.4</cell></row><row><cell>MDN</cell><cell>AM</cell><cell>29.6</cell><cell>15.4</cell><cell>32.8</cell><cell>41.6</cell></row><row><cell>MDN (Ours)</cell><cell>JM</cell><cell>36.0</cell><cell>23.4</cell><cell>41.8</cell><cell>50.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Analysis of variants of our proposed method</cell></row><row><cell>on VQG-COCO Dataset as mentioned in section 4.4</cell></row><row><cell>and different ways of getting a joint embedding (Atten-</cell></row><row><cell>tion (AtM), Hadamard (HM), Addition (AM) and Joint</cell></row><row><cell>(JM) method as given in section 4.1.3) for each method.</cell></row><row><cell>Refer section 5.1 for more details.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table 2 for</head><label>2</label><figDesc>VQA 1.0 and table 3 for VQG-COCO dataset. The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question. In both the tables, the first block consists of the current stateof-the-art methods on that dataset and the second contains the baselines. We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics. We improve over the previous state-of-the-art(Yang et al., 2015)   for VQA dataset by around 6% in BLEU score and 10% in METEOR score. In the VQG-COCO dataset, we improve over(Mostafazadeh et al.,  2016)  by 3.7% and (Jain et al., 2017) by 3.5% in terms of METEOR scores.</figDesc><table><row><cell cols="4">5.3 Statistical Significance Analysis</cell><cell></cell></row><row><cell>We</cell><cell>have</cell><cell>analysed</cell><cell>Statistical</cell><cell>Signifi-</cell></row><row><cell cols="5">cance (Demšar, 2006) of our MDN model</cell></row><row><cell cols="5">for VQG for different variations of the mixture</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>State-of-the-Art comparison on VQA-1.0 Dataset. The first block consists of the state-of-the-art results, second block refers to the baselines mentioned in section 5.2, third block provides the results for the variants of mixture module present in section 4.1.3.</figDesc><table><row><cell>Context</cell><cell>BLEU1</cell><cell>METEOR</cell><cell>ROUGE</cell><cell>CIDEr</cell></row><row><cell>Natural2016</cell><cell>19.2</cell><cell>19.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Creative2017</cell><cell>35.6</cell><cell>19.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Image Only</cell><cell>20.8</cell><cell>8.6</cell><cell>22.6</cell><cell>18.8</cell></row><row><cell>Caption Only</cell><cell>21.1</cell><cell>8.5</cell><cell>25.9</cell><cell>22.3</cell></row><row><cell>Tag-Hadamard</cell><cell>24.4</cell><cell>10.8</cell><cell>24.3</cell><cell>55.0</cell></row><row><cell>PlaceCNN-Joint</cell><cell>25.7</cell><cell>10.8</cell><cell>24.5</cell><cell>56.1</cell></row><row><cell>Diff.Image-Joint</cell><cell>30.4</cell><cell>11.7</cell><cell>26.3</cell><cell>38.8</cell></row><row><cell>MDN-Joint (Ours)</cell><cell>36.0</cell><cell>23.4</cell><cell>41.8</cell><cell>50.7</cell></row><row><cell>Humans2016</cell><cell>86.0</cell><cell>60.8</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>State-of-the-Art (SOTA) comparison on VQG- COCO Dataset. The first block consists of the SOTA results, second block refers to the baselines mentioned in section 5.2, third block shows the results for the best method for different ablations mentioned in table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Hugo Larochelle, and Aaron Courville. 2017. Guesswhat?! visual object discovery through multi-modal dialogue. In Proc. of CVPR. NIPS'14, pages 3104-3112. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566-4575.</figDesc><table><row><cell>Elad Hoffer and Nir Ailon. 2015. Deep metric learning Conference on Neural Information Processing Sys-</cell><cell>Harm De Vries, Florian Strub, Sarath Chandar, Olivier (Volume 1: Long Papers), volume 1, pages 1802-</cell></row><row><cell>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. 2014. Neural machine translation by jointly using triplet network. In International Workshop on Similarity-Based Pattern Recognition, pages 84-92. Springer. Unnat Jain, Ziyu Zhang, and Alexander G Schwing. 2017. Creativity: Generating diverse questions using variational autoencoders. In IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR). tems, Oriol Vinyals, Alexander Toshev, Samy Bengio, and</cell><cell>Pietquin, Janez Demšar. 2006. Statistical comparisons of clas-sifiers over multiple data sets. Journal of Machine learning research, 7(Jan):1-30. Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-1813. Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. 2016. Image question answering using con-volutional neural network with dynamic parameter prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 30-38.</cell></row><row><cell>learning to align and translate. arXiv preprint Dumitru Erhan. 2015. Show and tell: A neural im-</cell><cell>ing to ask: Neural question generation for reading Kishore Papineni, Salim Roukos, Todd Ward, and Wei-</cell></row><row><cell>arXiv:1409.0473. Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016. age caption generator. In Proceedings of the IEEE</cell><cell>comprehension. arXiv preprint arXiv:1705.00106. Jing Zhu. 2002. Bleu: a method for automatic eval-</cell></row><row><cell>Densecap: Fully convolutional localization net-Conference on Computer Vision and Pattern Recog-</cell><cell>uation of machine translation. In Proceedings of</cell></row><row><cell>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An works for dense captioning. In Proceedings of the nition, pages 3156-3164.</cell><cell>Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Sri-the 40th annual meeting on association for compu-</cell></row><row><cell>IEEE Conference on Computer Vision and Pattern automatic metric for mt evaluation with improved Recognition, pages 4565-4574. Huijuan Xu and Kate Saenko. 2016. Ask, attend and correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, volume 29, pages 65-72. answer: Exploring question-guided spatial attention Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-for visual question answering. In European Confer-tions. In Proceedings of the IEEE conference semantic alignments for generating image descrip-ence on Computer Vision, pages 451-466. Springer.</cell><cell>vastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xi-tational linguistics, pages 311-318. Association for aodong He, Margaret Mitchell, John Platt, et al. Computational Linguistics. 2015. From captions to visual concepts and back. Badri Patro and Vinay P Namboodiri. 2018. Differen-In Proceedings of the IEEE conference on computer ceedings of the IEEE conference on computer vision vision and pattern recognition. tial attention for visual question answering. In Pro-</cell></row><row><cell>on computer vision and pattern recognition, pages Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, 3128-3137. Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Andrej Karpathy, Armand Joulin, and Fei Fei F Li. Neural image caption generation with visual at-2014. Deep fragment embeddings for bidirectional tention. In International Conference on Machine formation processing systems, pages 1889-1897. image sentence mapping. In Advances in neural in-Learning, pages 2048-2057.</cell><cell>and pattern recognition, pages 7680-7688. Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Mengye Ren, Ryan Kiros, and Richard Zemel. 2015. Hockenmaier, and David Forsyth. 2010. Every pic-Exploring models and data for image question an-ture tells a story: Generating sentences from images. swering. In Advances in Neural Information Pro-In European conference on computer vision, pages 15-29. Springer. cessing Systems, pages 2953-2961.</cell></row><row><cell>Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-Lee. 2016. Attribute2image: Conditional image ing Li, Yejin Choi, Alexander C Berg, and Tamara L generation from visual attributes. In European Berg. 2011. Baby talk: Understanding and generat-Conference on Computer Vision, pages 776-791.</cell><cell>Florian Schroff, Dmitry Kalenichenko, and James Darja Fišer, Tomaž Erjavec, and Nikola Ljubešić. Philbin. 2015. Facenet: A unified embedding for 2016. Janes v0. 4: Korpus slovenskih spletnih face recognition and clustering. In Proceedings of uporabniških vsebin. Slovenščina, 2(4):2. the IEEE Conference on Computer Vision and Pat-</cell></row><row><cell>ing image descriptions. In Proceedings of the 24th Springer. CVPR. Citeseer. Yezhou Yang, Yi Li, Cornelia Fermuller, and Yiannis Chin-Yew Lin. 2004. Rouge: A package for auto-Aloimonos. 2015. Neural self talk: Image under-matic evaluation of summaries. In Text summariza-standing via continuous questioning and answering. shop. tion branches out:Proceedings of the ACL-04 work-arXiv preprint arXiv:1512.03460.</cell><cell>tern Recognition, pages 815-823. Andrea Frome, Yoram Singer, Fei Sha, and Jitendra Malik. 2007. Learning globally-consistent local dis-Iulian Vlad Serban, Alberto García-Durán, Caglar tance functions for shape-based image retrieval and Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron classification. In Computer Vision, 2007. ICCV Courville, and Yoshua Bengio. 2016. Generating 2007. IEEE 11th International Conference on, pages The 30m factoid question-answer corpus. arXiv 1-8. IEEE. factoid questions with recurrent neural networks:</cell></row><row><cell>Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Tsung-Yi Lin, Michael Maire, Serge Belongie, James and Alex Smola. 2016. Stacked attention networks Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, for image question answering. In Proceedings of the and C Lawrence Zitnick. 2014. Microsoft coco: IEEE Conference on Computer Vision and Pattern ence on Computer Vision, pages 740-755. Springer. Common objects in context. In European Confer-Recognition, pages 21-29.</cell><cell>preprint arXiv:1603.06807. Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Kevin J Shih, Saurabh Singh, and Derek Hoiem. 2016. 2016. Multimodal compact bilinear pooling for Where to look: Focus regions for visual question visual question answering and visual grounding. on Computer Vision and Pattern Recognition, pages arXiv preprint arXiv:1606.01847. answering. In Proceedings of the IEEE Conference</cell></row><row><cell>Richard Zhang, Phillip Isola, and Alexei A Efros. 2016. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Colorful image colorization. In European Confer-attention for visual question answering. In Advances Parikh. 2016. Hierarchical question-image co-ence on Computer Vision, pages 649-666. Springer.</cell><cell>4613-4621. Siddha Ganju, Olga Russakovsky, and Abhinav Gupta. 2017. What's in a question: Using visual questions deep convolutional networks for large-scale image as a form of supervision. In CVPR. Karen Simonyan and Andrew Zisserman. 2014. Very</cell></row><row><cell>In Neural Information Processing Systems, pages Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude 289-297. Oliva, and Antonio Torralba. 2017. Places: A 10 million image database for scene recognition. IEEE Lin Ma, Zhengdong Lu, and Hang Li. 2016. Learning Transactions on Pattern Analysis and Machine In-neural network. In Thirtieth AAAI Conference on to answer questions from image using convolutional telligence.</cell><cell>recognition. arXiv preprint arXiv:1409.1556. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to a Richard Socher, Andrej Karpathy, Quoc V Le, Christo-machine? dataset and methods for multilingual im-pher D Manning, and Andrew Y Ng. 2014. age question. In Advances in Neural Information describing images with sentences. Transactions Processing Systems, pages 2296-2304. Grounded compositional semantics for finding and</cell></row><row><cell>Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M.F. Moura, Devi Parikh, and Dhruv Batra. 2017. Visual Dialog. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Artificial Intelligence. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question answering Mateusz Malinowski and Mario Fritz. 2014. A multi-world approach to question answering about real-world scenes based on uncertain input. In Advances in Neural Information Processing Systems. in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4995-5004.</cell><cell>of the Association of Computational Linguistics, Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes. 2015. Visual turing test for com-puter vision systems. Proceedings of the National Academy of Sciences of the United States of Amer-ica, 112(12):3618-3623. 2(1):207-218. Florian Strub, Harm De Vries, Jeremie Mary, Bilal Piot, Aaron Courville, and Olivier Pietquin. 2017. End-to-end optimization of goal-driven and visu-</cell></row><row><cell></cell><cell>ally grounded dialogue systems. arXiv preprint</cell></row><row><cell></cell><cell>arXiv:1703.05423.</cell></row><row><cell></cell><cell>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.</cell></row><row><cell></cell><cell>770-Sequence to sequence learning with neural net-</cell></row><row><cell></cell><cell>778. works. In Proceedings of the 27th International</cell></row></table><note>K Barnard, P Duygulu, and D Forsyth. 2003. N. de freitas, d. Blei, and MI Jordan," Matching Words and Pictures", submitted to JMLR. Alan R Chappell, Andrew J Cowell, David A Thur- man, and Judi R Thomson. 2004. Supporting mutual understanding in a visual dialogue between analyst and computer. In Proceedings of the Human Fac- tors and Ergonomics Society Annual Meeting, vol- ume 48, pages 376-380. SAGE Publications Sage CA: Los Angeles, CA. Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, and Devi Parikh. 2017. Evaluating vi- sual conversational agents via cooperative human- ai games. In Proceedings of the Fifth AAAI Con- ference on Human Computation and Crowdsourcing (HCOMP). Xinlei Chen and C Lawrence Zitnick. 2015. Mind's eye: A recurrent visual representation for image cap- tion generation. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2422-2431. Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. 2016. Human Atten- tion in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In Con- ference on Empirical Methods in Natural Language Processing (EMNLP).Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. 2007. Information-theoretic metric learning. In Proceedings of the 24th interna- tional conference on Machine learning, pages 209- 216. ACM.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pagesNasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar- garet Mitchell, Xiaodong He, and Lucy Vander- wende. 2016. Generating natural questions about an image. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Context</cell><cell>BLEU-1</cell><cell>Meteor</cell><cell>Rouge</cell><cell>CIDer</cell></row><row><cell></cell><cell>Tag-n3-add</cell><cell>22.4</cell><cell>9.1</cell><cell>22.2</cell><cell>26.7</cell></row><row><cell></cell><cell>Tag-n3-con</cell><cell>24.8</cell><cell>10.6</cell><cell>24.4</cell><cell>53.2</cell></row><row><cell></cell><cell>Tag-n3-joint</cell><cell>22.1</cell><cell>8.9</cell><cell>21.7</cell><cell>24.6</cell></row><row><cell></cell><cell>Tag-n3-conv</cell><cell>24.1</cell><cell>10.3</cell><cell>24.0</cell><cell>47.9</cell></row><row><cell></cell><cell>Tag-v3-add</cell><cell>24.1</cell><cell>10.2</cell><cell>23.9</cell><cell>46.7</cell></row><row><cell></cell><cell>Tag-v3-con</cell><cell>24.5</cell><cell>10.7</cell><cell>24.2</cell><cell>52.3</cell></row><row><cell></cell><cell>Tag-v3-joint</cell><cell>22.5</cell><cell>9.1</cell><cell>22.1</cell><cell>25.6</cell></row><row><cell></cell><cell>Tag-v3-conv</cell><cell>23.2</cell><cell>9.0</cell><cell>24.2</cell><cell>38.0</cell></row><row><cell></cell><cell>Tag-q3-add</cell><cell>24.5</cell><cell>10.5</cell><cell>24.4</cell><cell>51.4</cell></row><row><cell></cell><cell>Tag-q3-con</cell><cell>24.6</cell><cell>10.8</cell><cell>24.3</cell><cell>55.0</cell></row><row><cell></cell><cell>Tag-q3-joint</cell><cell>22.1</cell><cell>9.0</cell><cell>22.0</cell><cell>25.9</cell></row><row><cell></cell><cell>Tag-q3-conv</cell><cell>24.3</cell><cell>10.4</cell><cell>24.0</cell><cell>48.6</cell></row><row><cell cols="2">: Analysis of different Tags for VQG-COCO-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dataset. We analyse noun tag (Tag-n), verb tag (Tag-v)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and question tag (Tag-wh) for different fusion methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">namely joint, attention, Hadamard and addition based</cell><cell></cell><cell></cell><cell></cell></row><row><cell>fusion.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">B Dataset and Training Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B.1 Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">We conduct our experiments on two types of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dataset: VQA dataset (Antol et al., 2015), which</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">contains human annotated questions based on</cell><cell></cell><cell></cell><cell></cell></row><row><cell>images on MS-COCO dataset.</cell><cell>Second one</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">is VQG-COCO dataset based on natural ques-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tion (Mostafazadeh et al., 2016).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>B.1.1 VQA dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">VQA dataset(Antol et al., 2015) is built on com-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">plex images from MS-COCO dataset. It contains</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Combination of 3 tags of each category for</cell></row><row><cell>hadamard mixture model namely addition, concatena-</cell></row><row><cell>tion, multiplication and 1d-convolution</cell></row><row><cell>a total of 204721 images, out of which 82783 are</cell></row><row><cell>for training, 40504 for validation and 81434 for</cell></row><row><cell>testing. Each image in the MS-COCO dataset is</cell></row><row><cell>associated with 3 questions and each question has</cell></row><row><cell>10 possible answers. So there are 248349 QA pair</cell></row><row><cell>for training, 121512 QA pairs for validating and</cell></row><row><cell>244302 QA pairs for testing. We used pre-trained</cell></row><row><cell>caption generation model (Karpathy et al., 2014)</cell></row><row><cell>to extract captions for VQA dataset.</cell></row><row><cell>B.1.2 VQG dataset</cell></row><row><cell>The VQG-COCO dataset(Mostafazadeh et al.,</cell></row><row><cell>2016), is developed for generating natural and en-</cell></row><row><cell>gaging questions that are based on common sense</cell></row><row><cell>reasoning. This dataset contains a total of 2500</cell></row><row><cell>training images, 1250 validation images and 1250</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>VQG-COCO-dataset, Analysis of question generation approaches:sampling vs Argmax in MDN-Joint model for K=5 only. We see that Argmax clearly outperforms the sampling method.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">The human annotated questions are (b) for the first image and (a) for the second image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The github link for MDN-VQG Model is https:// github.com/MDN-VQG/EMNLP-MDN-VQG</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
