<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Charting the Right Manifold: Manifold Mixup for Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
							<email>msingh@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media and Data Science Research lab</orgName>
								<address>
									<country>Adobe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
							<email>abhsinha@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media and Data Science Research lab</orgName>
								<address>
									<country>Adobe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
							<email>nupkumar@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Media and Data Science Research lab</orgName>
								<address>
									<country>Adobe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
							<email>vineethnb@iith.ac.in</email>
							<affiliation key="aff1">
								<orgName type="institution">IIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Media and Data Science Research lab</orgName>
								<address>
									<country>Adobe</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Charting the Right Manifold: Manifold Mixup for Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help of only a few labeled examples. A recent regularization technique -Manifold Mixup focuses on learning a generalpurpose representation, robust to small changes in the data distribution. Since the goal of few-shot learning is closely linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data. This work investigates the role of learning relevant feature manifold for few-shot tasks using self-supervision and regularization techniques. We observe that regularizing the feature manifold, enriched via self-supervised techniques, with Manifold Mixup significantly improves few-shot learning performance. We show that our proposed method S2M2 beats the current state-of-the-art accuracy on standard fewshot learning datasets like CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet by 3 − 8%. Through extensive experimentation, we show that the features learned using our approach generalize to complex few-shot evaluation tasks, cross-domain scenarios and are robust against slight changes to data distribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional networks (CNN's) have become a regular ingredient for numerous contemporary computer vision tasks. They have been applied to tasks such as object recognition, semantic segmentation, object detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> to achieve state-of-the-art performance. However, the at par performance of deep neural networks * Authors contributed equally † Work done during Adobe MDSR internship requires huge amount of supervisory examples for training. Generally, labeled data is scarcely available and data collection is expensive for several problem statements. Hence, a major research effort is being dedicated to fields such as transfer learning, domain adaptation, semi-supervised and unsupervised learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref> to alleviate this requirement of enormous amount of examples for training. A related problem which operates in the low data regime is few-shot classification. In few-shot classification, the model is trained on a set of classes (base classes) with abundant examples in a fashion that promotes the model to classify unseen classes (novel classes) using few labeled instances. The motivation for this stems from the hypothesis that an appropriate prior should enable the learning algorithm to solve consequent tasks more easily. Biologically speaking, humans have a high capacity to generalize and extend the prior knowledge to solve new tasks using only small amount of supervision. One of the promising approach to few-shot learning utilizes meta-learning framework to optimize for such an initialization of model parameters such that adaptation to the optimal weights of classifier for novel classes can be reached with few gradient updates <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b39">40]</ref>. Some of the work also includes leveraging the information of similarity between images <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16]</ref> and augmenting the training data by hallucinating additional examples <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b55">56]</ref>. Another class of algorithms <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17]</ref> learns to directly predict the weights of the classifier for novel classes.</p><p>Few-shot learning methods are evaluated using N -way K-shot classification framework where N classes are sampled from a set of novel classes (not seen during training) with K examples for each class. Usually, the few-shot classification algorithm has two separate learning phases. In the first phase, the training is performed on base classes to develop robust and general-purpose representation aimed to be useful for classifying novel classes. The second phase of training exploits the learning from previous phase in the arXiv:1907.12087v4 <ref type="bibr">[cs.</ref>LG] 18 Jan 2020 form of a prior to perform classification over novel classes. The transfer learning approach serves as the baseline which involves training a classifier for base classes and then subsequently learning a linear classifier on the penultimate layer of the previous network to classify the novel classes <ref type="bibr" target="#b6">[7]</ref>.</p><p>Learning feature representations that generalize to novel classes is an essential aspect of few-shot learning problem. This involves learning a feature manifold that is relevant for novel classes. Regularization techniques enables the models to generalize to unseen test data that is disjoint from training data. It is frequently used as a supplementary technique alongside standard learning algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b27">28]</ref>. In particular for classification problems, Manifold Mixup <ref type="bibr" target="#b61">[62]</ref> regularization leverages interpolations in deep hidden layer to improve hidden representations and decision boundaries at multiple layers.</p><p>In Manifold Mixup <ref type="bibr" target="#b61">[62]</ref>, the authors show improvement in classification task over standard image deformations and augmentations. Also, some work in self-supervision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b10">11]</ref> explores to predict the type of augmentation applied and enforces feature representation to become invariant to image augmentations to learn robust visual features. Inspired by this link, we propose to unify the training of few-shot classification with self-supervision techniques and Manifold Mixup <ref type="bibr" target="#b61">[62]</ref>. The proposed technique employs self-supervision loss over the given labeled data unlike in semi-supervised setting that uses additional unlabeled data and hence our approach doesn't require any extra data for training.</p><p>Many of the recent advances in few-shot learning exploit the meta-learning framework, which simulates the training phase as that of the evaluation phase in the few-shot setting. However, in a recent study <ref type="bibr" target="#b6">[7]</ref>, it was shown that learning a cosine classifier on features extracted from deeper networks also performs quite well on few-shot tasks. Motivated by this observation, we focus on utilizing self-supervision techniques augmented with Manifold Mixup in the domain of few-shot tasks using cosine classifiers.</p><p>Our main contributions in this paper are the following:</p><p>• We find that the regularization technique of Manifold Mixup <ref type="bibr" target="#b61">[62]</ref> being robust to small changes in data distribution enhances the performance of few-shot tasks.</p><p>• We show that adding self-supervision loss to the training procedure, enables robust semantic feature learning that leads to a significant improvement in few-shot classification. We use rotation <ref type="bibr" target="#b17">[18]</ref> and exemplar <ref type="bibr" target="#b10">[11]</ref> as the self-supervision tasks.</p><p>• We observe that applying Manifold Mixup regularization over the feature manifold enriched via the self-supervision tasks further improves the performance of few-shot tasks. The proposed methodology (S2M2) outperforms the state-of-the-art methods by 3-8% over the CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet datasets.</p><p>• We conduct extensive ablation studies to verify the efficacy of the proposed method. We find that the improvements made by our methodology become more pronounced with increasing N in the N -way K-shot evaluation and also in the cross-domain evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is associated with various recent development made in learning robust general-purpose visual representations, specifically few-shot learning, self-supervised learning and generalization boosting techniques.</p><p>Few-shot learning: Few-shot learning involves building a model using available training data of base classes that can classify unseen novel classes using only few examples. Few-shot learning approaches can be broadly divided into three categories -gradient based methods, distance metric based methods and hallucination based methods.</p><p>Some gradient based methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b0">1]</ref> aim to use gradient descent to quickly adapt the model parameters suitable for classifying the novel task. The initialization based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b39">40]</ref> specifically advocate to learn a suitable initialization of the model parameters, such that adapting from those parameters can be achieved in a few gradient steps. Distance metric based methods leverage the information about similarity between images to classify novel classes with few examples. The distance metric can either be cosine similarity <ref type="bibr" target="#b62">[63]</ref>, euclidean distance <ref type="bibr" target="#b57">[58]</ref>, CNN based distance module <ref type="bibr" target="#b59">[60]</ref>, ridge regression <ref type="bibr" target="#b2">[3]</ref> or graph neural network <ref type="bibr" target="#b15">[16]</ref>. Hallucination based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b55">56]</ref> augment the limited training data for a new task by generating or hallucinating new data points.</p><p>Recently, <ref type="bibr" target="#b6">[7]</ref> introduced a modification for the simple transfer learning approach, where they learn a cosine classifier <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17]</ref> instead of a linear classifier on top of feature extraction layers. The authors show that this simple approach is competitive with several proposed few-shot learning approaches if a deep backbone network is used to extract the feature representation of input data.</p><p>Self-supervised learning: This is a general learning framework which aims to extract supervisory signals by defining surrogate tasks using only the structural information present in the data. In the context of images, a pretext task is designed such that optimizing it leads to more semantic image features that can be useful for other vision tasks. Self-supervision techniques have been successfully applied to diverse set of domains, ranging from robotics to computer vision <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b44">45]</ref>. In the context of visual data, the surrogate loss functions can be derived by leveraging the invariants in the structure of the image. In this paper, we focus on self-supervised learning techniques to enhance the representation and learn a relevant feature manifold for few-shot classification setting. We now briefly describe the recent developments in self-supervision techniques.</p><p>C. Doersch et al. <ref type="bibr" target="#b8">[9]</ref> took inspiration from spatial context of a image to derive supervisory signal by defining the surrogate task of relative position prediction of image patches. Motivated by the task of context prediction, the pretext task was extended to predict the permutation of the shuffled image patches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. <ref type="bibr" target="#b17">[18]</ref> leveraged the rotation invariance of images to create the surrogate task of predicting the rotation angle of the image. Also, the authors of <ref type="bibr" target="#b12">[13]</ref> proposed to decouple representation learning of the rotation as pretext task from class discrimination to obtain better results. Along the lines of context-based prediction, <ref type="bibr" target="#b47">[48]</ref> uses generation of the contents of image region based on context pixel (i.e. in-painting) and in <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70]</ref> the authors propose to use gray-scale image colorization as a pretext task.</p><p>Apart from enforcing structural constraints, <ref type="bibr" target="#b5">[6]</ref> uses cluster assignments as supervisory signals for unlabeled data and works by alternating between clustering of the image descriptors and updating the network by predicting the cluster assignments. <ref type="bibr" target="#b46">[47]</ref> defines pretext task that uses lowlevel motion-based grouping cues to learn visual representation. Also, <ref type="bibr" target="#b41">[42]</ref> proposes to obtain supervision signal by enforcing the additivity of visual primitives in the patches of images and <ref type="bibr" target="#b43">[44]</ref> proposed to learn feature representations by predicting the future in latent space by employing autoregressive models.</p><p>Some of the pretext tasks also work by enforcing constraints on the representation of the feature. A prominent example is the exemplar loss from <ref type="bibr" target="#b10">[11]</ref> that promotes representation of image to be invariant to image augmentations. Additionally, some research effort have also been put in to define the pretext task as a combination of multiple pretext task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>. For instance, in <ref type="bibr" target="#b31">[32]</ref> representation learning is augmented with pretext tasks of jigsaw puzzle <ref type="bibr" target="#b40">[41]</ref>, colorization <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70]</ref> and in-painting <ref type="bibr" target="#b47">[48]</ref>.</p><p>Generalization: Employing regularization techniques for training deep neural networks to improve their generalization performances have become standard practice in the deep learning community. Few of the commonly used regularization techniques are -dropout <ref type="bibr" target="#b58">[59]</ref>, cutout <ref type="bibr" target="#b7">[8]</ref>, Mixup <ref type="bibr" target="#b27">[28]</ref>, Manifold Mixup <ref type="bibr" target="#b61">[62]</ref>. Mixup <ref type="bibr" target="#b27">[28]</ref> is a specific case of Manifold Mixup <ref type="bibr" target="#b61">[62]</ref> where the interpolation of only input data is applied. The authors in <ref type="bibr" target="#b61">[62]</ref> claim that Manifold Mixup leads to smoother decision boundaries and flattens the class representations thereby leading to feature representation that improve the performance over a held-out validation dataset. We apply a few of these generalization techniques during the training of the backbone network over the base tasks and find that the features learned via such regularization lead to better generalization over novel tasks too. Authors of <ref type="bibr" target="#b35">[36]</ref> provide a summary of popular regularization techniques used in deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The few-shot learning setting is formalized by the availability of a dataset with data-label pairs D = {(x i , y i ) : i = 1, · · · , m} where x ∈ R d and y i ∈ C, C being the set of all classes. We have sufficient number of labeled data in a subset of C classes (called base classes), while very few labeled data for the other classes in C (called novel classes). Few-shot learning algorithms generally train in two phases: the first phase consists of training a network over base class data</p><formula xml:id="formula_0">D b = {(x i , y i ), i = 1, · · · , m b } where {y i ∈ C b ⊂ C}</formula><p>to obtain a feature extractor, and the second phase consists of adapting the network for novel class data</p><formula xml:id="formula_1">D n = {(x i , y i ), i = 1, · · · , m n } where {y i ∈ C n ⊂ C} and C b ∪ C n = C.</formula><p>We assume that there are N b base classes (cardinality of C b ) and N n novel classes (cardinality of C n ). The general goal of few-shot learning algorithms is to learn rich feature representations from the abundant labeled data of base classes N b , such that the features can be easily adapted for the novel classes using only few labeled instances.</p><p>In this work, in the first learning stage, we train a N b -way neural network classifier: <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17]</ref> and f θ is the convolutional feature extractor, with θ parametrizing the neural network model. The model is trained with classification loss and an additional auxiliary loss which we explain soon. The second phase involves fine-tuning of the backbone model, f θ , by freezing the feature extractor layers and training a new N n -way cosine classifier c Wn on data from k randomly sampled novel classes in D n with only classification loss. <ref type="figure" target="#fig_0">Figure 1</ref> provides an overview of our approach S2M2 for few-shot learning . Importantly, in our proposed methodology, we leverage self-supervision and regularization techniques <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b10">11]</ref> to learn general-purpose representation suitable for fewshot tasks. We hypothesize that using robust features which describes the feature manifold well is important to obtain better performance over the novel classes in the few-shot setting. In the subsequent subsections, we describe our training procedure to use self-supervision methods (such as rotation <ref type="bibr" target="#b17">[18]</ref> and exemplar <ref type="bibr" target="#b10">[11]</ref>) to obtain a suitable feature manifold, following which using Manifold Mixup regularization <ref type="bibr" target="#b61">[62]</ref> provides a robust feature extractor backbone. We empirically show that this proposed methodology achieves the new state-of-the-art result on standard few-shot learning benchmark datasets.</p><formula xml:id="formula_2">g = c W b • f θ (1) on D b , where c W b is a cosine classifier</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Manifold Mixup for Few-shot Learning</head><p>Higher-layer representations in neural network classifiers have often been visualized as lying on a meaningful manifold, that provide the relevant geometry of data to solve a given task <ref type="bibr" target="#b1">[2]</ref>. Therefore, linear interpolation of feature vectors in that space should be relevant from the perspective of classification. With this intuition, Manifold Mixup <ref type="bibr" target="#b61">[62]</ref>, a recent work, leverages linear interpolations in neural network layers to help the trained model generalize better. In particular, given input data x and x with corresponding feature representations at layer l given by f l θ (x) and f l θ (x ) respectively. Assuming we use Manifold Mixup on the base classes in our work, the loss for training L mm is then formulated as:</p><formula xml:id="formula_3">L mm = E (x,y)∈D b L M ix λ (f l θ (x), f l θ (x )), M ix λ (y, y ) (2) where M ix λ (a, b) = λ · a + (1 − λ) · b<label>(3)</label></formula><p>The mixing coefficient λ is sampled from a β(α, α) distribution and loss L is standard cross-entropy loss. We hypothesize that using Manifold Mixup on the base classes provides robust feature presentations that lead to state-ofthe-art results in few-shot learning benchmarks. Training using loss L mm encourages the model to predict less confidently on linear interpolations of hidden representations. This encourages the feature manifold to have broad regions of low-confidence predictions between different classes and thereby smoother decision boundaries, as shown in <ref type="bibr" target="#b61">[62]</ref>. Also, models trained using this regularizer lead to flattened hidden representations for each class with less number of directions of high variance i.e. the representations of data from each class lie in a lower dimension subspace. The above-mentioned characteristics of the method make it a suitable regularization technique for generalizing to tasks with potential distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Charting the Right Manifold</head><p>We observed that Manifold Mixup does result in higher accuracy on few-shot tasks, as shown in Section 4.2.3. However, it still lags behind existing state-of-the-art performance, which begs the question: Are we charting the right manifold? In few-shot learning, novel classes introduced during test time can have a different data distribution when compared to base classes. In order to counter this distributional shift, we hypothesize that it is important to capture the right manifold when using Manifold Mixup for the base classes. To this end, we leverage self-supervision methods. Self-supervision techniques have been employed recently in many domains for learning rich, generic and meaningful feature representations. We show that the simple idea of adding auxiliary loss terms from self-supervised techniques while training the base classes provides feature representations that significantly outperform state-of-the-art for classifying on the novel classes. We now describe the selfsupervised methods used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Self-Supervision: Towards the Right Manifold</head><p>In this work, we use two pretext tasks that have recently been widely used for self-supervision to support our claim. We describe each of these below.</p><p>Rotation <ref type="bibr" target="#b17">[18]</ref>: In this self-supervised task, the input image is rotated by different angles, and the auxiliary aim of the model is to predict the amount of rotation applied to image. In the image classification setting, an auxiliary loss (based on the predicted rotation angle) is added to the standard classification loss to learn general-purpose representations suitable for image understanding tasks. In this work, we use a 4-way linear classifier, c Wr , on the penultimate feature representation f θ (x r ) where x r is the image x rotated by r degrees and r ∈ C R = {0 • , 90 • , 180 • , 270 • }, to predict one of 4 classes in C R . In other words, similar to Eqn 1, our pretext task model is given by g r = c Wr • f θ . The self-supervision loss is given by:</p><formula xml:id="formula_4">L rot = 1 |C R | * x∈D b r∈C R L(c Wr (f θ (x r )), r)<label>(4)</label></formula><formula xml:id="formula_5">L class = E (x,y)∈D b ,r∈C R L(x r , y)<label>(5)</label></formula><p>where |C R | denotes the cardinality of C R . As the selfsupervision loss is defined over the given labeled data of D b , no additional data is required to implement this method. L is the standard cross-entropy loss, as before.</p><p>Exemplar <ref type="bibr" target="#b10">[11]</ref>: Exemplar training aims at making the feature representation invariant to a wide range of image transformations such as translation, scaling, rotation, contrast and color shifts. In a given mini-batch M , we create 4 copies of each image through random augmentations. These 4 copies are the positive examples for each image and every other image in the mini-batch is a negative example. We then use hard batch triplet loss <ref type="bibr" target="#b25">[26]</ref> with soft margin on f θ (x) on the mini-batch to bring the feature representation of positive examples close together. Specifically, the loss is given as:</p><formula xml:id="formula_6">L e = 1 4 * |M | x∈M 4 k=1 log 1 + exp − max p∈{1,··· ,4} D x i k , x i p + min p∈{1..4},i =j D(x i k , x j p )<label>(6)</label></formula><p>Here, D is the Euclidean distance in the feature representation space f θ (x) and x i k is the k th exemplar of x with class label i (the appropriate augmentation). The first term inside the exp term is the maximum among distances between an image and its positive examples which we want to reduce. The second term is the minimum distance between the image and its negative examples which we want to maximize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">S2M2: Self-Supervised Manifold Mixup</head><p>The few-shot learning setting relies on learning robust and generalizable features that can separate base and novel classes. An important means to this end is the ability to compartmentalize the representations of base classes with generous decision boundaries, which allow the model to generalize to novel classes. Manifold Mixup provides an effective methodology to flatten representations of data from a given class into a compact region, thereby supporting this objective. However, while <ref type="bibr" target="#b61">[62]</ref> claims that Manifold Mixup can handle minor distribution shifts, the semantic difference between base and novel classes in the few-shot setting may be more than what it can handle. We hence propose the use of self-supervision as an auxiliary loss while training the base classes, which allows the learned backbone model, f θ , to provide feature representations with sufficient decision boundaries between classes, that allow the model to extend to the novel classes. This is evidenced in our results presented in Section 4.2.3. Our overall methodology is summarized in the steps below, and the pseudo-code of the proposed approach for training the backbone is presented in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end</head><p>Step 1: Self-supervised training: Train the backbone model using self-supervision as an auxiliary loss along with classification loss i.e. L + L ss where L ss ∈ {L e , L rot }.</p><p>Step 2: Fine-tuning with Manifold Mixup: Fine-tune the above model with Manifold Mixup loss L mm for a few more epochs.</p><p>After obtaining the backbone, a cosine classifier is learned over it to adapt to few-shot tasks. S2M2 R and S2M2 E are two variants of our proposed approach which uses L rot and L e as auxiliary loss in Step 1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we present our results of few-shot classification task on different datasets and model architectures. We first describe the datasets, evaluation criteria and implementation details 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We perform experiments on four standard datasets for few-shot image classification benchmark, mini-ImageNet <ref type="bibr" target="#b62">[63]</ref>, tiered-ImageNet <ref type="bibr" target="#b51">[52]</ref>, CUB <ref type="bibr" target="#b63">[64]</ref> and CIFAR-FS <ref type="bibr" target="#b3">[4]</ref>. mini-ImageNet consists of 100 classes from the ImageNet <ref type="bibr" target="#b52">[53]</ref> which are split randomly into 64 base, 16 validation and 20 novel classes. Each class has 600 samples of size 84 × 84. tiered-ImageNet consists of 608 classes randomly picked from ImageNet <ref type="bibr" target="#b52">[53]</ref>    Evaluation Criteria: We evaluate experiments on 5-way 1-shot and 5-way 5-shot [63] classification setting i.e using 1 and 5 labeled instances of each of the 5 classes as training data and Q instances each from the same classes as testing data. For tiered-ImageNet, mini-ImageNet and CIFAR-FS we report the average classification accuracy over 10000 tasks where Q = 599 for 1-Shot and Q = 595 for 5-Shot tasks respectively. For CUB we report average classification accuracy with Q = 15 over 600 tasks. We compare our approach S2M2 R against the current state-of-the-art methods, LEO <ref type="bibr" target="#b53">[54]</ref> and DCO <ref type="bibr" target="#b36">[37]</ref> in Section 4.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We perform experiments on three different model architecture: ResNet-18, ResNet-34 <ref type="bibr" target="#b21">[22]</ref> and WRN-28-10 <ref type="bibr" target="#b66">[67]</ref> which is a Wide Residual Network of 28 layers and width factor 10. For tiered-ImageNet we only perform experiments with WRN-28-10 architecture. Average pooling is applied at the last block of each architecture for getting feature vectors. ResNet-18 and ResNet-34 models have 512 dimensional output feature vector and WRN-28-10 has 640 dimensional feature vector. For training ResNet-18 and ResNet-34 architectures, we use Adam <ref type="bibr" target="#b32">[33]</ref> optimizer for mini-ImageNet and CUB whereas SGD optimizer for CIFAR-FS. For WRN-28-10 training, we use Adam optimizer for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Evaluation over Few-shot Tasks</head><p>In this subsection, we report the result of few shot learning over our proposed methodology and its variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Using Manifold Mixup Regularization</head><p>All experiments using Manifold Mixup <ref type="bibr" target="#b61">[62]</ref> randomly sample a hidden layer (including input layer) at each step to  <ref type="bibr" target="#b37">[38]</ref> plot of feature vectors of images from novel classes of mini-ImageNet using Baseline++, Rotation, S2M2 R (left to right). apply mixup as described in equation 3 for the mini-batch with mixup coefficient (λ) sampled from a β(α, α) distribution with α = 2. We compare the performance of Manifold Mixup <ref type="bibr" target="#b61">[62]</ref> with Baseline++ <ref type="bibr" target="#b6">[7]</ref> and Mixup <ref type="bibr" target="#b27">[28]</ref>. The results are shown in table 2. We can see that the boost in fewshot accuracy from the two aforementioned mixup strategies is significant when model architecture is deep (WRN-28-10). For shallower backbones (ResNet-18 and ResNet-34), the results are not conclusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Using Self-supervision as Auxiliary Loss</head><p>We evaluate the contribution of rotation prediction <ref type="bibr" target="#b17">[18]</ref> and exemplar training <ref type="bibr" target="#b10">[11]</ref> as an auxiliary task during backbone training for few-shot tasks. Backbone model is trained with both classification loss and auxiliary loss as explained in section 3.2.1. For exemplar training, we use random cropping, random horizontal/vertical flip and image jitter randomization <ref type="bibr" target="#b67">[68]</ref> to produce 4 different positive variants of each image in the mini-batch. Since exemplar training is computationally expensive, we fine-tune the baseline++ model for 50 epochs using both exemplar and classification loss.</p><p>The comparison of above techniques with Baseline++ is shown in table 2. As we see, by selecting rotation and exemplar as an auxiliary loss there is a significant improvement from Baseline++ ( 7 − 8%) in most cases. Also, the improvement is more prominent for deeper backbones like WRN-28-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Our Approach: S2M2</head><p>We first train the backbone model using self-supervision (exemplar or rotation) as auxiliary loss and then fine-tune it with Manifold Mixup as explained in section 3.2.2. The results are shown in table 2. We compare our approach with current state-of-the-art <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b36">37]</ref> and other existing few-shot methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b60">61]</ref> in <ref type="table" target="#tab_1">Table 1</ref>. As we can observe from table, our approach S2M2 R beats the most recent state-of-the-art results , LEO <ref type="bibr" target="#b53">[54]</ref> and DCO <ref type="bibr" target="#b36">[37]</ref>, by a significant margin on all four datasets. We find that using only rotation prediction as an auxiliary task during backbone training also outperforms the existing state-of-the-art methods on all datasets except CIFAR-FS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Ablation Studies</head><p>To understand the significance of learned feature representation for few-shot tasks, we perform various experiments and analyze the findings in this section. We choose mini-ImageNet as the primary dataset with WRN-28-10 backbone for the following experiments.</p><p>Effect of varying N in N -way classification: For extensive evaluation, we test our proposed methodology in complex few-shot settings. We vary N in N -way K-shot evaluation criteria from 5 to 10, 15 and 20. The corresponding results are reported in table 3. We observe that our approach S2M2 R outperforms other techniques by a significant margin. The improvement becomes more pronounced for N &gt; 5. <ref type="figure" target="#fig_2">Figure 2</ref> shows the 2-dimensional UMAP <ref type="bibr" target="#b37">[38]</ref> plot of feature vectors of novel classes obtained from different methods. It shows that our approach has more segregated clusters with less variance. This supports our hypothesis that using both self supervision and Manifold Mixup regularization helps in learning feature representations with well separated margin between novel classes.</p><p>Cross-domain few-shot learning: We believe that in practical scenarios, there may be a significant domain-shift between the base classes and novel classes. Therefore, to further highlight the significance of selecting the right manifold for feature space, we evaluate the few-shot classification performance over cross-domain dataset : mini-ImageNet =⇒ CUB (coarse-grained to fine-grained distribution) using Baseline++, Manifold Mixup <ref type="bibr" target="#b61">[62]</ref>, Rotation <ref type="bibr" target="#b67">[68]</ref> and S2M2 R . We train the feature backbone with the base classes of mini-ImageNet and evaluate its performance    Generalization performance of supervised learning over base classes: The results in table 2 and 3 empirically support the hypothesis that our approach learns a feature manifold that generalizes to novel classes and also results in improved performance on few-shot tasks. This generalization of the learned feature representation should also hold for base classes. To investigate this, we evaluate the performance of backbone model over the validation set of the ImageNet dataset and the recently proposed ImageNetV2 dataset <ref type="bibr" target="#b50">[51]</ref>. ImageNetV2 was proposed to test the generalizability of the ImageNet trained models and consists of images having slightly different data distribution from the Im-ageNet. We further test the performance of backbone model over some common visual perturbations and adversarial attack. We randomly choose 3 of the 15 different perturbation techniques -pixelation, brightness, contrast , with 5 varying intensity values , as mentioned in the paper <ref type="bibr" target="#b24">[25]</ref>. For adversarial attack, we use the FGSM [19] with = 1.0/255.0. All the evaluation is over the 64 classes of mini-ImageNet used for training the backbone model. The results are shown in table 6. It can be seen that S2M2 R has the best generalization performance for the base classes also.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of using the union of base and validation classes:</head><p>We test the performance of few-shot tasks after merging the validation classes into base classes. In table 5, we see a considerable improvement over the other approaches using the same extended data, supporting the generalizability claim   of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different levels of self-supervision:</head><p>We conduct a separate experiment to evaluate the performance of the model by varying the difficulty of self-supervision task; specifically the number of angles to predict in rotation task. We change the number of rotated versions of each image to 1 (0 •  <ref type="figure" target="#fig_4">Figure 3</ref> shows that the performance improves with increasing the number of rotation variants till 4, after which the performance starts to decline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We observe that learning feature representation with relevant regularization and self-supervision techniques lead to consistent improvement of few-shot learning tasks on a diverse set of image classification datasets. Notably, we demonstrate that feature representation learning using both self-supervision and classification loss and then applying Manifold Mixup over it, outperforms prior state-of-the-art approaches in few-shot learning. We do extensive experiments to analyze the effect of architecture and efficacy of learned feature representations in few-shot setting. This work opens up a pathway to further explore the techniques in self-supervision and generalization techniques to improve computer vision tasks specifically in low-data regime. Finally, our findings highlight the merits of learning a robust representation that helps in improving the performance of few-shot tasks. <ref type="figure">Figure 4</ref>: UMAP (2-dim) <ref type="bibr" target="#b37">[38]</ref> plot for feature vectors of examples from novel classes of CIFAR-FS using Baseline++ <ref type="bibr" target="#b6">[7]</ref>, Rotation, S2M2 R (top to bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>5-way 10-way 15-way 20-way 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot  <ref type="table">Table 7</ref>: Mean few-shot accuracy on CIFAR-FS as N increases in N -way K-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ablation Studies</head><p>In this section, we perform additional experiments to study the efficacy of our approach S2M2 R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Effect of varying N in N -way classification on CIFAR-FS</head><p>We vary N in N -way K-shot evaluation criteria from 5 to 10, 15 and 20 for CIFAR-FS dataset. The corresponding results are reported in table 7. We observe that our approach S2M2 R outperforms other techniques by a significant margin. The improvement becomes more pronounced for N &gt; 5. <ref type="figure">Figure 4</ref> shows the 2-dimensional UMAP <ref type="bibr" target="#b37">[38]</ref> plot of feature vectors of novel classes obtained from different methods. We obtain similar results for CIFAR-FS as that in the case of mini-ImageNet. We show that our approach has more segregated clusters with less variance. This supports our hypothesis that using both self-supervision and Manifold Mixup regularization helps in learning feature representations with well separated margin between novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Visualizing important regions in images responsible for classification</head><p>We visualize the relevant pixels responsible for classifying a particular image to the correct class. We define the relevance of the pixels as the top-1 percentile of the pix-els sorted by the magnitude of the gradient with respect to the correct class of the image. For this experiment, we use models trained using the Baseline++ <ref type="bibr" target="#b6">[7]</ref> and S2M2 R methods to visualize the relevant pixels. In <ref type="figure">figure 5</ref>, we show the relevant pixels of the image highlighted in white for visualization. Qualitatively speaking, we observe that relevant pixels for model trained using S2M2 R tends to focus more on the object belonging to the specified class and not in the background. <ref type="figure">Figure 5</ref>: Each row visualizes the relevant pixels for classification with respect to a trained model for a image sampled from the base class of mini-ImageNet ( house finch, beer bottle, green mamba, Saluki). Images in each row are arranged in the order with labels as original image, relevant pixels by Baseline++ model and relevant pixels by S2M2 R model (from left to right) respectively. The relevant pixels is defined as the Top-1 percentile of pixels responsible for classification (pixels marked in white color).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowchart for our proposed approach (S2M2) for few-shot learning. The auxiliary loss is derived from Manifold Mixup regularization and self-supervision tasks of rotation and exemplar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 S2M2</head><label>1</label><figDesc>feature backbone training begin Input: {x, y} ∈ D b ; α; {x , y } ∈ D val Output: Backbone model f θ Feature extractor backbone f θ training Initialize f θ for epochs∈ {1, 2, ..., 400} do Training data of size B -(X(i), Y (i)). L(θ, X(i), Y (i)) = L class + Lss θ → θ − η * ∇L(θ, X(i), Y (i)) end val acc prev = 0.0 val acc list = [ ]Fine-tuning f θ with Manifold Mixup while val acc &gt; val acc prev doTraining data of size B -(X(i), Y (i)). L(θ, X(i), Y (i)) = Lmm + 0.5(L class + Lss) θ → θ − η * ∇L(θ, X(i), Y (i)) val acc = Accuracy x,y∈D val (Wn(f θ (x)), y) Append val acc to val acc list Update val acc prev with val acc end return fine-tuned backbone f θ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>UMAP (2-dim)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Effect of increasing the number of self-supervised (degrees of rotation) labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>which are split randomly into 351 base, 97 validation and 160 novel classes. In total, there are 779, 165 images of size 84 × 84. CUB contains 200 classes with total 11, 788 images of size 84 × 84. The base, validation and novel split is 100, 50 and ± 0.89 66.62 ± 0.83 51.67 ± 1.81 70.30 ± 0.08 71.29 ± 0.95 80.33 ± 0.70 58.9 ± 1.9 71.5 ± 1.0 ProtoNet [58] 54.16 ± 0.82 73.68±0.65 53.31 ± 0.89 72.69 ± 0.74 71.88±0.91 87.42 ± 0.48 55.5 ± 0.7 72.0 ± 0.6 RelationNet [61] 52.19 ± 0.83 70.20 ± 0.66 54.48 ± 0.93 71.32 ± 0.78 68.65 ± 0.91 81.12 ± 0.63 55.0 ± 1.0 69.3 ± 0.8 LEO [54] 61.76 ± 0.08 77.59 ± 0.12 66.33 ± 0.05 81.44 ± 0.09 68.22 ± 0.22 * 78.27 ± 0.16 * --DCO [37] 62.64 ± 0.61 78.63 ± 0.46 65.99 ± 0.72 81.56 ± 0.53 --72.0 ± 0.7 84.2 ± 0.5 Baseline++ 57.53 ± 0.10 72.99 ± 0.43 60.98 ± 0.21 75.93 ± 0.17 70.4 ± 0.81 82.92 ± 0.78 67.50 ± 0.64 80.08 ± 0.32 Manifold Mixup 57.16 ± 0.17 75.89 ± 0.13 68.19 ± 0.23 84.61 ± 0.16 73.47 ± 0.89 85.42 ± 0.53 69.20 ± 0.2 83.42 ± 0.15 Rotation 63.9 ± 0.18 81.03 ± 0.11 73.04 ± 0.22 87.89 ± 0.14 77.61 ± 0.86 89.32 ± 0.46 70.66 ± 0.2 84.15 ± 0.14 S2M2 R 64.93 ± 0.18 83.18 ± 0.11 73.71 ± 0.22 88.59 ± 0.14 80.68 ± 0.81 90.85 ± 0.44 74.81 ± 0.19 87.47 ± 0.13</figDesc><table><row><cell>Method</cell><cell cols="2">mini-ImageNet</cell><cell cols="2">tiered-ImageNet</cell><cell>CUB</cell><cell></cell><cell cols="2">CIFAR-FS</cell></row><row><cell></cell><cell>1-Shot</cell><cell>5-Shot</cell><cell>1-Shot</cell><cell>5-Shot</cell><cell>1-Shot</cell><cell>5-Shot</cell><cell>1-Shot</cell><cell>5-Shot</cell></row><row><cell>MAML [14]</cell><cell>54.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with prior/current state of the art methods on mini-ImageNet, tiered-ImageNet, CUB and CIFAR-FS dataset. The accuracy with * denotes our implementation of LEO using their publicly released code ± 0.32 74.02 ± 0.<ref type="bibr" target="#b12">13</ref> 54.41 ± 0.21 74.14 ± 0.19 57.53 ± 0.10 72.99 ± 0.43 Mixup (α = 1) 56.12 ± 0.17 73.42 ± 0.13 56.19 ± 0.17 73.05 ± 0.12 59.65 ± 0.34 77.52 ± 0.52 Manifold Mixup 55.77 ± 0.23 71.15 ± 0.12 55.40 ± 0.37 70.0 ± 0.11 57.16 ± 0.17 75.89 ± 0.13 Rotation 58.96 ± 0.24 76.63 ± 0.12 61.13 ± 0.2 77.05 ± 0.35 63.9 ± 0.18 81.03 ± 0.11 Exemplar 56.39 ± 0.17 76.33 ± 0.14 56.87 ± 0.17 76.90 ± 0.17 62.2 ± 0.45 78.8 ± 0.15 S2M2 E 56.80 ± 0.2 76.54 ± 0.14 56.92 ± 0.18 76.97 ± 0.18 62.33 ± 0.25 79.35 ± 0.16 S2M2 R 64.06 ± 0.18 80.58 ± 0.12 63.74 ± 0.18 79.45 ± 0.12 64.93 ± 0.18 83.18 ± 0.11 CUB Baseline++ 67.68 ± 0.23 82.26 ± 0.15 68.09 ± 0.23 83.16 ± 0.3 70.4 ± 0.81 82.92 ± 0.78 Mixup (α = 1) 68.61 ± 0.64 81.29 ± 0.54 67.02 ± 0.85 84.05 ± 0.5 68.15 ± 0.11 85.30 ± 0.43 Manifold Mixup 70.57 ± 0.71 84.15 ± 0.54 72.51 ± 0.94 85.23 ± 0.51 73.47 ± 0.89 85.42 ± 0.53 Rotation 72.4 ± 0.34 84.83 ± 0.32 72.74 ± 0.46 84.76 ± 0.62 77.61 ± 0.86 89.32 ± 0.46 Exemplar 68.12 ± 0.87 81.87 ± 0.59 69.93 ± 0.37 84.25 ± 0.56 71.58 ± 0.32 84.63 ± 0.57 S2M2 E 71.81 ± 0.43 86.22 ± 0.53 72.67 ± 0.27 84.86 ± 0.13 74.89 ± 0.36 87.48 ± 0.49 S2M2 R 71.43 ± 0.28 85.55 ± 0.52 72.92 ± 0.83 86.55 ± 0.51 80.68 ± 0.81 90.85 ± 0.44 CIFAR-FS Baseline++ 59.67 ± 0.90 71.40 ± 0.69 60.39 ± 0.28 72.85 ± 0.65 67.5 ± 0.64 80.08 ± 0.32 Mixup (α = 1) 56.60 ± 0.11 71.49 ± 0.35 57.60 ± 0.24 71.97 ± 0.14 69.29 ± 0.22 82.44 ± 0.27 Manifold Mixup 60.58 ± 0.31 74.46 ± 0.13 58.88 ± 0.21 73.46 ± 0.14 69.20 ± 0.2 83.42 ± 0.15 Rotation 59.53 ± 0.28 72.94 ± 0.19 59.32 ± 0.13 73.26 ± 0.15 70.66 ± 0.2 84.15 ± 0.14 Exemplar 59.69 ± 0.19 73.30 ± 0.17 61.59 ± 0.31 74.17 ± 0.37 70.05 ± 0.17 84.01 ± 0.22 S2M2 E 61.95 ± 0.11 75.09 ± 0.16 62.48 ± 0.21 73.88 ± 0.30 72.63 ± 0.16 86.12 ± 0.26 S2M2 R 63.66± 0.17 76.07± 0.19 62.77± 0.23 75.75± 0.13 74.81 ± 0.19 87.47 ± 0.13</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">ResNet-18</cell><cell cols="2">ResNet-34</cell><cell>WRN-28-10</cell></row><row><cell></cell><cell></cell><cell>1-Shot</cell><cell>5-Shot</cell><cell>1-Shot</cell><cell>5-Shot</cell><cell>1-Shot</cell><cell>5-Shot</cell></row><row><cell></cell><cell>Baseline++</cell><cell>53.56</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mini-ImageNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on mini-ImageNet, CUB and CIFAR-FS dataset over different network architecture.</figDesc><table /><note>50 classes. CIFAR-FS is created by randomly splitting 100 classes of CIFAR-100 [34] into 64 base, 16 validation and 20 novel classes. The images are of size 32 × 32.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Mean few-shot accuracy on mini-ImageNet as N increases in N -way K-shot classification.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>± 0.75 64.98 ± 0.68 Baseline++ 40.44 ± 0.75 56.64 ± 0.72 Manifold Mixup 46.21 ± 0.77 66.03 ± 0.71 Rotation 48.42 ± 0.84 68.40 ± 0.75 S2M2 R 48.24 ± 0.84 70.44 ± 0.75</figDesc><table><row><cell></cell><cell cols="2">mini-ImageNet =⇒ CUB</cell></row><row><cell></cell><cell>1-Shot</cell><cell>5-Shot</cell></row><row><cell>DCO [37]</cell><cell>44.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison in cross-domain dataset scenario.</figDesc><table><row><cell>Method</cell><cell cols="2">Base + Validation</cell></row><row><cell></cell><cell>1-Shot</cell><cell>5-Shot</cell></row><row><cell>LEO [54]</cell><cell cols="2">61.76 ± 0.08 77.59 ± 0.12</cell></row><row><cell>DCO [37]</cell><cell cols="2">64.09 ± 0.62 80.00 ± 0.45</cell></row><row><cell>Baseline++</cell><cell cols="2">61.10 ± 0.19 75.23 ± 0.12</cell></row><row><cell cols="3">Manifold Mixup 61.10 ± 0.27 77.69 ± 0.21</cell></row><row><cell>Rotation</cell><cell cols="2">65.98 ± 0.36 81.67 ± 0.08</cell></row><row><cell>S2M2 R</cell><cell cols="2">67.13 ± 0.13 83.6 ± 0.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Effect of using the union of base and validation class for training the backbone f θ .</figDesc><table /><note>over the novel classes of CUB (to highlight the domain- shift). We report the corresponding results in table 4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.75 81.<ref type="bibr" target="#b46">47</ref> 70.54 47.11 74.36 19.75 Rotation 82.21 83.91 71.9 50.84 76.26 20.5 Manifold Mixup 83.75 87.19 75.22 57.57 78.54 44.97 S2M2 R 85.28 88.41 75.66 60.0 79.77 28.0</figDesc><table><row><cell>Methods</cell><cell>I</cell><cell>I2</cell><cell>P</cell><cell>C</cell><cell>B</cell><cell>Adv</cell></row><row><cell>Baseline++ 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Validation set top-1 accuracy of different approaches over</figDesc><table><row><cell>base classes and it's perturbed variants (I:ImageNet; I2:ImageNetv2;</cell></row><row><cell>P:Pixelation noise; C: Contrast noise; B: Brightness; Adv: Aversarial</cell></row><row><cell>noise)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>, 2 (0 • , 180 • ), 4 (0 • ,90 • ,180 • ,270 • ) and 8 (0 • ,45 • ,90 • ,135 • ,180 • ,225 • ,270 • ,315 • ) and record the performance over the novel tasks for each of the corresponding 4 variants.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Baseline++ 67.5 80.08 53.39 68.89 44.73 60.59 38.22 54.68 Manifold Mixup 69.45 83.31 57.06 75.53 49.04 68.60 43.54 62.80 Rotation 70.5 84.03 57.37 73.60 48.49 66.25 42.28 61.10 S2M2 R 74.45 87.50 62.28 78.47 53.49 71.88 47.59 66.37</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/nupurkmr9/S2M2 fewshot</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>abs/1805.08136</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural networks for pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning by rotation feature decoupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N D D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P. Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to cluster in order to transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grasp2vec: Learning object representations from self-supervised grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kukačka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10686</idno>
		<title level="m">Regularization for deep learning: A taxonomy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Metalearning with differentiable convex optimization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On first-order metalearning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cross and learn: Cross-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In GCPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deltaencoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno>abs/1711.06025</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. CoRR, abs/1605.07146</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">S 4 l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
