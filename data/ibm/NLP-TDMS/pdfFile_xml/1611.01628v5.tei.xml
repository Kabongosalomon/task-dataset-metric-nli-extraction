<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reference-Aware Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
							<email>zichaoy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<email>pblunsom@google.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DeepMind</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
							<email>lingwang@google.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reference-Aware Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a general class of language models that treat reference as discrete stochastic latent variables. This decision allows for the creation of entity mentions by accessing external databases of referents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality). Beyond simple copying, our coreference model can additionally refer to a referent using varied mention forms (e.g., a reference to "Jane" can be realized as "she"), a characteristic feature of reference in natural languages. Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step <ref type="bibr" target="#b23">(Wen et al., 2015;</ref><ref type="bibr" target="#b13">Luong et al., 2015)</ref>. Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source <ref type="bibr" target="#b4">(Gülçehre et al., 2016;</ref><ref type="bibr" target="#b3">Gu et al., 2016;</ref><ref type="bibr" target="#b12">Ling et al.,</ref>   2016; <ref type="bibr" target="#b14">Merity et al., 2016)</ref>. However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. <ref type="figure" target="#fig_0">Figure 1</ref> depicts examples of REs in the context of the three tasks that we consider in this work. First, many models need to refer to a list of items <ref type="bibr" target="#b10">(Kiddon et al., 2016;</ref><ref type="bibr" target="#b23">Wen et al., 2015)</ref>. In the task of recipe generation from a list of ingredients <ref type="bibr" target="#b10">(Kiddon et al., 2016)</ref>, the generation of the recipe will frequently refer to these items. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in the recipe "Blend soy milk and . . . ", soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user's query <ref type="bibr" target="#b26">(Young et al., 2013;</ref><ref type="bibr" target="#b23">Wen et al., 2015;</ref><ref type="bibr" target="#b18">Sordoni et al., 2015;</ref><ref type="bibr" target="#b16">Serban et al., 2016;</ref><ref type="bibr" target="#b2">Bordes and Weston, 2016;</ref><ref type="bibr" target="#b24">Williams and Zweig, 2016;</ref><ref type="bibr" target="#b17">Shang et al., 2015;</ref><ref type="bibr" target="#b22">Wen et al., 2016)</ref>. Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system says "the nirala is a nice restaurant", it refers to the restaurant name the nirala from the database. Finally, we address references within a document <ref type="bibr" target="#b15">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b21">Wang and Cho, 2015)</ref>, as the generation of words will often refer to previously generated words. For instance the same entity will often be referred to throughout a document. In <ref type="figure" target="#fig_0">Figure 1</ref>, the entity you refers to I in a previous utterance. In this case, copying is insufficient-although the referent is the same, the form of the mention is different.</p><p>In this work we develop a language model that has a specific module for generating REs. A series of decisions (should I generate a RE? If yes, which entity in the context should I refer to? How should the RE be rendered?) augment a traditional recurrent neural network language model and the two components are combined as a mixture model. Selecting an entity in context is similar to familiar models of attention <ref type="bibr" target="#b1">(Bahdanau et al., 2014)</ref>, but rather than being a soft decision that reweights representations of elements in the context, it is treated as a hard decision over contextual elements which are stochastically selected and then copied or, if the task warrants it, transformed (e.g., a pronoun rather than a proper name is produced as output). In cases when the stochastic decision is not available in training, we treat it as a latent variable and marginalize it out. For each of the three tasks, we pick one representative application and demonstrate our reference aware model's efficacy in evaluations against models that do not explicitly include a reference operation.</p><p>Our contributions are as follows:</p><p>• We propose a general framework to model reference in language. We consider reference to entries in lists, tables, and document context. We instantiate these tasks into three specific applications: recipe generation, dialogue modeling, and coreference based language models.</p><p>• We develop the first neural model of reference that goes being copying and can model (conditional on context) how to form the mention.</p><p>• We perform comprehensive evaluation of our models on the three data sets and verify our proposed models perform better than strong baselines.</p><p>2 Reference-aware language models</p><p>Here we propose a general framework for reference-aware language models. We denote each document as a series of tokens x 1 , . . . , x L , where L is the number of tokens. Our goal is to maximize p(x i | c i ), the probability of each word x i given its previous context c i = x 1 , . . . , x i−1 . In contrast to traditional neural language models, we introduce a variable z i at each position, which controls the decision on which source x i is generated from. Then the conditional probability is given by:</p><formula xml:id="formula_0">p(x i , z i | c i ) = p(x i | z i , c i )p(z i | c i ), (1)</formula><p>where z i has different meanings in different contexts. If x i is from a reference list or a database, then z i is one dimensional and z i = 1 denotes x i is generated as a reference. z i can also model more complex decisions. In coreference based language model, z i denotes a series of sequential decisions, such as whether x i is an entity, if yes, which entity x i refers to. When z i is not observed, we will train our model to maximize the marginal probability</p><formula xml:id="formula_1">over z i , i.e., p(x i |c i ) = z i p(x i |z i , c i )p(z i |c i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reference to lists</head><p>We begin to instantiate the framework by considering reference to a list of items. Referring to a list of items has broad applications, such as generating documents based on summaries etc. Here we specifically consider the application of recipe generation conditioning on the ingredient lists. Table. 1 illustrates the ingredient list and recipe for Spinach and Banana Power Smoothie. We can see that the ingredients soy milk, spinach leaves, and banana occur in the recipe.  </p><formula xml:id="formula_2">{x ij } L j=1 . The corresponding recipe is y = {y v } K v=1 .</formula><p>We would like to model p(y|X) = Π v p(y v |X, y &lt;v ).</p><p>We first use a LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> to encode each ingredient:</p><formula xml:id="formula_3">h i,j = LSTM E (W E x ij , h i,j−1 ) ∀i.</formula><p>Then, we sum the resulting final state of each ingredient to obtain the starting LSTM state of the decoder. We use an attention based decoder:</p><formula xml:id="formula_4">s v = LSTM D ([W E y v−1 , d v−1 ], s v−1 ), p copy v = ATTN({{h i,j } T i=1 } L j=1 , s v ), d v = ij p v,i,j h i,j , p(z v |s v ) = sigmoid(W [s v , d v ]), p vocab v = softmax(W [s v , d v ]),</formula><p>where ATTN(h, q) is the attention function that returns the probability distribution over the set of vectors h, conditioned on any input representation q. A full description of this operation is described in <ref type="bibr" target="#b1">(Bahdanau et al., 2014)</ref>. The decision to copy from the ingredient list or generate a new word from the softmax is performed using a switch, denoted as p(z v |s v ). We can obtain a probability distribution of copying each of the words in the ingredients by computing p</p><formula xml:id="formula_5">copy v = ATTN({{h i,j } T i=1 } L j=1 , s v ) in the attention mech- anism.</formula><p>Objective: We can obtain the value of z v through a string match of tokens in recipes with tokens in ingredients. If a token appears in the ingredients, we set z v = 1 and z v = 0 otherwise. We can train the model in a fully supervised fashion, i.e., we can obtain the probability of y v as</p><formula xml:id="formula_6">p(y v , z v |s v ) = p copy v (y v )p(1|s v ) if z v = 1 and p vocab v (y v )(1 − p(1|s i,v )) otherwise.</formula><p>However, it may be not be accurate. In many cases, the tokens that appear in the ingredients do not specifically refer to ingredients tokens. For examples, the recipe may start with "Prepare a cup of water". The token "cup" does not refer to the "cup" in the ingredient list "1 cup plain soy milk".</p><p>To solve this problem, we treat z i as a latent variable, we wish to maximize the marginal probability of y v over all possible values of z v . In this way, the model can automatically learn when to refer to tokens in the ingredients. Thus, the probability of generating token y v is defined as:</p><formula xml:id="formula_7">p(y v |s v ) = p vocab v (y v )p(0|s v ) + p copy v (y v )p(1|s v ) = p vocab v (y v )(1 − p(1|s v )) + p copy v (y v )p(1|s v ).</formula><p>If no string match is found for y v , we simply set p copy v (y v ) = 0 in the above objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reference to databases</head><p>We then consider the more complicated task of reference to database entries. Referring to databases is quite common in question answering and dialogue systems, in which databases are external knowledge and they are resorted to reply users' query. In our paper, we consider the application of task-oriented dialogue systems in the domain of restaurant recommendations. Different from lists that are one dimensional, databases are twodimensional and referring to table entries requires sophisticated model design.</p><p>To better understand the model, we first make a brief introduction of the data set. We use dialogues from the second Dialogue State Tracking Challenge (DSTC2) <ref type="bibr" target="#b6">(Henderson et al., 2014)</ref>. Table. 3 is one example dialogue from this dataset.</p><p>We can observe from this example, users get recommendations of restaurants based on queries that specify the area, price and food type of the restaurant. We can support the system's decisions by incorporating a mechanism that allows the model to query the database to find restaurants that satisfy the users' queries. A sample of our database (refer to data preparation part on how we construct the database) is shown in <ref type="table" target="#tab_3">Table 2</ref>. We can observe that each restaurant contains 6 attributes that are generally referred in the dialogue dataset. As such, if the user requests a restaurant that serves "indian" food, we wish to train a model that can search for entries whose "food"    <ref type="figure">Figure 3</ref>: Hierarchical RNN Seq2Seq model. The red box denotes attention mechanism over the utterances in the previous turn.</p><p>column contains "indian". Now, we describe how we deploy a model that fulfills these requirements. We first introduce the basic dialogue framework in which we incorporates the table reference module. Basic Dialogue Framework: We build a basic dialogue model based on the hierarchical RNN model described in <ref type="bibr" target="#b16">(Serban et al., 2016)</ref>, as in dialogues, the generation of the response is not only dependent on the previous sentence, but on all sentences leading to the response. We assume that a dialogue is alternated between a machine and a user. An illustration of the model is shown in <ref type="figure">Figure 3</ref>. Consider a dialogue with T turns, the utterances from a user and a machines are denoted</p><formula xml:id="formula_8">as X = {x i } T i=1 and Y = {y i } T i=1 respectively, where i is the i-th utterance. We define x i = {x ij } |x i | j=1 , y i = {y iv } |y i | v=1</formula><p>, where x ij (y iv ) denotes the j-th (v-th) token in the i-th utterance from the user (the machines). The dialogue sequence starts with a machine utterance and is given by</p><formula xml:id="formula_9">{y 1 , x 1 , y 2 , x 2 , . . . , y T , x T }.</formula><p>We would like to model the utterances from the machine</p><formula xml:id="formula_10">p(y 1 , y 2 , . . . , y T |x 1 , x 2 , . . . , x T ) = i p(y i |y &lt;i , x &lt;i ) = i,v p(y i,v |y i,&lt;v , y &lt;i , x &lt;i ).</formula><p>We encode y &lt;i and x &lt;i into continuous space in a hierarchical way with LSTM: Sentence Encoder: For a given utterance x i , We encode it as</p><formula xml:id="formula_11">h x i,j = LSTM E (W E x i,j , h x i,j−1 ). The representa- tion of x i is given by the h x i = h x i,|x i | .</formula><p>The same process is applied to obtain the machine utterance representation h y i = h y i,|y i | . Turn Encoder: We further encode the sequence {h y 1 , h x 1 , ..., h y i , h x i } with another LSTM encoder. We shall refer the last hidden state as u i , which can be seen as the hierarchical encoding of the previous i utterances. Decoder: We use u i−1 as the initial state of decoder LSTM and decode each token in y i . We can express the decoder as:</p><formula xml:id="formula_12">s y i,v = LSTM D (W E y i,v−1 , s i,v−1 ), p y i,v = softmax(W s y i,v ).</formula><p>We can also incoroprate the attetionn mechanism in the decoder. As shown in <ref type="figure">Figure.</ref> 3, we use the attention mechanism over the utterance in the previous turn. Due to space limit, we don't present the attention based decoder mathmatically and readers can refer to <ref type="bibr" target="#b1">(Bahdanau et al., 2014)</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Incorporating Table Reference</head><p>We now extend the decoder in order to allow the model to condition the generation on a database. Pointer Switch: We use z i,v ∈ {0, 1} to denote the decision of whether to copy one cell from the table. We compute this probability as follows:</p><formula xml:id="formula_13">p(z i,v |s i,v ) = sigmoid(W s i,v ).</formula><p>Thus, if z i,v = 1, the next token y i,v is generated from the database, whereas if z i,v = 0, then the following token is generated from a softmax. We now describe how we generate tokens from the database.  <ref type="table" target="#tab_6">z   Yes  No   U   decoder  Table Pointer</ref> Step 1: attribute attn</p><p>Step 3: row attn</p><p>Step <ref type="formula">5</ref>  To encode the table, we first build an attribute vector and then an encoding vector for each cell. The attribute vector is simply an embedding lookup g c = W E s c . For the encoding of each cell, we first concatenate embedding lookup of the cell with the corresponding attribute vector g c and then feed it through a one-layer MLP as follows: then e r,c = tanh(W [W E t r,c , g c ]).  <ref type="figure" target="#fig_3">Figure 4</ref>. The attention over cells in the table is conditioned on a given vector q, similarly to the attention model for sequences. However, rather than a sequence of vectors, we now operate over a table.</p><p>Step 1: Attention over the attributes to find out the attributes that a user asks about, p a = ATTN({g c }, q). Suppose a user says cheap, then we should focus on the price attribute.</p><p>Step 2: Conditional row representation calculation, e r = c p a c e r,c ∀r. So that e r contains the price information of the restaurant in row r.</p><p>Step 3: Attention over e r to find out the restaurants that satisfy users' query, p r = ATTN({e r }, q). Restaurants with cheap price will be picked.</p><p>Step 4: Using the probabilities p r , we compute the weighted average over the all rows e c = r p r r e r,c . {e r } contains the information of cheap restaurant.</p><p>Step 5: Attention over columns {e r } to compute the probabilities of copying each column p c = ATTN({e c }, q).</p><p>Step 6: To get the probability matrix of copying each cell, we simply compute the outer product p copy = p r ⊗ p c . The overall process is as follows: If z i,v = 1, we embed the above attention process in the decoder by replacing the conditioned state q with the current decoder state s y i,v . Objective: As in previous task, we can train the model in a fully supervised fashion, or we can treat the decision as a latent variable. We can get p(y i,v |s i,v ) in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reference to document context</head><p>Finally, we address the references that happen in a document itself and build a language model that uses coreference links to point to previous entities. Before generating a word, we first make the decision on whether it is an entity mention. If so, we decide which entity this mention belongs to, then we generate the word based on that entity. Denote the document as X = {x i } L i=1 , and the entities are</p><formula xml:id="formula_14">E = {e i } N i=1 , each entity has M i mentions, e i = {m ij } M i j=1 , such that {x m ij } M i j=1</formula><p>refer to the same entity. We use a LSTM to model the document, the hidden state of each token is h i = LSTM(W E x i , h i−1 ). We use a set h e = {h e 0 , h e 1 , ..., h e M } to keep track of the entity states, where h e j is the state of entity j. Word generation: At each time step before generating the next word, we predict whether the word is an entity mention:</p><formula xml:id="formula_15">p coref (v i |h i−1 , h e ) = ATTN(h e , h i−1 ), d i = v i p(v i )h e v i , p(z i |h i−1 ) = sigmoid(W [h i−1 , d i ]),</formula><p>where z i denotes whether the next word is an entity and if yes v i denotes which entity the next word corefers to. If the next word is an entity mention, then p(x i |v i , h i−1 , h e ) =  <ref type="figure">Figure 5</ref>: Coreference based language model, example taken from <ref type="bibr" target="#b25">Wiseman et al. (2016)</ref>.</p><formula xml:id="formula_16">softmax(W 1 tanh(W 2 [h i−1 , h e v i ])) else p(x i |h i−1 ) = softmax(W 1 h i−1 ). Hence, p(x i |x &lt;i ) =      p(x i |h i−1 )p(z i |h i−1 , h e ) if z i = 0. p(x i |v i , h i−1 , h e )× p coref (v i |h i−1 , h e )p(z i |h i−1 , h e ) if z i = 1.</formula><p>Entity state update: Since there are multiple mentions for each entity and the mentions appear dynamically, we need to keep track of the entity state in order to use coreference information in entity mention prediction. We update the entity state h e at each time step. In the beginning, h e = {h e 0 }, h e 0 denotes the state of an virtual empty entity and is a learnable variable. If z i = 1 and v i = 0, then it indicates the next word is a new entity mention, then in the next step, we append h i to h e , i.e., h e = {h e , h i }, if z i = 1 and v i &gt; 0, then we update the corresponding entity state with the new hidden state, h e [v i ] = h i . Another way to update the entity state is to use one LSTM to encode the mention states and get the new entity state. Here we use the latest entity mention state as the new entity state for simplicity. The detailed update process is shown in <ref type="figure">Figure 5</ref>.</p><p>Note that the stochastic decisions in this task are more complicated than previous two tasks. We need to make two sequential decisions: whether the next word is an entity mention, and if yes, which entity the mention corefers to. It is intractable to marginalize these decisions, so we train this model in a supervised fashion (refer to data preparation part on how we get coreference annotations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data sets and preprocessing</head><p>Recipes: We crawled all recipes from www. allrecipes.com. There are about 31, 000 recipes in total, and every recipe has an ingredient list and a corresponding recipe. We exclude the recipes that have less than 10 tokens or more than 500 tokens, those recipes take about 0.1% of all data set. On average each recipe has 118 tokens and 9 ingredients. We random shuffle the whole data set and take 80% as training and 10% for validation and test. We use a vocabulary size of 10,000 in the model. Dialogue: We use the DSTC2 data set. We only use the dialogue transcripts from the data set. There are about 3,200 dialogues in total. The table is not available from DSTC2. To reconstruct the table, we crawled TripAdvisor for restaurants in the Cambridge area, where the dialog dataset was collected. Then, we remove restaurants that do not appear in the data set and create a database with 109 restaurants and their attributes (e.g. food type). Since this is a small data set, we use 5fold cross validation and report the average result over the 5 partitions. There may be multiple tokens in each table cell, for example in Table. 2, the name, address, post code and phone number have multiple tokens, we replace them with one special token. For the name, address, post code and phone number of the j-th row, we replace the tokens in each cell with NAME j, ADDR j, POSTCODE j, PHONE j. If a table cell is empty, we replace it with an empty token EMPTY. We do a string match in the transcript and replace the corresponding tokens in transcripts from the table with the special tokens. Each dialogue on average has 8 turns (16 sentences). We use a vocabulary size of 900, including about 400 table tokens and 500 words. Coref LM: We use the Xinhua News data set from Gigaword Fifth Edition and sample 100,000 documents that has length in range from 100 to 500. Each document has on average 234 tokens, so there are 23 million tokens in total. We process the documents to get coreference annotations and use the annotations, i.e., z i , v i , in training. We take 80% as training and 10% as validation and test respectively. We ignore the entities that have only one mention and for the mentions that have multiple tokens, we take the token that is most frequent in the all the mentions for this entity. After preprocessing, tokens that are entity mentions take about 10% of all tokens. We use a vocabulary size of 50,000 in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines, model training and evaluation</head><p>We compare our model with baselines that do not model reference explicitly. For recipe generation and dialogue modeling, we compare our model with basic seq2seq and attention model. We also apply attention mechanism over the table for dialogue modeling as a baseline. For coreference based language model, we compare our model with simple RNN language model.</p><p>We train all models with simple stochastic gradient descent with gradient clipping. We use a one-layer LSTM for all RNN components. Hyperparameters are selected using grid search based on the validation set. Evaluation of our model is challenging since it involves three rather different applications. We focus on evaluating the accuracy of predicting the reference tokens, which is the goal of our model. Specifically, we report the perplexity of all words, words that can be generated from reference and non-reference words. The perplexity is calculated by multiplying the probability of decision at each step all together. Note that for non-reference words, they also appear in the vocabulary. So it is a fair comparison to models that do not model reference explicitly. For the recipe task, we also generate the recipes using beam size of 10 and evaluate the generated recipes with BLEU. We didn't use BLEU for dialogue generation since the database entries take only a very small part of all tokens in utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and analysis</head><p>The results for recipe generation, dialogue and coref based language model are shown in <ref type="table" target="#tab_10">Table 4</ref>, 5, and 6 respectively. The recipe results in Table 4 verifies that modeling reference explicitly improves performance. Latent and Pointer perform better than Seq2Seq and Attn model. The Latent model performs better than the Pointer model since tokens in ingredients that match with recipes do not necessarily come from the ingredients. Imposing a supervised signal gives wrong information to the model and hence makes the result worse. With latent decision, the model learns to when to copy and when to generate it from the vocabulary.</p><p>The findings for dialogue basically follow that of recipe generation, as shown in <ref type="table" target="#tab_11">Table 5</ref> The coref based LM results are shown in Table 6. We find that coref based LM performs much better on the entity perplexity, but is a little bit worse for non-entity words. We found it was an optimization problem and the model was stuck in a local optimum. So we initialize the Pointer model with the weights learned from LM, the Pointer model performs better than LM both for entity perplexity and non-entity words perplexity.</p><p>In Appendix A, we also visualize the heat map of table reference and list items reference. The visualization shows that our model can correctly predict when to refer to which entries according to context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In terms of methodology, our work is closely related to previous works that incorporate copying mechanism with neural models <ref type="bibr" target="#b4">(Gülçehre et al., 2016;</ref><ref type="bibr" target="#b3">Gu et al., 2016;</ref><ref type="bibr" target="#b12">Ling et al., 2016;</ref>. Our models are similar to models proposed in <ref type="bibr" target="#b14">Merity et al., 2016)</ref>,    where the generation of each word can be conditioned on a particular entry in knowledge lists and previous words. In our work, we describe a model with broader applications, allowing us to condition, on databases, lists and dynamic lists. In terms of applications, our work is related to chit-chat dialogue <ref type="bibr" target="#b18">Sordoni et al., 2015;</ref><ref type="bibr" target="#b16">Serban et al., 2016;</ref><ref type="bibr" target="#b17">Shang et al., 2015)</ref> and task oriented dialogue <ref type="bibr" target="#b23">(Wen et al., 2015;</ref><ref type="bibr" target="#b2">Bordes and Weston, 2016;</ref><ref type="bibr" target="#b24">Williams and Zweig, 2016;</ref><ref type="bibr" target="#b22">Wen et al., 2016)</ref>. Most of previous works on task oriented dialogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly. Recipe generation was proposed in <ref type="bibr" target="#b10">(Kiddon et al., 2016)</ref>. They use attention mechanism over the checklists, whereas our work models ex-plicit references to them. Context dependent language models <ref type="bibr" target="#b15">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b9">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b15">Mikolov et al., 2010;</ref><ref type="bibr" target="#b21">Wang and Cho, 2015)</ref> are proposed to capture long term dependency of text. There are also lots of works on coreference resolution <ref type="bibr" target="#b5">(Haghighi and Klein, 2010;</ref><ref type="bibr" target="#b25">Wiseman et al., 2016)</ref>. We are the first to combine coreference with language modeling, to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three applications, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore further based on our framework. The current evaluation method is based on perplexity and BLEU. In task oriented dialogues, we can also try human evaluation to see if the model can reply users' query accurately. It is also interesting to use reinforcement learning to learn the actions in each step in coref based LM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Reference-aware language models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Recipe pointer Let the ingredients of a recipe be X = {x i } T i=1 and each ingredient contains L tokens x i =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Decoder with table pointer. We denote a table with R rows and C columns as {t r,c}, r ∈ [1, R], c ∈ [1, C],where t r,c is the cell in row r and column c. The attribute of each column is denoted as s c , where c is the c-th attribute. t r,c and s c are one-hot vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>e r,c ∀c, p c = ATTN({e c }, q), p copy = p r ⊗ p c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Work completed at DeepMind.</figDesc><table><row><cell cols="3">a) reference to a list</cell><cell></cell><cell></cell></row><row><cell>recipe</cell><cell cols="2">1) soy milk 2) leaves</cell><cell cols="2">Blend soy milk and …</cell></row><row><cell></cell><cell cols="2">3) banana</cell><cell></cell><cell></cell></row><row><cell cols="3">b) reference to a table</cell><cell></cell><cell></cell></row><row><cell>dialogue</cell><cell>name</cell><cell>price</cell><cell>food</cell><cell>M: the nirala is a</cell></row><row><cell></cell><cell cols="3">the nirala moderate lebanese</cell><cell>nice restuarant</cell></row><row><cell></cell><cell>ali baba</cell><cell>moderate</cell><cell>indian</cell><cell></cell></row><row><cell cols="4">c) reference to document context</cell><cell></cell></row><row><cell>coref</cell><cell cols="3">[I] 1 think…Go ahead [Linda] 2</cell><cell>… thanks goes to [you] 1 …</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ingredients and recipe for Spinach and Banana Power Smoothie.</figDesc><table><row><cell>Ingredients</cell><cell>Recipe</cell></row><row><cell>1 cup plain soy milk</cell><cell>Blend soy milk and spinach leaves</cell></row><row><cell>3/4 cup packed fresh spinach leaves</cell><cell>together in a blender until smooth. Add</cell></row><row><cell>1 large banana, sliced</cell><cell>banana and pulse until thoroughly blended.</cell></row><row><cell>Table 1:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Fragment of database for dialogue system.</figDesc><table><row><cell>M: Hello, welcome to the Cambridge</cell></row><row><cell>restaurant system? You can ask for</cell></row><row><cell>restaurants by area, price range</cell></row><row><cell>or food type . How may I help you?</cell></row><row><cell>U: i want a moderately priced</cell></row><row><cell>restaurant in the north town</cell></row><row><cell>M: the nirala is a nice restaurant in</cell></row><row><cell>the north of town in the moderate price</cell></row><row><cell>range</cell></row><row><cell>U: what is the address</cell></row><row><cell>M: Sure , the nirala is on 7 Milton</cell></row><row><cell>Road Chesterton</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Example dialogue, M stands for Machine and U stands for User</figDesc><table><row><cell></cell><cell>decoder</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>turn encoder</cell><cell></cell></row><row><cell>attn</cell><cell></cell><cell></cell><cell></cell></row><row><cell>M</cell><cell>U</cell><cell>M</cell><cell>U</cell></row><row><cell></cell><cell></cell><cell>sentence encoder</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table Encoding :</head><label>Encoding</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table Pointer :</head><label>Pointer</label><figDesc>The detailed process of calculating the probability distribution over the table is shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>um and [I] 1 think that is whats -Go ahead [Linda] 2 . Well and thanks goes to [you] 1 and to [the media] 3 to help [us] 4 ...So [our] 4 hat is off to all of [you] 5 ...</figDesc><table><row><cell></cell><cell>empty state</cell><cell cols="2">new entity</cell><cell></cell><cell>2</cell><cell>Linda</cell><cell>entity 1</cell><cell>2</cell><cell>Linda</cell></row><row><cell>entity state</cell><cell></cell><cell></cell><cell>1</cell><cell>I</cell><cell>1</cell><cell>I</cell><cell>1</cell><cell>You</cell></row><row><cell>update process</cell><cell>0</cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell><cell></cell><cell>attn</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>attn</cell><cell>[I] 1</cell><cell>push state</cell><cell></cell><cell>push state</cell><cell>[You] 1</cell><cell>update state</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>…</cell><cell></cell><cell>…</cell></row><row><cell></cell><cell>um</cell><cell cols="2">and</cell><cell>[I] 1</cell><cell cols="2">[Linda] 2</cell><cell>of</cell><cell>[You] 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>. Conditioning table performs better in predicting table tokens in general. Table Pointer has the lowest perplexity for tokens in the table. Since the table tokens appear rarely in the dialogue transcripts, the overall perplexity does not differ much and the non-table token perplexity are similar. With attention mechanism over the table, the perplexity of table token improves over basic Seq2Seq model, but still not as good as directly pointing to cells in the table, which shows the advantage of modeling reference explicitly. As expected, using sentence attention improves significantly over models without sentence attention. Surprisingly, Table Latent performs much worse than Table Pointer. We also measure the perplexity of table tokens that appear only in test set. For models other than Table Pointer, because the tokens never appear in the training set, the perplexity is quite high, while Table Pointer can predict these tokens much more accurately. This verifies our conjecture that our model can learn reasoning over databases.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Recipe results, evaluated in perplexity and BLEU score. All means all tokens, Ing denotes tokens from recipes that appear in ingredients. Word means non-table tokens. Pointer and Latent differs in that for Pointer, we provide supervised signal on when to generate a reference token, while in Latent it is a latent decision.</figDesc><table><row><cell>Model</cell><cell>All</cell><cell>Table</cell><cell>Table OOV</cell><cell>Word</cell></row><row><cell>Seq2Seq</cell><cell cols="2">1.35±0.01 4.98±0.38</cell><cell cols="2">1.99E7±7.75E6 1.23±0.01</cell></row><row><cell>Table Attn</cell><cell cols="2">1.37±0.01 5.09±0.64</cell><cell cols="2">7.91E7±1.39E8 1.24±0.01</cell></row><row><cell>Table Pointer</cell><cell cols="2">1.33±0.01 3.99±0.36</cell><cell cols="2">1360 ± 2600 1.23±0.01</cell></row><row><cell>Table Latent</cell><cell cols="2">1.36±0.01 4.99±0.20</cell><cell cols="2">3.78E7±6.08E7 1.24±0.01</cell></row><row><cell>+ Sentence Attn</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seq2Seq</cell><cell cols="4">1.28±0.01 3.31±0.21 2.83E9 ± 4.69E9 1.19±0.01</cell></row><row><cell>Table Attn</cell><cell cols="2">1.28±0.01 3.17±0.21</cell><cell cols="2">1.67E7±9.5E6 1.20±0.01</cell></row><row><cell>Table Pointer</cell><cell cols="2">1.27±0.01 2.99±0.19</cell><cell cols="2">82.86±110 1.20±0.01</cell></row><row><cell>Table Latent</cell><cell cols="2">1.28±0.01 3.26±0.25</cell><cell cols="2">1.27E7±1.41E7 1.20±0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Dialogue perplexity results. Table means tokens from table,Table OOV denotes table tokensthat do not appear in the training set. Sentence Attn denotes we use attention mechanism over tokens in utterances from the previous turn.</figDesc><table><row><cell>Model</cell><cell cols="2">val All Entity Word</cell><cell cols="2">test All Entity Word</cell></row><row><cell>LM</cell><cell>33.08</cell><cell cols="2">44.52 32.04 33.08</cell><cell>43.86 32.10</cell></row><row><cell>Pointer</cell><cell>32.57</cell><cell cols="2">32.07 32.62 32.62</cell><cell>32.07 32.69</cell></row><row><cell>Pointer + init</cell><cell>30.43</cell><cell cols="2">28.56 30.63 30.42</cell><cell>28.56 30.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Coreference based LM. Pointer + init means we initialize the model with the LM weights.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Visualization M: Hello , welcome to the Cambridge restaurant system ? You can ask for restaurants by area , price range or food type . How may I help you ? U: and expensive restaurant that serves tuscan food M: Sorry there is no tuscan restaurant in the expensive price range U: french food M: cote is a great restaurant serving french food and it is in the expensive price range U: may i have the phone number M: The phone number of cote is 01223 311053 . U: thank you good bye    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A neural knowledge language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Sungjin Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1608.00318</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>abs/1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ç Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1603.08148</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dialog state tracking challenge 2 &amp; 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03962</idno>
		<title level="m">Document context language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02364</idno>
		<title level="m">Neural responding machine for short-text conversation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meg</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Deep Learning Workshop</title>
		<meeting>ICML Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03729</idno>
		<title level="m">Largercontext language modelling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Endto-end lstm-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01269</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03035</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
