<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Divide and Conquer: A Deep CASA Approach to Talker-independent Monaural Speaker Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Divide and Conquer: A Deep CASA Approach to Talker-independent Monaural Speaker Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Monaural speech separation</term>
					<term>speaker separa- tion</term>
					<term>computational auditory scene analysis</term>
					<term>deep CASA</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address talker-independent monaural speaker separation from the perspectives of deep learning and computational auditory scene analysis (CASA). Specifically, we decompose the multi-speaker separation task into the stages of simultaneous grouping and sequential grouping. Simultaneous grouping is first performed in each time frame by separating the spectra of different speakers with a permutation-invariantly trained neural network. In the second stage, the frame-level separated spectra are sequentially grouped to different speakers by a clustering network. The proposed deep CASA approach optimizes framelevel separation and speaker tracking in turn, and produces excellent results for both objectives. Experimental results on the benchmark WSJ0-2mix database show that the new approach achieves the state-of-the-art results with a modest model size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Divide and Conquer: A Deep CASA Approach to Talker-independent Monaural Speaker Separation Yuzhou Liu and DeLiang Wang</head><p>Abstract-We address talker-independent monaural speaker separation from the perspectives of deep learning and computational auditory scene analysis (CASA). Specifically, we decompose the multi-speaker separation task into the stages of simultaneous grouping and sequential grouping. Simultaneous grouping is first performed in each time frame by separating the spectra of different speakers with a permutation-invariantly trained neural network. In the second stage, the frame-level separated spectra are sequentially grouped to different speakers by a clustering network. The proposed deep CASA approach optimizes framelevel separation and speaker tracking in turn, and produces excellent results for both objectives. Experimental results on the benchmark WSJ0-2mix database show that the new approach achieves the state-of-the-art results with a modest model size.</p><p>Index Terms-Monaural speech separation, speaker separation, computational auditory scene analysis, deep CASA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PEECH usually occurs simultaneously with interference in real acoustic environments. Interference suppression is needed in a wide variety of speech applications, including automatic speech recognition, speaker identification, and hearing aids. One particular kind of interference is the speech signal from competing speakers. Although human listeners excel at attending to a target speaker even without any spatial cues <ref type="bibr" target="#b3">[4]</ref>, speech separation remains a challenge for machines despite decades of research. In this study, we address monaural (one microphone) speaker separation, mainly in the case of two concurrent speakers, which is also known as co-channel speech separation.</p><p>A traditional approach to monaural speech separation is computational auditory scene analysis (CASA) <ref type="bibr" target="#b34">[35]</ref>, which is inspired by human auditory scene analysis (ASA) mechanisms <ref type="bibr" target="#b2">[3]</ref>. CASA addresses speech separation in two main stages: simultaneous grouping and sequential grouping. With an acoustic mixture decomposed into a matrix of timefrequency (T-F) units, simultaneous grouping aggregates T-F units overlapping in time to short segments, each originating from the same source. In sequential grouping, segments are grouped across time into auditory streams, each corresponding to a distinct source. For example, an unsupervised speaker separation method <ref type="bibr" target="#b13">[14]</ref> first generates T-F segments based on This work was supported in part by two NIDCD Grants (R01 DC012048 and R01 DC015521), and in part by the Ohio Supercomputer Center.</p><p>Y. <ref type="bibr">Liu</ref>  pitch and onset/offset analysis, and then uses clustering to sequentially group T-F segments into speakers.</p><p>Recently deep learning has been employed to address speaker separation. The general idea is to train a deep neural network (DNN) to predict T-F masks or spectra of two speakers in a mixture <ref type="bibr" target="#b6">[7]</ref> [16] <ref type="bibr" target="#b41">[42]</ref>. There are usually two output layers in such a DNN, one for an individual speaker. These studies assume that the two speakers do not change between training and testing. It has been shown that such talker-dependent training leads to significant intelligibility improvement for hearing impaired listeners <ref type="bibr" target="#b10">[11]</ref>. However, talker-dependent training does not generalize to untrained speakers. Talkerindependent speaker separation has to address the permutation problem <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b20">[21]</ref>, i.e., how the output layers are tied to the underlying speakers. The details of the permutation problem are introduced in Section II-A.</p><p>Frame-level permutation invariant training (denoted by tPIT) <ref type="bibr" target="#b20">[21]</ref> tackles this problem by examining all possible label permutations within each frame during training, and uses the one with the lowest frame-level loss to train the separation network. A locally optimized output-speaker pairing can thus be reached, which leads to excellent frame-level separation performance. However, the correct speaker assignment in tPIT's output may swap frequently across frames. In other words, the frame-level optimized outputs cannot be readily streamed into underlying speakers without reorganization. To address this issue, an utterance-level PIT (uPIT) algorithm <ref type="bibr" target="#b20">[21]</ref> is proposed to align each speaker to a fixed output layer throughout a whole training utterance. Recent uPIT improvements include new network structure <ref type="bibr" target="#b22">[23]</ref>  <ref type="bibr" target="#b40">[41]</ref> and new training objectives <ref type="bibr" target="#b22">[23]</ref>. TasNet <ref type="bibr" target="#b26">[27]</ref> extends uPIT to the waveform domain using a convolutional encoder-decoder structure. FurcaNeXt <ref type="bibr" target="#b31">[32]</ref> integrates gated activations and ensemble learning into TasNet, and reports very high performance.</p><p>Deep clustering (DC) <ref type="bibr" target="#b11">[12]</ref> looks at the permutation problem from a different perspective. In DC, a recurrent neural network (RNN) with bi-directional long short-term memory (BLSTM) is trained to assign one embedding vector to each T-F unit of the mixture spectrogram. The Frobenius norm between the affinity matrix of embedding vectors and the affinity matrix of the ideal speaker assignment (or the ideal binary mask) is used as the training objective. DC avoids the permutation problem due to the permutation-invariant property of affinity matrices. As training unfolds, embedding vectors of T-F units dominated by the same source are drawn closer together, and embeddings of those units dominated by different sources become farther apart. Clustering these embedding vectors using the K-means algorithm assigns each T-F unit to one of the speakers in the mixture, which can be viewed as binary masking for speech separation. In <ref type="bibr" target="#b25">[26]</ref>, a concept of attractors is introduced to DC to enable ratio masking and real-time processing. Alternative training objectives, together with a chimera network which simultaneously estimates DC embeddings and uPIT outputs, are proposed in <ref type="bibr" target="#b36">[37]</ref>. In <ref type="bibr" target="#b37">[38]</ref>, iterative phase reconstruction is integrated into the chimera network to alleviate phase distortions. In <ref type="bibr" target="#b38">[39]</ref>, a phase prediction network is further added to <ref type="bibr" target="#b37">[38]</ref> to estimate the clean phase of each speaker source.</p><p>DC and PIT represent major approaches to talkerindependent speaker separation. There are, however, limitations. As indicated in <ref type="bibr" target="#b20">[21]</ref> [25], uPIT sacrifices frame-level performance for better assignments at the utterance level. The speaker tracking mechanism in uPIT works poorly for samegender mixtures. On the other hand, DC is better at speaker tracking, but its frame-level separation is suboptimal compared to ratio masking used in tPIT.</p><p>Inspired by CASA, PIT and DC, we proposed a deep learning based two-stage method in our preliminary study <ref type="bibr" target="#b24">[25]</ref> to perform talker-independent speaker separation. The method consists of two stages, a simultaneous grouping stage and a sequential grouping stage. In the first stage, a tPIT-BLSTM is trained to predict the spectra of the two speakers at each frame without speaker assignment. This stage separates spectral components of the two speakers at the same frame, corresponding to simultaneous grouping in CASA. In the sequential grouping stage, frame-level separated spectra and the mixture spectrogram are fed to another BLSTM to predict embedding vectors for the estimated spectra, such that the embedding vectors corresponding to the same speaker are close together, and those corresponding to different speakers are far apart. A constrained K-means algorithm is then employed to group the two spectral estimates at the same frame across time to different speakers. This stage corresponds to sequential grouping in CASA.</p><p>In this study, we adopt the same divide-and-conquer strategy but improve its realization in major ways, resulting in what we call a deep CASA approach. In the simultaneous grouping stage, we utilize a UNet <ref type="bibr" target="#b29">[30]</ref> convolutional neural network (CNN) with densely-connected layers <ref type="bibr" target="#b14">[15]</ref> to improve the performance of frame-level separation. A frequency mapping layer is added to deal with inconsistencies between different frequency bands. To overcome the effects of noisy phase in inverse short-time Fourier transform (STFT), we explore complex STFT objectives and time-domain objectives as the training targets. In the sequential grouping stage, we introduce a new embedding representation and weighted objective function. In addition, we leverage the latest development in temporal convolutional networks (TCNs) <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b21">[22]</ref> [27] <ref type="bibr" target="#b28">[29]</ref>, and use a TCN for sequential grouping, which greatly improves speaker tracking. A new dropout scheme is proposed for TCNs to overcome the overfitting problem. The evaluation results and comparisons demonstrate the resulting system achieves better frame-level separation and speaker tracking at the same time compared to uPIT and <ref type="bibr" target="#b24">[25]</ref>.</p><p>The rest of the paper is organized as follows. Section II presents details on monaural speaker separation and permutation invariant training. The proposed algorithm, including the simultaneous and sequential grouping stages, is introduced in Section III. Section IV presents experimental results, comparisons and analysis. Conclusion and related issues are discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MONAURAL SPEAKER SEPARATION AND PERMUTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INVARIANT TRAINING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Monaural Speaker Separation</head><p>The goal of monaural speaker separation is to estimate C independent speech signals x c (n), c = 1, ..., C, from a single-channel recording of speech mixture y(n), where y(n) = C c=1 x c (n) and n indexes time. In this work, we focus on the co-channel situation where C = 2.</p><p>Many deep learning based speaker separation systems <ref type="bibr" target="#b6">[7]</ref> [16] <ref type="bibr" target="#b41">[42]</ref> address this problem in the T-F domain, where STFT is calculated using an analysis window w(n) with FFT length N and frame shift R:</p><formula xml:id="formula_0">Y (t, f ) = ∞ n=−∞ w(n − tR)y(n)e −j2πf n/N (1) X c (t, f ) = ∞ n=−∞ w(n − tR)x c (n)e −j2πf n/N<label>(2)</label></formula><p>where t and f denote the frame and frequency, respectively. The magnitude STFT of the mixture signal |Y (t, f )|, together with other spectral features, are fed into a neural network to predict a T-F mask M c (t, f ) for each speaker c. The masks are multiplied by the mixture to estimate the original sources:</p><formula xml:id="formula_1">|X c (t, f )| = M c (t, f ) |Y (t, f )|<label>(3)</label></formula><p>Here denotes element-wise multiplication, and |X c (t, f )| denotes the estimated magnitude STFT of speaker c. An estimate of complex STFTX c (t, f ) can be obtained by coupling |X c (t, f )| with noisy phase. In the end, separated waveforms are resynthesized using inverse STFT (iSTFT):</p><formula xml:id="formula_2">x c (n) = ∞ t=−∞ w(n−tR) 1 N N −1 f =0X c (t,f )e j2πf n/N ∞ t=−∞ w 2 (n−tR)<label>(4)</label></formula><p>Various training targets of |X c (t, f )| have been explored for masking based speech separation <ref type="bibr" target="#b35">[36]</ref>. Phase-sensitive approximation (PSA) is found to be effective as it accounts for errors introduced by the noisy phase <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b20">[21]</ref>. In PSA, the desired reconstructed signal is defined as:</p><formula xml:id="formula_3">|X c (t, f )| cos(φ c (t, f )), where φ c (t, f ) is the phase difference between Y (t, f ) and X c (t, f ).</formula><p>Overall, the training loss at each frame is computed as:</p><formula xml:id="formula_4">J P SA t = F f =1 2 c=1 ||M c (t, f ) |Y (t, f )| − |X c (t, f )| cos(φ c (t, f ))|| (5)</formula><p>where || · || denotes the l 1 norm.</p><p>The above formulation works well only when each output layer is tied to a training target signal with similar characteristics. For instance, we may tie each output to a specific speaker, leading to talker-dependent training. We may also tie two outputs with male and female speakers respectively, leading to gender-dependent training. However, for talker-independent training data, how to select output-speaker pairing becomes a nontrivial problem. Think of a training set consisting of three female speakers. For speaker 1-2 mixtures, we can tie output1 to speaker1, and output2 to speaker2. For speaker 1-3 mixtures, again output1 can be tied to speaker1, and output2 tied to speaker3. However, it is hard to decide the pairing arrangement for speaker 2-3 mixtures. If output-speaker pairing is not arranged properly, conflicting gradients may be generated during training, preventing the neural network from converging. This is referred as the permutation problem <ref type="bibr" target="#b11">[12]</ref> [21].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Permutation Invariant Training</head><p>Frame-level PIT <ref type="bibr" target="#b20">[21]</ref> overcomes the permutation problem by providing target speakers as a set instead of an ordered list, and output-speaker pairing c ↔ θ c (t), for a given frame t, is defined as the pairing that minimizes the loss function over all possible speaker permutations P . For tPIT, the framelevel training loss in Eq. 5 is rewritten as:</p><formula xml:id="formula_5">J tP IT −P SA t = min θc(t)∈P f,c ||M c |Y | − |X θc(t) | cos(φ θc(t) )|| (6)</formula><p>We omit (t, f ) in M, Y, X, and φ for brevity.</p><p>tPIT does a good job in separating two speakers at the frame level <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b24">[25]</ref>. However, due to its locally optimized training objective, an output layer may be tied to different speakers at different frames, and the correct speaker assignment may swap frequently. If we reassign the outputs with respect to the minimum loss for each speaker, tPIT can almost perfectly reconstruct both speakers <ref type="bibr" target="#b24">[25]</ref>.</p><p>Optimal speaker assignments are not obtainable in practice as the targets are not given beforehand. To address this issue, uPIT fixes output-speaker pairing c ↔ θ c (t) for a whole utterance, which corresponds to the pairing that provides the minimum utterance-level loss over all possible permutations.</p><p>As reported in <ref type="bibr" target="#b20">[21]</ref> [25], uPIT considerably improves the separation performance with a default output assignment. But it has the following shortcomings. First, uPIT's output-speaker pairing is fixed throughout a whole utterance, which prevents frame-level loss to be optimized as in tPIT. As a result, uPIT always underperforms tPIT if their outputs are optimally reassigned. Second, uPIT addresses separation and speaker tracking simultaneously and due to limited modeling capacity of a neural network, uPIT does not work well for speaker tracking, especially for same-gender mixtures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP CASA APPROACH TO MONAURAL SPEAKER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEPARATION</head><p>We employ a divide and conquer idea to break down monaural speaker separation into two stages. In the simultaneous grouping stage, a tPIT based neural network separates spectral components of different speakers at the frame-level. The sequential grouping stage then streams frame-level estimates belonging to the same speaker. Unlike uPIT, separation and tracking are optimized in turn in the deep CASA framework. The two stages are detailed in the following subsections.</p><p>A. Simultaneous Grouping Stage 1) Baseline system: We adopt the tPIT framework described in <ref type="bibr" target="#b24">[25]</ref> as the baseline simultaneous grouping system. The magnitude STFT of the mixture is used as the input.</p><p>BLSTM is employed as the learning machine. The system is trained using the loss function in Eq. 6. In the end, frame-level spectral estimates are passed to the second stage for sequential grouping.</p><p>2) Alternative training targets for tPIT: As mentioned, the PSA training target partially accounts for STFT phase, unlike the ideal binary mask (IBM) and ideal ratio mask (IRM). However, PSA cannot completely restore the phase information in clean sources, because it uses noisy phase during iSTFT. Recently, complex ratio masking <ref type="bibr" target="#b39">[40]</ref> (cRM) attempts to restore clean phase. The complex ideal ratio mask (cIRM) is defined in the complex STFT domain, with real and imaginary parts. When applied to the complex STFT of the mixture, it perfectly reconstructs clean sources:</p><formula xml:id="formula_6">X c (t, f ) = cIRM c (t, f ) ⊗ Y (t, f )<label>(7)</label></formula><p>where ⊗ denotes point-wise complex multiplication. We propose complex ratio masking to perform monaural speaker separation. Instead of directly using the cIRM as the training target, we first multiply the complex mixture by the estimated complex mask cRM c to perform complex domain reconstruction:X</p><formula xml:id="formula_7">c (t, f ) = cRM c (t, f ) ⊗ Y (t, f )<label>(8)</label></formula><p>The reconstructed sources are then compared with clean sources to form the training objective:</p><formula xml:id="formula_8">J tP IT −CA t = min θc(t)∈P f,c [ |Re(X c − X θc(t) )| + |Im(X c − X θc(t) )| ] (9)</formula><p>where the l 1 norm is applied to both the real and imaginary parts of the loss. We call this training objective complex approximation (CA). We also consider a training objective based on time-domain signal-to-noise ratio (SNR). The proposed framework consists of two steps: First, we organize all frame-level complex estimatesX c with respect to the minimum frame-level loss, so that each organized outputX θc(t) corresponds to a single speaker. The frame-level loss for organization can be defined in three domains: the complex STFT, magnitude STFT and time domain. In each domain, we compare the estimates and ground-truth targets, and calculate the l 1 norm of the difference as the loss. We found the complex STFT loss to be slightly better. Second, we apply iSTFT (Eq. 4) toX θc(t) (t, f ), and compute utterance-level SNR for the final time-domain estimatesx θc(t) (n):</p><formula xml:id="formula_9">J tP IT −SN R = 2 c=1 10 log n x c (n) 2 n (x c (n) −x θc(t) (n)) 2 (10)</formula><p>3) Convolutional neural networks for simultaneous grouping: Motivated by the recent success of DenseNet <ref type="bibr" target="#b14">[15]</ref> and UNet <ref type="bibr" target="#b29">[30]</ref> in music source separation <ref type="bibr" target="#b17">[18]</ref> [33], we propose a Dense-UNet structure for simultaneous grouping.</p><p>The proposed Dense-UNet is shown in <ref type="figure">Fig. 1</ref>, and it is based on a UNet architecture <ref type="bibr" target="#b29">[30]</ref>. It consists of a series of convolutional layers, downsampling layers and upsampling layers. The first half of the network encodes utterance-level STFT feature maps into a higher level of abstraction. this half, allowing the network to model large T-F contexts. Convolutional layers and upsampling layers are alternated in the second half to project the encoded features back to its original resolution. In this study, we use strided 2 × 2 depthwise convolutional layers <ref type="bibr" target="#b4">[5]</ref> as downsampling layers.</p><p>Strided transpose convolutional layers are used as upsampling layers. Skip connections are added between layers at the same hierarchical level in the encoder and decoder to preserve raw information in the mixture. Next, we replace convolutional layers in the original UNet with densely-connected CNN blocks (DenseNet) <ref type="bibr" target="#b14">[15]</ref>. The basic idea of DenseNet is to decompose one convolutional layer with many channels into a sequence of densely connected convolutional layers with fewer channels, where each layer is connected to every other layer in a feed-forward fashion:</p><formula xml:id="formula_10">z l = H l ([z l−1 , z l−2 , ..., z 0 ])<label>(11)</label></formula><p>where z 0 denotes the input feature map, z l the output of the l th layer, [...] concatenation, and H l the l th convolutional layer followed by ELU (exponential linear unit) activation <ref type="bibr" target="#b5">[6]</ref> and layer normalization <ref type="bibr" target="#b0">[1]</ref>. The DenseNet structure has shown excellent performance in image classification <ref type="bibr" target="#b14">[15]</ref> and music source separation <ref type="bibr" target="#b32">[33]</ref>. In this study, all output layers z l in a dense block have the same number of channels, denoted by K. The total number of layers in each dense block is denoted by L. As shown in <ref type="figure">Fig. 1</ref>, we alternate 9 dense blocks with 4 downsampling layers and 4 upsampling layers. After the last dense block, we use a 1 × 1 CNN layer to reorganize the feature map, and then output two masks. In CNNs, convolutional kernels are usually applied across the entire input field. This is reasonable in the case of visual processing, where similar patterns can appear anywhere in the visual field with translation and rotation. However, in the auditory representation of speech, patterns that occur in different frequency bands are usually different. A generic CNN kernel may result in inconsistent outputs at different frequencies. To address this problem, Takahashi and Mitsufuji <ref type="bibr" target="#b32">[33]</ref> split the spectral input into several subbands, and train band-dependent CNNs, leading to a substantial rise in model size.</p><p>We propose a frequency mapping layer which effectively alleviates this problem with a significant reduction of parameters. The basic idea is to project inconsistent frequency outputs to an organized space using a fully-connected layer. We replace one CNN layer in each dense block with a frequency mapping layer. The input to a frequency mapping layer is a concatenation of CNN layers z 0 l = [z l−1 , z l−2 , ..., z 0 ] ∈ R T ×F ×K , where T and F denote time and frequency respectively, K the number of channels in the input. z 0 l is passed to a 1 × 1 convolutional layer, followed by ELU activation and layer normalization, to reduce the number of channels to K. The resulting output is denoted by z 1 l ∈ R T ×F ×K . We then transpose the F and K dimension of z 1 l to get z 2 l ∈ R T ×K×F . Next, z 2 l is fed to a 1 × 1 convolutional layer, followed by ELU activation and layer normalization, to output z 3 l ∈ R T ×K×F . This layer can also be viewed as a frequencywise fully connected layer, which takes all frequency estimates as the input and reorganize them in a different space. Finally, z 3 l is transposed back, and the output of the frequency mapping layer z l ∈ R T ×F ×K is generated.</p><p>B. Sequential Grouping Stage 1) Baseline system: In this stage, we group frame-level spectral estimates across time using a clustering network, which corresponds to sequential grouping in CASA. In deep clustering based speaker separation, T-F level embedding vectors estimated by BLSTM are clustered into different speakers. We extend this framework to frame-level speaker tracking. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates our sequential grouping. We first stack the mixture spectrogram and two spectral estimates (including real, imaginary and magnitude STFT) as the input to the system. A neural network then projects frame-level inputs to a D-dimensional unit-length embedding vector V(t) ∈ R D . The target label is a two-dimensional indicator vector, denoted by A(t). During the training of tPIT, if the minimum loss is achieved whenX 1 (t) is paired with speaker 1, andX 2 (t) is paired with speaker 2, we set A(t) to [1 0]. Otherwise, A(t) is set to [0 1]. In other words, A(t) indicates the optimal output assignment of each frame. V(t) and A(t) can be reshaped into a T × D matrix V, and a T × 2 matrix A, respectively. A permutation independent objective function <ref type="bibr" target="#b11">[12]</ref> is:</p><formula xml:id="formula_11">J DC = ||VV T − AA T || 2 F<label>(12)</label></formula><p>where || · || F is the Frobenius norm. Optimizing J DC forces V(t) corresponding to the same optimal assignment to get closer during training, and otherwise to become farther apart.</p><p>Because we care more about the speaker assignment of frames where the two outputs are substantially different, a weight w(t) = |LD(t)| t |LD(t)| is used during training where LD(t) represents the frame-level loss difference (LD) between the two possible speaker assignments. LD(t) is large if two conditions are both satisfied: 1) the frame-level energy of the mixture is high; 2) the two frame-level outputs,X 1 (t, f ) and X 2 (t, f ), are quite different, so that the losses with respect to different speaker assignments are significantly different. w(t) can be used to construct a diagonal matrix W = diag(w(t)). The final weighted objective function is:</p><formula xml:id="formula_12">J DC−W = ||W 1/2 (VV T − AA T )W 1/2 || 2 F<label>(13)</label></formula><p>This objective function emphasizes frames where the speaker assignment plays an important role. During inference, the K-means algorithm is first applied to cluster V(t) into two groups. We then organize frame-level outputs according to their K-means labels. Finally, iSTFT is employed to convert complex outputs to the time domain.</p><p>2) Temporal convolutional networks for sequential grouping: Temporal convolutional networks (TCNs) have been used as a replacement for RNNs, and have shown comparable or better performance in various tasks <ref type="bibr" target="#b1">[2]</ref> [22] <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b28">[29]</ref>. In TCNs, a series of dilated convolutional layers are stacked to form a deep network, which enables very long memory. In this study, we adopt a TCN similar to TasNet <ref type="bibr" target="#b26">[27]</ref> for sequential grouping, as illustrated in <ref type="figure">Fig. 3</ref>.</p><p>In the proposed TCN, input features are first passed to a 2-D dense CNN block, a 1 × 1 convolutional layer and a layer normalization module, to perform frame-level feature preprocessing. The 1 × 1 convolutional layer here refers to a 1-D CNN layer with a kernel size of 1. The preprocessed features are then passed to a series of dilated convolutional blocks, with an exponentially increasing dilation factor (2 0 , 2 1 , ..., 2 M −1 ) to exploit large temporal contexts. Next, the M stacked dilated convolutional blocks are repeated 3 times to further increase the receptive field. Lastly, the outputs are fed into a 1 × 1 convolutional layer for embedding estimation.</p><p>In each dilated convolutional block, a bottleneck input with B channels I 0 ∈ R T ×B is first passed to a 1 × 1 convolutional layer, followed by PReLU (parametric rectified linear unit) activation <ref type="bibr" target="#b9">[10]</ref> and layer normalization, to extend the number of channels to H, with output denoted by I 1 ∈ R T ×H . A depthwise dilated convolutional layer <ref type="bibr" target="#b4">[5]</ref> with kernel S ∈ R 3×H , followed by PReLU activation and layer normalization, is then employed to capture the temporal context. The number 3 here indicates the size of the temporal filter in each channel, and there are H depthwise separable filters in the kernel. We adopt non-causal filters to exploit both past and future information, with a dilation factor from 2 0 ,... 2 M −1 , as in <ref type="bibr" target="#b26">[27]</ref>. The output of this part is denoted by I 2 ∈ R T ×H , which is then passed to a 1 × 1 convolutional layer to project the number of channels back to B, denoted by I 3 ∈ R T ×B . In the end, an identity residual connection combines I 3 and I 0 and forms the final output.</p><p>Overfitting is a major concern in sequence models. If not regularized properly, sequence models tend to memorize the patterns in the training data, and get trapped in local minima. To address this issue, various dropout techniques <ref type="bibr" target="#b8">[9]</ref> [28] <ref type="bibr" target="#b30">[31]</ref> have been proposed for RNNs. Consistent improvement has been achieved if dropout is applied to recurrent connections <ref type="bibr" target="#b27">[28]</ref>. Meanwhile, a simple dropout scheme for TCNs is used in <ref type="bibr" target="#b1">[2]</ref>, i.e., dropping I 3 in each dilated convolutional block, but it does not yield satisfactory performance in our experience. Based on these findings, we design a new dropout scheme for the TCN model, denoted by dropDilation. In dropDilation, the dilated connections in depthwise dilated convolutional layers are dropped with a probability of (1 − p), where p denotes the keep rate. To be more specific, a binary mask,</p><formula xml:id="formula_13">m = [m −d 1 m d ] T ∈ R 3×1</formula><p>, is multiplied with each depthwise dilated convolutional kernel S ∈ R 3×H during training, with m −d and m d drawn independently from a Bernoulli distribution Bernoulli(p). In dropDilation, we only drop the dilated connections while keeping the direct connections to preserve local information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION AND COMPARISON A. Experimental Setup</head><p>We use the WSJ0-2mix dataset, a monaural two-talker speaker separation dataset introduced in <ref type="bibr" target="#b11">[12]</ref>, for evaluations. WSJ0-2mix has a 30-hour training set and a 10-hour validation set generated by selecting random speaker pairs in the Wall Street Journal (WSJ0) training set si tr s, and mixing them at various SNRs between 0 dB and 5 dB. Evaluation is conducted on the 5-hour open-condition (OC) test set, which is similarly generated using 16 untrained speakers from the WSJ0 development set si dt 05 and si et 05. All mixtures are sampled at 8 kHz. STFT with a frame length of 32ms, a frame shift of 8 ms, and a square root hanning window is taken for the whole system.</p><p>We report results in terms of signal-to-distortion ratio improvement (∆SDR) <ref type="bibr" target="#b33">[34]</ref>, perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b16">[17]</ref>, and extended short-time objective intelligibility (ESTOI) <ref type="bibr" target="#b18">[19]</ref>, to measure source separation performance, speech quality and speech intelligibility, respectively. We also report the final result in terms of scale-invariant signal-tonoise ratio improvement (∆SI-SNR) <ref type="bibr" target="#b26">[27]</ref> for a systematical comparison with other competitive systems.</p><p>B. Models 1) Simultaneous grouping models: Two models are evaluated for simultaneous grouping: BLSTM and Dense-UNet.</p><p>The baseline BLSTM contains 3 BLSTM layers, with 896×2 units in each layer. In each dense block of Dense-UNet, the number of channels K is set to 64, the total number of dense layers L is set to 5, and all CNN layers have a kernel size of 3 × 3 and a stride of 1 × 1. The middle layer in each dense block is replaced with a frequency mapping layer. We use valid padding (a term in CNN literature referring to no input padding) for the last CNN layer in each dense block, and same padding (padding the input with zeros so that the output has the same dimension as the original input) for all other layers. The input STFT is zero-padded accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Min Loss</head><p>Grouped Output1</p><p>Grouped Output2</p><p>Frequency Mapping  <ref type="figure">Fig. 3</ref>. Diagram of the TCN used in sequential grouping. Outputs from the previous stage are fed into a series of dilated convolutional blocks to predict frame-level embedding vectors. The dilation factor of each block is marked on the right. The detailed structure of a dilated convolutional block is illustrated in the large gray box. The network within the dashed box can be also used for uPIT based speaker separation.</p><p>For both models, when trained with J tP IT −P SA t , the magnitude STFT of the mixture is adopted as the input, and ELU activation is applied to output layers for phase-sensitive mask estimation. If J tP IT −CA t or J tP IT −SN R is used for training, a stack of real and imaginary STFT is used as the input, and linear output layers are used to predict the real and imaginary parts of complex ratio masks separately.</p><p>Both networks are trained with the Adam optimization algorithm <ref type="bibr" target="#b19">[20]</ref> and dropout regularization <ref type="bibr" target="#b12">[13]</ref>. The initial learning rate is set to 0.0002 for BLSTM, and 0.0001 for Dense-UNet. Learning rate adjustment and early stopping are employed based on the loss on the validation set.</p><p>2) Sequential grouping models: Two models are evaluated for sequential grouping: BLSTM and TCN. Both models are trained on top of a well-tuned simultaneous grouping model.</p><p>The baseline BLSTM contains 4 BLSTM layers, with 300×2 units in each layer. In TCN, the maximum dilation factor is set to 2 6 = 64, to reach a theoretical receptive field of 8.128s. The number of bottleneck units B is selected as 256. The number of units in depthwise dilated convolutional layers H is set to 512. Same padding is employed in all CNN layers. DropDilation with p = 0.7 is applied during training.</p><p>A 2-D dense CNN block is used in both models for framelevel feature preprocessing, with K = 16, L = 4, a kernel size of 1 × 3 (T × F ) and a stride of 1 × 1. The dimensionality of embedding vectors D is set to 40. Both networks are trained with the Adam optimization algorithm, with an initial learning rate of 0.001 for BLSTM, and 0.00025 for TCN. Learning rate adjustment and early stopping are again adopted.</p><p>3) One stage uPIT models: To systematically evaluate the proposed methods, we train a Dense-UNet and a TCN with SNR objectives and uPIT training criterion, i.e., J uP IT −SN R . Other training recipes follow those in Section IV-B1 and IV-B2..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Comparisons</head><p>We first evaluate the simultaneous grouping stage. <ref type="table" target="#tab_3">Table I</ref> summarizes the performance of tPIT models with respect  to different network structures and training objectives. For all models, outputs are organized with the optimal speaker assignment before evaluation. Scores of mixtures are presented in the first row. Compared to BLSTM, Dense-UNet drastically reduces the number of trainable parameters to 4.7 million, and introduces significant performance gain. The frequency mapping layers in our Dense-UNet introduce a 0.3 dB increment in ∆SDR, 0.1 increment in PESQ, 0.8% increment in ESTOI and a parameter reduction of 0.9 million. Next, we switch from magnitude STFT to complex STFT, and change the training objective to J tP IT −CA t . This change leads to large improvement, revealing the importance of phase information for source separation. The SNR objective further outperforms the CA objective. We thus adopt tPIT Dense-UNet trained with J tP IT −SN R for simultaneous grouping in the following evaluations. <ref type="table" target="#tab_3">Table II compares</ref>  ment is improved by a large margin over tPIT. However, since frame-level loss is not optimized in uPIT, there is a significant gap between uPIT and tPIT with optimal assignment. <ref type="figure" target="#fig_1">Fig. 4</ref> illustrates the differences between tPIT and uPIT based Dense-UNet in more details. Because SNR objectives lead to less structured outputs in the T-F domain, the models illustrated in the figure are trained with CA objectives. Speaker assignment swaps frequently in the default outputs of tPIT. However, if we organize the outputs with the optimal assignment, the outputs almost perfectly match the clean sources, as shown in the fourth row. On the other hand, the default outputs of uPIT are much closer to the clean sources compared to tPIT. However, for this same-gender mixture, uPIT makes several assignment mistakes in the default outputs, e.g., from 2s to 2.5s, and from 5s to 5.2s. If we optimally organize uPIT's outputs, as in the last row, we can see uPIT exhibits much worse frame-level performance than tPIT. In some frames, e.g., around 4.9s, the predicted frequency patterns are totally mixed up. These observations reveal uPIT's limitations in both framelevel separation and speaker tracking for challenging speaker pairs.</p><p>Next, we evaluate different sequential grouping models in <ref type="table" target="#tab_3">Table III</ref>. The first two models are trained on top of the tPIT Dense-UNet with the SNR objective. As shown in the table,  <ref type="bibr" target="#b1">[2]</ref>. In the last four rows of <ref type="table" target="#tab_3">Table III</ref>, we report the results of uPIT models. The first uPIT model is trained using Dense-UNet, and it significantly underperforms both deep CASA systems. Even if the outputs are optimally reassigned, uPIT Dense-UNet still systematically underperforms deep CASA (tPIT Dense-UNet + TCN), due to its frame-level separation errors. We also train a TCN model with uPIT objectives, and it yields much worse results than uPIT Dense-UNet.</p><p>To further analyze the differences between deep CASA and uPIT, we present frame assignment error (FAE) for the best performing deep CASA system and the two uPIT based models in <ref type="table" target="#tab_3">Table IV</ref>. FAE is defined as the percentage of incorrectly assigned frames in terms of the minimum framelevel loss. As shown in the table, uPIT Dense-UNet generates the highest FAE, because the network is not specifically designed for sequence modeling. uPIT TCN slightly outperforms uPIT Dense-UNet due to its long receptive field. However, because uPIT TCN does not handle frequency patterns as well, its overall separation performance is worse than uPIT Dense-UNet. Deep CASA cuts FAE by half compared to uPIT models. Such results demonstrate the benefits of the proposed divide-and-conquer strategy, which optimizes framelevel separation and speaker tracking in turn, and achieves better performance in both objectives. <ref type="table">Table V</ref> compares deep CASA and uPIT systems for different gender combinations. Both systems achieve better results on male-female combinations than same gender conditions. The performance gap is larger for female-female mixtures, consistent with the observation in <ref type="bibr" target="#b23">[24]</ref>. This might be due to the unbalanced gender distribution in WSJ0-2mix OC, which contains 1086 male-male mixtures, but only 394 femalefemale mixtures. On the other hand, the performance gap between different gender combinations is much smaller in deep CASA than in uPIT, likely because deep CASA is better at speaker tracking. <ref type="figure" target="#fig_2">Fig. 5</ref> illustrates the results of deep CASA. As shown in the second row, tPIT Dense-UNet trained with SNR objectives generates entirely different default outputs compared to the same model trained with CA (cf. <ref type="figure" target="#fig_1">Fig. 4</ref>). The optimal assignments alternate almost every frame, leading to striped patterns. To study the phenomenon, we analyze the overall training process of tPIT Dense-UNet trained with J tP IT −SN R . At the beginning, the SNR objective leads to similar outputs as the CA objective. However, because there is 75% overlap between neighboring frames in the proposed STFT, models trained with SNR only need to make accurate predictions every other frame, with frames in between left blank. Such patterns start to occur after a few hundred training steps. The competing speaker then gradually fills in the blanks, and the striped patterns are thus formed. As shown in <ref type="figure" target="#fig_2">Fig. 5(f)</ref>, the K-means labels predicted by the sequential grouping system almost perfectly match the optimal labels in speech-dominant frames. However, organizing the default outputs with respect to the K-means labels leads to magnitude STFT that is quite different from the clean sources. Residual patterns from the interfering speaker still exist in some frames. If we convert the complex outputs in <ref type="figure" target="#fig_2">Fig. 5</ref>(g) and 5(h) to the time-domain, these residual patterns will be cancelled by the overlap-andadd operation in iSTFT due to their opposite phases. In the last row, we apply iSTFT and STFT in turn to the organized complex outputs, and the new results can almost perfectly match the clean sources.</p><p>Simultaneous and sequential grouping are optimized in turn in the above deep CASA systems. We now consider joint optimization, where the two stages are trained together with small learning rates (1/8 of the initial learning rates) for 40 epochs. For the simultaneous grouping module, we organize the outputs using estimated K-means labels, and compare them with the clean sources to form an SNR objective. Meanwhile, the sequential grouping module is trained using the weighted objective in Eq. 13. As joint training unfolds, we observe smoother outputs. Joint optimization introduces slight but consistent improvement in all three metrics (0.1 dB ∆SDR, 0.02 PESQ, and 0.3% ESTOI).</p><p>Finally, <ref type="table" target="#tab_3">Table VI</ref> compares the deep CASA system with joint optimization and other state-of-the-art talker-independent methods on WSJ0-2mix OC. For all methods, we list the best reported results, and leave unreported fields blank. The numbers of parameters in different methods are estimated according to the papers. The uPIT system <ref type="bibr" target="#b20">[21]</ref> is the basis of this study. TasNet <ref type="bibr" target="#b26">[27]</ref> extends uPIT to the waveform domain, where a TCN is utilized for separation. We have also trained a similar uPIT TCN in this work. However, due to the different domains of signal representation, our uPIT TCN yields slightly worse results than TasNet, which suggests that better performance may be achieved by extending the deep CASA framework to the time domain. In <ref type="bibr" target="#b38">[39]</ref>, a phase prediction network is trained on top of a DC network. It yields high PESQ. FurcaNeXt <ref type="bibr" target="#b31">[32]</ref> produces very high ∆SDR. The deep CASA system generates slightly lower ∆SDR results, but has much fewer parameters. In addition, deep CASA yields the best results in terms of ∆SI-SNR, PESQ and ESTOI. The last three rows present the results of the IBM, IRM and ideal phase-sensitive mask (PSM) with the STFT configuration in Section IV-A. Deep CASA systematically outperforms the ideal masks in terms of SDR and SI-SNR. However, there is still room for improvement in terms of PESQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUDING REMARKS</head><p>We have proposed a deep CASA approach to talkerindependent monaural speaker separation. Simultaneous grouping is first conducted to separate two speakers at the frame level. Sequential grouping is then employed to stream  separated frame-level spectra into two sources. The deep CASA algorithm optimizes frame-level separation and speaker tracking in turn in the two-stage framework, leading to much better performance than DC and PIT. Our contributions also include novel techniques such as complex ratio masking, SNR objectives, Dense-UNet with frequency mapping layers and TCN with dropDilation. Experimental results on the benchmark WSJ0-2mix dataset show that the proposed algorithm produces the state-of-the-art results, with a modest model size.</p><p>A major difference between our sequential grouping stage and deep clustering is that embedding operates at the T-F unit level in DC, and at the frame level in deep CASA. There are several advantages to our approach. First, DC excels at speaker tracking due to clustering, but it is not better than ratio masking for frame-level separation. Therefore, divide and conquer is a natural choice. Second, deep CASA is more flexible. Almost all DC based algorithms are built on time-frequency processing. Our sequential grouping works on frame-level outputs, which can be produced by estimating magnitude STFT, complex masks, or even time-domain signals. In addition, we reduce the computational complexity of clustering from O(F T ) in DC to O(T ) in deep CASA.</p><p>Although the deep CASA algorithm is formulated for two speakers, it can be extended to three or more speakers. First, additional output layers can be added in the simultaneous grouping stage. In sequential grouping, we can employ the setup in <ref type="bibr" target="#b24">[25]</ref> to predict one embedding vector for each framelevel spectral estimate. A constrained K-means algorithm can then be used to assign each frame-level embedding to a different speaker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Diagram of the sequential grouping stage. We use BLSTM or TCN as the neural network in this stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>tPIT and uPIT based Dense-UNet in terms of both optimal and default output assignments. Both models are trained with SNR objectives. Thanks to the utterance-level output-speaker pairing, uPIT's default assign-Speaker separation results of PIT based models in log-scale magnitude STFT. Two models, tPIT Dense-UNet and uPIT Dense-UNet, are trained with CA objectives. The complex outputs from the models are converted to log magnitude STFT for visualization. (a) A male-male test mixture. (b) Speaker1 in the mixture. (c) Speaker2 in the mixture. (d) tPIT's output1 with default assignment. (e) tPIT's output2 with default assignment. (f) tPIT's output1 with optimal assignment. (g) tPIT's output2 with optimal assignment. (h) uPIT's output1 with default assignment. (i) uPIT's output2 with default assignment. (j) uPIT's output1 with optimal assignment. (k) uPIT's output2 with optimal assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Speaker separation results of the deep CASA system, with tPIT Dense-UNet trained with SNR objectives for simultaneous grouping and TCN for sequential grouping. The same test mixture is used as in Fig. 4. The complex outputs from the models are converted to log magnitude STFT for visualization. (a). Speaker1 in the mixture. (b) Speaker2 in the mixture. (c) tPIT's output1 with default assignment. (d) tPIT's output2 with default assignment. (e) Optimal assignment (black and white bars represent two different assignments). (f) K-means assignment. (g) tPIT's output1 with K-means assignment. (h) tPIT's output2 with K-means assignment. (i) tPIT's output1 with K-means assignment after iSTFT and STFT. (j) tPIT's output2 with K-means assignment after iSTFT and STFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is with the Department of Computer Science and Engineering, The Ohio State University, Columbus, OH 43210-1277 USA (e-mail: liuyuz@cse.ohio-state.edu). D. L. Wang is with the Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH 43210-1277 USA (e-mail: dwang@cse.ohio-state.edu).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Convolutional layers and downsampling layers are alternated in Diagram of the Dense-UNet used in simultaneous grouping. Gray blocks denote dense CNN layers. DS blocks denote downsampling layers and US blocks denote upsampling layers. Skip connections are added to connect layers at the same level. The inputs, masks and outputs can be defined in either magnitude or complex STFT domain.</figDesc><table><row><cell></cell><cell cols="2">Frequency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Mapping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mask1</cell><cell>STFT1</cell></row><row><cell></cell><cell>Mixture STFT</cell><cell>D S</cell><cell>D S</cell><cell>D S</cell><cell>D S</cell><cell>U S</cell><cell>U S</cell><cell>U S</cell><cell>U S</cell><cell>1x1 conv</cell><cell>Mixture STFT</cell><cell></cell><cell>Frame-level CPSD</cell><cell>Frame-level CPSD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mask2</cell><cell>STFT2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mask</cell><cell>Mask</cell></row><row><cell></cell><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSTM</cell><cell>LSTM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>STFT</cell><cell>STFT</cell></row><row><cell></cell><cell>Min Loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Grouped</cell><cell>Grouped</cell></row><row><cell>Target1</cell><cell></cell><cell></cell><cell cols="2">Target2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output1</cell><cell>Output2</cell></row><row><cell cols="4">Frame-level Pair-wise Loss</cell><cell></cell><cell></cell><cell cols="2">Next Stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Label</cell></row><row><cell>Output1</cell><cell></cell><cell></cell><cell cols="2">Output2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K-means</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Embedding</cell></row><row><cell>Mask1</cell><cell>Mixture STFT</cell><cell></cell><cell></cell><cell>Mask2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NN</cell></row><row><cell></cell><cell>NN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output1</cell><cell>Mixture</cell><cell>Output2</cell></row><row><cell></cell><cell>Mixture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>STFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc>∆SDR, PESQ AND ESTOI FOR SIMULTANEOUS GROUPING MODELS WITH OPTIMAL OUTPUT ASSIGNMENT ON WSJ0-2MIX OC.</figDesc><table><row><cell></cell><cell cols="3">Objective # of param. ∆SDR (dB)</cell><cell cols="2">PESQ ESTOI (%)</cell></row><row><cell>Mixture</cell><cell>-</cell><cell>-</cell><cell>0.0</cell><cell>2.02</cell><cell>56.1</cell></row><row><cell>tPIT BLSTM</cell><cell>PSA</cell><cell>46.3M</cell><cell>13.0</cell><cell>3.13</cell><cell>86.7</cell></row><row><cell>tPIT Dense-UNet</cell><cell>PSA</cell><cell>4.7M</cell><cell>14.7</cell><cell>3.41</cell><cell>90.5</cell></row><row><cell>tPIT Dense-UNet</cell><cell>CA</cell><cell>4.7M</cell><cell>18.6</cell><cell>3.57</cell><cell>93.8</cell></row><row><cell>tPIT Dense-UNet</cell><cell>SNR</cell><cell>4.7M</cell><cell>19.1</cell><cell>3.63</cell><cell>94.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II ∆SDR,</head><label>II</label><figDesc>PESQ AND ESTOI FOR TPIT AND UPIT BASED DENSE-UNET TRAINED WITH SNR OBJECTIVES.</figDesc><table><row><cell></cell><cell cols="4">Output Assign. ∆SDR (dB) PESQ ESTOI (%)</cell></row><row><cell>tPIT Dense-UNet</cell><cell>Optimal Default</cell><cell>19.1 0.0</cell><cell>3.63 1.99</cell><cell>94.3 55.8</cell></row><row><cell>uPIT Dense-UNet</cell><cell>Optimal Default</cell><cell>17.0 15.2</cell><cell>3.40 3.24</cell><cell>91.6 88.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III COMPARISON</head><label>III</label><figDesc></figDesc><table><row><cell cols="5">OF DIFFERENT SEQUENTIAL GROUPING METHODS ON</cell></row><row><cell></cell><cell cols="2">WSJ0-2MIX OC.</cell><cell></cell><cell></cell></row><row><cell>Simul. Group.</cell><cell cols="2">Seq. Group. ∆SDR (dB)</cell><cell cols="2">PESQ ESTOI (%)</cell></row><row><cell>tPIT Dense-UNet</cell><cell>BLSTM</cell><cell>16.4</cell><cell>3.31</cell><cell>90.8</cell></row><row><cell>tPIT Dense-UNet</cell><cell>TCN</cell><cell>17.9</cell><cell>3.49</cell><cell>92.9</cell></row><row><cell>uPIT Dense-UNet</cell><cell>-</cell><cell>15.2</cell><cell>3.24</cell><cell>88.9</cell></row><row><cell>uPIT Dense-UNet</cell><cell>Optimal</cell><cell>17.0</cell><cell>3.40</cell><cell>91.6</cell></row><row><cell>uPIT TCN</cell><cell>-</cell><cell>13.5</cell><cell>3.06</cell><cell>85.9</cell></row><row><cell>uPIT TCN</cell><cell>Optimal</cell><cell>14.9</cell><cell>3.19</cell><cell>88.1</cell></row><row><cell cols="5">TCN substantially outperforms BLSTM, both having around</cell></row><row><cell cols="5">8 million parameters. The dropDilation technique in our TCN</cell></row><row><cell cols="5">introduces 0.5 dB ∆SDR gain compared to conventional</cell></row><row><cell>dropout</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV FRAME</head><label>IV</label><figDesc>ASSIGNMENT ERRORS FOR DIFFERENT METHODS FOR FRAMES WITH SIGNIFICANT ENERGY (AT LEAST -20 DB RELATIVE TO MAXIMUM FRAME-LEVEL ENERGY).</figDesc><table><row><cell>Simul. Group.</cell><cell cols="3">Seq. Group. Frame Assign. Errors (%)</cell><cell></cell></row><row><cell>tPIT Dense-UNet</cell><cell>TCN</cell><cell>1.38</cell><cell></cell><cell></cell></row><row><cell>uPIT Dense-UNet</cell><cell>-</cell><cell>3.49</cell><cell></cell><cell></cell></row><row><cell>uPIT TCN</cell><cell>-</cell><cell>3.07</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">∆SDR, PESQ AND ESTOI FOR DEEP CASA AND UPIT FOR DIFFERENT</cell></row><row><cell cols="3">GENDER COMBINATIONS.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Gender Comb.</cell><cell>∆SDR (dB)</cell><cell cols="2">PESQ ESTOI (%)</cell></row><row><cell></cell><cell>Female-Male</cell><cell>18.9</cell><cell>3.57</cell><cell>93.9</cell></row><row><cell>tPIT Dense-UNet + TCN Assign.</cell><cell>Female-Female</cell><cell>15.7</cell><cell>3.32</cell><cell>90.5</cell></row><row><cell></cell><cell>Male-Male</cell><cell>17.2</cell><cell>3.45</cell><cell>92.5</cell></row><row><cell></cell><cell>Female-Male</cell><cell>17.4</cell><cell>3.42</cell><cell>91.9</cell></row><row><cell>uPIT Dense-UNet</cell><cell>Female-Female</cell><cell>10.9</cell><cell>2.89</cell><cell>83.5</cell></row><row><cell></cell><cell>Male-Male</cell><cell>13.6</cell><cell>3.12</cell><cell>86.7</cell></row><row><cell></cell><cell>Female-Male</cell><cell>19.4</cell><cell>3.64</cell><cell>94.4</cell></row><row><cell>tPIT Dense-UNet + Opt. Assign.</cell><cell>Female-Female</cell><cell>18.8</cell><cell>3.61</cell><cell>93.9</cell></row><row><cell></cell><cell>Male-Male</cell><cell>18.7</cell><cell>3.62</cell><cell>94.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI NUMBER</head><label>VI</label><figDesc>OF PARAMETERS, ∆SDR, ∆SI-SNR, PESQ AND ESTOI FOR VARIOUS STATE-OF-THE-ART SYSTEMS EVALUATED ON WSJ0-2MIX OC.</figDesc><table><row><cell></cell><cell cols="2"># of param. ∆SDR (dB)</cell><cell cols="3">∆SI-SNR (dB) PESQ ESTOI (%)</cell></row><row><cell>Mixture</cell><cell>-</cell><cell>0.0</cell><cell>0.0</cell><cell>2.02</cell><cell>56.1</cell></row><row><cell>uPIT [21]</cell><cell>94.6M</cell><cell>10.0</cell><cell>-</cell><cell>2.84</cell><cell>-</cell></row><row><cell>TasNet [27]</cell><cell>8.8M</cell><cell>15.0</cell><cell>14.6</cell><cell>3.25</cell><cell>-</cell></row><row><cell>Wang et al. [39]</cell><cell>56.6M</cell><cell>15.4</cell><cell>15.2</cell><cell>3.45</cell><cell>-</cell></row><row><cell>FurcaNeXt [32]</cell><cell>51.4M</cell><cell>18.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep CASA</cell><cell>12.8M</cell><cell>18.0</cell><cell>17.7</cell><cell>3.51</cell><cell>93.2</cell></row><row><cell>IBM</cell><cell>-</cell><cell>13.8</cell><cell>13.4</cell><cell>3.28</cell><cell>89.1</cell></row><row><cell>IRM</cell><cell>-</cell><cell>13.0</cell><cell>12.7</cell><cell>3.68</cell><cell>92.9</cell></row><row><cell>PSM</cell><cell>-</cell><cell>16.7</cell><cell>16.4</cell><cell>3.98</cell><cell>96.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Auditory scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Informational and energetic masking effects in the perception of two simultaneous talkers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Brungart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="1101" to="1109" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech separation of a target speaker based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSP</title>
		<meeting>ICSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An algorithm to increase intelligibility for hearing impaired listeners in the presence of a competing talker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delfarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Vasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="4230" to="4239" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An unsupervised approach to cochannel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (PESQ) An objective method for end-to-end speech quality assessment of narrowband telephone networks and speech codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Recommendation P.862</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Singing voice separation with deep U-Net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMIR</title>
		<meeting>ISMIR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2009" to="2022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CBLDNN-based speakerindependent speech separation via generative adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="711" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Listening and grouping: an online autoregressive approach for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="692" to="703" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A CASA approach to deep learning based speaker-independent co-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5399" to="5403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speaker-independent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="787" to="796" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">TasNet: Surpassing ideal time-frequency masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07454</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TCNN: temporal convolutional neural network for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recurrent dropout without memory loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05118</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">FurcaNeXt: End-toend monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04891</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-scale multi-band DenseNets for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">Computational Auditory Scene Analysis: Principles, Algorithms and Applications</title>
		<editor>D. L. Wang and G. Brown</editor>
		<imprint>
			<publisher>Wiley-IEEE Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2708" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09010</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Single channel speech separation with constrained utterance level permutation invariant training using grid LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A deep ensemble learning method for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="967" to="977" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
