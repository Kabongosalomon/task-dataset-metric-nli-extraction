<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exemplar Guided Face Image Super-Resolution without Facial Landmarks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Dogan</surname></persName>
							<email>doganb@student.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>D-ITET</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
							<email>shgu@ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>D-ITET</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
							<email>radu.timofte@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>D-ITET</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exemplar Guided Face Image Super-Resolution without Facial Landmarks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, due to the ubiquitous visual media there are vast amounts of already available high-resolution (HR) face images. Therefore, for super-resolving a given very lowresolution (LR) face image of a person it is very likely to find another HR face image of the same person which can be used to guide the process. In this paper, we propose a convolutional neural network (CNN)-based solution, namely GWAInet, which applies super-resolution (SR) by a factor 8× on face images guided by another unconstrained HR face image of the same person with possible differences in age, expression, pose or size. GWAInet is trained in an adversarial generative manner to produce the desired high quality perceptual image results. The utilization of the HR guiding image is realized via the use of a warper subnetwork that aligns its contents to the input image and the use of a feature fusion chain for the extracted features from the warped guiding image and the input image. In training, the identity loss further helps in preserving the identity related features by minimizing the distance between the embedding vectors of SR and HR ground truth images. Contrary to the current state-of-the-art in face super-resolution, our method does not require facial landmark points for its training, which helps its robustness and allows it to produce fine details also for the surrounding face region in a uniform manner. Our method GWAInet produces photo-realistic images in upscaling factor 8× and outperforms state-of-theart in quantitative terms and perceptual quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face image super-resolution or face hallucination aims at reconstructing details / high-frequencies in low-resolution (LR) face images. This is an important problem due to the increasing need for high-resolution (HR) face images for different applications such as security, surveillance or other application that involves face recognition.</p><p>Due to the increasing interest in visual media and the development of the social media, it is very likely that given a LR face image of a person, we can find another HR face guiding face input our result ground truth image of the same person possibly taken at a different time in different conditions. This guiding face could be used in the super-resolution (SR) process to guide the hallucination of high frequencies/details, which might increase the quality of the HR result and help to preserve the identity related features. <ref type="figure" target="#fig_0">Fig. 1</ref> shows such a case and our result. The current state-of-the-art face image super-resolution approach <ref type="bibr" target="#b26">[27]</ref> proposed the use of a guiding image together with a facial landmark detector, where an additional loss term is optimized such that the facial landmarks of the warped guiding image and those of the ground truth image are close to each other. However, this approach seems to produce fine details for the face region in a non-uniform and unpredictable manner, resulting in SR images that look only partially sharp.</p><p>Although the recently proposed CNN-based SR solutions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref> provide state-of-the-art quantitative results in terms of peak signal-to-noise ratio (PSNR) when they optimize for reconstruction losses such as L1 or L2 in image space, the results are smooth without the fine details required for a good perceptual quality. This problem is more visible with the increase of the upscaling factor <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5]</ref>. On top of that, the PSNR measure is unable to capture perceptually important differences between two images as it relies on the differences between pixel-level values at the same position <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b14">15]</ref>. One way to introduce perceptually important features into the SR image is to use generative adversarial networks (GANs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5]</ref>. These networks help to create realistic SR images that look like HR images, which are naturally sharper and contain fine details.</p><p>In this paper, we introduce a novel CNN architecture capable of generating high quality HR face images with an upscaling factor 8×. During the SR process, the network utilizes the LR face image and the extra information provided by another HR face image of the same person while making the necessary processing through a warping subnetwork on this guiding HR image. By addressing the possible differences in contents (e.g. expression, pose, size) between two images the warper facilitates the extraction and integration of information from the guiding image. Contrary to the current state-of-the-art approach <ref type="bibr" target="#b26">[27]</ref>, our method does not require facial landmarks during training. This makes the network to learn and process the whole face region in a uniform manner and adds robustness. We add also an identity loss to further help in preserving the identity related features by minimizing the distance between the embedding vectors of HR result and ground truth images. Utilization of the guiding image expresses itself qualitatively as an improvement in visual content quality by correcting the inaccurate facial details. Finally, the adversarial loss that is incorporated via a GAN setting, will introduce fine details to the SR image and produce face images that are hardly distinguishable from real HR face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Convolutional Neural Networks (CNNs) and Image Super-Resolution (SR). CNNs have emerged as a successful method in many computer vision applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. Deep learning with CNNs has also become very widely used in image SR <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. CNNs have outperformed previous works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b0">1]</ref> both quantitatively and qualitatively. These networks, on the other hand, are mainly used for SR of single or multiple LR images and do not utilize a guiding HR image for the given LR image. In our work, we use the network given in <ref type="bibr" target="#b28">[29]</ref> as the main structural element of our subnetworks and specifically work on face images while utilizing the additional information provided by another HR face image of the same person.</p><p>Spatial Transformer Networks. Spatial transformer networks are modules that can be incorporated into an existing network and trained in an end-to-end fashion without any modification to the learning scheme or the loss function <ref type="bibr" target="#b20">[21]</ref>. They increase the spatial invariance of the network and provide invariance for large transformations <ref type="bibr" target="#b20">[21]</ref>. They are used to spatially transform input feature maps and consist of a localisation network and a sampler. In our work, we use the ideas from spatial transformer networks to create a flow field, which is then used in combination with a bilinear sampler to warp the guiding image, thus making the guiding image aligned with the contents of the input image.</p><p>Face Hallucination. CNNs have also shown great success in the field of face hallucination, where we apply superresolution on face images <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48]</ref>. <ref type="bibr" target="#b46">[47]</ref> applies face hallucination on tiny 16 × 16 faces. In <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48]</ref>, the authors again work on tiny 16 × 16 images but use spatial transformer networks <ref type="bibr" target="#b20">[21]</ref> in their generator architecture to alleviate the effects of misalignment of input images. Zhou et al. <ref type="bibr" target="#b50">[51]</ref> fuse two channels of information, namely extracted facial features and the LR input image, in order to overcome problems related with appearance variations and misalignment. However, they use resource intensive fully-connected layers in upscaling process and follow a simple fusion operation by just summing the upscaled LR input image via bicubic interpolation with the HR image created from the facial features. In our approach, on the other hand, we cope with the effects of appearance variations through the use of spatial transformer networks. However, contrary to <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48]</ref>, we introduce the spatial transformer network as a subnetwork that is only applied to the input image rather than intermediate feature maps and contrary to <ref type="bibr" target="#b50">[51]</ref>, we use resource efficient convolutional layers in upscaling process and follow a complex feature fusion for the information coming from two channels. Most importantly, these works do not incorporate the use of an additional HR image of the same person. In a very recent work <ref type="bibr" target="#b26">[27]</ref>, Li et al. use a guiding image and a warper subnetwork to cope with appearance variations between the LR input and the HR guiding image. However, they apply direct concatenation of warped guiding image and upscaled LR image at the input of the generator, which is different than our feature extraction and fusion based approach through the use of secondary feature extractor subnetwork, which is called GFEnet, for the warped guiding image. They also use landmark loss and total variation loss for their warping subnetwork in the joint training phase, whereas we do not incorporate these losses in our overall objective, thus our network does not require facial landmarks during training. Another difference is that they use conditional adversarial networks <ref type="bibr" target="#b19">[20]</ref> for generating the adversarial loss, whereas we use a Wasserstein generative adversarial network with gradient penalty (WGAN-GP) <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" target="#b26">[27]</ref> is the only recent paper known to us that uses an additional guiding image in face hallucination. As in our approach, Zhang et al. <ref type="bibr" target="#b49">[50]</ref> also use an identity loss in face hallucination problem, which is calculated between the SR image and the ground truth image.</p><p>Generative Adversarial Networks (GANs). Although the SR methods using CNN architectures provide state-ofthe-art quantitative results in PSNR terms when optimized for reconstruction losses such as L1 or L2 in image space, they produce overly-smooth visuals and lack the ability to produce images with fine details. The PSNR metric does not correlate well with the human perception of image quality <ref type="bibr" target="#b15">[16]</ref>. This is due to the fact that the reconstruction loss is calculated in image space and the optimum solution is the average of all possible solutions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>. GANs <ref type="bibr" target="#b12">[13]</ref>  have become successful in creating realistically looking images thanks to their adversarial loss. As a result of this, many methods make use of GANs. <ref type="bibr" target="#b11">[12]</ref> uses both a loss in feature space and an adversarial loss, in addition to the reconstruction loss in image space to generate sharp and natural looking images. Besides adversarial loss and reconstruction loss in image space, <ref type="bibr" target="#b30">[31]</ref> uses an additional image gradient difference loss between the input and the output that sharpens the image prediction to predict future images from a video sequence. In <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref>, they use adversarial loss and pixel loss to super-resolve tiny face images such that the resulting SR images have high frequency components. In <ref type="bibr" target="#b25">[26]</ref>, they use adversarial loss and feature loss for VGG-19 <ref type="bibr" target="#b33">[34]</ref> network to produce sharp and photo-realistic SR images. In <ref type="bibr" target="#b26">[27]</ref>, they also include adversarial loss in their total loss to improve the output visual quality of face restoration tasks from degraded observations. In our work, we use adversarial loss together with the L1 reconstruction loss in image space. Reconstruction loss drives the networks to match the contents of the output SR image with the contents of the input LR image. The adversarial loss, on the other hand, tries to ensure that the SR image contains high frequency features that make it photo-realistic. As a result of this loss combination, we get an SR image that agrees with the input LR image in terms of facial feature location and the coarse specifications for these facial features but also agrees with the specifications imposed by the distribution of the HR face images. We specifically use WGAN-GP, which optimizes for a different metric than the traditional GANs and is found to be more stable and easier to train <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Our proposed GWAInet solution (Guidance, Warper, Adversarial loss, Identity loss network) produces a SR image I SR from a LR input image I LR and a HR guiding image I GI . I LR is obtained from a ground truth highresolution image I GT by downscaling with bicubic interpolation in scale 8×. I GI is another HR face image of the person, to whom the tuple (I LR , I GT ) belongs to. We also denote the image that is obtained by warping I GI as I GW .</p><p>In the following, we provide detailed information about our GWAInet method. First, we briefly describe the WGAN-GP and then we present the network architecture of our model. Finally, we describe the loss functions used in guiding the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">WGAN-GP [14]</head><p>We use a generative adversarial network (GAN) approach <ref type="bibr" target="#b12">[13]</ref> to generate perceptually good and sharp images. Specifically, we use a Wasserstein GAN with gradient penalty (WGAN-GP) <ref type="bibr" target="#b13">[14]</ref>. With the help of this new architecture, we are aiming to make the SR images from our proposed network GWAInet as indistinguishable as possible from the HR images in our dataset. This is possible due to the structure of WGAN-GP and its training objective.</p><p>GANs consist of a generator subnetwork G and a discriminator subnetwork D, where the aim of G is to create samples that are as close as possible to the real data samples and the aim of the D is to classify these fake samples from the real ones. If we denote the real data distribution by p(x) and generated data distribution by q(y), then objective can be formulated as <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_0">min G max D E x ∼p(x) [logD(x )] + E y ∼q(y) [1 − logD(y )]<label>(1)</label></formula><p>In the traditional GAN setting, for an optimal discriminator, we are trying to minimize the JS-divergence between the real and generated data distributions <ref type="bibr" target="#b12">[13]</ref>. With this approach, training the model is a difficult process. This is due to the fact that in many practical problems, the real and the generated data distribution are disjoint in some low dimensional manifold in a high dimensional space, which makes it easier to find a perfect discriminator <ref type="bibr" target="#b1">[2]</ref>. When the discriminator becomes perfect, the gradient coming from the JS-divergence vanishes <ref type="bibr" target="#b12">[13]</ref>. There is an alternative method to avoid vanishing gradients by maximizing E y ∼q(y) [−logD(y )] for G, however it is shown that this method causes unstable updates <ref type="bibr" target="#b1">[2]</ref>.</p><p>Wasserstein generative adversarial network (WGAN) introduces a new loss function for GAN training, which depends on the Earth-Mover distance <ref type="bibr" target="#b2">[3]</ref> formulated as:</p><formula xml:id="formula_1">W (p, q) = inf γ∈ (p,q) E (x ,y )∼γ(x,y) [ x − y ] (2)</formula><p>In equation 2, denotes the set of all joint distributions γ(x, y), where y γ(x, y) = p(x) and x γ(x, y) = q(y). γ(x, y) can be seen as the amount of earth that should be transported from x to y to transform p into q. WGAN-GP is the improved version of WGAN, with an addition of gradient penalty term in the cost function instead of weight clipping procedure <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>The complete network architecture of the proposed solution is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The complete model, called GWAInet, consists of four network components, namely Gnet, Wnet, Cnet and Inet.</p><p>Warper (Wnet). Wnet's aim is to produce the flow field required to warp the guiding image such that it is well aligned with the contents of the input LR image, removing any difference in pose or size of the faces in both images. This warping procedure allows the extra information provided by I GI to be better utilized. Wnet is essentially the localisation network for a spatial transformer network <ref type="bibr" target="#b20">[21]</ref>. It produces the transformation parameters that is fed into the bilinear sampler along with I GI . Before the first layer of Wnet, upscaling via bicubic interpolation is applied to I LR , which scales the spatial dimensions by 8, producing the image I LRU . After that, I GI and I LRU are concatenated along the depth axis. The resulting tensor forms the input of Wnet. Wnet outputs a 3D flow field with 2 depth channels. At each pixel location, the first value determines the sampling motion horizontally and the second value determines the sampling motion vertically. It should be noted that flow field values are not scaled into a specific range, meaning that no constraints are applied at the output. The flow field and I GI are used in the bilinear sampling module to produce the warped guiding image I GW . Let us denote the flow field as Ω ∈ R h HR ×w HR ×2 and denote the pixel location grid for</p><formula xml:id="formula_2">I GW as δ ∈ R h HR ×w HR ×2 , where δ(i, j, 0) = 2×i</formula><p>h HR −1 − 1 and δ(i, j, 1) = 2×j w HR −1 − 1 ∀i ∈ {0, 1, ..., h HR − 1} and ∀j ∈ {0, 1, ..., w HR − 1}. Note that the grid values are in the range [−1, +1] instead of [0, h HR − 1] for δ(i, j, 0) and [0, w HR − 1] for δ(i, j, 1). In other words, in our setting, we assume that the top left corner of the image has coordinates (-1,-1) and the bottom right corner of the image has coordinates (+1,+1). Using Ω and δ, the sampling grid ρ ∈ R h HR ×w HR ×2 can be calculated as <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_3">ρ = Ω + δ<label>(3)</label></formula><p>This sampling grid dictates where to sample from the original input image, I GI , for an output pixel in the output image, I GW , which is the warped guiding image. After the calculation of ρ, the values are scaled back to [0, h HR − 1] for δ(i, j, 0) and [0, w HR − 1] for δ(i, j, 1) using the inverses of the previously given transforms. If we let I(i, j, c) represents the pixel intensity value at the (height = i, width = j, channel = c) location of the some image I, then the pixel intensity values at the output of the bilinear sampling module can be calculated using the following formula <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_4">I GW (i, j, c) = (a,b)∈H I GI (a, b, c) max {0, 1 − |ρ(i, j, 0) − a|} max {0, 1 − |ρ(i, j, 1) − b|}<label>(4)</label></formula><p>In equation 4, H refers to the 4 closest pixel indices with respect to the coordinate given by height = ρ(i, j, 0) and width = ρ(i, j, 1). Wnet can be trained end-to-end with a loss function using gradient based methods due to the fact that I GW is sub-differentiable with respect to the parameters of the Wnet <ref type="bibr" target="#b20">[21]</ref>. Generator (Gnet). Gnet is the network that applies SR on the I LR while using the additional information provided by the warped guiding image I GW . It consists of two smaller subnetworks, which are called SRnet and GFEnet. These two subnetworks represent two channels of information. SRnet takes only I LR as input, whereas GFEnet takes only I GW as input.</p><p>SRnet is the same baseline architecture used in <ref type="bibr" target="#b28">[29]</ref>. The architecture is given in <ref type="figure" target="#fig_1">Figure 2</ref>. It consists of 16 residual blocks <ref type="bibr" target="#b16">[17]</ref>, whose architecture can be found in the supplementary material. In our setting, scale parameter is set to α res = 1, which is recommended in <ref type="bibr" target="#b28">[29]</ref>. Throughout SRnet, spatial dimensions of I LR is preserved via zero padding. After the merging point of the global skip connection, upscaling blocks come, whose main responsibility is to gradually upscale the feature maps such that their spatial dimensions match with the spatial dimensions of I GT . The upscaling is done via efficient sub-pixel convolutional layers <ref type="bibr" target="#b32">[33]</ref>, that is in each upscaling block, 2× upscaling is performed by cascading a convolutional layer and a pixel shuffler layer. These convolutional layers apply a 3×3 filter with stride 1 and they have 256 feature maps.</p><p>GFEnet, which is used as a feature extractor for the I GW , consists of 3 downscaling blocks and 12 residual blocks. As can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>, each downscaling block is used to downscale the spatial dimensions of its input by 2. In each downscaling block, first, a convolutional layer with 3 × 3 kernel, 64 feature maps and stride 1 is applied, which is followed by a ReLU. Then another convolutional layer with 3 × 3 kernel, 64 feature maps and stride 2 is applied, which is again followed by ReLU. Downscaling of the I GW is done through series of stride 2 convolutions instead of maxpooling operation. The motivation is to let the model learn the downscaling procedure instead of fixing it <ref type="bibr" target="#b34">[35]</ref>. After every 4th residual block, the current features that come from GFEnet and features that come from SRnet are fused. This feature fusion is done via concatenation along the depth axis. Since only convolutions with 64 output feature maps are used in both subnetworks, after the concatenation, a feature map of depth 128 is obtained. A convolution operation follows this concatenation before the signal resulting from the fusion operation enters to the next residual block of SRnet. After 12th residual block, which also means that after the 3rd feature fusion, GFEnet reaches to an end.</p><p>Critic (Cnet). The critic network is the same discriminator network that is used in DCGAN architecture <ref type="bibr" target="#b31">[32]</ref> except we do not use batch normalization layers <ref type="bibr" target="#b18">[19]</ref>. The exact specifications of the architecture is given in the supplementary material.</p><p>Outputs of Gnet, I SR , form the generated samples, which should be criticized as fake images by the critic. The real images, which are samples from the real data distribution, are the same images that are used as the ground truth HR images for the LR input images. These should be criticized as real images by the critic.</p><p>Identity Encoder (Inet). We use a Siamese network <ref type="bibr" target="#b8">[9]</ref> for generating embedding vectors related with the identity of the person. We have selected VGG-16 network <ref type="bibr" target="#b33">[34]</ref> as the architecture of our siamese Inet, whose details can be found in the supplementary material. Given a SR face image I SR and its corresponding HR ground truth image I GT , Inet is used to evaluate their similarity. This similarity information is then used to penalize I SR that has characteristics that differ from the characteristics of its corresponding I GT . To learn the parameters θ Inet , we cast the problem as a binary classification problem, in which Inet tries to predict whether the two input images belong to the same person. This procedure can be guided by cross-entropy loss function, where the output is equal to</p><formula xml:id="formula_5">y = sigmoid(w T |Inet(x 1 ; θ Inet )− Inet(x 2 ; θ Inet )| + b)<label>(5)</label></formula><p>where Inet(x; θ Inet ), w ∈ R 4096 and b ∈ R. Note that the parameters w and b are only used during pretraining of Inet. Moreover, during the optimization of Wnet, Gnet and Cnet, θ Inet is frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Functions</head><p>Content loss L content . Content loss is equal to L1 loss in our setting and can be calculated as:</p><formula xml:id="formula_6">L content = 1 3h HR w HR h HR j=1 w HR k=1 3 c=1 |I SR (j, k, c) − I GT (j, k, c)| (6)</formula><p>L content ensures that contents of super-resolved image I SR match with those of I GT . Although this loss is vital in keeping the connection between I SR and I GT , and optimizes for high PSNR values, it results in I SR images that are formed by smooth regions and that lack high-frequency details <ref type="bibr" target="#b25">[26]</ref>.</p><p>Adversarial loss L adv . The aim of adversarial loss is to to make SR images look perceptually good and photorealistic, making generated SR data distribution and real HR data distribution as close as possible to each other. With the help of the adversarial loss, the SR image will have fine details and the network will combat the smoothing effect caused by the content loss.</p><p>The adversarial loss incurred by the WGAN-GP for the generator is equal to −L f ake . Note that L f ake = D(I SR ; θ Cnet ), where θ Cnet represents the parameters of the critic and D(I SR ; θ Cnet ) represents the output of the critic for I SR image.</p><p>Identity loss L id . Identity loss is calculated as the squared Euclidean norm of the distance between the embedding vectors of I SR and I GT . Thus,</p><formula xml:id="formula_7">L id = Inet(I SR ; θ Inet ) − Inet(I GT ; θ Inet ) 2 4096 (7)</formula><p>L id is used to penalize I SR that has characteristics that differ from the characteristics of its corresponding I GT , thus increasing the perceptual quality of the SR image.</p><p>Critic loss L c . We can calculate the loss incurred by the WGAN-GP for the critic as:</p><formula xml:id="formula_8">L c = L f ake − L real + λ gp L gp<label>(8)</label></formula><p>where L real = D(I GT ; θ Cnet ) and D(I GT ; θ Cnet ) represents the output of the critic for I GT image. λ gp is the coefficient for the gradient penalty and L gp , which is the gradient penalty term, is a function of I SR , I GT , θ Cnet and ∼ U nif orm [0, 1]. Exact details are given in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Overall objective. The overall objective function for the optimization of θ W net and θ Gnet can be written as:</p><formula xml:id="formula_9">L total = L content + λ adv L adv + λ id L id<label>(9)</label></formula><p>where λ adv and λ id are the weighting coefficients for L adv and L id respectively. The overall objective function for the optimization of θ Cnet is directly equal to L c . Optimization of these two objectives are done in an alternating fashion as also described in <ref type="bibr" target="#b13">[14]</ref>. Note that during this training procedure θ Inet is frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>CelebA <ref type="bibr" target="#b29">[30]</ref>. We used this dataset in developing our network and we moved to the dataset of <ref type="bibr" target="#b26">[27]</ref> for comparing our method with the state-of-the-art. We use the aligned and cropped version of the CelebA dataset. We select the same train-validation-test partitioning used by the creators of the dataset. We have removed all of the identities that has a single image from the dataset, resulting in 162,734 training, 19,862 validation and 19,959 test images. It should be noted that the identities in each set are disjoint. To remove as much background as possible and to focus on the faces, we further crop the images to dimensions 168 × 144. For a given LR input image, the guiding image is sampled uniformly from the remaining HR images of the same person.</p><p>Dataset of <ref type="bibr" target="#b26">[27]</ref>. This dataset is a subset of VggFace2 <ref type="bibr" target="#b7">[8]</ref> and CASIA-WebFace <ref type="bibr" target="#b44">[45]</ref> datasets. All of the images are collected from the wild and therefore include different expressions, pose and illumination conditions. For each identity, pairs of HR guiding and ground truth images are available. All HR images have spatial dimensions 256 × 256. Different from <ref type="bibr" target="#b26">[27]</ref>, we randomly select 2,273 among 20,273 training images as validation images, which means that we are working with a smaller training set of size 18,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Settings</head><p>As a preprocessing step, we scale the input pixel intensity values from [0, 255] to [0, 1] and then subtract the mean of the training dataset. We also scale the range of the I GT to [−1, 1].</p><p>We always use Adam optimizer <ref type="bibr" target="#b23">[24]</ref>. During training, whenever λ adv = 0, we use the suggested parameters β 1 = 0.9, β 2 = 0.999 and = 10 −8 <ref type="bibr" target="#b23">[24]</ref>. Whenever λ adv = 0, we use β 1 = 0.5 and β 2 = 0.9. During this adversarial training, we always apply 5 critic updates per each generator update and we set λ gp = 10.</p><p>Training on CelebA. This dataset is used only during the development of the proposed method. The identity loss is not used during the training of GWAInet, thus λ id in <ref type="figure">Equation 9</ref> is always set to 0. The training consists of three steps. We first pretrain the Wnet using L1 reconstruction loss between the warped guiding image and the ground truth image with learning rate 0.0001 for 1.25 epochs with batch size 4. In the second step, we train the whole net- work by setting λ adv = 0 in Equation 9 with learning rate 0.0001 and batch size 16 until the validation PSNR reaches its peak. Then we set λ adv = 0.001, and continue training for 4 epochs using batch size 4 and then another 2 epochs with learning rate 0.00005.</p><p>Training on the dataset of <ref type="bibr" target="#b26">[27]</ref>. We train the full model on this dataset. The training of GWAInet consists of two steps. We first train the network by setting λ adv , λ id = 0 in Equation 9 with learning rate 0.0001 and batch size 48 until the validation PSNR reaches its peak. Then we set λ adv = 0.001 and λ id = 0.05, and continue training for 8 epochs using batch size 16. During the training of GWAInet, parameters of Inet is fixed. The Inet is pretrained on the same training set for 12 epochs with learning rate 0.0001 and batch size 8. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on CelebA</head><p>After training on the CelebA dataset, we obtain the model GWAnet, which is the same model as the proposed full model GWAInet except that it does not include the identity loss in its optimization objective. Note that the identity loss is not related with the utilization of the guiding image because it is calculated between the super-resolved image and the ground truth image.</p><p>Model without guiding image.  outperforms BAnet in generating perceptually good looking face images due to the fact that GWAnet provides better visual content quality by correcting the inaccurate facial details through the use of I GI . In this context, visual content quality refers to the extent that the characteristics of the facial contents of I SR image match with those of I GT image. As can be seen in <ref type="figure" target="#fig_2">Figure 3</ref>, BAnet is fully capable of generating photo-realistic images as well as GWAnet but the point that sets GWAnet apart from BAnet is its ability to complete the missing facial details in I SR image by utilizing the guiding image I GI . The improvements express themselves as location and shape improvements of facial features such as eyes, eyebrows, nose, mouth, hair and wrinkles.</p><p>Guiding image with a different identity In order to evaluate the magnitude of the effect of the guiding image in generating I SR , we have carried over an experiment, where for a given identity, we feed a randomly selected guiding image with a different identity. We denote the resulting model as GWAnet-R. The comparison of I SR images for GWAnet and GWAnet-R is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In general, when the guiding image has a different identity, the resulting differences from the standard model are noticeable. For some cases, as exemplified by the second row in <ref type="figure" target="#fig_3">Figure 4</ref>, the complete facial structure of the person changes. The mentioned differences mainly express themselves as location and shape differences of eyes and eyebrows as shown in the first row in <ref type="figure" target="#fig_3">Figure 4</ref>. There are also cases, where wrinkles appear or disappear according to the selected guiding image. The qualitative differences between GWAnet and GWAnet-R suggest that apart from being an additional information about high-resolution face images, the identity of the guiding image also plays an important role in generating high quality face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art Methods</head><p>We compare our results quantitatively with the state-ofthe-art face hallucination methods CBN <ref type="bibr" target="#b51">[52]</ref>, WaveletSR <ref type="bibr" target="#b17">[18]</ref>, TDAE <ref type="bibr" target="#b47">[48]</ref>, GFRNet <ref type="bibr" target="#b26">[27]</ref> and super-resolution methods SRCNN <ref type="bibr" target="#b9">[10]</ref>, VDSR <ref type="bibr" target="#b22">[23]</ref>, SRGAN <ref type="bibr" target="#b25">[26]</ref>. For all those methods, we directly use the results reported in <ref type="bibr" target="#b26">[27]</ref>. Moreover, we compare our results qualitatively with GFRNet <ref type="bibr" target="#b26">[27]</ref>, which is the current state-of-the-art in face hallucination. Note that all of our experiments are performed for upscaling factor 8×.</p><p>Quantitative comparison. The quantitative results are shown in <ref type="table">Table 1</ref>. As can be seen from <ref type="table">Table 1</ref>, GWAInet outperforms the state-of-the-art in VggFace2 dataset by 1.47dB. It is the second best method in WebFace dataset and lags behind GFRNet <ref type="bibr" target="#b26">[27]</ref> by 0.1dB. However, we should note that the training of GWAInet is not optimized for highest PSNR due to the adversarial loss and identity loss terms in the overall objective, which conflict with the objective of maximizing PSNR. PSNR is not well capable of capturing perceptual quality in an image <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b14">15]</ref>. Moreover, it is possible to get highest PSNR values by training GWAInet shorter or longer with very small decrease in perceptual quality. We did not follow such a path because the focal point of this paper is presenting the capability of GWAInet in producing perceptually high quality SR images.</p><p>Qualitative comparison. As can be seen from <ref type="figure">Figure 5</ref>, our method GWAInet produces better looking and sharper face images than the state-of-the-art. GFRNet <ref type="bibr" target="#b26">[27]</ref> only sharpens a small area in the face region, whereas our method GWAInet introduces high frequency details for all parts of the image, including the hair. Moreover, GFRNet <ref type="bibr" target="#b26">[27]</ref> generally completely hallucinates the face of the per-Guiding face GFRNet <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours -Full</head><p>Ground truth Guiding face GFRNet <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> Ours -Full Ground truth <ref type="figure">Figure 5</ref>: Comparison with state-of-the-art. Our full model GWAInet produces perceptually high quality images while retaining the facial features related with the identity. To obtain the results for GFRNet, their publicly available model is used <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>  <ref type="table">Table 1</ref>: PSNR (dB) values for all models on the dataset of <ref type="bibr" target="#b26">[27]</ref>. Upscaling factor is 8× for all experiments. All results apart from the results of our models are taken from <ref type="bibr" target="#b26">[27]</ref>. Red and blue markers indicate the first and second highest value, respectively.</p><p>son such that the super-resolved face does not look like the same identity. Our method, on the other hand, is completely faithful to the identity of the person while super-resolving the face image. Furthermore, GFRNet <ref type="bibr" target="#b26">[27]</ref> most of the time outputs a super-resolution image that is blurry and that contains artifacts, whereas our method GWAInet produces sharp, visually appealing and photo-realistic results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel solution, namely GWAInet, for the task of face image super-resolution. Our GWAInet utilizes the additional information provided by a high-resolution guiding image of the same person. Our network does not use facial landmarks during training and is capable to produce fine details for the whole face region in a uniform manner. Moreover, in the training, the employed identity loss further helps in preserving the identity related features by minimizing the distance between the embedding vectors of the super-resolved and HR ground truth images. GWAInet produces photo-realistic images in upscaling factor 8× and outperforms state-of-the-art in PSNR terms and also for perceptual quality of super-resolved images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>This supplementary material provides the details on the network architectures used in our proposed solution from the main paper. <ref type="table">Table 2</ref> provides the descriptions of the Warper (Wnet), Critic (Cnet), and Identity Encoder (Inet) subnetworks as employed in our proposed GWAInet (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Our Network Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wnet</head><p>Cnet Inet k3n64s1* k5n64s2** k3n64s1* k3n64s2* k5n128s2** k3n64s1* k3n64s1* k5n256s2** max pool(k2s2) k3n64s2* k5n512s2** k3n128s1* k3n64s1* fc(1) k3n128s1* k3n64s2* max pool(k2s2) k3n64s1 k3n256s1* skip start k3n256s1* 8x ResBlock max pool(k2s2) k3n64s1 k3n512s1* skip end k3n512s1* pixel shuffler 2x k3n512s1* pixel shuffler 2x max pool(k2s2) pixel shuffler 2x k3n512s1* k3n2s1 k3n512s1* k3n512s1* max pool(k2s2) fc(4096)* fc(4096)* fc(4096) <ref type="table">Table 2</ref>: Architectures of Wnet, Cnet and Inet. Note that * and ** symbols refer to ReLU and LeakyReLU (α = 0.2) layers, respectively. k3n64s1 represents a convolution operation with kernel size 3x3, 64 feature maps and stride 1. The structure of the residual block is shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Residual Block</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Exemplar guided face image super-resolution result (8×) of our proposed GWAInet approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Proposed GWAInet and its Warper (Wnet), Generator (Gnet), Critic (Cnet) and Identity Encoder (Inet) subnetworks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CelebA results without (BAnet) and with (GWAnet) the use of the guiding face. [8× upscaling, LR input spatial dimensions 21 × 18]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of I SR face super resolved images for the cases when a guiding image with the same identity is used (GWAnet) and when a guiding image with a different identity is used (GWAnet-R) for CelebA dataset. [8× upscaling, LR input spatial dimensions21 × 18]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Residual block as introduced in<ref type="bibr" target="#b28">[29]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To evaluate the importance of the guiding image and the subnetworks related with the guiding image, i.e. Wnet and GFEnet component of Gnet, we create a new model called BAnet, which only includes Cnet and SRnet component of Gnet. It is trained exactly in the same fashion as GWAnet. GWAnet, with the help of the guiding image, almost always improves the quality of the face image by adding some missing details about the facial features of the person over BAnet. GWAnet</figDesc><table><row><cell>IGI</cell><cell>IGI</cell><cell>ILR</cell><cell>ISR</cell><cell>ISR</cell><cell>ground truth</cell></row><row><cell>same identity</cell><cell>different identity</cell><cell>input</cell><cell>same identity</cell><cell>different identity</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. [8× upscaling, LR input spatial dimensions 32 × 32]</figDesc><table><row><cell>Method</cell><cell cols="2">VggFace2 [8] WebFace [45]</cell></row><row><cell>SRCNN [10]</cell><cell>22.30</cell><cell>23.50</cell></row><row><cell>VDSR [23]</cell><cell>22.50</cell><cell>23.65</cell></row><row><cell>SRGAN [26]</cell><cell>23.01</cell><cell>24.49</cell></row><row><cell>CBN [52]</cell><cell>21.84</cell><cell>23.10</cell></row><row><cell>WaveletSR [18]</cell><cell>20.87</cell><cell>21.63</cell></row><row><cell>TDAE [48]</cell><cell>20.19</cell><cell>20.24</cell></row><row><cell>GFRNet [27]</cell><cell>24.10</cell><cell>27.21</cell></row><row><cell>Ours (Full-GWAInet)</cell><cell>25.57</cell><cell>27.11</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our codes, models and results are publicly available on the project page: https://github.com/berkdogan2/GWAInet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly supported by ETH Zurich General Fund (OK), Huawei, and a GPU grant from Nvidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<title level="m">Towards Principled Methods for Training Generative Adversarial Networks. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai. Found</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07517</idno>
		<title level="m">pirm challenge on perceptual image superresolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Super-Resolution with Deep Convolutional Sufficient Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention-Aware Face Hallucination via Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
	<note>CVPR &apos;05</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="184" to="199" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating Images with Perceptual Similarity Metrics based on Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative Adversarial Networks. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A modified psnr metric based on hvs for quality assessment of color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhateja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Communication and Industrial Application</title>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Benchmarking of quality metrics on ultra-high definition video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 18th International Conference on Digital Signal Processing (DSP)</title>
		<imprint>
			<date type="published" when="2013-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1698" to="1706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial Transformer Networks. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning warped guidance for blind face restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning Warped Guidance for Blind Face Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<title level="m">Striving for Simplicity: The All Convolutional Net. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V S</forename><surname>Praveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON IMAGE PROCESS-ING</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals, Systems Computers</title>
		<imprint>
			<date type="published" when="2003-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to super-resolve blurry face and text images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning Face Representation from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Face hallucination with tiny unaligned images by transformative discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5367" to="5375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Super-identity convolutional neural network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning face hallucination in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3871" to="3877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep cascaded bi-network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
