<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Vial</surname></persName>
							<email>loic.vial@univ-grenoble-alpes.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIG</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
							<email>benjamin.lecouteux@univ-grenoble-alpes.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIG</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
							<email>didier.schwab@univ-grenoble-alpes.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIG</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation, by exploiting the semantic relationships between senses such as synonymy, hypernymy and hyponymy, in order to compress the sense vocabulary of Princeton WordNet, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduce the size of neural WSD models, with the benefit of improving their coverage without additional training data, and without impacting their precision. In addition to our methods, we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels, given a predefined sense inventory.</p><p>Various approaches have been proposed to achieve WSD: Knowledge-based methods rely on dictionaries, lexical databases, thesauri or knowledge graphs as primary resources, and use algorithms such as lexical similarity measures <ref type="bibr" target="#b9">(Lesk, 1986)</ref> or graph-based measures <ref type="bibr" target="#b15">(Moro et al., 2014)</ref>. Supervised methods, on the other hand, exploit sense annotated corpora as training instances for a classifier such as SVM <ref type="bibr" target="#b0">(Chan et al., 2007;</ref><ref type="bibr" target="#b30">Zhong and Ng, 2010)</ref>, or more recently by a neural network <ref type="bibr">(Kågebäck and Salomonsson, 2016)</ref>. Finally, unsupervised methods automatically iden-tify the different senses of words from unannotated or parallel corpora (e.g. <ref type="bibr" target="#b5">Ide et al. (2002)</ref>).</p><p>Supervised methods are by far the most predominant as they generally offer the best results in evaluation campaigns (for instance <ref type="bibr" target="#b16">(Navigli et al., 2007)</ref>). State of the art classifiers used to combine specific features such as the parts of speech and the lemmas of surrounding words <ref type="bibr" target="#b30">(Zhong and Ng, 2010)</ref>, but they are now replaced by neural networks which learn their own representation of words <ref type="bibr" target="#b24">(Raganato et al., 2017b;</ref><ref type="bibr" target="#b8">Le et al., 2018)</ref>.</p><p>One major bottleneck of supervised systems is the restricted quantity of manually sense annotated corpora: In the annotated corpus SemCor <ref type="bibr" target="#b12">(Miller et al., 1993)</ref>, the largest manually sense annotated corpus available, words are annotated with 33 760 different sense keys, which corresponds to only approximately 16% of the sense inventory of WordNet <ref type="bibr" target="#b13">(Miller, 1995)</ref>, the lexical database of reference widely used in WSD. Many works try to leverage this problem by creating new sense annotated corpora, either automatically <ref type="bibr" target="#b18">(Pasini and Navigli, 2017)</ref>, semi-automatically <ref type="bibr">(Taghipour and Ng, 2015)</ref>, or through crowdsourcing <ref type="bibr" target="#b29">(Yuan et al., 2016)</ref>.</p><p>In this work, the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet, such as the hypernymy, the hyponymy, the meronymy, the antonymy, etc. Our method is based on the observation that a sense and its closest related senses (its hypernym or its hyponyms for instance) all share a common idea or concept, and so a word can sometimes be disambiguated using only related concepts. Consequently, we do not need to know every sense of WordNet to disambiguate all words of WordNet.</p><p>For instance, let us consider the word "mouse" and two of its senses which are the computer mouse and the animal mouse. We only need to know the notions of "animal" and "electronic de-vice" to distinguish them, and all notions that are more specialized such as "rodent" or "mammal" are therefore superfluous. By grouping them, we can benefit from all other instances of electronic devices or animals in a training corpus, even if they do not mention the word "mouse". Contributions: In this paper, we hypothesize that only a subset of WordNet senses could be considered to disambiguate all words of the lexical database. Therefore, we propose two different methods for building this subset and we call them sense vocabulary compression methods. By using these techniques, we are able to greatly improve the coverage of supervised WSD systems, nearly eliminating the need for a backoff strategy that is currently used in most systems when dealing with a word which has never been observed in the training data. We evaluate our method on a state of the art WSD neural network, based on pretrained contextualized word vector representations, and we present results that significantly outperform the state of the art on every standard WSD evaluation task. Finally, we provide a documented tool for training and evaluating neural WSD models, as well as our best pretrained model in a dedicated GitHub repository 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In WSD, several recent advances have been made in the creation of new neural architectures for supervised models and the integration of knowledge into these systems. Multiple works also exploit the idea of grouping together related senses. In this section, we give an overview of these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WSD Based on a Language Model</head><p>In this type of approach, that has been initiated by <ref type="bibr" target="#b29">Yuan et al. (2016)</ref> and reimplemented by <ref type="bibr" target="#b8">Le et al. (2018)</ref>, the central component is a neural language model able to predict a word with consideration for the words surrounding it, thanks to a recurrent neural network trained on a massive quantity of unannotated data.</p><p>Once the language model is trained, it is used to produce sense vectors that result from averaging the word vectors predicted by the language model at all positions of words annotated with the given sense.</p><p>At test time, the language model is used to predict a vector according to the surrounding context, 1 https://github.com/getalp/disambiguate and the sense closest to the predicted vector is assigned to each word.</p><p>These systems have the advantage of bypassing the problem of the lack of sense annotated data by concentrating the power of abstraction offered by recurrent neural networks on a good quality language model trained in an unsupervised manner. However, sense annotated corpora are still indispensable to contruct the sense vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">WSD Based on a Softmax Classifier</head><p>In these systems, the main neural network directly classifies and attributes a sense to each input word through a probability distribution computed by a softmax function. Sense annotations are simply seen as tags put on every word, like a POS-tagging task for instance.</p><p>We can distinguish two separate branches of these types of neural networks: 1. Those in which we have several distinct and token-specific neural networks (or classifiers) for every different word in the dictionary <ref type="bibr" target="#b4">(Iacobacci et al., 2016;</ref><ref type="bibr">Kågebäck and Salomonsson, 2016</ref>), each of them being able to manage a particular word and its particular senses. For instance, one of the classifiers is specialized in choosing between the four possible senses of the noun "mouse". This type of approach is particularly fitted for the lexical sample tasks, where a small and finite set of very ambiguous words have to be sense annotated in several contexts, but it can also be used in all-words word sense disambiguation tasks.</p><p>2. Those in which we have a larger and general neural network that is able to manage all different words and assign a sense in the set of all existing sense in the dictionary used <ref type="bibr" target="#b24">(Raganato et al., 2017b)</ref>. The advantage of the first branch of approaches is that in order to disambiguate a word, limiting our choice to one of its possible senses is computationally much easier than searching through all the senses of all words. To put things in perspective, the average number of senses of polysemous words in WordNet is approximately 3, whereas the total number of senses considering all words is 206 941.</p><p>The second approach, however, has an interesting property: all senses reside in the same vector space and hence share features in the hidden layers of the network. This allows the model to predict an identical sense for two different words (i.e. synonyms), but it also offers the possibility to predict a sense for a word not present in the dictionary (e.g. neologism, spelling mistake...).</p><p>Finally, in two recent articles, <ref type="bibr" target="#b10">Luo et al. (2018a)</ref> and <ref type="bibr" target="#b11">Luo et al. (2018b)</ref> have proposed an improvement of these type of architectures, by computing an attention between the context of a target word and the gloss of its different senses. Thus, their work is one of the first to incorporate knowledge from WordNet into a WSD neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sense Clustering Methods</head><p>Several works exploit the idea of grouping together mutiple WordNet sense tags in order to create a coarser sense inventory which can potentially be more useful in some NLP tasks.</p><p>In the works of <ref type="bibr" target="#b1">Ciaramita and Altun (2006)</ref>, the authors propose a supervised system that learns and predicts "Supersense" tags, which belong to the set of the broad semantic categories of senses, organizing the sense inventory of WordNet. This tagset consists, in their work, of 26 categories for nouns (such as "food", "person" or "object"), and 15 categories for verbs (such as "emotion" or "weather"). By predicting supersense tags instead of the usual fine-grained sense tags of WordNet, the output vocabulary of their system is shrinked to only 41 different classes, and this leads to a small and easy-to-train model able to perform partial WSD, which could be useful and sufficient for other NLP tasks where the fine-grained distinction is not necessary.</p><p>In <ref type="bibr">Izquierdo et al. (2007)</ref>, the authors propose several methods for creating "Basic Level Concepts" (BLC), groups of related senses with a generally smaller size than supersenses, and which can be controlled by a threshold variable. Their methods rely on the semantic relationships between senses of WordNet, and, in the same way as <ref type="bibr" target="#b1">Ciaramita and Altun (2006)</ref>, they evaluated their clusters on a modified WSD task, where supersenses or BLC have to be predicted instead of the original sense tags from WordNet.</p><p>The main difference between our work and these works is that our end goal is to improve finegrained WSD systems. Even though our methods generate clusters of related senses, we guarantee that two different senses of a lemma reside in two different clusters, so at the end, even if our supervised system produces a cluster tag for a target word, we are still able to find back the true sense tag, by simply keeping track of which sense key of its lemma belongs to the predicted group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sense Vocabulary Compression</head><p>Current state of the art supervised WSD systems such as <ref type="bibr" target="#b29">Yuan et al. (2016)</ref>, <ref type="bibr" target="#b24">Raganato et al. (2017b)</ref>, <ref type="bibr" target="#b10">Luo et al. (2018a)</ref> and <ref type="bibr" target="#b8">Le et al. (2018)</ref> are all confronted to the following issues: 1. Due to the small number of manually sense annotated corpora available, a target word may never be observed during the training, and therefore the system is not able to annotate it. 2. For the same reason, a word may have been observed, but not all of its senses. In this case the system is able to annotate the word, but if the expected sense has never been observed, the output will be wrong, regardless of the architecture of the supervised system. 3. Training a neural network to predict a tag which belongs to the set of all WordNet senses can become extremely slow and requires a lot of parameters with a large output vocabulary. And this vocabulary goes up to 206 941 if we consider all word-senses of WordNet. In order to overcome all these issues, we propose a method for grouping together multiple sense tags that refer in fact to the same concept. In consequence, the output vocabulary decreases, the ability of the trained system to generalize improves, as well as its coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From Senses to Synsets: A Vocabulary Compression Based on Synonymy</head><p>In the lexical database WordNet, senses are organized in sets of synonyms called synsets. A synset is technically a group of one or more word-senses that have the same definition and consequently the same meaning. For instance, the first senses of "eye", "optic" and "oculus" all refer to a common synset which definition is "the organ of sight". Illustrated in <ref type="figure">Figure 1</ref>, the word-sense to synset mapping is hence a way of compressing the output vocabulary, and it is already applied in many works <ref type="bibr" target="#b29">(Yuan et al., 2016;</ref><ref type="bibr" target="#b8">Le et al., 2018)</ref>, while not being always explicitly stated. This method clearly helps to improve the coverage of supervised systems however. Indeed, if the verb "help" is observed in the annotated data in its first sense, the context surrounding the target word can be used to later annotate the verb "assist" or "aid" with the same valid synset tag.  <ref type="figure">Figure 1</ref>: Word-sense to synset mapping (compression through synonymy) applied on the first two senses of the words "help", "aid" and "assist".</p><p>Going further, other information from WordNet can help the system to generalize. Our first new method takes advantage of the hypernymy and hyponymy relationships to achieve the same idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compression through Hypernymy and Hyponymy Relationships</head><p>According to <ref type="bibr" target="#b21">Polguère (2003)</ref>, hypernymy and hyponymy are two semantic relationships which correspond to a particular case of sense inclusion: the hyponym of a term is a specialization of this term, whereas its hypernym is a generalization. For instance, a "mouse" is a type of "rodent" which is in turn a type of "animal". In WordNet, these relationships bind nearly every noun together in a tree structure 2 that goes from the generic root, the node "entity" to the most specific leaves, for instance the node "whitefooted mouse". These relationships are also present on several verbs: for instance "add" is a way of "compute" which is a way of "reason".</p><p>For the sake of WSD, just like grouping together the senses of the same synset helps to better generalize, we hypothesize that grouping together the synsets of the same hypernymy relationship also helps in the same way. The general idea of our method is that the most specialized concepts in WordNet are often superfluous for WSD.</p><p>Indeed, considering a small subset of WordNet that only consists of the word "mouse", its first sense (the small rodent), its fourth sense (the elec- tronic device), and all of their hypernyms. This is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We can see that every concept that is more specialized than the concepts "artifact" and "living_thing" could be removed. We could map every tag of "mouse#1" to the tag of "living_thing#1" and we could still be able to disambiguate this word, but with a benefit: all other "living things" and animals in the sense annotated data could be tagged with the same sense. They would give examples of what is an animal and then show how to differentiate the small rodent from the hand-operated electronic device.</p><p>Therefore, the goal of our method is to map every sense of WordNet to its highest ancestor in the hypernymy hierarchy, but with the following constraints: First, this ancestor must discriminate all the different senses of the target word. Second, we need to preserve the hypernyms that are indispensable to discriminate the senses of the other words in the dictionary. For instance, we cannot map "mouse#1" to "living_thing#1", because the more specific tag "animal#1" is essential to distinguish the two senses of the word "prey" (one sense describes a person, the other describes an animal). Our method thus works in two steps: 1. We mark as "necessary" the children of the first common ancestor of every pair of senses of every word of WordNet. 2. We map every sense to its first ancestor in the hypernymy hierarchy that has been previously marked as "necessary".</p><p>As a result, the most specific synsets of the tree that are not indispensable for discriminating any word of the lexical inventory are automatically removed from the vocabulary. In other words, the set of synsets that is left in the vocabulary is the smallest subset of all synsets that are necessary to distinguish every sense of every word of WordNet, following the hypernym and hyponym links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compression through all semantic relationships</head><p>In addition to hypernymy and hyponymy, Word-Net contains several other relationships between synsets, such as the instance relationship (e.g. "Albert Einstein" is an instance of "physicist'), the meronymy (X is part of Y, or X is a member of Y) and its counterpart the holonymy, the antonymy (X is the opposite of Y), etc. We hence propose a second method for sense vocabulary compression, that considers all the semantic relationships offered by WordNet, in order to form clusters of related synsets.</p><p>For instance, using all semantic relationships, we could form a cluster containing "physicist", "physics" (domain category), "Albert Einstein" (instance of), "astronomer" (hyponym), but also further related senses such as "photon", because it is a meronym of "radiation", which is a hyponym of "energy", which belongs to the same domain category of "physics".</p><p>Our method works by constructing these clusters iteratively: First, we initialize the set of clusters C with one synset in each cluster.</p><p>C ={c 0 , c 1 , ..., c n } S = {s 0 , s 1 , ..., s n } C ={{s 0 }, {s 1 }, ..., {s n }} Then at each step, we sort C by sizes of clusters, and we peek the smallest one c x and the smallest related cluster to c x , c y . We define a cluster being related to another if they contain at least one synset that have a semantic link together. We merge c x and c y together, and we verify that the operation still allows to discriminate the different senses of all words in the lexical database. If it is not the case, we cancel the merge and we try another semantic link. If no link is possible, we try to create one with the next smallest cluster, and if no further link can be created, the algorithm stops.</p><p>In <ref type="figure">Figure 3</ref>, we show a possible set of clusters that could result from our method, focusing on two senses of the word "Weber" and only on a few relationships.  <ref type="figure">Figure 3</ref>: Example of clusters of sense that could result from our method, if we limit our view to two senses of the word "Weber" and only some relationship links.</p><p>This method produces clusters significantly larger than the method based on hypernyms. On average, a cluster has 5 senses with the hypernym method, whereas it has 17 senses with this method. This method, unlike the previous one, is also stochastic, because the formation of clusters depends on the underlying order of iteration when multiple clusters are the same size. However, because we always sort clusters by size before creating a link, we observed that the final vocabulary size (i.e. number of clusters) is always between 11 000 and 13 000. In the following, we consider a resulting mapping where the algorithm stopped after 105 774 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Vocabulary size  <ref type="table">Table 1</ref>: Effects of the sense vocabulary compression on the vocabulary size and on the coverage of the SemCor.</p><p>In <ref type="table">Table 1</ref>, we show the effect of the common compression through synonyms, our first proposed compression through hypernyms, and our second method of compression through all semantic relationships, on the size of the vocabulary of Word-Net sense tags, and on the coverage of the SemCor corpus. As we can see, the sense vocabulary size is drastically decreased, and the coverage of the same corpus really improved.</p><p>In order to evaluate our sense vocabulary compression methods, we applied them on a neural WSD system based on a softmax classifier capable of classifying a word in all possible synsets of Word-Net (see subsection 2.2). We implemented a system similar to <ref type="bibr" target="#b24">Raganato et al. (2017b)</ref>'s BiLSTM but with some key differences. In particular, we used BERT contextualized word vectors <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> in input of our network, Transformer encoder layers <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> instead of LSTM layers as hidden units, our output vocabulary only consists of sense tags seen during training (mapped according to the compression method used), and we ignore the network's predictions on words that are not annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>For BERT, we used the model named "bert-largecased" of the PyTorch implementation 3 , which consists of vectors of dimension 1024, trained on BooksCorpus and English Wikipedia.</p><p>Due to the fact that BERT's internal tokenizer sometimes split words in multiples tokens (i.e. ["rodent"] becomes ["rode", "##nt"]), we trained our system to predict a sense tag on the first token only of a splitted annotated word.</p><p>For the Transformer encoder layers, we used the same parameters as the "base" model of <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref>, that is 6 layers with 8 attention heads, a hidden size of 2048, and a dropout of 0.1.</p><p>Finally, because BERT already encodes the position of the words inside their vectors, we did not add any positional encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We compared our sense vocabulary compression methods on two training sets: The SemCor, and the concatenation of the SemCor and the Princeton WordNet Gloss Corpus (WNGC). The latter is a corpus distributed as part of WordNet since its version 3.0, and it consists of the definitions (glosses) of every synset of WordNet, with words manually or semi-automatically sense annotated. We used the version of these corpora given as part of the UFSAC 2.1 resource 4 <ref type="bibr" target="#b28">(Vial et al., 2018)</ref>.</p><p>We performed every training for 20 epochs. At the beginning of each epoch, we shuffled the training set. We evaluated our model at the end of every epoch on a development set, and we kept only the one which obtained the best F1 WSD score. The development set was composed of 4 000 random sentences taken from the Princeton WordNet Gloss Corpus for the models trained on the Sem-Cor, and 4 000 random sentences extracted from the whole training set for the other models. For each training set, we trained three systems: 1. A "baseline" system that predicts a tag belonging to all the synset tags seen during training, thus using the common vocabulary compression through synonyms method. 2. A "hypernyms" system which applies our vocabulary compression through hypernyms algorithm on the training corpus. 3. A "all relations" system which applies our second vocabulary compression through all relations on the training corpus. We trained with mini-batches of 100 sentences, truncated to 80 words, and we used Adam (Kingma and Ba, 2015) with a learning rate of 0.0001 as the optimization method.  All models have been trained on one Nvidia's Titan X GPU. The number of parameters of individual models are displayed in <ref type="table" target="#tab_3">Table 2</ref>. As we can see, our compression methods drastically reduce the number of parameters, by a factor of 1.2 to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We evaluated our models on all evaluation corpora commonly used in WSD, that is the English all-words WSD tasks of the evaluation campaigns SensEval/SemEval. We used the fine-grained evaluation corpora from the evaluation framework of <ref type="bibr" target="#b23">Raganato et al. (2017a)</ref>, which consists of Sen-sEval 2 (Edmonds and <ref type="bibr" target="#b3">Cotton, 2001)</ref>, SensEval 3 <ref type="bibr" target="#b25">(Snyder and Palmer, 2004)</ref>, SemEval 2007 task 17 <ref type="bibr" target="#b22">(Pradhan et al., 2007)</ref>, SemEval 2013 task 12 <ref type="bibr" target="#b17">(Navigli et al., 2013)</ref> and SemEval 2015 task 13 <ref type="bibr" target="#b14">(Moro and Navigli, 2015)</ref>, as well as the "ALL" corpus consisting of the concatenation of all pre-  <ref type="table">Table 3</ref>: F1 scores (%) on the English WSD tasks of the evaluation campaigns SensEval/SemEval. The task "ALL" is the concatenation of SE2, SE3, SE07 17, SE13 and SE15. The first sense is assigned on words for which none of its sense has been observed during the training. Results in bold are to our knowledge the best results obtained on the task. Scores prefixed by a dagger ( †) are not provided by the authors but are deduced from their other scores.</p><p>vious ones. We also compared our result on the coarse-grained task 7 of SemEval 2007 <ref type="bibr" target="#b16">(Navigli et al., 2007)</ref> which is not present in this framework.</p><p>For each evaluation, we trained 8 independent models, and we give the score obtained by an ensemble system that averages their predictions through a geometric mean.  <ref type="table">Table 4</ref>: Coverage of our systems on the task "ALL". "Backoff on Monosemics" means that monosemic words are considered annotated.</p><p>In the results in <ref type="table">Table 3</ref>, we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it.</p><p>Our methods greatly improves their coverage on the evaluation tasks however. As we can see in <ref type="table">Table 4</ref>, on the total of 7 253 words to annotate for the corpus "ALL", the baseline system trained on the SemCor is not able to annotate 491 of them, while the vocabulary compression through hypernyms reduces this number to 91 and 24 for the compression through all relations.</p><p>When adding the Princeton WordNet Gloss Corpus to the training set, only one word (the monosemic adjective "cytotoxic") cannot be annotated with the system that uses the compression through all relations because its sense has not been observed during training.</p><p>If we exclude the monosemic words, the system based on our compression method through all relations miss only one word (the adverb "eloquently") when trained on the SemCor, and has a coverage to 100% when the WNGC is addded.</p><p>In comparison to the other works, thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings, we outperform systematically the state of the art on every task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In order to give a better understanding of the origin of our scores, we provide a study of the impact of our main parameters on the results. In addition to the training corpus and the vocabulary compression method, we chose two parameters that differentiate us from the state of the art: the pretrained word embeddings model and the ensembling method, and we have made them vary.</p><p>For the word embeddings model, we experimented with BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> as in our main results, with ELMo <ref type="bibr" target="#b20">(Peters et al., 2018)</ref>, and with GloVe <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref>, the same pre-trained word embeddings used by <ref type="bibr" target="#b10">Luo et al. (2018a)</ref>. For ELMo, we used the model trained on  <ref type="table">Table 5</ref>: Ablation study on the task "ALL" (i.e. the concatenation of all SensEval/SemEval tasks). For systems that do not use ensemble, we display the mean score (x) of eight individually trained models along with its standard deviation (σ).</p><p>Wikipedia and the monolingual news crawl data from <ref type="bibr">WMT 2008</ref><ref type="bibr">WMT -2012</ref> For GloVe, we used the model trained on Wikipedia 2014 and Gigaword 5. <ref type="bibr">6</ref> Due to the fact that GloVe embeddings do not encode the position of the words (a word has the same vector representation in any context), we used bidirectional LSTM cells of size 1 000 for each direction, instead of Transformer encoders for this set of experiments. In addition, because the vocabulary of GloVe is finite and all words are lowercased, we lowercased the inputs, and we assigned a vector filled with zeros to outof-vocabulary words. For the ensembling method, we either perform ensembling as in our main results, by averaging the prediction of 8 models trained separately or we give the mean and the standard deviation of the scores of the 8 models evaluated separately.</p><p>As we can see in <ref type="table">Table 5</ref>, the additional training corpus (WNGC) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art. Using BERT instead of ELMo or GloVe improves respectively the score by approximately 3 and 5 points in every experiment, and adding the WNGC to the training data improves it by approximately 2 points. Finally, using ensembles adds roughly another 1 point to the final F1 score. 5 https://allennlp.org/elmo 6 https://nlp.stanford.edu/projects/glove/ Finally, through the scores obtained by invidual models (without ensemble), we can observe on the standard deviations that the vocabulary compression method through hypernyms never impact significantly the final score. However, the compression method through all relations seems to negatively impact the results in some cases (when using ELMo or GloVe especially).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented two new methods that improve the coverage and the capacity of generalization of supervised WSD systems, by narrowing down the number of different sense in WordNet in order to keep only the senses that are essential for differentiating the meaning of all words of the lexical database. On the scale of the whole lexical database, we showed that these methods can shrink the total number of different sense tags in WordNet to only 6% of the original size, and that the coverage of an identical training corpus has more than doubled. We implemented a state of the art WSD neural network and we showed that these methods compress the size of the underlying models by a factor of 1.2 to 2, and greatly improve their coverage on the evaluation tasks. As a result, we reach a coverage of 99.99% of the evaluation tasks (1 word missing on 7 253) when training a system on the SemCor only, and 100% when adding the WNGC to the training data, on the pol-ysemic words. Therefore, the need for a backoff strategy is nearly eliminated. Finally, our method combined with the recent advances in contextualized word embeddings and with a training corpus composed of sense annotated glosses, our system achieves scores that considerably outperform the state of the art on all WSD evaluation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>or assistance" "improve the condition of" "act as an assistant"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sense vocabulary compression trough hypernymy hierarchy applied on the first and fourth sense of the word "mouse". Dashed arrows mean that some nodes are skipped for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Number of parameters of neural models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>±0.38 77.08 ±0.17 76.52 ±0.36 ±0.27 74.36 ±0.27 68.77 ±0.30</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">F1 Score on task "ALL" (%)</cell><cell></cell></row><row><cell>Training Corpus</cell><cell cols="2">Input Embeddings Ensemble</cell><cell cols="2">Baseline</cell><cell cols="2">Hypernyms</cell><cell cols="2">All relations</cell></row><row><cell></cell><cell></cell><cell></cell><cell>x</cell><cell cols="2">σx</cell><cell cols="2">σx</cell><cell>σ</cell></row><row><cell>SemCor+WNGC</cell><cell>BERT</cell><cell>Yes</cell><cell>78.27</cell><cell>-</cell><cell>79.00</cell><cell>-</cell><cell>78.48</cell><cell>-</cell></row><row><cell cols="4">SemCor+WNGC 76.97 SemCor+WNGC BERT No ELMo Yes 75.16</cell><cell>-</cell><cell>74.65</cell><cell>-</cell><cell>70.58</cell><cell>-</cell></row><row><cell cols="4">SemCor+WNGC 74.56 SemCor+WNGC ELMo No GloVe Yes 72.23</cell><cell>-</cell><cell>72.74</cell><cell>-</cell><cell>71.42</cell><cell>-</cell></row><row><cell>SemCor+WNGC</cell><cell>GloVe</cell><cell>No</cell><cell cols="6">71.93 ±0.35 71.79 ±0.29 69.60 ±0.32</cell></row><row><cell>SemCor</cell><cell>BERT</cell><cell>Yes</cell><cell>76.02</cell><cell>-</cell><cell>76.73</cell><cell>-</cell><cell>75.40</cell><cell>-</cell></row><row><cell>SemCor</cell><cell>BERT</cell><cell>No</cell><cell cols="6">75.06 ±0.26 75.59 ±0.16 73.91 ±0.33</cell></row><row><cell>SemCor</cell><cell>ELMo</cell><cell>Yes</cell><cell>72.55</cell><cell>-</cell><cell>73.09</cell><cell>-</cell><cell>69.43</cell><cell>-</cell></row><row><cell>SemCor</cell><cell>ELMo</cell><cell>No</cell><cell cols="6">72.21 ±0.13 72.83 ±0.24 68.74 ±0.29</cell></row><row><cell>SemCor</cell><cell>GloVe</cell><cell>Yes</cell><cell>70.77</cell><cell>-</cell><cell>71.18</cell><cell>-</cell><cell>68.44</cell><cell>-</cell></row><row><cell>SemCor</cell><cell>GloVe</cell><cell>No</cell><cell cols="6">70.51 ±0.16 70.77 ±0.21 67.48 ±0.55</cell></row><row><cell cols="3">HCAN (Luo et al., 2018a) (fully reproducible state of the art)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SemCor+WordNet glosses</cell><cell>GloVe</cell><cell>No</cell><cell>71.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">LSTMLP (Yuan et al., 2016) (state of the art scores but use private data)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SemCor+1K (private)</cell><cell>private</cell><cell>No</cell><cell>71.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We computed that 41 607 on the 44 449 polysemous nouns of WordNet (94%) are part of this hierarchy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/huggingface/ pytorch-pretrained-BERT 4 https://github.com/getalp/UFSAC</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nus-pt: Exploiting parallel texts for word sense disambiguation in the english allwords tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, SemEval &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Senseval-2: Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, SENSEVAL &apos;01</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Embeddings for word sense disambiguation: An evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="897" to="907" />
		</imprint>
	</monogr>
	<note>Iacobacci et al.2016</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring the automatic selection of basic level concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions</title>
		<meeting>the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002-07" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="61" to="66" />
		</imprint>
	</monogr>
	<note>Proceedings of RANLP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word sense disambiguation using a bidirectional lstm</title>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on Cognitive Aspects of the Lexicon (CogALex)</title>
		<editor>Kågebäck and Salomonsson2016] Mikael Kågebäck and Hans Salomonsson</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference for Learning Representations</title>
		<meeting>the 3rd International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Kingma and Ba2015</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep dive into word sense disambiguation with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="354" to="365" />
		</imprint>
	</monogr>
	<note>Jacopo Urbani, and Piek Vossen. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using mrd: how to tell a pine cone from an ice cream cone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDOC &apos;86</title>
		<meeting>SIGDOC &apos;86<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging gloss knowledge in neural word sense disambiguation by hierarchical co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1402" to="1411" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating glosses into neural word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2473" to="2482" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology, HLT &apos;93</title>
		<meeting>the workshop on Human Language Technology, HLT &apos;93<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 13: Multilingual all-words sense disambiguation and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="288" to="297" />
		</imprint>
	</monogr>
	<note>Moro and Navigli2015</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Moro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 07: Coarse-grained english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval-2007</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SemEval-2013 Task 12: Multilingual Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Train-o-matic: Large-scale supervised word sense disambiguation in multiple languages without manual training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Lexicologie et sémantique lexicale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Polguère</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Les Presses de l&apos;Université de Montréal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 17: English lexical sample, srl and all words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, SemEval &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A unified evaluation framework and empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Raganato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1167" to="1178" />
		</imprint>
	</monogr>
	<note>Raganato et al.2017b. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">One Million Sense-Tagged Instances for Word Sense Disambiguation and Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. [Taghipour and Ng2015] Kaveh Taghipour and Hwee Tou Ng</title>
		<meeting>SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. [Taghipour and Ng2015] Kaveh Taghipour and Hwee Tou Ng<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="338" to="344" />
		</imprint>
	</monogr>
	<note>Proceedings of the Nineteenth Conference on Computational Natural Language Learning</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
		<editor>I. Guyon, U. V</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">UFSAC: Unification of Sense Annotated Corpora and Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation Conference (LREC)</title>
		<meeting><address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
	<note>Loïc Vial, Benjamin Lecouteux, and Didier Schwab</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semi-supervised word sense disambiguation with neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Ng2010</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations, ACLDemos &apos;10</title>
		<meeting>the ACL 2010 System Demonstrations, ACLDemos &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
