<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Geometrical and Statistical Alignment for Visual Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Multimedia Research Lab</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
							<email>wanqing@uow.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Multimedia Research Lab</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
							<email>philipo@uow.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Multimedia Research Lab</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Geometrical and Statistical Alignment for Visual Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel unsupervised domain adaptation method for cross-domain visual recognition. We propose a unified framework that reduces the shift between domains both statistically and geometrically, referred to as Joint Geometrical and Statistical Alignment (JGSA). Specifically, we learn two coupled projections that project the source domain and target domain data into lowdimensional subspaces where the geometrical shift and distribution shift are reduced simultaneously. The objective function can be solved efficiently in a closed form. Extensive experiments have verified that the proposed method significantly outperforms several state-of-the-art domain adaptation methods on a synthetic dataset and three different real world cross-domain visual recognition tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A basic assumption of statistical learning theory is that the training and test data are drawn from the same distribution. Unfortunately, this assumption does not hold in many applications. For example, in visual recognition, the distributions between training and test can be discrepant due to the environment, sensor type, resolution, and view angle. In video based visual recognition, more factors are involved in addition to those in image based visual recognition. For example, in action recognition, the subject, performing style, and performing speed increase the domain shift further. Labelling data is labour intensive and expensive, thus it is impractical to relabel a large amount of data in a new domain. Hence, a realistic strategy, domain adaptation, can be used to employ previous labeled source domain data to boost the task in the new target domain. Based on the availability of target labeled data, domain adaptation can be generally divided into semi-supervised and unsupervised domain adaptation. The semi-supervised approach requires a certain amount of labelled training samples in the target domain and the unsupervised one requires none labelled data. However, in both semi-supervised and unsupervised domain adaptation, sufficient unlabeled target domain data are required. In this paper, we focus on unsupervised domain adaptation which is considered to be more practical and challenging.</p><p>The most commonly used domain adaptation approaches include instance-based adaptation, feature representation adaptation, and classifier-based adaptation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In unsupervised domain adaptation, as there is no labeled data in the target domain, the classifier-based adaptation is not feasible. Alternatively, we can deal with this problem by minimizing distribution divergence between domains as well as the empirical source error <ref type="bibr" target="#b2">[3]</ref>. It is generally assumed that the distribution divergence can be compensated either by an instance based adaptation method, such as reweighting samples in the source domain to better match the target domain distribution, or by a feature transformation based method that projects features of two domains into another subspace with small distribution shift. The instance-based approach requires the strict assumptions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> that 1) the conditional distributions of source and target domain are identical, and 2) certain portion of the data in the source domain can be reused for learning in the target domain through reweighting. While the feature transformation based approach relaxes these assumptions, and only assumes that there exists a common space where the distributions of two domains are similar. This paper follows the feature transformation based approach.</p><p>Two main categories of feature transformation methods are identified <ref type="bibr" target="#b4">[5]</ref> among the literature, namely data centric methods and subspace centric methods. The data centric methods seek a unified transformation that projects data from two domains into a domain invariant space to reduce the distributional divergence between domains while preserving data properties in original spaces, such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The data centric methods only exploit shared feature in two domains, which will fail when the two different domains have large discrepancy, because there may not exist such a common space where the distributions of two domains are the same and the data properties are also maximumly preserved in the mean time. For the subspace centric methods, the domain shift is reduced by manipulating the subspaces of the two domains such that the sub-space of each individual domain all contributes to the final mapping <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Hence, the domain specific features are exploited. For example, Gong et al. <ref type="bibr" target="#b9">[10]</ref> regard two subspaces as two points on Grassmann manifold, and find points on a geodesic path between them as a bridge between source and target subspaces. Fernando et al. <ref type="bibr" target="#b10">[11]</ref> align source and target subspaces directly using a linear transformation matrix. However, the subspace centric methods only manipulate on the subspaces of the two domains without explicitly considering the distribution shift between projected data of two domains. The limitations of both data centric and subspace centric methods will be illustrated on a synthetic dataset in Section 4.1.</p><p>In this paper, we propose a unified framework that reduces the distributional and geometrical divergence between domains simultaneously by exploiting both the shared and domain specific features. Specifically, we learn two coupled projections to map the source and target data into respective subspaces. After the projections, 1) the variance of target domain data is maximized to preserve the target domain data properties, 2) the discriminative information of source data is preserved to effectively transfer the class information, 3) both the marginal and conditional distribution divergences between source and target domains are minimized to reduce the domain shift statistically, and 4) the divergence of two projections is constrained to be small to reduce domain shift geometrically.</p><p>Hence, different from data centric based methods, we do not require the strong assumption that a unified transformation can reduce the distribution shift while preserving the data properties. Different from subspace centric based methods, we not only reduce the shift of subspace geometries but also reduce the distribution shifts of two domains. In addition, our method can be easily extended to a kernelized version to deal with the situations where the shift between domains are nonlinear. The objective function can be solved efficiently in a closed form. The proposed method has been verified through comprehensive experiments on a synthetic dataset and three different real world crossdomain visual recognition tasks: object recognition (Office, Caltech-256), hand-written digit recognition (USPS, MNIST), and RGB-D-based action recognition (MSRAc-tion3DExt, G3D, UTD-MHAD, and MAD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data centric approach</head><p>Pan et al. <ref type="bibr" target="#b5">[6]</ref> propose the transfer component analysis (TCA) to learn some transfer components across domains in RKHS using Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b12">[13]</ref>. TCA is a typical data centric approach that finds a unified transformation φ(·) that projects data from two domains into a new space to reduce the discrepancy. In TCA, the authors aim to minimize the distance between the sample means of the source and target data in the k-dimensional embeddings while preserving data properties in original spaces. Joint distribution analysis (JDA) <ref type="bibr" target="#b6">[7]</ref> improves TCA by considering not only the marginal distribution shift but also the conditional distribution shift using the pseudo labels of target domain. Transfer joint matching (TJM) <ref type="bibr" target="#b7">[8]</ref> improves TCA by jointly reweighting the instances and finding the common subspace. Scatter component analysis (SCA) <ref type="bibr" target="#b8">[9]</ref> takes the between and within class scatter of source domain into consideration. However, these methods require a strong assumption that there exist a unified transformation to map source and target domains into a shared subspace with small distribution shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Subspace Centric Approach</head><p>As mentioned, subspace centric approach can address the issue of data centric methods that only exploit common features of two domains. Fernando et al. <ref type="bibr" target="#b10">[11]</ref> proposed a subspace centric method, namely Subspace Alignment (SA). The key idea of SA is to align the source basis vectors (A) with the target one (B) using a transformation matrix M . A and B are obtained by PCA on source and target domains, respectively. Hence, they do not assume that there exist a unified transformation to reduce the domain shifts. However, the variance of projected source domain data will be different from that of target domain after mapping the source subspace using a linear map because of the domain shift. In this case, SA fails to minimize the distributions between domains after aligning the subspaces. In addition, SA cannot deal with situations where the shift between two subspaces are nonlinear. Subspace distribution alignment (SDA) <ref type="bibr" target="#b13">[14]</ref> improves SA by considering the variance of the orthogonal principal components. However, the variances are considered based on the aligned subspaces. Hence, only the magnitude of each eigen direction is changed which may still fail when the domain shift is large. This has been validated by the illustration of synthetic data in <ref type="figure" target="#fig_4">Figure 2</ref> and the experiment results on real world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint Geometrical and Statistical Alignment</head><p>This section presents the Joint Geometrical and Statistical Alignment (JGSA) method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>We begin with the definitions of terminologies. The source domain data denoted as X s ∈ R D×ns are draw from distribution P s (X s ) and the target domain data denoted as X t ∈ R D×nt are draw from distribution P t (X t ), where D is the dimension of the data instance, n s and n t are number of samples in source and target domain respectively. We focus on the unsupervised domain adaptation problem. In unsupervised domain adaptation, there are sufficient labeled</p><formula xml:id="formula_0">source domain data, D s = {(x i , y i )} ns i=1 , x i ∈ R D , and un- labeled target domain data, D t = {(x j )} nt j=1 , x j ∈ R D ,</formula><p>in the training stage. We assume the feature spaces and label spaces between domains are the same: X s = X t and Y s = Y t . Due to the dataset shift, P s (X s ) = P t (X t ). Different from previous domain adaptation methods, we do not assume that there exists a unified transformation φ(·) such that P s (φ(X s )) = P t (φ(X t )) and P s (Y s |φ(X s )) = P t (Y t |φ(X s )), since this assumption becomes invalid when the dataset shift is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Formulation</head><p>To address limitations of both data centric and subspace centric methods, the proposed framework (JGSA) reduces the domain divergence both statistically and geometrically by exploiting both shared and domain specific features of two domains. The JGSA is formulated by finding two coupled projections (A for source domain, and B for target domain) to obtain new representations of respective domains, such that 1) the variance of target domain is maximized, 2) the discriminative information of source domain is preserved, 3) the divergence of source and target distributions is small, and 4) the divergence between source and target subspaces is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Target Variance Maximization</head><p>To avoid projecting features into irrelevant dimensions, we encourage the variances of target domain is maximized in the respective subspaces. Hence, the variance maximization can be achieved as follows</p><formula xml:id="formula_1">max B T r(B T S t B)<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">S t = X t H t X T t<label>(2)</label></formula><p>is the target domain scatter matrix, H t = I t − 1 nt 1 t 1 T t is the centering matrix, 1 t ∈ R nt is the column vector with all ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Source Discriminative Information Preservation</head><p>Since the labels in the source domain are available, we can employ the label information to constrain the new representation of source domain data to be discriminative.</p><formula xml:id="formula_3">max A T r(A T S b A) (3) min A T r(A T S w A)<label>(4)</label></formula><p>where S w is the within class scatter matrix, and S b is the between class scatter matrix of the source domain data, which are defined as follows,</p><formula xml:id="formula_4">S w = C c=1 X (c) s H (c) s (X (c) s ) T (5) S b = C c=1 n (c) s (m (c) s −m s )(m (c) s −m s ) T (6) where X (c) s ∈ R D×n (c)</formula><p>s is the set of source samples belonging to class c, m (c)</p><formula xml:id="formula_5">s = 1 n (c) s n (c) s i=1 x (c) i ,m s = 1 ns ns i=1 x i , H (c) s = I (c) s − 1 n (c) s 1 (c) s (1 (c) s ) T is the centering matrix of data within class c, I (c) s ∈ R n (c) s ×n (c) s is the identity matrix, 1 s ∈ R n (c) s is the column vector with all ones, n (c)</formula><p>s is the number of source samples in class c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Distribution Divergence Minimization</head><p>We employ the MMD criteria <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> to compare the distributions between domains, which computes the distance between the sample means of the source and target data in the k-dimensional embeddings,</p><formula xml:id="formula_6">min A,B 1 n s xi∈Xs A T x i − 1 n t xj ∈Xt B T x j 2 F<label>(7)</label></formula><p>Long et al. <ref type="bibr" target="#b6">[7]</ref> has been proposed to utilize target pseudo labels predicted by source domain classifiers for representing the class-conditional data distributions in the target domain.</p><p>Then the pseudo labels of target domain are iteratively refined to reduce the difference in conditional distributions between two domains further. We follow their idea to minimize the conditional distribution shift between domains,</p><formula xml:id="formula_7">min A,B C c=1 1 n (c) s xi∈X (c) s A T x i − 1 n (c) t xj ∈X (c) t B T x j 2 F (8)</formula><p>Hence, by combining the marginal and conditional distribution shift minimization terms, the final distribution divergence minimization term can be rewritten as</p><formula xml:id="formula_8">min A,B T r [A T B T ] M s M st M ts M t A B<label>(9)</label></formula><p>where</p><formula xml:id="formula_9">Ms = Xs(Ls + C c=1 L (c) s )X T s , Ls = 1 n 2 s 1s1 T s , (L (c) s )ij = 1 (n (c) s ) 2 xi, xj ∈ X (c) s 0 otherwise (10) Mt = Xt(Lt + C c=1 L (c) t )X T t , Lt = 1 n 2 t 1t1 T t , (L (c) t )ij =    1 (n (c) t ) 2 xi, xj ∈ X (c) t 0 otherwise (11) Mst = Xs(Lst + C c=1 L (c) st )X T t , Lst = − 1 nsnt 1s1 T t , (L (c) st )ij =    − 1 n (c) s n (c) t xi ∈ X (c) s , xj ∈ X (c) t 0 otherwise (12) Mts = Xt(Lts + C c=1 L (c) ts )X T s , Lts = − 1 nsnt 1t1 T s , (L (c) ts )ij =    − 1 n (c) s n (c) t xj ∈ X (c) s , xi ∈ X (c) t 0 otherwise<label>(13)</label></formula><p>Note that this is different from TCA and JDA, because we do not use a unified subspace because there may not exist such a common subspace where the distributions of two domains are also similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Subspace Divergence Minimization</head><p>Similar to SA <ref type="bibr" target="#b10">[11]</ref>, we also reduce the discrepancy between domains by moving closer the source and target subspaces. As mentioned, an additional transformation matrix M is required to map the source subspace to the target subspace in SA. However, we do not learn an additional matrix to map the two subspaces. Rather, we optimize A and B simultaneously, such that the source class information and the target variance can be preserved, and the two subspaces move closer in the mean time. We use following term to move the two subspaces close:</p><formula xml:id="formula_10">min A,B A − B 2 F<label>(14)</label></formula><p>By using term <ref type="bibr" target="#b13">(14)</ref> together with (9), both shared and domain specific features are exploited such that the two domains are well aligned geometrically and statistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Overall Objective Function</head><p>We formulate the JGSA method by incorporating the above five quantities ((1), (3), (4), <ref type="bibr" target="#b8">(9)</ref>, and (14)) as follows: We follow <ref type="bibr" target="#b8">[9]</ref> to further impose the constraint that T r(B T B) is small to control the scale of B. Specifically, we aim at finding two coupled projections A and B by solving the following optimization function,</p><formula xml:id="formula_11">max A,B T r [A T B T ] βS b 0 0 µSt A B T r [A T B T ] Ms + λI + βSw Mst − λI Mts − λI Mt + (λ + µ)I A B<label>(15)</label></formula><p>where I ∈ R d×d is the identity matrix.</p><p>Minimizing the denominator of (15) encourages small marginal and conditional distributions shifts, and small within class variance in the source domain. Maximizing the numerator of (15) encourages large target domain variance, and large between class variance in the source domain. Similar to JDA, we also iteratively update the pseudo labels of target domain data using the learned transformations to improve the labelling quality until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>To optimize <ref type="bibr" target="#b14">(15)</ref>, we rewrite [A T B T ] as W T . Then the objective function and corresponding constraints can be rewritten as:</p><formula xml:id="formula_12">max W T r W T βS b 0 0 µSt W T r W T Ms + λI + βSw Mst − λI Mts − λI Mt + (λ + µ)I W<label>(16)</label></formula><p>Note that the objective function is invariant to rescaling of W . Therefore, we rewrite objective function <ref type="formula" target="#formula_1">(16)</ref> as</p><formula xml:id="formula_13">max W T r W T βS b 0 0 µSt W (17) s.t. T r W T Ms + λI + βSw Mst − λI Mts − λI Mt + (λ + µ)I W = 1</formula><p>The Lagrange function of <ref type="formula" target="#formula_1">(17)</ref> is</p><formula xml:id="formula_14">L = T r W T βS b 0 0 µSt W + T r W T Ms + λI + βSw Mst − λI Mts − λI Mt + (λ + µ)I W − I Φ<label>(18)</label></formula><p>By setting the derivative ∂L ∂W = 0, we get: <ref type="bibr" target="#b18">19)</ref> where Φ = diag(λ 1 , ..., λ k ) are the k leading eigenvalues and W = [W 1 , ..., W k ] contains the corresponding eigenvectors, which can be solved analytically through generalized eigenvalue decomposition. Once the transformation matrix W is obtained, the subspaces A and B can be obtained easily. The pseudo code of JGSA is summarised in Algorithm 1.</p><formula xml:id="formula_15">βS b 0 0 µSt W = Ms + λI + βSw Mst − λI Mts − λI Mt + (λ + µ)I W Φ<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Kernelization Analysis</head><p>The JGSA method can be extended to nonlinear problems in a Reproducing Kernel Hilbert Space (RKHS) using some kernel functions φ. We use the Representer Theorem P = Φ(X)A and Q = Φ(X)B to kernelize our method, where X = [X s , X t ] denotes all the source and target training samples, Φ(X) = [φ(x 1 ), ..., φ(x n )] and n is the number of all samples. Hence, the objective function becomes,  </p><p>where all the X t 's are replaced by Φ(X t ) and all the X s 's are replaced by Φ(X s ) in S t , S w , S b , M s , M t , M st , and M ts in the kernelized version. We replace P and Q with Φ(X)A and Φ(X)B and obtain the objective function as follows,</p><formula xml:id="formula_17">max A,B T r [A T B T ] βS b 0 0 µSt A B T r [A T B T ] Ms + λK + βSw Mst − λK Mts − λK Mt + (λ + µ)K A B (21) where S t =K tKt T , S w = K s H (c) s K T s , with K = Φ(X) T Φ(X), K s = Φ(X) T Φ(X s ), K t = Φ(X) T Φ(X t ),K t = K t − 1 t K − K t 1 n + 1 t K1 n , 1 t ∈ R nt×n and 1 n ∈ R n×n are matrices with all 1 n . In S b , m (c) s = 1 n (c) s n (c) s i=1 k (c) i ,m s = 1 ns ns i=1 k i , with k i = Φ(X) T φ(x i ). In MMD terms, M s = K s (L s + C c=1 L (c) s )K T s , M t = K t (L t + C c=1 L (c) t )K T t , M st = K s (L st + C c=1 L (c) st )K T t , M ts = K t (L ts + C c=1 L (c)</formula><p>ts )K T s . Once the kernelized objective function (21) is obtained, we can simply solve it in the same way as the original objective function to compute A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first conduct experiments on a synthetic dataset to verify the effectiveness of the JGSA methods. Then we evaluate our method for cross-domain object recognition, cross-domain digit recognition, and cross dataset RGB-D-based action recognition. The codes are available online 1 . We compare our method with several state-of-the-art methods: subspace alignment (SA) <ref type="bibr" target="#b10">[11]</ref>, subspace distribution alignment (SDA) <ref type="bibr" target="#b13">[14]</ref>, geodesic flow kernel (GFK) <ref type="bibr" target="#b9">[10]</ref>, transfer component analysis (TCA) <ref type="bibr" target="#b5">[6]</ref>, joint distribution analysis (JDA) <ref type="bibr" target="#b6">[7]</ref>, transfer joint matching (TJM) <ref type="bibr" target="#b7">[8]</ref>, scatter component analysis (SCA) <ref type="bibr" target="#b8">[9]</ref>, optimal transport (OTGL) <ref type="bibr" target="#b14">[15]</ref>, and kernel manifold alignment (KEMA) <ref type="bibr" target="#b15">[16]</ref>. We use the parameters recommended by the original papers for all the baseline methods. For JGSA, we fix λ = 1, µ = 1 in all the experiments, such that the distribution shift, subspace shift, and target variance are treated as equally important. We empirically verified that the fixed parameters can obtained promising results on different types of tasks. Hence, the subspace dimension k, number of iteration T , and regularization parameter β are free parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Data</head><p>Here, we aim to synthesize samples of data to demonstrate that our method can keep the domain structures as well as reduce the domain shift. The synthesized source and target domain samples are both draw from a mixture of three RBFian distributions. Each RBFian distribution represents one class. The global means, as well as the means of the third class are shifted between domains. The original data are 3-dimensional. We set the dimensionality of the subspaces to 2 for all the methods. <ref type="figure" target="#fig_4">Figure 2</ref> illustrates the original synthetic dataset and domain adaptation results of different methods on the dataset. It can be seen that after SA method the divergences between domains are still large after aligning the subspaces. Hence, the aligned subspaces are not optimal for reduce the domain shift if the distribution divergence is not considered. The SDA method does not demonstrate obvious improvement over SA, since the variance shift is reduced based upon the aligned subspaces (which may not be optimal) as in SA. TCA method reduces the domain shift effectively. However, two of the classes are mixed up since there may not exist a unified subspace to reduce domain shift and preserve the original information simultaneously. Even with conditional distribution shift reduction (JDA) or instances reweighting (TJM), the class-1 and class-2 still cannot be distinguished. SCA considers the total scatter, domain scatter, and class scatter using a unified mapping. However, there may not exist such a common subspace that satisfies all the constraints.</p><p>Obviously, JGSA aligns the two domains well even though the shift between source and target domains is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Real World Datasets</head><p>We evaluate our method on three cross-domain visual recognition tasks: object recognition (Office, Caltech-256), hand-written digit recognition (USPS, MNIST), and RGB-D-based action recognition (MSRAction3DExt, G3D, UTD-MHAD, and MAD). The sample images or video frames are shown in <ref type="figure" target="#fig_3">Figure 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Setup</head><p>Object Recognition We adopt the public Office+Caltech object datasets released by Gong et al. <ref type="bibr" target="#b9">[10]</ref>. This dataset contains images from four different domains: Amazon (images downloaded from online merchants), Webcam (low-resolution images by a web camera), DSLR (highresolution images by a digital SLR camera), and Caltech-256. Amazon, Webcam, and DSLR are three datasets studied in <ref type="bibr" target="#b16">[17]</ref> for the effects of domain shift. Caltech-256 <ref type="bibr" target="#b17">[18]</ref> contains 256 object classes downloaded from Google images. Ten classes common to four datasets are selected: backpack, bike, calculator, head-phones, keyboard, laptop, monitor, mouse, mug, and projector. Two types of features are considered: SURF descriptors (which are encoded with 800-bin histograms with the codebook trained from a subset of Amazon images), and Decaf 6 features (which are the activations of the 6th fully connected layer of a convolutional network trained on imageNet). As suggested by <ref type="bibr" target="#b9">[10]</ref>, 1-Nearest Neighbor Classifier (NN) is chosen as the base classifier. For the free parameters, we set k = 30, T = 10, and β = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digit Recognition</head><p>For cross-domain hand-written digit recognition task, we use MNIST <ref type="bibr" target="#b18">[19]</ref> and USPS <ref type="bibr" target="#b19">[20]</ref> datasets to evaluate our method. MNIST dataset contains a training set of 60,000 examples, and a test set of 10,000 examples of size 28×28. USPS dataset consists of 7,291 training images and 2,007 test images of size 16×16. Ten shared classes of the two datasets are selected. We follow the settings of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> to construct a pair of cross-domain datasets USPS → MNIST by randomly sampling 1,800 images in USPS to form the source data, and randomly sampling 2,000 images in MNIST to form the target data. Then source and target pair are switched to form another dataset MNIST → USPS. All images are uniformly rescaled to size 16×16, and each image is represented by a feature vector encoding the gray-scale pixel values. For the free parameters, we set k = 100, T = 10, and β = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB-D-based Action Recognition</head><p>For cross-dataset RGB-D-based Action Recognition, four RGB-D-based Action Recognition datasets are selected, namely MSRAc-tion3DExt <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, UTD-MHAD <ref type="bibr" target="#b22">[23]</ref>, G3D <ref type="bibr" target="#b23">[24]</ref>, and MAD <ref type="bibr" target="#b24">[25]</ref>. All the four datasets are captured by both RGB and depth sensors. We select the shared actions between MSRAction3DExt and other three datasets to form 6 dataset pairs. There are 8 common actions between MSRAction3DExt and G3D: wave, forward punch, hand clap, forward kick, jogging, tennis swing, tennis serve, and golf swing. There are 10 common actions between MSRAc-tion3DExt and UTD-MHAD: wave, hand catch, right arm high throw, draw x, draw circle, two hand front clap, jogging, tennis swing, tennis serve, and pickup and throw. There are 7 shared actions between MSRAction3DExt and MAD: wave, forward punch, throw, forward kick, side kick, jogging, and tennis swing forehand. The local HON4D <ref type="bibr" target="#b25">[26]</ref> feature is used for the cross-dataset action recognition tasks. We extract local HON4D descriptors around 15 skeleton joints by following the process similar to <ref type="bibr" target="#b25">[26]</ref>. The selected joints include head, neck, left knee, right knee, left elbow, right elbow, left wrist, right wrist, left shoulder, right shoulder, hip, left hip, right hip, left ankle, and right ankle. We use a patch size of 24×24×4 for depth map with resolution of 320 × 240 and 48 × 48 × 4 for depth map with resolution of 640 × 480 , then divide the patches into a 3 × 3 × 1 grid. Since most of the real world applications of action recognition are required to recognize unseen data in the target domain, we further divide the target domain into training and test sets using cross-subject protocol, where half of the subjects are used as training and the rest subjects are used as test when a dataset is evaluated as target domain. Note that the target training set is also unlabeled. For the free parameters, we set k = 100 and β = 0.01. To avoid overfitting to the target training set, we set T = 1 in action recognition tasks. LibLINEAR <ref type="bibr" target="#b26">[27]</ref> is used for action recognition by following the original paper <ref type="bibr" target="#b25">[26]</ref>.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results and Discussion</head><p>The results on three types of real world cross domain (object, digit, and action) datasets are shown in Table 1, 2,and 3. The JGSA primal represents the results of JGSA method on original data space, while the JGSA linear and JGSA RBF represent the results with linear kernel and RBF kernel respectively. We follow JDA to report the results on digit datasets in the original feature space. For the action recognition task, it is hard to do eigen decomposition in the original space due to the high dimensionality, hence, the results are obtained using linear kernel. It can be observed that JGSA outperforms the state-of-the-art domain adaptation methods on most of the datasets. As mentioned, the general drawback of subspace centric approach is that the distribution shifts between domains are not explicitly reduced. The data centric methods reduce the distribution shift explicitly. However, a unified transformation may not exist to both reduce distribution shift and preserve the properties of original data. Hence, JGSA outperforms both subspace centric and data centric methods on most of the datasets. We also compare the primal and kernelized versions of the algorithm on the object recognition task ( <ref type="table" target="#tab_0">Table 1</ref>). The results show that the primal and kernelized versions can obtain similar results on average. To evaluate the effectiveness of pseudo labelling, we compare our method with a semi-supervised method KEMA <ref type="bibr" target="#b15">[16]</ref>. We use the same Decaf 7 feature on 8 Office-Caltech dataset pairs as did in KEMA. Our method obtains 90.18% (linear) and 89.91% (RBF), both of which are higher than 89.1% reported in KEMA. We also evaluated the runtime complexity on the crossdomain object datasets (SURF with linear kernel). The average runtime is 28.97s, which is about three times as long as the best baseline method (JDA). This is because JGSA learns two mappings simultaneously, the size of matrix for eigen decomposition is doubled compared to JDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Parameter Sensitivity</head><p>We analyse the parameter sensitivity of JGSA on different types of datasets to validate that a wide range of parameter values can be chosen to obtain satisfactory perfor-mance. The results on different types of datasets have validated that the fixing λ = 1 and µ = 1 is sufficient for all the three tasks. Hence, we only evaluate other three parameters (k, β, and T ). We conduct experiments on the USPS→MNIST, W→A (SURF descriptor with linear kernel), and MSR→MAD datasets for illustration, which are shown in <ref type="figure">Figure 3</ref>. The solid line is the accuracy on JGSA using different parameters, and the dashed line indicates the results obtained by the best baseline method on each dataset. Similar trends are observed on other datasets.</p><p>β is the trade-off parameter of within and between class variance of source domain. If β is too small, the class information of source domain is not considered. If β is too big, the classifier would be overfit to the source domain. However, it can be seen from <ref type="figure">Figure 3a</ref>, a large range of β (β ∈ [2 −15 , 0.5]) can be selected to obtain better results than the best baseline method. <ref type="figure">Figure 3b</ref> illustrates the relationship between various k and the accuracy. We can choose k ∈ [20, 180] to obtain better results than the best baseline method.</p><p>For the number of iteration T, the results on object and digit recognition tasks can be converged to the optimum value after several iteration. However, for the action recognition, the accuracy has no obvious change <ref type="figure">(Figure 3c</ref>). This may be because we use a different protocol for action recognition as mentioned in Section 4.2.1. After iterative labelling (which is done on the target training set), the mappings may be sufficiently good for fitting the target training set, but it is not necessarily the case for the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel framework for unsupervised domain adaptation, referred to as Joint Geometrical and Statistical Alignment (JGSA). JGSA reduces the domain shifts by taking both geometrical and statistical properties of source and target domain data into consideration and exploiting both shared and domain specific features. Comprehensive experiments on synthetic data and three different types of real world visual recognition tasks validate the effectiveness of JGSA compared to several state-of-theart domain adaptation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>max µ{Target Var.} + β{Between Class Var.} {Distribution shift} + λ{Subspace shift} + β{Within Class Var.} where λ, µ, β are trade-off parameters to balance the importance of each quantity, and Var. indicates variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 3 4 5 6 Update</head><label>13456</label><figDesc>Joint Geometrical and Statistical Alignment Input : Data and source labels: Xs, Xt, Ys; Parameters: λ = 1, µ = 1, k, T , β. Output: Transformation matrices: A and B; Embeddings: Zs, Zt; Adaptive classifier: f . 1 Construct St, S b , Sw, Ms, Mt, Mst, and Mts according to (2), (3), (4), (10), (11), (12), and (13); Initialize pseudo labels in target domainŶt using a classifier trained on original source domain data; 2 repeat Solve the generalized eigendecompostion problem in Equation (19) and select the k corresponding eigenvectors of k leading eigenvalues as the transformation W , and obtain subspaces A and B;Map the original data to respective subspaces to get the embeddings: Zs = A T Xs, Zt = B T Xt;Train a classifier f on {Zs, Ys} to update pseudo labels in target domainŶt = f (Zt); Ms, Mt, Mst, and Mts according to<ref type="bibr" target="#b9">(10)</ref>,(11),<ref type="bibr" target="#b11">(12)</ref>, and (13). 7 until Convergence;<ref type="bibr" target="#b7">8</ref> Obtain the final adaptive classifier f on {Zs, Ys}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>P T Q T ] Ms + λI + βSw Mst − λI Mts − λI Mt + (λ + µ)I P Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Sample images of object datasets, digit datasets, and sample video frames of depth map of RGB-D-based action datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Comparisons of baseline domain adaptation methods and the proposed JGSA method on the synthetic data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 −15 2 −13 2 −11 2 −9 2 −7 2 −5 2 −3 2 −Figure 3 :</head><label>22222223</label><figDesc>Parameter sensitivity study of JGSA on different types of datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="16">Accuracy(%) on cross-domain object datasets. Notation for datasets: Caltech:C; Amazon:A; Webcam:W; DSLR:D.</cell></row><row><cell>Feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SURF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decaf6</cell><cell></cell><cell></cell></row><row><cell>data</cell><cell>Raw</cell><cell>SA</cell><cell>SDA</cell><cell>GFK</cell><cell>TCA</cell><cell>JDA</cell><cell>TJM</cell><cell>SCA</cell><cell>JGSA primal</cell><cell>JGSA linear</cell><cell>JGSA RBF</cell><cell>JDA</cell><cell>OTGL</cell><cell>JGSA primal</cell><cell>JGSA linear</cell><cell>JGSA RBF</cell></row><row><cell>C→A</cell><cell>36.01</cell><cell>49.27</cell><cell>49.69</cell><cell>46.03</cell><cell>45.82</cell><cell>45.62</cell><cell>46.76</cell><cell>45.62</cell><cell>51.46</cell><cell>52.30</cell><cell>53.13</cell><cell>90.19</cell><cell>92.15</cell><cell>91.44</cell><cell>91.75</cell><cell>91.13</cell></row><row><cell>C→W</cell><cell>29.15</cell><cell>40.00</cell><cell>38.98</cell><cell>36.95</cell><cell>31.19</cell><cell>41.69</cell><cell>38.98</cell><cell>40.00</cell><cell>45.42</cell><cell>45.76</cell><cell>48.47</cell><cell>85.42</cell><cell>84.17</cell><cell>86.78</cell><cell>85.08</cell><cell>83.39</cell></row><row><cell>C→D</cell><cell>38.22</cell><cell>39.49</cell><cell>40.13</cell><cell>40.76</cell><cell>34.39</cell><cell>45.22</cell><cell>44.59</cell><cell>47.13</cell><cell>45.86</cell><cell>48.41</cell><cell>48.41</cell><cell>85.99</cell><cell>87.25</cell><cell>93.63</cell><cell>92.36</cell><cell>92.36</cell></row><row><cell>A→C</cell><cell>34.19</cell><cell>39.98</cell><cell>39.54</cell><cell>40.69</cell><cell>42.39</cell><cell>39.36</cell><cell>39.45</cell><cell>39.72</cell><cell>41.50</cell><cell>38.11</cell><cell>41.50</cell><cell>81.92</cell><cell>85.51</cell><cell>84.86</cell><cell>85.04</cell><cell>84.86</cell></row><row><cell>A→W</cell><cell>31.19</cell><cell>33.22</cell><cell>30.85</cell><cell>36.95</cell><cell>36.27</cell><cell>37.97</cell><cell>42.03</cell><cell>34.92</cell><cell>45.76</cell><cell>49.49</cell><cell>45.08</cell><cell>80.68</cell><cell>83.05</cell><cell>81.02</cell><cell>84.75</cell><cell>80.00</cell></row><row><cell>A→D</cell><cell>35.67</cell><cell>33.76</cell><cell>33.76</cell><cell>40.13</cell><cell>33.76</cell><cell>39.49</cell><cell>45.22</cell><cell>39.49</cell><cell>47.13</cell><cell>45.86</cell><cell>45.22</cell><cell>81.53</cell><cell>85.00</cell><cell>88.54</cell><cell>85.35</cell><cell>84.71</cell></row><row><cell>W→C</cell><cell>28.76</cell><cell>35.17</cell><cell>34.73</cell><cell>24.76</cell><cell>29.39</cell><cell>31.17</cell><cell>30.19</cell><cell>31.08</cell><cell>33.21</cell><cell>32.68</cell><cell>33.57</cell><cell>81.21</cell><cell>81.45</cell><cell>84.95</cell><cell>84.68</cell><cell>84.51</cell></row><row><cell>W→A</cell><cell>31.63</cell><cell>39.25</cell><cell>39.25</cell><cell>27.56</cell><cell>28.91</cell><cell>32.78</cell><cell>29.96</cell><cell>29.96</cell><cell>39.87</cell><cell>41.02</cell><cell>40.81</cell><cell>90.71</cell><cell>90.62</cell><cell>90.71</cell><cell>91.44</cell><cell>91.34</cell></row><row><cell>W→D</cell><cell>84.71</cell><cell>75.16</cell><cell>75.80</cell><cell>85.35</cell><cell>89.17</cell><cell>89.17</cell><cell>89.17</cell><cell>87.26</cell><cell>90.45</cell><cell>90.45</cell><cell>88.54</cell><cell>100</cell><cell>96.25</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>D→C</cell><cell>29.56</cell><cell>34.55</cell><cell>35.89</cell><cell>29.30</cell><cell>30.72</cell><cell>31.52</cell><cell>31.43</cell><cell>30.72</cell><cell>29.92</cell><cell>30.19</cell><cell>30.28</cell><cell>80.32</cell><cell>84.11</cell><cell>86.20</cell><cell>85.75</cell><cell>84.77</cell></row><row><cell>D→A</cell><cell>28.29</cell><cell>39.87</cell><cell>38.73</cell><cell>28.71</cell><cell>31.00</cell><cell>33.09</cell><cell>32.78</cell><cell>31.63</cell><cell>38.00</cell><cell>36.01</cell><cell>38.73</cell><cell>91.96</cell><cell>92.31</cell><cell>91.96</cell><cell>92.28</cell><cell>91.96</cell></row><row><cell>D→W</cell><cell>83.73</cell><cell>76.95</cell><cell>76.95</cell><cell>80.34</cell><cell>86.10</cell><cell>89.49</cell><cell>85.42</cell><cell>84.41</cell><cell>91.86</cell><cell>91.86</cell><cell>93.22</cell><cell>99.32</cell><cell>96.29</cell><cell>99.66</cell><cell>98.64</cell><cell>98.64</cell></row><row><cell cols="2">Average 40.93</cell><cell>44.72</cell><cell>44.52</cell><cell>43.13</cell><cell>43.26</cell><cell>46.38</cell><cell>46.33</cell><cell>45.16</cell><cell>50.04</cell><cell>50.18</cell><cell>50.58</cell><cell>87.44</cell><cell>88.18</cell><cell>89.98</cell><cell>89.76</cell><cell>88.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) on cross-domain digit datasets. 67.78 65.00 61.22 56.33 67.28 63.28 65.11 80.44 USPS→MNIST 44.70 48.80 35.70 46.45 51.20 59.65 52.25 48.00 68.15 Average 55.32 58.29 50.35 56.84 53.77 63.47 57.77 56.56 74.30</figDesc><table><row><cell>data</cell><cell>Raw</cell><cell>SA</cell><cell>SDA</cell><cell>GFK</cell><cell>TCA</cell><cell>JDA</cell><cell>TJM</cell><cell>SCA</cell><cell>JGSA primal</cell></row><row><cell cols="2">MNIST→USPS 65.94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) on cross-dataset RGB-D-based action datasets. 77.08 73.96 68.75 82.29 70.83 70.83 89.58 G3D→MSR 54.47 68.09 67.32 50.58 65.37 63.04 55.25 66.93 MSR→UTD 66.88 73.75 73.75 65.00 77.50 65.00 64.38 76.88 UTD→MSR 62.93 67.91 66.67 57.63 61.06 60.12 55.14 61.37 MSR→MAD 80.71 85.00 83.57 79.29 82.86 82.14 78.57 86.43 MAD→MSR 80.09 81.48 80.56 81.02 83.33 79.63 79.63 85.65 Average 69.67 75.55 74.30 67.05 75.40 70.13 67.30 77.81</figDesc><table><row><cell>data</cell><cell>Raw</cell><cell>SA</cell><cell>SDA</cell><cell>TCA</cell><cell>JDA</cell><cell>TJM</cell><cell>SCA</cell><cell>JGSA linear</cell></row><row><cell>MSR→G3D</cell><cell>72.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.uow.edu.au/˜jz960/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer learning for visual categorization: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1019" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A literature review of domain adaptation with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Margolis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zero-shot domain adaptation via kernel regression on the grassmannian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<idno>1.1-1.12. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. the 1st International Workshop on DIFFerential Geometry in Computer Vision for Analysis of Shapes, Images and Trajectories</title>
		<meeting>the 1st International Workshop on DIFFerential Geometry in Computer Vision for Analysis of Shapes, Images and Trajectories</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint crossdomain classification and subspace learning for unsupervised adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Subspace distribution alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal transport for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kernel manifold alignment for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">148655</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
	<note>in European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="509" />
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">G3D: A gaming action dataset and real time action recognition evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequential max-margin event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="410" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
