<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and Baseline Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozge</forename><forename type="middle">Mercanoglu</forename><surname>Sincan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Ankara University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hacer</forename><forename type="middle">Yalim</forename><surname>Keles</surname></persName>
							<email>hkeles@ankara.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Ankara University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and Baseline Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Turkish Sign language recognition</term>
					<term>deep learning</term>
					<term>CNN</term>
					<term>LSTM</term>
					<term>BLSTM</term>
					<term>feature pooling</term>
					<term>temporal attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sign language recognition is a challenging problem where signs are identified by simultaneous local and global articulations of multiple sources, i.e. hand shape and orientation, hand movements, body posture, and facial expressions. Solving this problem computationally for a large vocabulary of signs in real life settings is still a challenge, even with the state-of-the-art models. In this study, we present a new large-scale multi-modal Turkish Sign Language dataset (AUTSL) with a benchmark and provide baseline models for performance evaluations. Our dataset consists of 226 signs performed by 43 different signers and 38,336 isolated sign video samples in total. Samples contain a wide variety of backgrounds recorded in indoor and outdoor environments. Moreover, spatial positions and the postures of signers also vary in the recordings. Each sample is recorded with Microsoft Kinect v2 and contains color image (RGB), depth, and skeleton modalities. We prepared benchmark training and test sets for user independent assessments of the models. We trained several deep learning based models and provide empirical evaluations using the benchmark; we used Convolutional Neural Networks (CNNs) to extract features, unidirectional and bidirectional Long Short-Term Memory (LSTM) models to characterize temporal information. We also incorporated feature pooling modules and temporal attention to our models to improve the performances. We evaluated our baseline models on AUTSL and Montalbano datasets. Our models achieved competitive results with the state-of-the-art methods on Montalbano dataset, i.e. 96.11% accuracy. In AUTSL random train-test splits, our models performed up to 95.95% accuracy. In the proposed user-independent benchmark dataset our best baseline model achieved 62.02% accuracy. The gaps in the performances of the same baseline models show the challenges inherent in our benchmark dataset. AUTSL benchmark dataset is publicly available at https://cvml.ankara.edu.tr.</p><p>In the literature, the Sign Language Recognition (SLR) research is carried out in two different branches: The first one is isolated SLR [1-5] where a given spatio-temporal sequence is mapped to a sign; the second one is continuous SLR <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> where it is mapped to a sequence of signs. Isolated SLR can be considered as a special kind of action recognition problem. However, since the hands and face usually cover a small region in video frames, the accurate recognition of a sign imposes different challenges; relatively smaller regions need to be attended accurately. In this research we are focusing on isolated recognition of Turkish Sign Language (TSL) with a large corpus of signs with various challenges.</p><p>Although SLR is an active research area, there is a lack of realistic large-scale sign language datasets. Therefore, most studies are trained and evaluated on either private or public small-scale datasets in the literature <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. However, in order to train a deep learning based sign language recognition model, the amount of training data is crucial. In recent years, larger datasets have been published <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>, which contain a large vocabulary size <ref type="bibr" target="#b20">[21]</ref>, large number of samples <ref type="bibr" target="#b2">[3]</ref>, with many signers <ref type="bibr" target="#b1">[2]</ref>. These datasets help building practical SLR models. Although each of them has several challenges, video samples usually have a plain background. This makes it difficult to develop models that can be used in daily life. In the field of TSL, some early domain specific research are conducted for special purposes, e.g., <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> aims to assist TSL education, [25] implements human computer interaction systems in health and finance domains. Due to the absence of publicly available large-scale TSL datasets, researchers have to create their own small scale datasets for the development of special purpose SLR systems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>. There is a need for a new publicly accessible large-scale TSL dataset to provide the ground for various researches in this domain, especially using the recent deep learning techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sign language is a visual language that is performed with hand gestures, facial expressions, and body posture. It is used by deaf and speech-impaired people in communication. Since most of hearing people do not know sign language, there is a need to map signs to their associated meanings with computer vision based methods to help communication of the deaf-mute people with the rest of the community.</p><p>Recognition of signs using computational models is a challenging problem for a number of reasons. First, it requires fine-grained analysis of the local and global motion of multiple body parts, i.e. hand, arms, and face. For some pairs of signs, hand gestures look very similar, yet the differences in the facial expressions identify the meaning. In some cases, a very similar hand gesture can impose a different meaning depending on the number of repetitions. Another challenge is the variations of how a sign is performed by different signers, i.e. body and pose variations, duration variance of different parts of the signs etc. Also, variation in the illumination and background makes the problem harder, which is inherently problematic in computer vision. These problems becomes more challenging when the corpus of the signs increases.</p><p>In this study, we present a large-scale isolated Turkish Sign Language dataset with Kinect version 2.0 that provides RGB, depth, and skeleton data. It consists of 226 signs performed by 43 different signers and 38,336 isolated video samples. Our dataset differentiates from other publicly available large-scale datasets in that it has 20 different backgrounds with different challenges. We have focused on user-independent recognition of signs, which we believe is crucial for a model to be useful in practice. Therefore, we provide a benchmark that provides training and test video sets with separate signers in them; the signers in the test set do not appear in the training set. We think that our dataset will both contribute to the progress of studies in the field of TSL and can be a benchmark in general in the isolated SLR domain due to the challenges it provides.</p><p>In this paper, we evaluate our dataset with several deep learning based models that is configured to work with RGB and RGB+Depth (RGB-D) data without any explicit segmentation. The models are trained primarily in CNN + LSTM structure, where features are extracted from each frame separately using a 2D-CNN model and temporal relations of the frames are captured by an LSTM model. In addition to the basic model, different sub-models are integrated in between CNN and LSTM to encode the extracted features in multiple scales and to identify spatio-temporal regions of attention. For this purpose, we plugged in a feature pooling model (FPM) after the CNN model to obtain multi-scale representation of the features using single scale input; and we integrated spatio-temporal attention to the LSTM features. We then generated another model by replacing the LSTM with a bidirectional LSTM (BLSTM) model in the best performing model alternative. We provide empirical results of each model alternative using provided test data, using RGB and RGB-D modalities.</p><p>The rest of this paper is organized as follows. We examine existing SLR datasets and related works in Section 2. We then introduce our new AUTSL (Ankara University Turkish Sign Language) dataset in Section 3. We give the details of our baseline models in Section 4. Then, we provide our empirical evaluations in Section 5 and conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Similar to many pattern recognition systems, sign recognition systems are composed of two primary components: (1) feature extraction, <ref type="bibr" target="#b1">(2)</ref> classification. Extracting the best feature representation of the signs from video streams is a crucial step to obtain higher classification accuracies. Therefore, some previous works explicitly segment hands or/and face before extracting the features; they use colored gloves <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> or data gloves <ref type="bibr" target="#b28">[29]</ref> to track movements of the hands and deal with segmentation and occlusion problems more accurately. However, the requirement of wearing gloves at all times is not practical in daily life and data gloves with probes often limit the natural movements of the signers. Some other works propose segmenting hand regions by the help of hand motion speed and trajectory information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> or skin color detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. Skin color detection is one of the most popular segmentation method. However, it is sensitive to illumination changes. Also, face and hands could be confused easily with skin-like objects in the background. With the emergence of Microsoft Kinect technology, new modalities such as depth and skeleton are also provided with the RGB data. Some studies utilize depth data for accurate segmentation of the hands <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Depth data is more robust to illumination changes and cluttered background compared to RGB data. It works well to track a large object, e.g., the human body. Skeleton data provides some of the body key-points at the junctions, e.g., neck, elbow, wrist etc. However, it does not cover the details in the fingers of hands, which is crucial for discriminating local hand gestures. Therefore, it is still difficult to segment the human hand with complex articulations even with the different modalities provided with Kinect <ref type="bibr" target="#b31">[32]</ref>.</p><p>Early studies utilized handcrafted features, such as scale invariant feature transform (SIFT) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>, histogram of gradient (HOG) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. After feature extraction, features are fed into a classifier such as support vector machine (SVM) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>, K-nearest neighbour (K-NN) <ref type="bibr" target="#b18">[19]</ref>, or sequence models such as Hidden Markov Models (HMMs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. Also, some studies use dynamic time warping (DTW), a time series matching algorithm, for recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>. In parallel to the success of the deep learning based models in other domains, many works in the SLR domain recently conduct research using deep neural networks. In these approaches, instead of hand-crafted feature extraction, Convolutional Neural Networks (CNNs) are utilized effectively <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. While some of these studies do not require any segmentation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref>, some studies prefer to use neural networks, such as Fast R-CNN and Faster R-CNN, in order to locate the hand region <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. Recently, attention based models have been successfully applied in other computer vision tasks, such as image captioning <ref type="bibr" target="#b37">[38]</ref> and action recognition <ref type="bibr" target="#b38">[39]</ref>. These models learn the relevant spatial or temporal parts of the image or video automatically from data. These models have also been used in the SLR domain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>The sign language recognition literature is vast and a detailed review of all the literature is outside the scope of our paper. A recent detailed review of SLR works is provided in <ref type="bibr" target="#b40">[41]</ref>. In this section, we first overview the existing publicly available large-scale isolated sign language datasets. Then, we review deep learning based sign recognition language methods and attention based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sign Language Datasets</head><p>In the literature, most sign language datasets are small-scale in terms of number of signs, number of signers and total sample size, e.g., LSA64 <ref type="bibr" target="#b10">[11]</ref>, Purdue RVL-SLLL <ref type="bibr" target="#b11">[12]</ref>, PSL <ref type="bibr" target="#b12">[13]</ref>, RWTH BOSTON50 <ref type="bibr" target="#b13">[14]</ref>. LSA <ref type="bibr" target="#b10">[11]</ref> is an Argentinian Sign Language dataset that contains 64 signs that are performed by 10 signers. There are 3,200 RGB samples in total. The signers wore different colored gloves for each hand during recording. Purdue RVL-SLLL <ref type="bibr" target="#b11">[12]</ref> is an American Sign Language dataset that consists of motions, handshapes, signs and sentences performed by 14 signers. It contains 2,576 RGB videos in total. PSL Kinect 30 <ref type="bibr" target="#b12">[13]</ref> and PSL ToF 84 <ref type="bibr" target="#b12">[13]</ref> are Polish Sign Language datasets that consist of 30 and 84 signs, and in total 300 and 1680 samples, respectively. Both datasets provide RGB and depth modalities. RWTH BOSTON50 <ref type="bibr" target="#b13">[14]</ref> is an American Sign Language that contains 50 signs that are performed by 3 signers. It provides only 483 RGB samples in total. An extended list of sign language datasets can be found in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref>. Montalbano Italian gesture dataset <ref type="bibr" target="#b42">[43]</ref>, which has recently become one of the most widely used isolated SLR datasets, contains 20 gestures and approximately 14,000 samples in total. It contains 27 signers with variations in background, clothing and lighting. It was recorded with Microsoft Kinect v2 that provides RGB, depth, user segmentation, and skeleton modalities.</p><p>In recent years, a number of large-scale datasets have been published. <ref type="table" target="#tab_0">Table 1</ref> provides an overview  <ref type="bibr" target="#b36">[37]</ref> provides 1000 signs, 222 signers, and 25,513 samples. It is collected from a public video sharing platform, i.e. YouTube. Many of videos are performed by ASL students and teachers. In order to provide a basis for signer independent recognition systems, the signers in train, validation, and test set are distinct. It is worth to mention that some of the video links have expired and inaccessible in this dataset <ref type="bibr" target="#b44">[45]</ref>. CSL <ref type="bibr" target="#b1">[2]</ref> is a Chinese Sign Language dataset that consists of 500 signs performed by 50 different signers and 125,000 samples. It is recorded with Microsoft Kinect v2 that provides RGB, depth, and skeleton data. Besides being large-scale, this dataset also focusses on user-independent recognition of signs. They select different signers for the training and test sets. The videos are recorded in front of a white background. WLASL <ref type="bibr" target="#b2">[3]</ref> is another ASL dataset that consists of 2,000 signs performed by 119 signers and 21,083 samples. Each sign is performed by at least 3 different signers. The dataset consists of only RGB videos. It is collected from 20 different educational sign language websites that provide lookup functions for ASL signs and from ASL tutorial videos on YouTube. In the videos, signers are in a nearly-frontal view with plain background, generally wearing a black colored clothes. We noticed recently in <ref type="bibr" target="#b45">[46]</ref> that the authors also aim to provide a large-scale TSL dataset, with 744 signs, 6 signers, and 22,542 samples. Since the dataset is not released yet, we preferred not to include it in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our AUTSL dataset is a new large-scale Turkish Sign Language dataset with 226 signs, 38,336 samples in total. It is performed by 43 different signers. The average number of samples per sign in our dataset is 169.6, which is the second largest number of samples per sign after the CSL dataset <ref type="bibr" target="#b1">[2]</ref>. Our dataset differentiates from all aforementioned large-scale datasets in that it has 20 different backgrounds with many challenges, i.e. variation in the lighting, different indoor and outdoor background objects etc. Some of our videos have dynamic backgrounds; some videos that are recorded in the outdoor environments have background objects that move with the wind, and in some recordings, people are passing by behind the signers in the background. In this sense, the samples are collected to provide realistic scenarios for daily use-cases. The details of our AUTSL dataset is given in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning based SLR Approaches</head><p>In recent years, most studies have been proposed with deep learning based methods. In ChaLearn 2014 Looking at People Challenge gesture recognition track <ref type="bibr" target="#b42">[43]</ref>, the winner of the competition <ref type="bibr" target="#b46">[47]</ref> proposed a deep neural network, which outperforms other traditional methods.</p><p>In deep learning based methods, basic approach for feature extraction is using CNNs. After feature extraction, while some studies use fully connected layers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>, most studies use recurrent neural net-works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48]</ref> on top of the CNN models. <ref type="bibr" target="#b4">[5]</ref> use combination of video in multiple modalities (RGB, depth, intensity), articulated pose and audio streams as inputs. After feature extraction with CNNs, they fuse streams with a set of fully connected layers. They observe that fusing multiple modalities at multiple-scales leads to a significant increase in recognition rates. In <ref type="bibr" target="#b3">[4]</ref>, researchers compare the models that contain CNN architectures, temporal pooling, bidirectional LSTM, or temporal convolutions. They observe that incorporating temporal convolutions and bidirectional LSTM outperforms single-frame and temporal pooling architectures. In <ref type="bibr" target="#b34">[35]</ref>, Siamese CNN architecture is used to extract features from the RGB and depth data in parallel. Then, two types of recurrent neural network, LSTM and GRU, are experimented with. In our preliminary work <ref type="bibr" target="#b0">[1]</ref>, we used a feature extraction module (FPM), which is designed with parallel convolutions with different dilation rates, with a pretrained CNN network. Then LSTM is used to model the temporal characteristics of the stream. In the recent years, some studies use 3D-CNNs in order to capture spatial-temporal features together <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, pose based and visual appearance based approaches are compared. They compare 2D-CNNs with RNNs and 3D-CNNs for visual appearance based baselines. In their work, 3D-CNNs have higher network capacity, hence achieve better results. Moreover, their model is pretrained both with ImageNet <ref type="bibr" target="#b48">[49]</ref> and Kinetics action recognition dataset <ref type="bibr" target="#b49">[50]</ref>.</p><p>Recent studies also incorporate attention mechanisms into their deep networks in many tasks with promising results. In <ref type="bibr" target="#b47">[48]</ref>, an attention model is integrated to a bidirectional RNN for English-French machine translation. In <ref type="bibr" target="#b37">[38]</ref>, visual attention model is proposed for image caption generation. They incorporate attention mechanism to an LSTM that generates a weight for each spatial location. Attention weights encodes the importance and relevance of a location for producing the next word. In <ref type="bibr" target="#b38">[39]</ref>, researchers adapt the attention model of <ref type="bibr" target="#b37">[38]</ref> to action recognition problem. They incorporate spatial attention mechanisms into their deep networks to focus on the regions of interest. Since attention mechanisms achieve promising results in action recognition problem, it also attracts the researchers in the SLR domain. In <ref type="bibr" target="#b2">[3]</ref>, an attention based 3D-CNN network is proposed for CSL recognition. On the proposed method, they incorporate spatial attention into 3D-CNN to select skeleton joints of hand and the arm; spatial attention map peaks around these regions. They then feed extracted features into a bidirectional LSTM. They also incorporate temporal attention to LSTM in order to highlight significant video clips.</p><p>In <ref type="bibr" target="#b39">[40]</ref>, ASL fingerspelling recognition model is proposed with iterative visual attention mechanism for real-life data. Fingerspelling is a part of sign language in which words are signed letter by letter. It is usually used for spelling proper nouns, e.g., names of people. They use 2D-CNNs pretrained on ImageNet for feature extraction and they feed extracted features to LSTM. ASL fingerspelling signs are only one-handed and the attention mechanism enables the model to focus on active hand region. However, high resolution is needed to get sufficient information; therefore, they aim to retain the highest resolution available while zooming in with iterative attention. In <ref type="bibr" target="#b35">[36]</ref>, an attention-based recurrent encoder-decoders are proposed for ASL fingerspelling problem. In the decoding, temporal attention weights are used to focus on the important visual features when producing each output letter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AUTSL Dataset</head><p>In this section, we introduce our large-scale, multi-modal Turkish Sign Language dataset, named shortly as AUTSL 1 . Our motivation is to collect a large dataset with different challenging backgrounds that is suitable for modelling a realistic SLR system with real life scenarios. Main characteristics of our dataset are summarized in <ref type="table" target="#tab_1">Table 2</ref>. We record our dataset using Microsoft Kinect v2, hence it contains RGB, depth, and skeleton modalities. We apply some clipping and resizing operations to RGB and depth data and provide them with the resolution of 512x512. The skeleton data contains spatial coordinates, i.e. (x, y), of the 25 junction points on the signer body.</p><p>Our dataset consists of 226 signs. When choosing our signs, we paid attention to selecting the signs that are used frequently in daily spoken language. Moreover, we considered to keep a balance in the  dataset content to increase the variety of the signs with respect to the motion characteristics of hands while keeping similarly performed different signs at the same time. In this process, we worked with a group of TSL instructors. The selected signs cover a wide variety in terms of hand shape and hand movements. In some of the signs, hands hide each other, e.g., "ayakkabi" (shoe), "bal" (honey), or face, e.g., "beklemek" (wait), "uzgun" (unhappy). In some signs, hands move in the direction of depth, e.g., "</p><p>itmek" (push), "terzi" (tailor). In some signs, the right hand and left hand are in a cross position, e.g., "yardim" (help), "tehlike" (danger). Some of our signs are compound signs formed by making two consecutive signs. Some of these consecutive signs are also included in our dataset as single signs. For example, "hastane" (hospital) sign is formed by making "doktor" (doctor) and "bina" (building) signs consecutively. The signs for hospital and doctor are both included in our dataset. Similarly, "yemek" (eat) and "ocak" (cooker) signs and the compound versions of two, "yemek pisirmek" (cooking) are also included.</p><p>We also paid a lot of attention to create AUTSL with various and challenging backgrounds. It contains 20 different backgrounds. For some backgrounds in this set, we also recorded some videos by changing the camera field-of-view, or by adding or removing some objects to/from the background scene to increase the appearance variance more. In <ref type="figure" target="#fig_0">Fig. 1</ref>, we depict examples of different backgrounds from AUTSL dataset. As shown in the figure, backgrounds contain several challenges; some outdoor recordings contain dynamic backgrounds, i.e. moving trees, or people are passing by behind the signer. Videos contain various lighting conditions, from sunlight to artificial light. Therefore, video frames contain illumination changes and some shadowed or bright-dark areas.</p><p>In our dataset, signs are performed by 43 different signers; 6 of them are TSL instructors, 3 are TSL translators, 1 is deaf, 1 is coda (Children of Deaf Adults), 25 are TSL course students and 7 are trained signers who learned the signs in our dataset. 10 of these signers are men and 33 are women; and also, 2 of our signers are left-handed. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the distribution of the samples over signs and signers. As shown in the figure, we have a balanced dataset according to the sign distribution. On the other hand, the total number of samples for some signers is higher than that of others ( <ref type="figure" target="#fig_1">Fig. 2b</ref>). This is because they are recorded multiple times with different clothes or in different background settings.</p><p>One of the factors that make our dataset challenging is that it contains very similar signs. For example, as shown in <ref type="figure" target="#fig_2">Fig. 3a</ref>, although "doktor" (doctor) and "dakika" (minute) signs contain exactly the same hand gesture, they are differentiated according to the repetition cycle of the same gesture. The sign for "doktor" is made by touching the wrist once, while in the sign for "dakika", twice or more. Also, some signs are performed quite similarly in terms of hand shape, hand orientation, hand position or hand movement; changing only one of these factors may mean another sign. For instance, "dolu" (full) and "dede" (grandfather) signs are very similar <ref type="figure" target="#fig_2">(Fig. 3b</ref>). Although hand shapes, hand rotations and hand positions are very similar, there is only a subtle difference in hand movement. Fingers do not move in the sign of "dolu", while fingers swing slightly in "dede". In <ref type="figure" target="#fig_2">Fig. 3c</ref>, there is only a subtle difference in the position of the hand between "devlet" (government) and "mudur" (manager) signs. In the sign of "mudur", the index finger touches the nose, and in the "devlet", it touches under the eye.</p><p>In this work, we created a benchmark for user-independent recognition of the signs to observe the performances of the models in a more realistic setting. Therefore, we select 36 signers for training and validation, and the remaining 7 signers for testing. In this setting, our test set contains 9 different backgrounds, 3 of which are not included in the training and validation sets. Our training set contains 27,676 (72%), validation set contains 4,884 (13%), and test set contains 5,776 (15%) samples. In the test set, some signers has relatively more samples than others. Therefore, we will refer to this test set as the imbalanced test set. We also created a balanced test set by making the number of samples of each signer close to each-other by reducing the samples from the signers with excessive samples using random selection. As a result, balanced test set is a subset of the imbalanced test set, which consists of 3,742 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Methods</head><p>In order to set a baseline for the evaluation of our AUTSL dataset, we experimented with several deep learning based models. In this section, we first provide the details of the individual components of our models. Following that, we explain our proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Components of the Models</head><p>CNN Model: Recently, CNNs became the most preferred feature extraction methods in the SLR domain. As we used in our preliminary work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>, we also selected to use VGG16 model <ref type="bibr" target="#b50">[51]</ref> in this work. VGG16 is one of the most used CNN models that is pretrained on ImageNet <ref type="bibr" target="#b48">[49]</ref> dataset to extract features. We use all the convolutional layers of VGG16 model until the last max pooling layer. Since the low-level and mid-level convolutional layers extract generic features, such as edges, corners, common object parts etc., we used the low and mid-level layers as they are without changing the learned parameters. Since high-level layers are more specialized to the objects that are included in the trained dataset, we decided to fine-tune the last two convolutional layers (conv5 2, conv5 3) using our dataset. Before training, we resize the pixel resolutions of the video frames to 256 x 256. When the input images are 256 x 256 x 3, the size of the extracted feature maps at the end of the last convolutional layer become 16 x 16 x 512.</p><p>Feature Pooling Module: In <ref type="bibr" target="#b51">[52]</ref>, it is shown that using FPM is effective to extract features at multiple scales when single scale input is provided. The idea behind FPM layers is to increase the fieldof-views to different sizes in the network using dilated convolutions. We want to assess the performance of FPM in this dataset, considering that multi-scale interpretation of the spatial features may help the network be more aware of the context, i.e. hand, face, body, etc. We showed in our preliminary work <ref type="bibr" target="#b0">[1]</ref> that the FPM module is also effective in isolated sign recognition, using Montalbano dataset. Similar to our preliminary work, we placed the FPM model in this work on top of the last CNN layer.</p><p>FPM module is composed of parallel convolutions with different dilation rates. As seen in <ref type="figure" target="#fig_3">Figure  4</ref>, our FPM module consists of 2x2 max pooling with dilation rate 2 followed by a 1x1 convolution, a normal 3x3 convolution, and two 3x3 dilated convolutions with dilation rates 2 and 4. All the convolutions are implemented with padding, hence the spatial dimensions of the inputs are preserved at the end. The resultant features from the parallel CNN layers are concatenated at the end of FPM. All 4 convolutional layers have 128 output feature planes. Therefore, the resultant shape of the features is 16 x 16 x 512 in our experiments.</p><p>LSTM: In the literature, recurrent neural networks are commonly used to capture temporal relationship in sequences. In this paper, we use LSTMs <ref type="bibr" target="#b52">[53]</ref> for sequence modelling. After empirically evaluating 1024, 512 and 256 hidden units for LSTMs, we set the number of hidden units to 512 in our architecture, which performed the best with the validation data. We use random initialization for the hidden and cell states of the first LSTMCell. Bidirectional LSTM: BLSTMs <ref type="bibr" target="#b53">[54]</ref> can be considered as extensions to the conventional unidirectional LSTMs, where context of a sequence for each state is coded using the past and the future frames simultaneously. This is achieved using two LSTM models, one for the forward pass, i.e. from the beginning to the end frames; and the other for the backward pass, i.e. from the end to the beginning frames. Hence, each hidden state can aggregate information from the past and the future frames. In our experiments, the i th hidden state is calculated as a concatenation of the corresponding forward and backward hidden states as in <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_0">h i = [ − → h i + ← − h i ]<label>(1)</label></formula><p>We set the number of hidden units to 512 for both forward and backward LSTMs. Therefore, the hidden state sizes of BLSTM become 1024 in our experiments.</p><p>Attention Model: We integrate a temporal attention mechanism to LSTM and BLSTM models in order to select the most effective video frames in classification. We adapt the temporal attention model proposed by <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55]</ref> to the isolated SLR problem.</p><p>In our simple LSTM model, we use the last hidden state, h t , for prediction of a sign. However, in attention-based LSTM, we produce a context vector, c, using a weighted sum of all hidden states that are generated for each frame in a video by the LSTM model. This context vector is sent to the fully connected layer for the prediction of the sign. Each hidden state contributes the context vector according to its attention weight. Context vector is calculated as follows:</p><formula xml:id="formula_1">c = T i=1 α i h i (2) α i = exp(e i ) T k=1 exp(e k )<label>(3)</label></formula><formula xml:id="formula_2">e i = v T tan(W h i + b)<label>(4)</label></formula><p>where α i is the attention weight for the hidden state corresponding to the input frame features, x i . It is calculated by normalizing the attention scores, i.e. e i , with the softmax function as in <ref type="formula" target="#formula_1">(3)</ref>. Thus, the sum of the weights of all frames is normalized to 1. The higher the score for an input frame, the higher its contribution to the context vector. e i is produced by a neural network which generates a score for the input features, x i , depending on its hidden state, h i . This neural network is our attention network and it is parametrized by v, W, b, where v∈R d , W ∈R dxd , b∈R d . These parameters are learned during training the models. In this setting, d is the dimension of hidden unit in the LSTM, which is 512 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models</head><p>We construct five deep neural networks for the empirical evaluations. In all the models, we use CNNs to extract spatial features from each frame. In our experiments, we investigate the contributions of using a feature pooling module and temporal attention model as we described in Section 4.1. We also compare the performances using simple unidirectional LSTM and bidirectional LSTMs. All our networks, as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, are separately trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN + LSTM Model:</head><p>In our models, we conduct our experiments using only RGB and RGB-D modalities, with minor modifications. In order to use the depth data, which is represented as a single channel gray-scale image for each frame, with the pretrained VGG model, we repeat the same depth data into three color channels as in <ref type="bibr" target="#b55">[56]</ref>. Then, RGB and depth modalities are given as inputs to the two parallel VGG models with exact same architectures and applying similar training regime as we described in the previous section. CNN networks extract features and generate two feature matrices, i.e. one for the RGB data and one for the depth data. Then, we apply global average pooling and reduce the feature map dimensions to a vector of size 512 for each modality, separately. In the RGB only model, we feed 512-dimensional feature vectors into the LSTM model. On the other hand, in the RGB-D network, we concatenate two feature vectors with late fusion and obtain a 1024-dimensional feature vector. LSTM model generates scores using the the last hidden state vector, i.e. h t , after passing it to the Fully Connected (FC) layer. Since we have 226 signs, FC layer is set to have 226 neural units. The scores of the FC layers are fed to a softmax classifier. We refer to this model as CNN + LSTM from now on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN + FPM + LSTM Model:</head><p>In the second model, our motivation is to represent the generated features in multiple-scales, so that we can get more contextual clues for classification of individual signs. We add an FPM module after the last CNN layer for that purpose. After that, we apply global average pooling to the extracted features. In the RGB-D model, we again concatenate the two feature vectors with a late fusion. Then, we send extracted features to LSTM. All the architectures, i.e. CNN and LSTM, are the same with the previous model, except for the addition of the FPM module in between these models. As we stated before, all the parameters of this network is trained end-to-end from scratch. We refer to this model as CNN + FPM + LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN + LSTM + Attention Model:</head><p>Attention mechanisms have recently shown considerable improvements to many computer vision tasks. Therefore, we also want to investigate the contribution of attention to the classification performance with our dataset. The architecture is designed as follows: First, we extract the features with CNN and then apply global average pooling as in CNN + LSTM model. The only difference of this method from CNN + LSTM model is that we incorporate a temporal attention mechanism to the features that are passed to the LSTM model. We produce a context vector, c, using all the hidden states as we explained in detail in Section 4.1. We then send this context vector, instead of the last hidden state, to the FC layer. Finally, we use a softmax classifier. This model is referred to as CNN + LSTM + Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN + FPM + LSTM + Attention Model:</head><p>In this model, we observe the contribution of using both FPM and a temporal attention mechanism. At first, we extract features with CNN and pass the resultant feature maps to FPM. Then, we use the attention-based LSTM. Here again, we send the context vector to the FC layer and use softmax classifier. This model is referred to as CNN + FPM + LSTM+ Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN + FPM + BLSTM + Attention Model:</head><p>Finally, we want to investigate the classification performance using bidirectional LSTMs with AUTSL dataset. We configured the components of the model as in the CNN + FPM + LSTM + Attention model, but we use attention based BLSTM instead of LSTM this time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We evaluate our baseline models on our new large-scale AUTSL dataset and Montalbano Italian gesture dataset. For AUTSL dataset, our main experiments are configured in a user-independent setting; we use 36 signers for training and validation, and the remaining 7 signers for testing. We also conducted experiments by randomly selecting the training, validation and test set to evaluate our model performances in user-dependent test setting. In addition to the AUTSL experiments, we also trained our best performing model using the Montalbano dataset. In this section, we first give the evaluation metric. Then, we provide our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Metric</head><p>In order to evaluate the performances of the models, we use the recognition rate, r, as defined in <ref type="bibr" target="#b56">[57]</ref>:</p><formula xml:id="formula_3">r = 1 n n i=1 f (p(i), y(i))<label>(5)</label></formula><p>where n is the total number of samples; p is the predicted label; y is the true label; if p(i) = y(i), f (p(i), y(i)) = 1, otherwise f (p(i), y(i)) = 0.</p><p>We will refer to this metric as top-1 recognition rate, since we are only evaluating a model's best guess. In AUTSL dataset, some of the signs are quite similar to each other; they can be confused by the models. Therefore, in addition to top-1 recognition rate, we also considered top-3 and top-5 recognition performances of the models. Top-N recognition rate refers to the rate by which the true class label exists in a model's top-N predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Results on AUTSL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Training Details</head><p>We configured all our model experiments using the same hyperparameters. Since the videos in AUTSL dataset contain variable frame lengths, during training each sample is sent to the network separately; hence we set the batch size to 1. We implemented all the models using PyTorch library <ref type="bibr" target="#b57">[58]</ref>. In LSTM and BLSTM implementations with variable frame lengths, we use LSTMCells units of PyTorch. In order to avoid overfitting, we include dropout layers before sending the features to LSTM/BLSTM models and before the FC layer with dropout rate 0.25. We optimized the multi-class cross-entropy loss using Adam optimizer <ref type="bibr" target="#b58">[59]</ref>. We set the learning rate to 1e − 5 and reduce the learning rate to 2e − 6, if no improvement is observed in validation accuracies for ten epochs. If there is no improvement for ten of epochs again, we terminate the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>We conducted a number of experiments to measure the contribution of the use of FPM and the attention model. We also measure the contribution of using multiple modalities, i.e., RGB-D, versus using only RGB. <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref> shows the recognition rates of our baseline models using RGB-D and RGB data, respectively.</p><p>The challenges inherent in the AUTSL samples are visible in the recognition rates for user independent evaluations. The performance of the vanilla CNN + LSTM model using only RGB data is only 23% with the test data. When we fuse RGB and depth features, the recognition performance significantly increases up to 39.31%, around 16% higher than the RGB modality. This is something we expected, since AUTSL contains samples where hands move forward and backward with respect to the camera's optical axis. We think that RGB data alone is not sufficient to accurately discriminate such  signs. In this respect, multiple data modalities that we provide with AUTSL is necessary for better identifying some signs. Moreover, as the top-3 and top-5 performances are considered, the recognition rates increase around 20% and 27% in RGB-D data with respect to its top-1 accuracy, respectively. These results clearly reveal that the vanilla model confuses some signs; although the true sign is identified 66.64% of the time in its top-5 predictions, (and 59.13% of the time in its top-3), the model picks another similar sign in its top-1 order. For imbalanced test, the performance is quite similar; yet slightly worse than the balanced test. Remember that, in imbalanced test set, all the video samples that we have with the selected 7 signers are included. Apparently, the additional samples include more samples of the confused signs; also additional videos containing different backgrounds from the outdoor environment reduce the classification accuracies. For RGB data, top-3 and top-5 predictions are 14% and 20% higher than its top-1 predictions, respectively. Although this is a good sign, since it identifies comparatively a good deal of correct signs in its top-3 and top-5 predictions, it is less than RGB-D data with a high margin; in the balanced test, RDB+Depth top-5 predictions are around 23% higher than RGB top-5 predictions.</p><p>After observing the performance of RGB-D data <ref type="table" target="#tab_2">(Table 3)</ref>, we first completed the experiments by including FPM and attention modalities incrementally using these modalities together. Then after evaluating the performances, we repeated similar experiments using only RGB data. We aim to identify the setting with RGB only data that performs the best.</p><p>We will go over each case separately below:</p><p>Results of RGB-D Data: We first plug our FPM model to the vanilla CNN + LSTM model. FPM improved the recognition rates only slightly, i.e. 1.95% in the balanced test, 1.61% in the imbalanced test. The improvement with only FPM model is limited. We then integrated attention to the vanilla model, without FPM first, to see its effect alone to the classification performance. The temporal attention model that we integrated into LSTM model improved the results significantly, i.e. 18.49% top-1. This improvement reflects to top-3 and top-5 performances as well; top-3 recognition rate of the model becomes 76.24% and top-5 becomes 82.57% in the balanced test. The imbalanced test results are also improved in a parallel manner, i.e. 16.71% in top-1 accuracy. We then plugged in FPM model to the CNN + LSTM + Attention model to see its contribution again. It improves the top-1 performances slightly by 2.22%. After these observations, we set our baseline model for future researches with AUTSL dataset as CNN + FPM + LSTM + Attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of RGB Data:</head><p>We conducted similar experiments with RGB only data. Similar to RGB-D models, RGB model top-1 performances increased incrementally in the order we plugged temporal attention model and FPM model, from 23.00% to 42.14% and 44.89%, respectively. Attention model increased the performance significantly here as well, by more than 19% and FPM improved that performance 2.75% more. The addition of attention, however, increases the robustness of the predictions with RGB data more than we expected, as far as its top-3 and top-5 predictions are considered. The top-3 prediction of the RGB only model improves 24.80% more than the vanilla CNN + LSTM model. Similarly, top-5 predictions improves 27.55% more than the vanilla model's top-5 prediction. Still yet, there is quite a margin, i.e. 15.13%, between the top-1 predictions of RGB+Depth data and RGB only data with the balanced tests using CNN + FPM + LSTM + Attention models. The results with the imbalanced tests are also similar. Therefore, depth data provides a significant contribution to the recognition performance with AUTSL dataset.</p><p>In addition to using unidirectional LSTM model, we also tested the best model replacing it with a bidirectional LSTM model. The performances are similar, only slightly better in both RGB only and RGB-D modalities. Although the performance of BLSTM model is slightly higher than unidirectional LSTM, we want to underline an issue with BLSTMs that in a real-time application environment, where frames are evaluated online, backward evaluation requires buffering the incoming frames and evaluations can start only after all the frames of an isolated sign is completed. This complicates the process. Additional design issues would emerge while working in continuous sign recognition setting.</p><p>As mentioned earlier in Section 3, different signs have very similar gestures in our dataset. In continuous sign recognition, similar signs can be correctly discriminated from the context, the lack of context in isolated recognition makes correct classification harder. Therefore, considering top-1, top-3 and top-5 recognition rates are useful to interpret the performances of the models. As seen in the tables, when comparing the top-1 and the top-3 scores, there is a significant increase in the results. That is, even if a sign cannot be correctly classified in the first order, it can be classified correctly in the first 3 predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>About Confused Signs:</head><p>We examine the confusion matrix of our best model, CNN + FPM + BLSTM + Attention in the case of fusing RGB and depth data. On the balanced test set, there are around 17 samples for each sign. We observe that some of the signs are confused more with particular signs. One of the confused sign pairs, the sign "dede" (grandfather) and "dolu"(full), are shown in <ref type="figure" target="#fig_2">Fig.  3b</ref>. Although there are 17 samples from the sign "dede" in the test set, it is confused 10 times with the sign "dolu", because these two signs are performed very similarly in hand shape, hand rotation and hand position. We observe that the number of the correct predictions is quite low for some signs. These signs are generally confused with similar sign pairs in the dataset. The increase in the top-3 and top-5 evaluations also reveals this issue.</p><p>Visualization Results: After the quantitative analysis of the proposed models, we also observed the attended spatial regions of the test samples using Grad-CAM <ref type="bibr" target="#b59">[60]</ref> visualization technique. In addition, we analyzed the distribution of the temporal attention weights over the video frames to interpret the frames that contribute more to the classification <ref type="figure" target="#fig_5">(Fig. 6</ref>). In the visualizations, we used our CNN + FPM + BLSTM + Attention model that is trained using RGB data only. In general, the model learns to focus on the hands, arms, and faces of the signers in the spatial RGB domain. We generated visualizations considering the CNN output layer, before the FPM model. Since FPM model provides multi-scale interpretation of the CNN output features, visualizations generated by CNN outputs look more condensed and sharp on the image domain. Still, when we visualize the spatially attended regions, CNN models that are followed with an FPM model can focus the relevant regions more successfully.</p><p>In the time domain, we enclosed the attended frames, which have relatively high attention weight values throughout the whole video, within red a bounding box <ref type="figure" target="#fig_5">(Fig. 6</ref>). The distributions of the attention weights are visible in <ref type="figure" target="#fig_5">Fig. 6b and 6d</ref>. As can be seen from the enclosed frames, the attention model highlights the motion sequence that are particularly important for that sign. In other words, it learns to discard the initial and end parts of the video frames. We observed this pattern in almost all the signs. Depending on the particular sign, the weight distribution of the frames are also adapted successfully. This helps the discrimination of signs a lot, since in our dataset signers start performing the sign from a neutral position, i.e. hands are stationary down below, and end similarly, i.e. the   hands return back to neutral position. These analysis support the obtained quantitative increase in the classification accuracies when temporal attention is integrated to the models.</p><p>Visualizations of the attended regions are also useful to interpret the reason behind our model's poor performance for some signs. We show some samples that are all misclassified due to dynamic background in <ref type="figure" target="#fig_6">Fig. 7</ref>. In all these three samples, some people are passing by behind the signer, both in indoor and outdoor settings; both spatial attention and temporal attention is badly influenced by the appearance and motion of another person on the scene. Although they appear small in the background, far behind the signer, the spatial attention shifts to those people. The sample sign shown in <ref type="figure" target="#fig_6">Fig. 7</ref>middle part is the same sign with <ref type="figure" target="#fig_5">Fig. 6a</ref>. It is misclassified by our model this time. In addition to the attention shift in the spatial domain, the attention in the temporal domain is also affected by the motion in the background; our model attends to the last two frames this time, where the signer has already settled in neutral position of ending the sign. In that case, there is no spatially interesting motion of our signer to attend; so it focuses on the person in the background and misclassifies the sign that it was classifying correctly in the absence of disruption.</p><p>We also provide a sample visualization of the attended regions and frames for a confused sign pair <ref type="figure" target="#fig_7">(Fig. 8</ref>). The signs corresponding to "yavas" (slow) and "arkadas" (friend) are performed similarly in hand positions and shapes. Although the model pay attention to the hands and semantically relevant frames in time, the sign, which is depicted in the last row of <ref type="figure" target="#fig_7">Fig. 8</ref>, is misclassified as "arkadas" sign since they look quite similar. In such cases, the correct sign is usually included in the model's top-3 or top-5 predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training Times:</head><p>We trained our models on NVIDIA Tesla V100. <ref type="table" target="#tab_4">Table 5</ref> shows the average training time of an epoch in our models. Training with RGB-D data takes almost two times more than training with RGB data only. While adding FPM to the network cause an increase in time, adding an attention mechanism do not increase the time as much. Moreover, adding an attention model enables the models to converge faster, as seen in <ref type="figure" target="#fig_8">Fig. 9</ref>. For example, in the case of using only RGB modality, training and validation losses get close to zero at around 40 th epoch with our vanilla model, i.e. CNN + LSTM. On the other hand, attention-based models reach the same loss value at around 20 th epoch. Therefore, attention-based models converge faster than the other models in time. When we compare using single and multiple modalities, we observe that fusing RGB and depth data also reduces the total number of epochs during training. Moreover, validation losses are more stable in the case of using RGB-D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Signer Dependent Testing:</head><p>We also conducted some experiments by randomly selecting training, validation, and test sets to show the model performances by training the models using all the signers, i.e. signer dependent model training. In this setting, all the signers in the test set are also included in the training and validation set; we randomly selected 72% of all the videos for  training, 13% for validation and 15% for testing.</p><p>Since the model training takes too much time, we trained only two of our deep models; i.e. CNN + FPM + LSTM and CNN + FPM + LSTM + Attention models, end-to-end by using only RGB data, to obtain sample results to compare with corresponding user-independent models. The architectures and the model parameters are kept exacly the same with our previous experiments; only the traning and test data selections are different. The results are shown in <ref type="table" target="#tab_5">Table 6</ref>.</p><p>These models work significantly better than their corresponding user-independent counterparts ( <ref type="table" target="#tab_3">Table 4</ref>). We get 94.07% with CNN + FPM + LSTM model and 95.95% accuracy with CNN + FPM + LSTM + Attention in their top-1 accuracy. In this experiment, CNN + FPM + LSTM performs already very high, hence the amount of performance increase with added attention model is small. Also, both models reached more than 99% in their top-5 accuracies. These results show that our models work robustly when samples belonging to signers in the test set is viewed in the training set. When using the benchmark test data, which reflects the actual performances of the models in a realistic setting, the  Model Modality Recognition rate (%) <ref type="bibr" target="#b60">[61]</ref> RGB-D 97.50 <ref type="bibr" target="#b3">[4]</ref> RGB-D 97.23 Ours RGB-D 96.11 <ref type="bibr" target="#b4">[5]</ref> RGB-D 95.06 <ref type="bibr" target="#b0">[1]</ref> RGB-D 93.15 <ref type="bibr" target="#b61">[62]</ref> RGB-D 91.70 Ours RGB 95.46 <ref type="bibr" target="#b62">[63]</ref> RGB 94.58</p><p>performances drop heavily in the models' top-1 accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment Results on Montalbano</head><p>Montalbano is a gesture dataset released by ChaLearn 2014 Looking at People Challenge which consists of 20 Italian gestures performed by 27 users. It contains 940 video sequences, each containing 10 to 20 gesture samples and around 14,000 samples in total (6,850 train, 3,454 validation, and 3,579 test samples). The videos are recorded with Microsoft Kinect in 640x480 pixel resolutions and four types of data are provided; RGB, depth, user segmentation, and skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Preprocessing</head><p>We preprocess the videos in the Montalbano dataset similar to our preliminary work <ref type="bibr" target="#b0">[1]</ref>. Since the problem we are dealing in this research is isolated sign language recognition, we created isolated sign samples from the Montalbano video sequences. We then cropped each frame from the upper body of the signers using the signer's shoulder center joint coordinates, using the skeleton data. After this operation, each frame size is fixed to 400x400 pixels. We kept the shoulder center point on the horizontal center line of the cropping square. In the vertical axis, the images are cropped by aligning the window to the upper part of the image. Furthermore, we also fixed the number of frames in all the videos to 40 frames as in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Training Details</head><p>We configured all our model experiments using the same hyperparameters as in the AUTSL experiments. However, in our experiments on the Montalbano dataset, we use the batch size as 16, since the videos have fixed number of frames. We set the initial learning rate as 1e-4 instead of 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Results</head><p>We evaluate our best model, CNN + FPM + BLSTM + Attention, on Montalbano dataset and we compare our results with the sate-of-art models that also work with Montalbano dataset in isolated recognition setting. <ref type="table" target="#tab_6">Table 7</ref> contains state-of-the-art model performances that use only RGB or RGB-D data. We achieved competitive results with the state-of-the-art models on this dataset without eager hyperparameter parameter tuning for this dataset; our model got 95.46% accuracy using only RGB data and 96.11% accuracy using RGB-D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a new large-scale isolated Turkish Sign Language dataset that we named shortly as AUTSL. Our dataset provides various challenges compared to many other large-scale sign language datasets; to the best of our knowledge, it is the first large-scale public TSL dataset containing a variety of different backgrounds from indoor and outdoor settings that are performed with several different signers. In addition to the challenges provided with AUTSL, we aimed to perform userindependent classification of the signs in this research. We provide a benchmark training and test sets that we used in this research publicly available for the researchers. We also provide several deep learning-based models aiming to serve as baselines for future researches with this benchmark.</p><p>We trained a series of models based on a vanilla CNN + LSTM architecture. We incrementally integrated FPM and temporal attention to the vanilla model to improve the classification performances. All the models are trained with RGB-D data and RGB only data. Finally, we trained the best models of both modalities using BLSTM models replacing the LSTMs. The best results are obtained using RGB-D data using the CNN + FPM + BLSTM + Attention architecture. In order to validate our baseline models, we also evaluated our best model architecture on Montalbano dataset and compared the performances with state-of-the-art approaches. Our models achieved competitive results with the state-of-the-art models using RGB and RGB-D data. We provided quantitative results using top-1, top-3 and top-5 classification accuracies of all the models on AUTSL dataset. The results reveal that some signs in our dataset are performed visually similarly and are misclassified by our models. Moreover, the challenges provided with variety of backgrounds that are gathered in unconstrained settings degrade the performance a lot. We provided sample visualizations of the spatially and temporally attended regions for some samples that support these claims.</p><p>Providing these baseline models to the community, we also plan to work with AUTSL benchmark more to increase the classification performance in the future. We are planning to make more research to improve the spatial and temporal attention of our models to make them more robust to dynamic backgrounds. Moreover, we will focus more on better discriminative training of our models to increase the classification accuracy of similar signs in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Examples of different backgrounds from AUTSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Distribution of (a) number of samples performed by each signer and (b) number of samples for each sign.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Some of the similar example signs to each other in our dataset. (a) "Doktor" (doctor) and "dakika" (minute) signs differ only in repetition of the hand movement. (b) "Dolu" (full) and "dede" (grandfather) differ only in finger movements. (c) "Devlet" (government) and "mudur" (manager) differ only in the position of the index finger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Feature Pooling Module (FPM)<ref type="bibr" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Architectures of our baseline models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Sample RGB video sequences and GradCAM<ref type="bibr" target="#b59">[60]</ref> visualizations of the attended regions for two signs: (a) "oda" (room), (c) "fotograf" (photograph). Note that red regions in a frame show highly attended parts, blue regions are attended less. (b, d) Temporal attention weights of the videos. The attended frames are enclosed within a red frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Sample misclassifications due to dynamic backgrounds: (a) "oda" (room), which is the same sign withFig. 6a. This time, it is misclassified because of the moving person in the background (b) "oda" (room) sign in indoor (c) "gecmis olsun" (get well).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Sample misclassification due to sign similarity. (a) "yavas" (slow) sign, (b) GradCAM visualization of (a), (c) "arkadas" (friend) sign.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>(a) CNN + LSTM (b) CNN+ FPM+ LSTM+ Attention (c) CNN + FPM + BLSTM + Attention (d) CNN + LSTM (e) CNN + FPM + LSTM + Attention (f ) CNN + FPM + BLSTM + Attention Training and validation loss curves, (a, b, c) using only RGB data, (d, e, f) using RGB+Depth data on AUTSL dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of existing large-scale isolated sign language recognition datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="3">Year Sign Language #Avg</cell><cell cols="3">#Signs #Signers #Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sample</cell><cell></cell><cell></cell><cell>samples</cell></row><row><cell></cell><cell></cell><cell></cell><cell>per sign</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASLLVD [44]</cell><cell>2012</cell><cell>American</cell><cell>3.6</cell><cell>2,742</cell><cell>6</cell><cell>9,794</cell></row><row><cell cols="2">DEVISIGN [21] 2014</cell><cell>Chinese</cell><cell>12</cell><cell>2,000</cell><cell>8</cell><cell>24,000</cell></row><row><cell>MS-ASL [37]</cell><cell>2019</cell><cell>American</cell><cell>25.5</cell><cell>1,000</cell><cell>222</cell><cell>25,513</cell></row><row><cell>CSL [2]</cell><cell>2019</cell><cell>Chinese</cell><cell>250</cell><cell>500</cell><cell>50</cell><cell>125,000</cell></row><row><cell>WLASL [3]</cell><cell>2020</cell><cell>American</cell><cell>10.5</cell><cell>2,000</cell><cell>119</cell><cell>21,083</cell></row><row><cell cols="2">AUTSL (Ours) 2020</cell><cell>Turkish</cell><cell>169.6</cell><cell>226</cell><cell>43</cell><cell>38,336</cell></row><row><cell cols="7">of the large-scale isolated sign language datasets. ASLLVD [44] has 2,742 signs in American Sign</cell></row><row><cell cols="7">Language (ASL). Although the dataset has large vocabulary size, it has only 9,794 samples in total (3.6</cell></row><row><cell cols="7">examples per sign on the average). This dataset aims to serve as the basis for development of sign lookup</cell></row><row><cell cols="7">technology in ASL. The video sequences are collected from four cameras simultaneously; two frontal</cell></row><row><cell cols="7">views, one side view, and one view zoomed in on the face of the signer. DEVISIGN [21] is a Chinese</cell></row><row><cell cols="7">Sign Language dataset that consists of 2,000 signs and 24,000 samples that are performed by 8 signers.</cell></row><row><cell cols="7">The videos are recorded with Microsoft Kinect v1, which provides RGB, depth, and skeleton data, in</cell></row><row><cell cols="4">a lab environment in front of a white wall. MS-ASL dataset</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The statistics of AUTSL dataset.</figDesc><table><row><cell>Property</cell><cell>Description</cell></row><row><cell>Number of signs</cell><cell>226</cell></row><row><cell>Number of signers</cell><cell>43</cell></row><row><cell>Total samples</cell><cell>38,336</cell></row><row><cell cols="2">Number of different backgrounds 20</cell></row><row><cell>Mean sample per sign</cell><cell>169.6</cell></row><row><cell>Modalities</cell><cell>RGB, depth, skeleton</cell></row><row><cell>RGB and depth resolution</cell><cell>512x512</cell></row><row><cell>FPS</cell><cell>30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Recognition rates (%) of our models using RGB+Depth data. CNN + LSTM 39.31 59.13 66.64 37.84 57.68 65.30 CNN + FPM + LSTM 41.26 60.60 68.76 39.45 58.50 66.55 CNN + LSTM + Attention 57.80 76.24 82.57 54.55 62.79 70.82 CNN + FPM + LSTM + Attention 60.02 78.00 83.93 56.99 75.79 82.22 CNN + FPM + BLSTM + Attention 62.02 78.11 83.45 59.24 76.03 81.60</figDesc><table><row><cell>Method</cell><cell>Balanced Test Set Imbalanced Test Set top-1 top-3 top-5 top-1 top-3 top-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Recognition rates (%) of our models using only RGB data. Attention 49.22 68.89 75.78 47.62 67.38 73.89</figDesc><table><row><cell>Method</cell><cell cols="2">Balanced Test Set Imbalanced Test Set top-1 top-3 top-5 top-1 top-3 top-5</cell></row><row><cell>CNN + LSTM</cell><cell>23.00 37.03 43.66 22.80 36.94</cell><cell>43.61</cell></row><row><cell>CNN + LSTM + Attention</cell><cell>42.14 61.83 71.21 40.89 60.68</cell><cell>69.42</cell></row><row><cell>CNN + FPM + LSTM + Attention</cell><cell>44.89 64.24 72.26 43.69 62.79</cell><cell>70.72</cell></row><row><cell>CNN + FPM + BLSTM +</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of training time per epoch in hour on AUTSL.</figDesc><table><row><cell>Model</cell><cell cols="2">RGB (hr) RGB+Depth (hr)</cell></row><row><cell>CNN + LSTM</cell><cell>2.2</cell><cell>4.3</cell></row><row><cell>CNN + FPM + LSTM</cell><cell>-</cell><cell>5.8</cell></row><row><cell>CNN + LSTM + Attention</cell><cell>2.5</cell><cell>4.5</cell></row><row><cell>CNN + FPM + LSTM + Attention</cell><cell>3.5</cell><cell>6</cell></row><row><cell>CNN + FPM + BLSTM + Attention</cell><cell>3.7</cell><cell>6.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Recognition rates (%) of our models on AUTSL random test set.</figDesc><table><row><cell></cell><cell>Randomly Selected</cell></row><row><cell>Method</cell><cell>Train-Test Set</cell></row><row><cell></cell><cell>top-1 top-3 top-5</cell></row><row><cell>CNN + FPM + LSTM</cell><cell>94.07 98.79 99.36</cell></row><row><cell cols="2">CNN + FPM + LSTM + Attention 95.95 98.96 99.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the method performances on Montalbano dataset in isolated setting using RGB or RGB-D data.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://cvml.ankara.edu.tr/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is part of a project funded by TUBITAK (The Scientific and Technological Research Council of Turkey) under the grant number 217E022. The numerical calculations reported in this paper were partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources). We would like to thank TUBITAK for the support. We also would like to thank to all our volunteer signers and to our TSL instructor Selda Demirci who contributed this research a lot voluntarily during data gathering.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Isolated sign language recognition with multi-scale features using lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Sincan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 27th Signal Processing and Communications Applications Conference (SIU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-based 3d-cnns for large-vocabulary sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2822" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1459" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond temporal pooling: Recurrence and temporal convolutions for gesture recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Herreweghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="430" to="439" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Moddrop: adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1692" to="1706" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video-based sign language recognition without temporal segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A deep neural framework for continuous sign language recognition by iterative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1880" to="1891" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical lstm for sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative alignment network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4165" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep sign: Enabling robust statistical continuous sign language recognition via hybrid cnn-hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1311" to="1325" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lsa64: an argentinian sign language dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Estrebou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Lanzarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XXII Congreso Argentino de Ciencias de la Computación</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Purdue rvl-slll asl database for automatic recognition of american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Fourth IEEE International Conference on Multimodal Interfaces</title>
		<meeting>Fourth IEEE International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="167" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognition of hand gestures observed by depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kapuscinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oszust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wysocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warchol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combination of tangent distance and an image distortion model for appearance-based sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Isolated sign language recognition using convolutional neural network hand modelling and hand energy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sign language recognition using sub-units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2205" to="2231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">American sign language recognition with the kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zafrulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brashear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Presti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chinese sign language recognition based on video sequence appearance modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 5th IEEE Conference on Industrial Electronics and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1537" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A kinect based sign language recognition system using spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Memiş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albayrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Machine Vision (ICMV 2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">9067</biblScope>
			<biblScope unit="page">90670</biblScope>
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Turkish sign language recognition using kinect skeleton and convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Unutmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Karaca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Güllü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 27th Signal Processing and Communications Applications Conference (SIU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Devisign: dataset and evaluation for 3d sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wanga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">tech. rep</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Signtutor: An interactive system for sign language tutoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Carrillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new robotic platform for sign language tutoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Uluer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akalın</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Köse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="571" to="585" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Query-by-sign system for turkish sign language broadcasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Alaydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saraçlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 26th Signal Processing and Communications Applications Conference (SIU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bosphorussign: a turkish sign language recognition corpus in health and finance domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgöz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kındıroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karabüklü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kelepir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Özsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1383" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Isolated sign language recognition using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Assan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="162" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time hand gesture recognition using a color glove</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lamberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Camastra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modelling and segmenting subunits for sign language recognition based on hand motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="623" to="633" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new data glove approach for malaysian sign language detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Shukor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Miskon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Jamaluddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Asyraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Bahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time hand gesture detection and recognition using bag-of-features and support vector machine techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Dardas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Georganas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and measurement</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3592" to="3607" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with depth images: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE RO-MAN: the 21st IEEE international symposium on robot and human interactive communication</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="411" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust part-based hand gesture recognition using kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1110" to="1120" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sign language recognition using dynamic time warping and hand shape distance based on histogram of oriented gradient features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jangyodsuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Conly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on PErvasive Technologies Related to Assistive Environments</title>
		<meeting>the 7th International Conference on PErvasive Technologies Related to Assistive Environments</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep attention network for joint hand gesture localization and recognition using static rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Isolated sign recognition with a siamese neural network of rgb and depth streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE EUROCON 2019-18th International Conference on Smart Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">American sign language fingerspelling recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Del Rio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brentari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ms-asl: A large-scale data set and benchmark for understanding american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01053</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems: Time Series Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fingerspelling recognition in the wild with iterative visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M D</forename><surname>Rio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brentari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sign language recognition systems: A decade systematic literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Computational Methods in Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sign language recognition, generation, and translation: An interdisciplinary perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boudreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Braffort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Verhoef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st International ACM SIGACCESS Conference on Computers and Accessibility</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chalearn looking at people challenge 2014: Dataset and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ponce-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="459" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Challenges in development of the american sign language lexicon video dataset (asllvd) corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neidle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thangali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, LREC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transferring cross-domain knowledge for video sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6205" to="6214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Özdemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kındıroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgöz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01283</idno>
		<title level="m">Bosphorussign22k sign language recognition dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-scale deep learning for gesture detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="474" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Iternational Conference on Learning Representations, ICLR</title>
		<meeting>Iternational Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Foreground segmentation using convolutional neural networks for multiscale feature encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="256" to="262" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Feed-forward networks with attention can solve some long-term memory problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08756</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for robust rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Chalearn looking at people rgb-d isolated and continuous datasets for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Konur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4207" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sign language recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="572" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynamic gesture recognition by using cnns and star rgb: A temporal information condensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L A</forename><surname>Samatelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Vassallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="238" to="254" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
