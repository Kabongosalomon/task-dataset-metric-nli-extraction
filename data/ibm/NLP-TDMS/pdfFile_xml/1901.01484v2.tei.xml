<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LANCZOSNET: MULTI-SCALE DEEP GRAPH CONVO- LUTIONAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
							<email>zhizhenz@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
							<email>zemel@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LANCZOSNET: MULTI-SCALE DEEP GRAPH CONVO- LUTIONAL NETWORKS</title>
					</analytic>
					<monogr>
						<title level="m">Uber ATG Toronto 2 , Vector Institute</title>
						<imprint>
							<biblScope unit="volume">3</biblScope>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019 University of Illinois at Urbana-Champaign 4 , Canadian Institute for Advanced Research 5</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the Lanczos network (LanczosNet), which uses the Lanczos algorithm to construct low rank approximations of the graph Laplacian for graph convolution. Relying on the tridiagonal decomposition of the Lanczos algorithm, we not only efficiently exploit multi-scale information via fast approximated computation of matrix power but also design learnable spectral filters. Being fully differentiable, LanczosNet facilitates both graph kernel learning as well as learning node embeddings. We show the connection between our LanczosNet and graph based manifold learning methods, especially the diffusion maps. We benchmark our model against several recent deep graph networks on citation networks and QM8 quantum chemistry dataset. Experimental results show that our model achieves the state-of-the-art performance in most tasks. Code is released at: https://github.com/lrjconan/LanczosNetwork.</p><p>Published as a conference paper at ICLR 2019 for moderately large graphs. Second, spectral filters within current graph convolution based models are mostly fixed. In the context of image processing, using a Gaussian kernel along with a spectral filter f (λ) = 2λ − λ 2 corresponds to running forward the heat equation (blurring) followed by running it backwards (sharpening) <ref type="bibr" target="#b8">[9]</ref>. Multi-scale kernels introduced in [10] extends the idea of forward-backward diffusion process and can be represented as polynomials of matrices related to a Gaussian kernel. Learning the spectral filters is thus beneficial since it learns the stochastic processes on the graph which produce useful representations for particular tasks. However, how to learn spectral filters which have large model capacity is largely underexplored.</p><p>In this paper, we propose the Lanczos network (LanczosNet) to overcome the aforementioned issues. First, based on the tridiagonal decomposition implied by the Lanczos algorithm, our model exploits the low rank approximation of the graph Laplacian. This approximation facilitates efficient computation of matrix powers thus gathering multi-scale information easily. Second, we design learnable spectral filters based on the approximation which effectively increase model capacity. In scenarios where one wants to learn the graph kernel and/or node embeddings, we propose another variant, i.e., adaptive Lanczos network (AdaLanczosNet), which back-propagates through the Lanczos algorithm. We show that our proposed model is closely related to graph based manifold learning approaches such as diffusion maps which could potentially inspire more work from the intersection between deep graph networks and manifold learning. We benchmark against 9 recent deep graph networks, including both convolutional and RNN based methods, on citation networks and a quantum chemistry graph regression problem, and achieve state-of-the-art results in most tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-structured data is ubiquitous in real world applications, social networks, gene expression regulatory networks, protein-protein interactions, and many other physical systems. How to model such data using machine learning, especially deep learning, has become a central research question <ref type="bibr" target="#b0">[1]</ref>. For supervised and semi-supervised tasks such as graph or node classification and regression, learning based models can be roughly categorized into two classes, formulated either in terms of graph convolutions <ref type="bibr" target="#b1">[2]</ref> or recurrent neural networks <ref type="bibr" target="#b2">[3]</ref>.</p><p>Methods based on recurrent neural networks (RNN), especially graph neural networks (GNN) <ref type="bibr" target="#b2">[3]</ref>, repeatedly unroll a message passing process over the graph by exchanging information between the nodes. In theory, a GNN can have as large a model capacity as its convolutional counterpart. However, due to the instability of RNN dynamics and difficulty of optimization, GNN and its variants are generally slower and harder to train.</p><p>In this paper we focus on graph convolution based methods. Built on top of the graph signal processing (GSP) approaches <ref type="bibr" target="#b3">[4]</ref>, these methods extend convolution operators to graphs by leveraging spectral graph theory, graph wavelet theory, etc. Graph convolutions can be stacked and combined with nonlinear activation functions to build deep models, just as in regular convolutional neural networks (CNN). They often have large model capacity and achieve promising results. Also, graph convolution can be easily implemented with modern scientific computing libraries.</p><p>There are two main issues with current graph convolution approaches. First, it is not clear how to efficiently leverage multi-scale information except by directly stacking multiple layers. Having an effective multi-scale scheme is key for enabling the model to be invariant to scale changes, and to capture many intrinsic regularities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Graph coarsening methods have been proposed to form a hierarchy of multi-scale graphs <ref type="bibr" target="#b6">[7]</ref>, but this coarsening process is fixed during both inference and learning which may cause some bias. Alternatively, the graph signal can be multiplied by the exponentiated graph Laplacian, where the exponent indicates the scale of the diffusion process on the graph <ref type="bibr" target="#b7">[8]</ref>. Unfortunately, the computation and memory cost increases linearly with the exponent, which prohibits the exploitation of long scale diffusion in practice. Other fast methods for computing matrix power such as exponentiating by squaring are very memory intensive, even</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section, we introduce some background material. A graph G with N nodes is denoted as G = (V, E, A), where A ∈ R N ×N is an adjacency matrix which could either be binary or real valued. X ∈ R N ×F is the compact representation of node features (or graph signal in the GSP literature). For any node v ∈ V, we denote its feature as a row vector X v,: ∈ R 1×F . We use X :,i to denote the i-th column of X.</p><p>Graph Fourier Transform Given input node features X, we now discuss how to perform a graph convolution. Based on the adjacency matrix A, we compute the graph Laplacian L which can be defined in different ways: (1) L = D − A; (2) L = I − D −1 A; (3) L = I − D − 1 2 AD − 1 2 , where D is a diagonal degree matrix and D i,i = N j=1 A i,j . The definition (3) is often used in the GSP literature due to the fact that it is real symmetric, positive semi-definite (PSD) and has eigenvalues lying in <ref type="bibr">[0,</ref><ref type="bibr" target="#b1">2]</ref>. In certain applications <ref type="bibr" target="#b10">[11]</ref>, it was found that adding self-loops, i.e., changing A to A + I, and using the affinity matrix S = D − 1 2 AD − 1 2 instead of L gives better results. Since S is real symmetric, based on spectral decomposition, we have S = U ΛU where U is an orthogonal matrix and its column vectors are the eigenvectors of S. The diagonal matrix Λ contains the sorted eigenvalues where Λ i,i = λ i and 1 ≥ λ 1 ≥ · · · ≥ λ N ≥ −1. Based on the eigenbasis, we can define the graph Fourier transform Y = U X and its inverse transformX = U Y following <ref type="bibr" target="#b11">[12]</ref>. Note that L = I − D − 1 2 AD − 1 2 shares the same eigenvectors with S = D − 1 2 AD − 1 2 and the eigenvalues of L are µ i = 1 − λ i . Therefore, L and S share the same graph Fourier transform which justifies the usages of S. Different forms of filters can be further constructed in the spectral domain.</p><p>Localized Polynomial Filter A τ -localized polynomial filter is typically adopted in GSP literature <ref type="bibr" target="#b11">[12]</ref></p><formula xml:id="formula_0">, g w (Λ) = τ −1 t=0 w t Λ t , where w = [w 0 , w 1 , . . . , w τ −1 ] ∈ R τ ×1</formula><p>is the filter coefficient, i.e., learnable parameter. The filter is τ -localized in the sense that the filtering leverages information from nodes which are at most τ -hops away. One prominent example of this class is the Chebyshev polynomial filter <ref type="bibr" target="#b6">[7]</ref>. Here the graph Laplacian is modified toL = 2L/λ max − I such that its eigenvalues fall into [−1, 1]. Then the Chebyshev polynomial recursion is applied:X(t) = 2LX(t − 1) −X(t − 2) whereX(0) = X andX(1) =LX. For a pair of input and output channels (i, j), the final filtering becomes, y i,j = [X(0) :,i , . . . ,X(τ − 1) :,i ]w i,j , where [·] means concatenation along columns and w i,j ∈ R τ ×1 . Chebyshev polynomials provide two benefits: they form an orthogonal basis of L 2 ([−1, 1], dy/ 1 − y 2 ) and one avoids the spectral decomposition ofL in the filtering. However, the functional form of the spectral filter is not learnable, and cannot adapt to the data.</p><p>In this paper, instead of using the modified graph LaplacianL, we use the aforementioned S. Therefore, we can write the localized polynomial filtering in a more general form as,</p><formula xml:id="formula_1">Y = τ −1 t=0 g t (S, . . . , S t , X)W t ,<label>(1)</label></formula><p>where g t is a function that takes node features X and powers of the affinity matrices up to the t-th order as input and outputs a N × F matrix. W t ∈ R F ×O is the corresponding filter coefficient and Y ∈ R N ×O is the output. One can easily verify that in the Chebyshev polynomial filter, any i-th column of the corresponding g t (X, S, . . . , S t ) lies in the Krylov subspace K t+1 (S, X :,i ) ≡ span{X :,i , SX :,i , . . . , S t X :,i }. This naturally motivates the usage of Krylov subspace methods, like the Lanczos algorithm <ref type="bibr" target="#b12">[13]</ref>, since it provides an orthonormal basis for the above Krylov subspace, thus making the filter coefficients compact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LANCZOS NETWORKS</head><p>In this section, we first introduce the Lanczos algorithm which approximates the affinity matrix S.</p><p>We present our first model, called Lanczos network (LanczosNet), in which we execute the Lanczos algorithm once per graph and fix the basis throughout inference and learning. Then we introduce the adaptive Lanczos network (AdaLanczosNet) in which we learn the graph kernel and/or node embedding by back-propagating through the Lanczos algorithm.</p><p>Algorithm 1 : Lanczos Algorithm 1: Input: S, x, K, 2: Initialization: β 0 = 0, q 0 = 0, and q 1 = x/ x 3: For j = 1, 2, . . . , K:</p><formula xml:id="formula_2">4: z = Sq j 5: γ j = q j z 6: z = z − γ j q j − β j−1 q j−1 7: β j = z 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>If β j &lt; , quit <ref type="bibr">9:</ref> q j+1 = z/β j 10:</p><p>11: Q = [q 1 , q 2 , · · · , q K ] 12: Construct T following Eq. (2) 13: Eigen decomposition T = BRB 14: Return V = QB and R. For i ∈ I:</p><formula xml:id="formula_3">10: Z = Z ∪ VR(I i )V Y −1 11: Y = concat(Z)W 12: If &lt; L 13: Y = Dropout(σ(Y )) 14: Return Y c .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LANCZOS ALGORITHM</head><p>Given the aforementioned affinity matrix S 1 and node features x ∈ R N ×1 , the N -step Lanczos algorithm computes an orthogonal matrix Q and a symmetric tridiagonal matrix T , such that Q SQ = T . We denote Q = [q 1 , · · · , q N ] where column vector q i is the i-th Lanczos vector. T is illustrated as below,</p><formula xml:id="formula_4">T =      γ 1 β 1 β 1 . . . . . . . . . . . . β N −1 β N −1 γ N      .<label>(2)</label></formula><p>One can verify that Q forms an orthonormal basis of the Krylov subspace K N (S, x) and the first K columns of Q forms the orthonormal basis of K K (S, x). The Lanczos algorithm is shown in detail in Alg. 1. Intuitively, if we investigate the j-th column of the system SQ = QT and rearrange terms, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">= 345645(7)</head><p>Short Range Spectral Filtering e.g., S = 1, 2, …  we obtain β j q j+1 = Sq j − β j−1 q j−1 − γ j q j , which clearly explains lines 4 to 6 of the pseudocode, i.e., it tries to solve the system in an iterative manner. Note that the most expensive operation in the algorithm is the matrix-vector multiplication in line 4. After obtaining the tridiagonal matrix T , we can compute the Ritz values and Ritz vectors which approximate the eigenvalues and eigenvectors of S by diagonalizing the matrix T . We only add this step in LanczosNet as we found back-propagating through the eigendecomposition in AdaLanczosNet is not numerically stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LANCZOSNET</head><p>In this section, we first show the construction of the localized polynomial filter based on the Lanczos algorithm's output and discuss its limitations. Then we explain how to construct the spectral filter using a particular low rank approximation and how to further make the filter learnable. At last, we elaborate how to construct multi-scale graph convolution and build a deep network.</p><p>Localized Polynomial Filter For the ease of demonstrating the concept of Krylov subspace, we consider a pair of input and output channels (i, j). We denote the input as X :,i ∈ R N ×1 and the output as Y :,j ∈ R N ×1 . Executing the Lanczos algorithm for K steps with the normalized X :,i as the starting vector, one can obtain the orthonormal basisQ of K K (S, X :,i ) and the corresponding tridiagonal matrixT . Recall that in the localized polynomial filtering, given the orthonormal basis of K K (S, X :,i ), one can write the graph convolution as</p><formula xml:id="formula_5">Y j =Qw i,j ,<label>(3)</label></formula><p>whereQ ∈ R N ×K depends on the X :,i and w i,j ∈ R K×1 is the learnable parameter. This filter has the benefit that the corresponding learnable coefficients are compact due to the orthonormal basis. However, if one wants to stack multiple graph convolution layers, the dependency ofQ on X :,i implies that a separate run of Lanczos algorithm is necessary for each graph convolution layer which is computationally demanding.</p><p>Spectral Filter Ideally, we would like to compute Lanczos vectors only once during the inference of a deep graph convolutional network. Luckily, this can be achieved if we take an alternative view of Lanczos algorithm. In particular, we can choose a random starting vector with unit norm and treat the K step Lanczos layer's output as the low rank approximation S ≈ QT Q . Note that here Q ∈ R N ×K has orthonormal columns and does not depend on the node features X i and T is a K × K tridiagonal matrix. Following <ref type="bibr" target="#b13">[14]</ref>, we prove the theorem below to bound the approximation error.</p><formula xml:id="formula_6">Theorem 1. Let U ΛU be the eigendecomposition of an N × N symmetric matrix S with Λ i,i = λ i , λ 1 ≥ · · · ≥ λ N and U = [u 1 , . . . , u N ]. Let U j ≡ span{u 1 , .</formula><p>. . , u j }. Assume K-step Lanczos algorithm starts with vector v and outputs the orthogonal Q ∈ R N ×K and tridiagonal T ∈ R K×K .</p><p>For any j with 1 &lt; j &lt; N and K &gt; j, we have,</p><formula xml:id="formula_7">S − QT Q 2 F ≤ j i=1 λ 2 i sin (v, U i ) j−1 k=1 (λ k − λ N )/(λ k − λ j ) cos (v, u i )T K−i (1 + 2γ i ) 2 + N i=j+1 λ 2 i , where T K−i (x) is the Chebyshev Polynomial of degree K − i and γ i = (λ i − λ i+1 )/(λ i+1 − λ N ).</formula><p>We leave the proof to the appendix. Note that the term ( N i=j+1 λ 2 i ) 1/2 is the Frobenius norm of the error between S and the best rank-j approximation S j . We decompose the tridiagonal matrix T = BRB , where the K × K diagonal matrix R contains the Ritz values and B ∈ R K×K is an orthogonal matrix. We have a low rank approximation of the affinity matrix S ≈ V RV , where V = QB. Therefore, we can rewrite the graph convolution as,</p><formula xml:id="formula_8">Y j = [X i , SX i , . . . , S K−1 X i ]w i,j ≈ [X i , V RV X i , . . . , V R K−1 V X i ]w i,j ,<label>(4)</label></formula><p>The difference between Eq. (3) and Eq. <ref type="formula" target="#formula_8">(4)</ref> is that the former uses the orthonormal basis while the latter uses the approximation of the direct basis of K K (S, X :,i ). Since we explicitly operate on the approximation of spectrum, i.e., Ritz value, it is a spectral filter. Such a filtering form will have significant computational benefits while considering the long range/scale dependency due to the fact that the t-th power of S can be approximated as S t ≈ V R t V , where we only need to raise the diagonal entries of R to the power t.</p><p>Learning the Spectral Filter Following the previous filter, one can naturally design learnable spectral filters. Denoting the diagonal entries of R and the column vectors of V , i.e., Ritz values and vectors, as {(r i , v i )|i = 1, . . . , K}, we perform i-th spectral filtering as follows,</p><formula xml:id="formula_9">L i = K k=1 f i (r 1 k , r 2 k , · · · , r K−1 k )v k v k<label>(5)</label></formula><p>where f i is a multi-layer perceptron (MLP). Therefore, we have the following graph convolution,</p><formula xml:id="formula_10">Y j = [X i ,L 1 X i , . . . ,L K−1 X i ]w i,j .<label>(6)</label></formula><p>Note that it includes the polynomial filter as a special case. When positive semi-definiteness is a concern, one can apply an activation function like ReLU to the output of the MLPs.</p><p>Multi-Scale Graph Convolution Using any above filter, one can construct a deep graph convolutional network which leverages multi-scale information. Taking the learnable spectral filter as an example, we can write one graph convolution layer in a compact way as below,</p><formula xml:id="formula_11">Y = L S1 X, . . . , L S M X,L 1 (I)X, . . . ,L N (I)X W,<label>(7)</label></formula><p>where weight W ∈ R (M +E)D×O , S is a set of M short scale parameters and I is a set of E long scale parameters. We consider a non-negative integer as scale parameter, e.g., S = {0, 1, . . . , 5}, I = {10, 20, . . . , 50}. Here we slightly abuse the notation and define the i-th spectral filtering aŝ</p><formula xml:id="formula_12">L i (I) = K k=1 f i (r I1 k , r I2 k , · · · , r I |I| k )v k v k .<label>(8)</label></formula><p>Note that the convolution corresponding to short scales is similar to <ref type="bibr" target="#b7">[8]</ref> where the number of matrixvector multiplications is tied to the maximum scale of S. In contrast, the convolution of long scales decouples the Lanczos step K and scale parameters I, thus permitting great freedom in tuning scales as hyperparameters. One can choose K properly to balance the computation cost and the accuracy of the low rank approximation. In our experiments, short scales are typically less than 10 which have reasonable computation cost. Moreover, the short scale part could sometimes remedy cases where the low rank approximation is crude. We set the long scale no larger than 100 in our experiments. If the maximum eigenvalue of S is 1, we can even raise the power to infinity, which corresponds to the equilibrium state of diffusion process on the graph.</p><p>To build a deep network, we can stack multiple such graph convolution layers where each layer has its own spectral filter weights. Nonlinear activation functions, e.g., ReLU, and/or Dropout can be added between layers. The inference algorithm of such a deep network is shown in Alg. 2. The overall computation graph of the model is illustrated in <ref type="figure" target="#fig_3">Fig. 1</ref>. With the top layer representation, one can use softmax to perform classification or a fully connected layer to perform regression. The Lanczos algorithm is run beforehand once per graph to construct the network and will not be invoked during inference and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ADALANCZOSNET</head><p>In this section, we explain another variant which back-propagates through the Lanczos algorithm. This facilitates learning the graph kernel and/or node embeddings.</p><p>Graph Kernel Assume we are given node features X and a graph G. We are interested in learning a graph kernel function with the hope that it can capture the intrinsic geometry of node representations. Given data points x i , x j ∈ X , we define the anisotropic graph kernel, k : X × X → R as,</p><formula xml:id="formula_13">k(x i , x j ) = exp − (f θ (x i ) − f θ (x j )) 2 .<label>(9)</label></formula><p>where f θ is an MLP. This class of anisotropic kernels is very expressive and includes self-tuning kernel <ref type="bibr" target="#b14">[15]</ref> and the Gaussian kernel with Mahalanobis distances <ref type="bibr" target="#b15">[16]</ref>. Moreover, for different kernel functions, the resulted graph Laplacians will converge to different limiting operators asymptotically. For example, even for isotropic Gaussian kernels, the graph Laplacian can converge pointwise to the Laplace-Beltrami, Fokker-Planck operator and heat kernel under different normalizations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In practice, we notice that choosing = (p,q)∈E (f θ (x p ) − f θ (x q )) 2 /|E| helps normalizing the pairwise distances, thus avoiding the gradient vanishing issue due to the exponential function. This type of learnable anisotropic diffusion is useful in two ways. First, it increases model capacity, thus potentially gaining better performance. Second, it can better adapt to the non-uniform density of the data points on the manifold or nonlinear measurements of the underlying data points on a maninfold. We can construct an adjacency matrix A such that</p><formula xml:id="formula_14">A i,j = k(x i , x j ) if (i, j) ∈ E and A i,j = 0</formula><p>otherwise. Then we can obtain the affinity matrix S = D − 1 2 AD − 1 2 .</p><p>Node Embedding In some applications, we do not observe the node features X but only the graph itself G, so we may need to learn an embedding vector per node. For example, this scenario applies in the quantum chemistry tasks where a node, i.e., an atom within a molecule, has rarely observed features. We can still use the above graph kernel to construct the affinity matrix which results in the same form except f is discarded. Learning embedding X naturally amounts to learning the similarities between nodes.</p><p>Tridiagonal Decomposition Although all operations in LanczosNet are differentiable, we empirically observe that backpropagation through the eigendecomposition of the tridiagonal matrix is numerically instable. The situation would be even worse if multiple eigenvalues are numerically close or one takes a large power in Eq. <ref type="bibr" target="#b6">(7)</ref>. Therefore, we instead directly leverage the approximated tridiagonal decomposition S ≈ QT Q which is obtained by running the Lanczos algorithm K steps. Then we can rewrite the spectral filtering as following,</p><formula xml:id="formula_15">L i (I) = Qg i (vec(T I1 ), vec(T I2 ), · · · , vec(T I |I| ))Q ,<label>(10)</label></formula><p>where vec(·) means vectorization and g i (·) = f i (·) + f i (·) , with the output of an MLP f i reshaped to a matrix of the same size as T . This ensures that the output is symmetric.</p><p>With the above parameterization of the graph Laplacian and tridiagonal decomposition, we can back-propagate the loss through the Lanczos algorithm to either the graph kernel parameters θ or the node embedding X. The overall model is similar to the LanczosNet except that the Lanczos algorithm needs to be invoked for each inference pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LANCZOS NETWORK AND DIFFUSION MAPS</head><p>In this section, we highlight the relationship between LanczosNet and an important example of graph based manifold learning algorithms, diffusion maps <ref type="bibr" target="#b16">[17]</ref>.</p><p>Diffusion Maps In diffusion maps, the weights in the adjacency matrix define a discrete random walk over the graph, where the Markov transition matrix P = D −1 A shows the transition probability in a single time step. Therefore, P t i,j sums the probability of all paths of length t that start at node i and end at node j. It is shown in <ref type="bibr" target="#b16">[17]</ref> that P can be used to define an inner product in a Hilbert space. Specifically, we use the eigenvalues and right eigenvectors {λ l , ψ l } N l=1 of P to define a diffusion mapping Φ t as,</p><formula xml:id="formula_16">Φ t (i) = λ t 1 ψ 1 (i), λ t 2 ψ 2 (i), . . . , λ t N ψ N (i) ,<label>(11)</label></formula><p>where ψ l (i) is the i-th entry of the eigenvector ψ l . Since the row stochastic matrix P is similar to S, i.e.,</p><formula xml:id="formula_17">P = D −1/2 SD 1/2 , we have ψ l = D −1/2 u l . The mapping Φ t satisfies N k=1 P t i,k P t j,k /D k,k = Φ t (i), Φ t (j) ,</formula><p>where ·, · is the inner product over Euclidean space. The diffusion distance between i and j, d 2</p><formula xml:id="formula_18">DM,t (i, j) = Φ t (i) − Φ t (j) 2 = N k=1 (P t i,k − P t j,k ) 2 /D k,k</formula><p>, is the weighted-l 2 proximity between the probability clouds of random walkers starting at i and ending at j after t steps. Since all eigenvalues of S reside in the interval [−1, 1], for some large t, λ t l in Eq. <ref type="formula" target="#formula_1">(11)</ref> is close to zero, and d DM,t can be well approximated by using only a few largest eigenvalues and their eigenvectors.</p><p>Connection to Graph Convolution Apart from using diffusion maps to embed node features X at different time scales, one can use it to compute the frequency representations of X as below,</p><formula xml:id="formula_19">X = Λ t U X,<label>(12)</label></formula><p>where U are the eigenvectors of S and define the graph Fourier transform. The frequency representa-tionX is weighted by the powers of the eigenvalues λ t l , suppressing entries with small magnitude of eigenvalues. Recall that in the convolution layer Eq. (6) of LanczosNet, we use multiple such frequency representations with different scales t and replace the eigenvalues Λ in Eq. (12) with their approximation, i.e., Ritz values. Therefore, in LanczosNet, spectral filters are actually applied to the frequency representations which are obtained by projecting the node features X onto multiple diffusion maps with different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>We can roughly categorize the application of machine learning, especially deep learning, to graph structured data into supervised/semi-supervised and unsupervised scenarios. For the former, a majority of work focuses on node/graph classification and regression <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. For the latter, unsupervised node/graph embedding learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> is common. Recently, generative models for graphs, such as molecule generation, has drawn some attention <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Graph Convolution Based Models The first class of learning models on graphs stems from graph signal processing (GSP) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> which tries to generalize convolution operators from traditional signal processing to graphs. Relying on spectral graph theory <ref type="bibr" target="#b25">[26]</ref> and graph wavelet theory <ref type="bibr" target="#b26">[27]</ref>, several definitions of frequency representations of graph signals have been proposed <ref type="bibr" target="#b3">[4]</ref>. Among these, spectral graph theory based one is popular, where graph Fourier transform and its inverse are defined based on the eigenbasis of the graph Laplacian. Following this line, many graph convolution based deep network models emerge. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref> are among the first to explore Laplacian based graph convolution within the context of deep networks. Meanwhile, <ref type="bibr" target="#b28">[29]</ref> performs graph convolution directly based on the adjacency matrix to predict molecule fingerprints. <ref type="bibr" target="#b29">[30]</ref> proposes a strategy to form same sized local neighborhoods and then apply convolution like regular CNNs. Chebyshev polynomials are exploited by <ref type="bibr" target="#b6">[7]</ref> to construct localized polynomial filters for graph convolution and are later simplified in graph convolutional networks (GCN) <ref type="bibr" target="#b10">[11]</ref>. Further accelerations for GCN based on importance sampling and control variate techniques have been proposed by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Several attention mechanisms have been introduced in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> to learn the weights over edges for GCNs. Notably, <ref type="bibr" target="#b7">[8]</ref> proposes diffusion convolutional neural networks (DCNN) which uses diffusion operator for graph convolution. Lanczos method has been explored for graph convolution in <ref type="bibr" target="#b34">[35]</ref> for the purpose of acceleration. Specifically, they only consider the localized polynomial filter case in our LanczosNet variant and do not explore the low rank decomposition, learnable spectral filter and graph kernel/node embedding learning as we do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Neural Networks based Models</head><p>The second class of models dates back to recursive neural networks <ref type="bibr" target="#b35">[36]</ref> which recurrently apply neural networks to trees following a particular order. Graph neural networks (GNN) <ref type="bibr" target="#b2">[3]</ref> generalize recursive neural networks to arbitrary graphs and exploit the synchronous schedule to propagate information on graphs. <ref type="bibr" target="#b36">[37]</ref> later proposes the gated graph neural networks (GGNN) which improves GNN by adding gated recurrent unit and training the network with back-propagation through time. <ref type="bibr" target="#b37">[38]</ref> learns graph embeddings via unrolling variational inference algorithms over a graph as a RNN. <ref type="bibr" target="#b38">[39]</ref> introduces random subgraph sampling and explores different aggregation functions to scale GNN to large graphs. <ref type="bibr" target="#b39">[40]</ref> proposes asynchronous propagation schedules based on graph partitions to improve GNN. Moreover, many applications have recently emerged for GNNs, including community detection <ref type="bibr" target="#b40">[41]</ref>, situation recognition <ref type="bibr" target="#b41">[42]</ref>, RGBD semantic segmentation <ref type="bibr" target="#b42">[43]</ref>, few-shot learning <ref type="bibr" target="#b20">[21]</ref>, probabilistic inference <ref type="bibr" target="#b43">[44]</ref>, continuous control of reinforcement learning <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> and so on.</p><p>Graph based Manifold Learning The non-linear dimensionality reduction methods, such as locally linear embedding (LLE) <ref type="bibr" target="#b46">[47]</ref>, ISOMAP <ref type="bibr" target="#b47">[48]</ref>, Hessian LLE <ref type="bibr" target="#b48">[49]</ref>, Laplacian eigenmaps <ref type="bibr" target="#b49">[50]</ref>, and diffusion maps <ref type="bibr" target="#b16">[17]</ref>, assume that the high-dimensional data lie on or close to a low dimensional manifold and use the local affinities in the weighted graph to learn the global features of the data. They are invaluable tools for embedding complex data in a low dimensional space and regressing functions over graphs. Spectral clustering <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>, semi-supervised learning <ref type="bibr" target="#b52">[53]</ref>, and out-of-sample extension <ref type="bibr" target="#b53">[54]</ref> share the similar geometrical consideration of the associated graphs. Anisotropic graph kernels are useful in many applications. For example, <ref type="bibr" target="#b14">[15]</ref> improves the spectral clustering results with a self-tuning diffusion kernel that takes into account the local variance at each node in the Gaussian kernel function. Similarly, <ref type="bibr" target="#b54">[55]</ref> uses the anisotropic Gaussian kernel defined by the local Mahalanobis distances to extract independent components from nonlinear measurements of independent stochastic Itô processes. Manifold learning with anisotropic kernel is also useful for data-driven dynamical system analysis, for example, detecting intrinsically slow variable for a stochastic dynamical system <ref type="bibr" target="#b55">[56]</ref>, filtering dynamical processes <ref type="bibr" target="#b56">[57]</ref>, and long range climate forecasting <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>. The anisotropic diffusion is able to use the local statistics of the measurements to convey the geometric information on the underlying factors rather than the specific realization or measurements at hand <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we compare our two model variants against 9 recent graph networks, including graph convolution networks for fingerprint (GCN-FP) <ref type="bibr" target="#b28">[29]</ref>, gated graph neural networks (GGNN) <ref type="bibr" target="#b36">[37]</ref>, diffusion convolutional neural networks (DCNN) <ref type="bibr" target="#b7">[8]</ref>, Chebyshev networks (ChebyNet) <ref type="bibr" target="#b6">[7]</ref>, graph convolutional networks (GCN) <ref type="bibr" target="#b10">[11]</ref>, message passing neural networks (MPNN) <ref type="bibr" target="#b61">[62]</ref>, graph sample and aggregate (GraphSAGE) <ref type="bibr" target="#b38">[39]</ref>, graph partition neural networks (GPNN) <ref type="bibr" target="#b39">[40]</ref>, graph attention networks (GAT) <ref type="bibr" target="#b32">[33]</ref>. We test them on two sets of tasks: (1) semi-supervised document classification on 3 citation networks <ref type="bibr" target="#b62">[63]</ref>, (2) supervised regression of molecule property on QM8 quantum chemistry dataset <ref type="bibr" target="#b63">[64]</ref>. For fair comparison, we only tune model-related hyperparameters in all our experiments and share the others, e.g., using the same batch size. We carefully tune hyperparameters based on cross-validation and report the best performance of each competitor. Please refer to the appendix for more details on hyperparameters. We implement all methods using PyTorch <ref type="bibr" target="#b64">[65]</ref> and release the code at https://github.com/lrjconan/LanczosNetwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">CITATION NETWORKS</head><p>Three citation networks used in this experiment are: Cora, Citeseer and Pubmed. For each network, nodes are documents and connected based on their citation links. Each node is associated with a bag-of-words feature vector. We use the same pre-processing procedure and follow the transductive setting as in <ref type="bibr" target="#b62">[63]</ref>. In particular, given a portion of nodes and their labeled content categories, e.g., history, science, the task is to predict the category for other unlabeled nodes within the same graph. The statistics of these datasets are summarized in the appendix. All experiments are repeated 10 times with different random seeds. During each run, all methods share the same random seed. We first experiment with the public data split and observe severe overfitting for almost all algorithms. To mitigate overfitting and test the robustness of models, we then increase the difficulty of the task by reducing the portion of training examples to several levels and randomly split data.   <ref type="table">Table.</ref> 1. We use the reported best hyperparameters when available for public split and do cross-validation otherwise. Hyperparameters are reported in the appendix. From the table, we see that for random splits with different portion of training examples, since each run of experiment uses a separate random split, the overall variance is larger than its public counterpart. We see that GAT achieves the best performance on the public split but performs poorly on random splits with different portions of training examples. This is partly due to the fact that GAT uses multiple dropout throughout the model which helps only if there is overfitting. We can see that either LanczosNet or AdaLanczosNet achieves state-of-the-art accuracy on random difficult splits and performs closely with respect to GAT on public splits. This may be attributed to the fact that with fewer training examples, the model requires longer scale schemes to spread supervised information over the graph. Our model provides an efficient way of leveraging such long scale information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">QUANTUM CHEMISTRY</head><p>We then benchmark all algorithms on the QM8 quantum chemistry dataset which comes from a recent study on modeling quantum mechanical calculations of electronic spectra and excited state energy of small molecules <ref type="bibr" target="#b63">[64]</ref>. The setup of QM8 is as follows. Atoms are treated as nodes and they are connected to each other following the structure of the corresponding molecule. Each edge is labeled with a chemical bond. Note that two atoms in one molecule can have multiple edges belong to different chemical bonds. Therefore a molecule is actually modeled as a multigraph. We also use explicit hydrogen in molecule graphs as suggested in <ref type="bibr" target="#b61">[62]</ref>. Since some models cannot leverage feature on edges easily, we use the molecule graph itself as the only input information for all models so that it is a fair comparison. As demonstrated in our ablation studies, learning node embeddings for atoms is very helpful. Therefore, we augment all competitors and our models with this component. The task is to predict 16 different quantities of electronic spectra and energy per molecule graph which boils down to a regression problem. There are 21786 molecule graphs in total of which the average numbers of nodes and edges are around 16 and 21. There are 6 different chemical bonds and 70 different atoms throughout the dataset. We use the split provided by DeepChem 2 which have 17428, 2179 and 2179 graphs for training, validation and testing respectively. Following <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b65">66]</ref>, we use mean squared error (MSE) as the loss for training and weighted mean absolute error (MAE) as the evaluation metric. We repeat all experiments 3 times with different random seeds and report the average performance and standard deviation. The same random seed is shared for all methods per run. Hyperparameters are reported in the appendix. The validation and test MAEs are shown in <ref type="table" target="#tab_2">Table 2</ref>. As you can see, LanczosNet and AdaLanczosNet achieve better performances than all other competitors. Note that DCNN also achieves good performance with the carefully chosen scale parameters since it is somewhat similar to our model in terms of leveraging multi-scale information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Validation MAE (×1.0e −3 ) Test MAE (×1.0e −3 ) GCN-FP <ref type="bibr" target="#b28">[29]</ref> 15.06 ± 0.04 14.80 ± 0.09 GGNN <ref type="bibr" target="#b36">[37]</ref> 12.94 ± 0.05 12.67 ± 0.22 DCNN <ref type="bibr" target="#b7">[8]</ref> 10.14 ± 0.05 9.97 ± 0.09 ChebyNet <ref type="bibr" target="#b6">[7]</ref> 10.24 ± 0.06 10.07 ± 0.09 GCN <ref type="bibr" target="#b10">[11]</ref> 11.68 ± 0.09 11.41 ± 0.10 MPNN <ref type="bibr" target="#b61">[62]</ref> 11.16 ± 0.13 11.08 ± 0.11 GraphSAGE <ref type="bibr" target="#b38">[39]</ref> 13.19 ± 0.04 12.95 ± 0.11 GPNN <ref type="bibr" target="#b39">[40]</ref> 12.81 ± 0.80 12.39 ± 0.77 GAT <ref type="bibr" target="#b32">[33]</ref> 11.39 ± 0.09 11.02 ± 0.06 LanczosNet 9.65 ± 0.19 9.58 ± 0.14 AdaLanczosNet 10.10 ± 0.22 9.97 ± 0.20  <ref type="table">Table 3</ref>: Ablation study on QM8 dataset. Empty cell means that the component is neither used nor applicable. X-MLP means a MLP with X hidden layers. 'one-hot' means the node embedding is fixed as the one-hot encoding throughout learning and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ABLATION STUDY</head><p>We also did a thorough ablation study of our modeling components on the validation set of QM8.</p><p>Multi-Scale Graph Convolution: We first study the effect of multi-scale graph convolution. In order to rule out the impact of other factors, we use LanczosNet, do not employ the learnable spectral filter and use the one-hot encoding as the node embedding. The results are shown in the first row of <ref type="table">Table 3</ref>. Using long scales for graph convolution clearly helps on this task. Combining both short and long scales further improves results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lanczos</head><p>Step: We then investigate the Lanczos step since it will have an impact on the accuracy of the low rank approximation induced by the Lanczos algorithm. The results are shown in the second row of <ref type="table">Table 3</ref>. We can see that the performance is better with a relatively small Lanczos step like 10 and 20 which makes sense since the average number of nodes in this dataset is around 16.</p><p>Learning Spectral Filter: We then study whether learning spectral filter will help improve performance. The results are shown in the third row of <ref type="table">Table 3</ref>. Adding a 3-layer MLP does help reduce the error compared to not using any learnable spectral filter. Note that the MLP consists of 128 hidden units per layer and uses ReLU as the nonlinearity. However, using a deeper MLP does not seem to be helpful which might be caused by the challenges in optimization.</p><p>Graph Kernel/Node Embedding: At last, we study the usefulness of adding graph kernel and node embeddings. We first fix the node embedding as one-hot encoding and learn a 3 layer MLP which is the function f θ in Eq. <ref type="bibr" target="#b8">(9)</ref>. Next, we learn the node embeddings directly. Intuitively, learning embeddings amounts to learn a separate function f per node whereas our graph kernel learning enforces that f is shared for all nodes, thus being more restrictive. As shown in the 3-rd and 4th rows of the table, learning node embeddings significantly improves the performance for both LanczosNet and AdaLanczosNet and is more effective than learning graph kernels. Also, tuning the scale parameters further boosts the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose LanczosNet which leverages the Lanczos algorithm to construct a low rank approximation of the graph Laplacian. It not only provides an efficient way to gather multi-scale information for graph convolution but also enables learning spectral filters. Additionally, we propose a model variant AdaLanczosNet which facilitates graph kernel and node embedding learning. We show that our model has a close relationship with graph based manifold learning, especially diffusion map. Experimental results demonstrate that our model outperforms a range of other graph networks, on challenging graph problems. We are currently exploring customized eigen-decomposition methods for tridiagonal matrices, which will potentially further improve our AdaLanczosNet. Overall, work in this direction holds promise for allowing deep learning to scale up to very large graph problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><formula xml:id="formula_20">tan (u j , K m ) ≤ sin (v, U j ) j−1 k=1 (λ k − λ n )/(λ k − λ j ) cos (v, u j )T m−j (1 + 2γ) ,</formula><p>where T m−j (x) is the Chebyshev Polynomial of degree m − j and γ = (λ j − λ j+1 )/(λ j+1 − λ N ).</p><formula xml:id="formula_21">Theorem 1. Let U ΛU be the eigendecomposition of an N × N symmetric matrix S with Λ i,i = λ i , λ 1 ≥ · · · ≥ λ N and U = [u 1 , . . . , u N ]. Let U j ≡ span{u 1 , .</formula><p>. . , u j }. Assume K-step Lanczos algorithm starts with vector v and outputs the orthogonal Q ∈ R N ×K and tridiagonal T ∈ R K×K . For any j with 1 &lt; j &lt; N and K &gt; j, we have,</p><formula xml:id="formula_22">S − QT Q 2 F ≤ j i=1 λ 2 i sin (v, U i ) j−1 k=1 (λ k − λ N )/(λ k − λ j ) cos (v, u i )T K−i (1 + 2γ i ) 2 + N i=j+1 λ 2 i , where T K−i (x) is the Chebyshev Polynomial of degree K − i and γ i = (λ i − λ i+1 )/(λ i+1 − λ N ).</formula><p>Proof. From Lanczos algorithm, we have SQ = QT . Therefore,</p><formula xml:id="formula_23">S − QT Q 2 F = S − SQQ 2 F = S(I − QQ ) 2 F<label>(13)</label></formula><p>Let P ⊥ Q ≡ I −QQ , the orthogonal projection onto the orthogonal complement of subspace span{Q}. Relying on the eigendecomposition, we have,</p><formula xml:id="formula_24">S − QT Q 2 F = U ΛU (I − QQ ) 2 F = ΛU (I − QQ ) 2 F = (I − QQ )U Λ 2 F = λ 1 P ⊥ Q u 1 , . . . , λ N P ⊥ Q u N 2 F ,<label>(14)</label></formula><p>where we use the fact that RA 2 F = A 2 F for any orthogonal matrix R and A 2 F = A 2 F . Note that for any j we have,</p><formula xml:id="formula_25">λ 1 P ⊥ Q u 1 , . . . , λ N P ⊥ Q u N 2 F = N i=1 λ 2 i P ⊥ Q u i 2 ≤ j i=1 λ 2 i P ⊥ Q u i 2 + N i=j+1 λ 2 i ,<label>(15)</label></formula><p>where we use the fact that for any i, P ⊥</p><formula xml:id="formula_26">Q u i 2 = u i 2 − u i − P ⊥ Q u i 2 ≤ u i 2 = 1.</formula><p>Note that we have span{Q} = span{v, Sv, . . . , S K−1 v} ≡ K K from the Lanczos algorithm. Therefore, we have,</p><formula xml:id="formula_27">P ⊥ Q u i = | sin (u i , K K )| ≤ | tan (u i , K K )|.<label>(16)</label></formula><p>Applying the above lemma with A = S, we finish the proof.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">LANCZOS ALGORITHM</head><p>Utilizing exact arithmetic, Lanczos vectors are orthogonal to each other. However, in floating point arithmetic, it is well known that the round-off error will make the Lanczos vectors lose orthogonality as the iteration proceeds. One could apply a full Gram-Schmidt (GS) process z = z − j−1 i=1 z q i q i after line 6 of Alg. 1 to ensure orthogonality. Other partial or selective re-orthogonalization could also be explored. However, since we found the orthgonality issue does not hurt overall performance with a small iteration number, e.g., K = 20, and the full GS process is computationally expensive, we do not add such a step. Although some customized eigendecomposition methods, e.g., implicit QL <ref type="bibr" target="#b67">[68]</ref>, exist for tridiagonal matrix, we leave it for future exploration due to its complicated implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">EXPERIMENTS</head><p>For ChebyNet, we do not use graph coarsening in all experiments due to its demanding computation cost for large graphs. Also, for small molecule graphs, coarsening generally does not help since it loses information compared to directly stacking another layer of original graph.</p><p>Citation Networks The statistics of three citation networks are summarized in <ref type="table" target="#tab_5">Table 4</ref>. We now report the important hyperparameters chosen via cross-validation for each method. All methods are trained with Adam with learning rate 1.0e −2 and weight decay 5.0e −4 . The maximum number of epoch is set to 200. Early stop with window size 10 is also adopted. We tune hyperparameters using Cora alone and fix them for citeseer and pubmed. For convolution based methods, we found 2 layers work the best. In GCN-FP, we set the hidden dimension to 64 and dropout to 0.5. In GGNN, we set the hidden dimension to 64, the propagate step to 2 and aggregation function to summation. In DCNN, we set the hidden dimension to 64, dropout to 0.5 and use diffusion scales {1, 2, 5}. In ChebyNet, we set the polynomial order to 5, the hidden dimension to 64 and dropout to 0.5. In GCN, we set the hidden dimension to 64 and dropout to 0.5. In MPNN, we use GRU as update function and set the hidden dimension to 64 and dropout to 0.5. No edge embedding is used as there is just one edge type. In GraphSAGE, we set the number of sampled neighbors to 500, the hidden dimension to 64, dropout to 0.5 and the aggregation function to average. In GAT, we set the number of heads per layer to 8, 1, hidden dimension per head to 8 and dropout to 0.6. In LanczosNet, we set the short and long diffusion scales to {1, 2, 5, 7} and {10, 20, 30} respectively. The hidden dimension is 64 and dropout is 0.5. Lanczos step is 20. 1-layer MLP with 128 hidden units and ReLU nonlinearity is used as the spectral filter. In AdaLanczosNet, we set the short and long diffusion scales to {1, 2, 5} and {10, 20} respectively. The hidden dimension is 64 and dropout is 0.5. Lanczos step is 20. 1-layer MLP with 128 hidden units and ReLU nonlinearity is used as the spectral filter.</p><p>Quantum Chemistry We now report the important hyperparameters chosen via cross-validation for each method. All methods are trained with Adam with learning rate 1.0e −4 and no weight decay. The maximum number of epoch is set to 200. Early stop with window size 10 is also adopted. For convolution based methods, we found 7 layers work the best. We augment all methods with 64dimension node embedding and add edge types by either feeding a multiple-channel graph Laplacian matrix or directly adding a separate message function per edge type. For all methods, no dropout is used since it slightly hurts the performance. In GCN-FP, we set the hidden dimension to 128. In GGNN, we set the hidden dimension to 128, the propagate step to 15 and aggregation function to average. In DCNN, we set the hidden dimension to 128 and use diffusion scales {3, 5, 7, 10, 20, 30}. In ChebyNet, we set the polynomial order to 5, the hidden dimension to 128. In GCN, we set the hidden dimension to 128. In MPNN, we use GRU as update function, set the number of propagation to 7, set the hidden dimension to 128, use a 1-layer MLP with 1024 hidden units and ReLU nonlinearity as the message function and set the number of unroll step of Set2Vec to 10. In GraphSAGE, we set the number of sampled neighbors to 40, the hidden dimension to 128 and the aggregation function to average. In GAT, we set the number of heads of all 7 layers to 8 and hidden dimension per head to 16. In LanczosNet, we do not use short diffusion scales and set long ones to {1, 2, 3, 5, 7, 10, 20, 30}. The hidden dimension is 128. Lanczos step is 20. 1-layer MLP with 128 hidden units and ReLU nonlinearity is used as the spectral filter. In AdaLanczosNet, we set the short and long diffusion scales to {1, 2, 3} and {5, 7, 10, 20, 30} respectively. The hidden dimension is 128. Lanczos step is 20. 3-layer MLP with 4096 hidden units and ReLU nonlinearity is used as the spectral filter.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 2 : LanczosNet 1 : 5 :</head><label>15</label><figDesc>Input: Signal X, Lanczos output V and R, scale index sets S and I, 2: Initialization: Y 0 = X 3: For = 1, 2, . . . , c : 4: Z = Y −1 , Z = {∅} For j = 1, 2, . . . , max(S):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>{</head><label></label><figDesc>" # , % # } ' = )*+,-./())</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>The illustration of the model. The learnable spectral filters have different parameters per layer. The nonlinear activation function σ could be applied before or after the concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>± 0.7 77.6 ± 1.7 79.7 ± 0.8 78.0 ± 1.2 80.5 ± 0.8 78.0 ± 1.1 74.5 ± 0.8 82.6 ± 0.7 79.5 ± 1.8 80.4 ± 1.1 3% 71.7 ± 2.4 73.1 ± 2.3 76.7 ± 2.5 62.1 ± 6.7 74.0 ± 2.8 72.0 ± 4.6 64.2 ± 4.0 56.8 ± 7.9 76.3 ± 2.3 77.7 ± 2.4 1% 59.6 ± 6.5 60.5 ± 7.1 66.4 ± 8.2 44.2 ± 5.6 61.0 ± 7.2 56.7 ± 5.9 49.0 ± 5.8 48.6 ± 8.0 66.1 ± 8.2 67.5 ± 8.7 0.5% 50.5 ± 6.0 48.2 ± 5.7 59.0 ± 10.7 33.9 ± 5.0 52.9 ± 7.4 46.5 ± 7.5 37.5 ± 5.4 41.4 ± 6.9 58.1 ± 8.2 60.8 ± 9.0 ± 0.9 64.6 ± 1.3 69.4 ± 1.3 70.1 ± 0.8 68.1 ± 1.3 64.0 ± 1.9 67.2 ± 1.0 72.2 ± 0.9 66.2 ± 1.9 68.7 ± 1.0 1% 54.3 ± 4.4 56.0 ± 3.4 62.2 ± 2.5 59.4 ± 5.4 58.3 ± 4.0 54.3 ± 3.5 51.0 ± 5.7 46.5 ± 9.3 61.3 ± 3.9 63.3 ± 1.8 0.5% 43.9 ± 4.2 44.3 ± 3.8 53.1 ± 4.4 45.3 ± 6.6 47.7 ± 4.4 41.8 ± 5.0 33.8 ± 7.0 38.2 ± 7.1 53.2 ± 4.0 53.8 ± 4.7 0.3% 38.4 ± 5.8 36.5 ± 5.1 44.3 ± 5.1 39.3 ± 4.9 39.2 ± 6.3 36.0 ± 6.1 25.7 ± 6.1 30.9 ± 6.9 44.4 ± 4.5 46.7 ± 5.6 ± 4.7 70.4 ± 4.5 73.1 ± 4.7 55.2 ± 6.8 73.0 ± 5.5 67.3 ± 4.7 65.4 ± 6.2 59.6 +-9.5 73.4 ± 5.1 72.8 ± 4.6 0.05% 63.2 ± 4.7 63.3 ± 4.0 66.7 ± 5.3 48.2 ± 7.4 64.6 ± 7.5 59.6 ± 4.0 53.0 ± 8.0 50.4 +-9.7 68.8 ± 5.6 66.0 ± 4.5 0.03% 56.2 ± 7.7 55.8 ± 7.7 60.9 ± 8.2 45.3 ± 4.5 57.9 ± 8.1 53.9 ± 6.9 45.4 ± 5.5 50.9 +-8.8 60.4 ± 8.6 61.0 ± 8.7</figDesc><table><row><cell>Cora</cell><cell>GCN-FP</cell><cell>GGNN</cell><cell>DCNN</cell><cell>ChebyNet</cell><cell>GCN</cell><cell>MPNN GraphSAGE</cell><cell>GAT</cell><cell>LNet</cell><cell>AdaLNet</cell></row><row><cell cols="2">Public 74.6 Citeseer GCN-FP</cell><cell>GGNN</cell><cell>DCNN</cell><cell>ChebyNet</cell><cell>GCN</cell><cell>MPNN GraphSAGE</cell><cell>GAT</cell><cell>LNet</cell><cell>AdaLNet</cell></row><row><cell cols="2">Public 61.5 Pubmed GCN-FP</cell><cell>GGNN</cell><cell>DCNN</cell><cell>ChebyNet</cell><cell>GCN</cell><cell>MPNN GraphSAGE</cell><cell>GAT</cell><cell>LNet</cell><cell>AdaLNet</cell></row><row><cell cols="10">Public 76.0 ± 0.7 75.8 ± 0.9 76.8 ± 0.8 69.8 ± 1.1 77.8 ± 0.7 75.6 ± 1.0 76.8 ± 0.6 76.7 +-0.5 78.3 ± 0.3 78.1 ± 0.4</cell></row><row><cell cols="2">0.1% 70.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy with 10 runs on citation networks. The public splits in Cora, Citeseer and Pubmed contain 5.2%, 3.6% and 0.3% training examples respectively.</figDesc><table /><note>Experimental results and exact portions of training examples are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean absolute error on QM8 dataset.</figDesc><table><row><cell>Model</cell><cell>Graph Kernel</cell><cell>Node Embedding</cell><cell>Spectral Filter</cell><cell>Short Scales</cell><cell>Long Scales</cell><cell>Lanczos Step</cell><cell>Validation MAE (×1.0e −3 )</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell>{1, 2, 3}</cell><cell></cell><cell></cell><cell>10.71</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell>{3, 5, 7}</cell><cell></cell><cell></cell><cell>10.60</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell></cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.54</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell>{3, 5 ,7}</cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.41</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell></cell><cell>{10, 20, 30}</cell><cell>5</cell><cell>10.49</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell></cell><cell>{10, 20, 30}</cell><cell>10</cell><cell>10.44</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell></cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.54</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell></cell><cell></cell><cell>{10, 20, 30}</cell><cell>40</cell><cell>10.49</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell>3-MLP</cell><cell>{3, 5 ,7}</cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.44</cell></row><row><cell>LanczosNet</cell><cell></cell><cell>one-hot</cell><cell>5-MLP</cell><cell>{3, 5 ,7}</cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.54</cell></row><row><cell>LanczosNet</cell><cell></cell><cell></cell><cell>3-MLP</cell><cell>{3, 5 ,7}</cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.26</cell></row><row><cell>LanczosNet</cell><cell></cell><cell></cell><cell>3-MLP</cell><cell></cell><cell>{1, 2, 3, 5, 7, 10, 20, 30}</cell><cell>20</cell><cell>9.56</cell></row><row><cell>AdaLanczosNet</cell><cell></cell><cell>one-hot</cell><cell>3-MLP</cell><cell>{3, 5, 7}</cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.99</cell></row><row><cell>AdaLanczosNet</cell><cell></cell><cell></cell><cell>3-MLP</cell><cell>{3, 5, 7}</cell><cell>{10, 20, 30}</cell><cell>20</cell><cell>10.20</cell></row><row><cell>AdaLanczosNet</cell><cell></cell><cell></cell><cell>3-MLP</cell><cell>{1, 2, 3}</cell><cell>{5, 7, 10, 20, 30}</cell><cell>20</cell><cell>9.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>RL thanks Roger Grosse for introducing the Lanczos algorithm to him. RL was supported by Connaught International Scholarships. RL, RU and RZ were supported in part by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.8 APPENDIX8.1 LOW RANK APPROXIMATIONWe first state the following Lemma from<ref type="bibr" target="#b66">[67]</ref> without proof and then prove our Theorem 1 following<ref type="bibr" target="#b13">[14]</ref>. Lemma 1. Let A ∈ R N ×N be symmetric and v an arbitrary vector. Define Krylov subspace K m ≡ span{v, Av, . . . , A m−1 v}. Let A = U ΛU be the eigendecomposition of A with Λ i,i = λ i and λ 1 ≥ · · · ≥ λ n . Denoting U = [u 1 , . . . , u N ] and U j = span{u 1 , . . . , u j }, then</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics. S 0 is portion of training examples in the public split. S 1 to S 3 are the ones of 3 random splits generated by us.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When faced with a non-symmetric matrix, one can resort to the Arnoldi algorithm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://deepchem.io/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNN</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph signal processing: Overview, challenges, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Kovačević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scale-space filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew P Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Geometric diffusions as a tool for harmonic analysis and structure definition of data: Multiscale methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">B</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion interpretation of nonlocal neighborhood filters for signal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoel</forename><surname>Shkolnisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale kernels for nyström based extension schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neta</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalia</forename><surname>Fishelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page" from="165" to="177" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An iteration method for the solution of the eigenvalue problem of linear differential and integral operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Lanczos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<publisher>United States Governm. Press Office Los</publisher>
			<pubPlace>Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-rank matrix approximation using the lanczos bidiagonalization process with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metric learning for kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Diffusion maps. Applied and computational harmonic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">From graph to manifold laplacian: The convergence rate. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<title level="m">Graph kernels. JMLR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rmi</forename><surname>Gribonval</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Accelerated filtering on graphs using lanczos method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Susnjara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kressner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04537</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph partition neural networks for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<title level="m">Community detection with graph neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Inference in probabilistic graphical models by graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kijung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xaq</forename><surname>Pitkow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07710</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nervenet: Learning structured policy with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01242</idno>
		<title level="m">Graph networks as learnable physics engines for inference and control</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A global geometric framework for nonlinear dimensionality reduction. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carrie</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grimes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diffusion maps, spectral clustering and eigenfunctions of fokker-planck operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Computer Science, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Non-linear independent component analysis with diffusion maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronald R Coifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Detecting intrinsic slow variables in stochastic dynamical systems by anisotropic diffusion maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Erban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">R</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coifman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Empirical intrinsic geometry for nonlinear modeling and time series filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Talmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronald R Coifman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dynamics-adapted cone kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Dynamical Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Analog forecasting with dynamics-adapted kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinearity</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Diffusion kernels on statistical manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Methods of information geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Shun-Ichi Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagaoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Electronic spectra from tddft and machine learning in chemical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Tapavicza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Beresford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parlett</surname></persName>
		</author>
		<title level="m">The Symmetric Eigenvalue Problem. SIAM</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Numerical recipes 3rd edition: The art of scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">P</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flannery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
