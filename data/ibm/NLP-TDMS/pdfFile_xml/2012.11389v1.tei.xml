<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KNOWLEDGE TRANSFER BASED FINE-GRAINED VISUAL CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">KNOWLEDGE TRANSFER BASED FINE-GRAINED VISUAL CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Fine-grained Visual Clas- sification</term>
					<term>Knowledge Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual classification (FGVC) aims to distinguish the sub-classes of the same category and its essential solution is to mine the subtle and discriminative regions. Convolution neural networks (CNNs), which employ the cross entropy loss (CE-loss) as the loss function, show poor performance since the model can only learn the most discriminative part and ignore other meaningful regions. Some existing works try to solve this problem by mining more discriminative regions by some detection techniques or attention mechanisms. However, most of them will meet the background noise problem when trying to find more discriminative regions. In this paper, we address it in a knowledge transfer learning manner. Multiple models are trained one by one, and all previously trained models are regarded as teacher models to supervise the training of the current one. Specifically, a orthogonal loss (OR-loss) is proposed to encourage the network to find diverse and meaningful regions. In addition, the first model is trained with only CE-Loss. Finally, all models' outputs with complementary knowledge are combined together for the final prediction result. We demonstrate the superiority of the proposed method and obtain state-of-the-art (SOTA) performances on three popular FGVC datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Compared with general image classification tasks, finegrained visual classification (FGVC) aims to identify subclasses of a given object class <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Due to different sub-classes of a common visual class only differ in subtle details, FGVC faces the challenges of large intra-class variations and small inter-class variations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Recently, convolution neural networks (CNNs) have undergone unprecedented success in visual representation learning. For classification tasks, it takes minimizing the crossentropy loss (CE-loss) between the class labels and the network predictions as the optimization objective, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(A). However, CE-loss usually makes the network to focus on the most discriminative region <ref type="bibr" target="#b3">[4]</ref> and ignore the other less significant but complementary local parts, which is not sufficient for FGVC. For instance, given an image containing a Corgi, the network may merely concentrate on its <ref type="figure">Fig. 1</ref>. For Corgis, round butts are their most significant features, but we can also easily recognize them through its cute heads or short legs.</p><p>round butt, and ignore the remaining parts such as its cute head and short legs which also provide effective information for classification, as shown in <ref type="figure">Figure 1</ref>.</p><p>To overcome these limitations, it is widely accepted that the essential solution is to mine discriminative information from various complementary local regions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. In the early work, people resort help of heavy supervision to detect multiple discriminative parts for classification. It requires not only the category labels of the image but also additional manual annotation such as object part bounding boxes which consumes lots of human labor <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Besides, the part annotations are hard to obtain during the inference phase, which reduces its usefulness and slows down the development of the community <ref type="bibr" target="#b1">[2]</ref>. Recently, weakly supervised detection or attention techniques become feasible substitutes, since only the category labels are needed for both the training and inference stage <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. These methods can be roughly divided into two types: (i) part-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref> which first locate several discriminative local parts and then extract features from them for classification, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (B), and (ii) adversarial erasing methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15</ref>] that encourage the model to learn more discriminative parts by progressively erasing the learned parts, as shown in <ref type="figure" target="#fig_0">Figure  2</ref> (C). However, part-based methods may bring noises from background since most models have pre-defined the number of parts and multiple disciminative parts may not consistently occur in each image, and adversarial erasing methods suffer the same problem when too many object parts are erased.</p><p>In this paper, instead of explicitly locating the discriminative parts, we tend to address the limitation of CE-loss by knowledge transfer, where the student model implicitly mines the complementary discriminative regions with the guidance of the teacher models and then their results with complementary knowledge are combined together for the final recognition result, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(D). Specifically, spatial attention maps are generated for both networks to represent their spatial concentrations where high response regions mean the discriminative parts and the low response regions indicate background. During the knowledge transfer stage, a orthogonal loss (OR-loss) is proposed to force the student model to mine discriminative information from the regions which are neither high response nor background on the spatial attention maps of the teacher models. Thanks to knowledge transfer, the student model not only obtains diverse and useful knowledge but also reduces the noises from background. Main contributions of this paper can be summarized as follows:</p><p>1. We apply knowledge transfer mechanism in FGVC,</p><p>where several networks are trained in sequence, and all previously trained ones will serve as teacher models for the current student model. In this way, instead of focusing on the most discriminative region, the current model is forced to mine meaningful information from other regions and offer complementary knowledge.</p><p>2. We propose a orthogonal loss (OR-loss), which can be used during knowledge transfer to guide the student network to mine discriminative information from the regions which neither have been focused by the teacher networks nor are background.</p><p>3. Our method achieves SOTA performance on three widely used FGVC datasets, which demonstrates the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-grained Visual Classification</head><p>Due to large intra-class variations and small inter-class variations, FGVC is much more challenging than general classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>, and the most effective solution for FGVC is to discover discriminative regions as many as possible. Recent works tackle FGVC from this perspective can be roughly divided into two types: (i) part-based methods [2, 3, 6, 10, 7], and (ii) adversarial erasing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. For part-based methods, the network first locates discriminative parts and then extracts features from these parts respectively. Fu et al. <ref type="bibr" target="#b0">[1]</ref> find region detection and fine-grained feature learning can reinforce each other, and build a series of networks which find discriminative regions for the next one. With similar motivation, Zhang et al. <ref type="bibr" target="#b4">[5]</ref> train several networks focusing on features of different granularities to produce diverse prediction distribution. Zheng et al. <ref type="bibr" target="#b1">[2]</ref> jointly learn part proposals and feature representations on each part. Nevertheless, these methods often suffer the problem that the number of found parts is pre-defined and discriminative parts may not consistently occur in each image.</p><p>For adversarial erasing methods, the algorithm progressively erasing learned discriminative parts to encourage the network to learn more. In this way, not only the most discriminative region but also other informative parts are highlighted. Choe et al. <ref type="bibr" target="#b12">[13]</ref> utilize attention mechanism to drop the regions with high response. Lee et al. <ref type="bibr" target="#b13">[14]</ref> randomly hide patches to discover more object regions. Zhang et al. <ref type="bibr" target="#b14">[15]</ref> find the complementary regions by two adversarial classifiers. However, it is hard to decide when the training process be stopped, and if too many object parts are erased the network tends to fix the background noise.</p><p>In this paper, we propose a knowledge transfer based method and change the point of view by utilizing spatial information of both discriminative regions and background to encourage the model mining complementary information in a more robust way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge Distillation</head><p>Knowledge distillation Learning has been widely studied and applied in machine learning. It is a technology to distill the knowledge of one model to another. Hinton et al. <ref type="bibr" target="#b15">[16]</ref> firstly propose the concept of dark knowledge based on teacherstudent network, where teacher network is often a more complex network with desirable performance and fine generalization. With teacher's help the simpler student model with less parameter also has similar performance with teacher. Remero et al. <ref type="bibr" target="#b16">[17]</ref> convey knowledge by feature maps, guiding student to acquire the ability to extract features. Zaforuyko et al. <ref type="bibr" target="#b17">[18]</ref> propose the method that the student model is trained with the guidance of the teacher's attention map. All aforementioned methods aim to make the student model has similar performance and fine generalization to the teacher model. However, in this paper, we guide the student model learn complementary knowledge with the teacher. We replace the KL divergence with the proposed OR-Loss and take spatial attention map as knowledge carrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head><p>In this section, we first revisit the CNN pipeline for general classification. Then two main parts of our knowledge transfer framework are introduced: the spatial attention maps which act as knowledge carrier, the orthogonal loss which induces the training of the student network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting CNN for classification</head><p>Given an input image, the network first extracts features F ∈ R C×H×W which consist of C channels with H × W spatial dimensions, and then feed into a classifier.</p><p>During training phase, minimizing the CE-loss between the labels and the predictions is employed as the optimizing objective. It has been proved that, with CE-loss only, the network is encouraged to locate the most discriminative region in the input image <ref type="bibr" target="#b3">[4]</ref> and ignores others. This natural character hinders the performance of CNN in FGVC tasks where various discriminative parts are sufficient. To address this limitation, in this paper, we train multiple networks in sequence with different concentrations that provide complementary knowledge during inference. Furthermore, network trained arbitrarily often extracts the most significant region. Hence, we propose a knowledge transfer framework with a newly proposed OR-Loss forcing the student model to mine complementary and meaningful information with the guidance of the teacher models.  <ref type="figure">Fig. 4</ref>. The architecture of the spatial attention map producer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Attention Map</head><p>During the knowledge transfer stage, our aim is transferring spatial information between two models, which makes spatial attention map an appropriate choice to be the information carrier since its response indicates spatial information of the model. The spatial attention map is obtained by the following procedure. First, the channel importance of a feature map is aggregated by global average pooling (GAP) operation. Then, we broadcast the GAP outputs along the channel dimension and element-wisely multiply them with the feature maps. At last, a channel-wise average pooling (CAP) operation is applied to the feature maps to compress the 3D features into a 2D spatial attention map M . In short, the process is denoted as follows:</p><formula xml:id="formula_0">M = CAP (GAP (F ) ⊗ F ),<label>(1)</label></formula><p>where ⊗ denotes element-wise multiplication, and M ∈ R 1×W ×H represents the spatial attention map. Further more, the spatial attention map is normalized to standard normal distribution with zero mean. It is operated as follows:</p><formula xml:id="formula_1">M norm = M − M mean M std ,<label>(2)</label></formula><p>where M mean and M std are mean value and standard deviation value of the spatial attention map M , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Orthogonal loss</head><p>On the attention map, high response regions stand for the discovered discriminative parts and low response regions indicate background. To ensure the different concentrations between the teacher models and the student model, an orthogonal loss (OR-loss) is applied to their spatial attention maps, which guarantee them to be diverse. Furthermore, we hope the knowledge of background is also transferred to the student model to avoid the background noise being mined by mistake. Hence, the absolute value of the teacher attention map is used when computing the ORloss:</p><formula xml:id="formula_2">M n teacher = |M norm | .<label>(3)</label></formula><p>Besides, only the high response regions of the student attention map need guidance of the OR-loss, so we only take the positive parts of the student attention map:</p><formula xml:id="formula_3">M student = max(M norm , 0),<label>(4)</label></formula><p>then the OR-loss can be denoted as:</p><formula xml:id="formula_4">L n OR = M n teacher ⊗ M student ,<label>(5)</label></formula><p>where ⊗ denotes element-wise multiplication. When the N th model is trained, where N ≥ 2, all previous trained networks will serve as the teacher model, and M n teacher denotes the knowledge carrier from the n th model network with n ∈ {1, N − 1}. During knowledge transfer phase, CE-loss is also applied to encourage the model learning discriminative information under the orthogonal loss. A hyper-parameter α is introduced to balance the value of CE-loss and OR-loss, and the total loss L total is expressed as follows:</p><formula xml:id="formula_5">L total = L CE + 1 N − 1 N −1 n=1 αL n OR ,<label>(6)</label></formula><p>where N represents the ensemble system consists of N models. All teacher and student networks constituted the ensemble system. To make a comprehension use of each model's discriminative information, we average inference results and get the final classification result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Protocol</head><p>The proposed model is evaluated on three public FGVC benchmarks: Caltech-UCSD Birds (CUB) <ref type="bibr" target="#b18">[19]</ref>, Stanford Cars (CAR) <ref type="bibr" target="#b19">[20]</ref>, and FGVC-Aircraft (AIR) <ref type="bibr" target="#b20">[21]</ref>. All our experiments are conducted without any bounding box annotations. To evaluate the performance of our method, we employ the top-1 accuracy as evaluation metric. <ref type="table">Table 1</ref>. Classification accuracy on CUB, CAR, and AIR. The best results on each dataset are in red, and the second best results are in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone CUB CAR AIR FT VGGNet <ref type="bibr" target="#b5">[6]</ref> VGG19 77.8 84.9 84.8 FT ResNet <ref type="bibr" target="#b5">[6]</ref> ResNet50 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In this paper, all our experiments are performed on 4 NVIDIA Geforce 1080Ti GPUs with 12G memory. Images are resized to 448 × 448 before fed into the networks. Then we adopt random crop and random flip as the data augmentations. After that, we feed these images into ResNet50 or VGG16, initialized by the ImageNet <ref type="bibr" target="#b21">[22]</ref>. We use a mini-batch size of 32. The network models are trained for 100 epochs, using Stochastic Gradient Descent as optimizer and Batch Normalization <ref type="bibr" target="#b22">[23]</ref> as regularizer. We initialize the learning rate of pretrained feature extractor and classifier to 0.001 and 0.01, respectively. And we use cosine annealing algorithm <ref type="bibr" target="#b23">[24]</ref> to gradually adjust learning rate every epoch. We empirically set momentum to 0.9, weight decay to 5e −4 , and the number of models N to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with SOTA methods</head><p>To verify the effectiveness of the proposed method, it is compared with other SOTA methods. <ref type="table">Table 1</ref> illustrates the experiment results on CUB, CAR, and AIR datasets, respectively. Our method achieves state-of-the-art (SOTA) performances on three datasets with ResNet50 as the backbone network. And when we use VGG16 as the base model, the proposed method also outperforms the baseline with a large margin of 8.4%, 7%, and 5.7%, which demonstrates the superiority of our framework can be generalized to any network architecture. As for representative part-based arts, MA-CNN <ref type="bibr" target="#b1">[2]</ref> adopts weakly-supervised object detection to locate different spatial regions for further prediction and RA-CNN <ref type="bibr" target="#b0">[1]</ref> creatively locates regions of different scales to obtain complementary information, which are suppressed by our framework on all of three datasets. For PMA <ref type="bibr" target="#b28">[29]</ref> supervised with only category label, which is a standard adversarial erasing framework for FGVC, we suppress it with significant improvements of 1.6%, 1.0%, and 2.5% on all three datasets, which demonstrates the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>To select the optimal hyper-parameter α and number of models N , we conduct a series of experiments on the CUB dataset with ResNet50 as the base model, and report the results in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Effects of the hyper-parameter α</head><p>It can be seen in <ref type="table" target="#tab_1">Table 2</ref>, when the hyper-parameter α is set to 0, the performance of ensemble system is not good enough. It indicates that with CE-loss, different networks only extract similar regions and ignore less informative parts which also benefit our final result. However, when the hyper-parameter α is between 0.05 and 1, the ensemble system obtains significant improvement, which verifies the validity of our method. The best result is obtained when α is set to 0.5. Besides, when setting α to 5, the performance drops a lot, since ORloss demonstrates the training and hurts the optimization of CE-loss. Eventually, we set 0.5 as the hyper-parameter α. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Effects of the Number of Models N</head><p>As for the performance of each model, <ref type="table" target="#tab_2">Table 3</ref> shows the first model achieves the best performance in our framework, since it focuses on the most discriminative region of the input. Although the discriminative region is suppressed during the training phase, the accuracy of each student model does not significantly drop compared to baseline models trained arbitrarily, which illustrates that other region has discriminative information too.</p><p>We conduct experiments about the number of models N ranging from 1 to 6. As shown in <ref type="table" target="#tab_2">Table 3</ref>, when we increase the number of models from 1 to 5, the classification performance of our ensemble system receives consecutive gains and steadily exceeds the performance of the baseline ensemble system which contains N models trained arbitrarily. It confirms the achievement of our motivation that mines and fuses complementary knowledge of less discriminative regions, with the guidance of both OR-loss and CE-loss. However, when N is set to 6, the accuracy of the ensemble system does not show any further improvements, which indicates the discriminative regions have already been excavated when N = 5. At last, we set the number of models N to 5. <ref type="figure" target="#fig_2">Figure 5</ref> displays original images and visualizations of the baseline network, the teacher network, and the first student network from Grad-CAM on CUB dataset based on ResNet50 model. Both of teacher and baseline network are trained with only CE-Loss. They only pay attention to the most discriminative region like bird's chest, and ignore lots of information of less discriminative regions. However, with assistance of the OR-loss, the student network mines complementary regions (e.g., bird's tail) from the teacher network. The visualization results indicate the effectiveness of OR-Loss of forcing the student network to focus on diverse regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we propose a knowledge transfer based approach for FGVC. Specifically, we train several models in sequence, and each of them are supervised by knowledge transferred from previous trained ones. During the training stage, under the guidance of both the CE-Loss and the proposed OR-Loss, the network is encouraged to find diverse and meaningful discriminative regions. Finally, to utilize the complementary knowledge of these models, we ensemble their inference outputs as the final prediction result. We demonstrate the superiority of our method and achieve the SOTA performance on CUB, CAR, and AIR datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>arXiv:2012.11389v1 [cs.CV] 21 Dec 2020 Illustration of features learned by general methods (A B, and C) and our proposed method (D). (A) Convolution neural networks trained with cross entropy (CE) loss tend to find the most discriminative parts; (B) Part-based methods first locate several discriminative local parts and then extract features from them for classification; (C) Adversarial erasing methods encourage the model to learn more discriminative parts by progressively erasing the learned parts; (D) With assistant of Orthogonal loss, the student model implicitly mines the complementary and meaningful regions with the guidance of the teacher model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The framework of our method. The upper components and the bottom components represent the forward propagation processes of the teacher model and the student model, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Activation map of selected results on the CUB dataset on Resnet50. First column represents the original image. The following three columns show visualizations of baseline network, teacher network, and the first student network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The performances of the proposed method with different loss weight α. The best result is in red. Acc. 87.2 87.7 87.9 88.5 88.2 87.2</figDesc><table><row><cell>α</cell><cell>0</cell><cell>0.05 0.1</cell><cell>0.5</cell><cell>1</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of our framework and the baseline ensemble system where all models are trained arbitrarily. Both the single model accuracy and the ensemble accuracy with different hyper-parameters N are listed. The best results are in red.</figDesc><table><row><cell>N</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>Single (Base)</cell><cell cols="6">87.2 86.8 87.1 87.0 86.6 87.0</cell></row><row><cell cols="7">Ensemble (Base) 87.2 87.8 88.2 88.3 88.6 88.5</cell></row><row><cell>Single (Ours)</cell><cell cols="6">87.2 86.1 87.0 86.5 86.4 86.6</cell></row><row><cell cols="7">Ensemble (Ours) 87.2 88.5 88.6 88.8 89.1 89.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning multiattention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast mode decision based on grayscale similarity and inter-view correlation for depth map coding in 3d-hevc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained vehicle classification with channel max pooling modified cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge,&quot; IJCV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bi-modal progressive mask attention for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
