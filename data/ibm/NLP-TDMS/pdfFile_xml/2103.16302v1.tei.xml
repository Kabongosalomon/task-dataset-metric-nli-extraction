<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Spatial Dimensions of Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heo</forename><surname>Byeongho</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Sangdoo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Dongyoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Sanghyuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choe</forename><surname>Junsuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Seong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Spatial Dimensions of Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of the spatial dimension conversion and its effectiveness on the transformer-based architecture. We particularly attend the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The architectures based on the self-attention mechanism have achieved great success in the field of Natural Language Processing (NLP) <ref type="bibr" target="#b34">[35]</ref>. There have been attempts to utilize the self-attention mechanism in computer vision. Non-local networks <ref type="bibr" target="#b36">[37]</ref> and DETR <ref type="bibr" target="#b3">[4]</ref> are representative works, showing that the self-attention mechanism is also effective in video classification and object detection tasks, respectively. Recently, Vision Transformer (ViT) <ref type="bibr" target="#b8">[9]</ref>, a transformer architecture consisting of self-attention layers, has been proposed to compete with ResNet <ref type="bibr" target="#b12">[13]</ref>, and shows that it can achieve the best performance without convolution operation on ImageNet <ref type="bibr" target="#b7">[8]</ref>. As a result, a new direction of network architectures based on self-attention mechanism, not convolution operation, has emerged in computer vision.</p><p>ViT is quite different from convolutional neural networks (CNN). Input images are divided into 16×16 patches and fed to the transformer network; except for the first embedding layer, there is no convolution operation in ViT, and the position interactions occur only through the self-attention layers. While CNNs have restricted spatial interactions, ViT allows all the positions in an image to interact through transformer layers. Although ViT is an innovative architecture and has proven its powerful image recognition ability, it follows the transformer architecture in NLP <ref type="bibr" target="#b34">[35]</ref> without any changes. Some essential design principles of CNNs, which have proved to be effective in the computer vision domain over the past decade, are not sufficiently reflected. We thus revisit the design principles of CNN architectures and investigate their efficacy when applied to ViT architectures.</p><p>CNNs start with a feature of large spatial sizes and a small channel size and gradually increase the channel size while decreasing the spatial size. This dimension conversion is indispensable due to the layer called spatial pooling. Modern CNN architectures, including AlexNet <ref type="bibr" target="#b20">[21]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>, and EfficientNet <ref type="bibr" target="#b32">[33]</ref>, follow this design principle. The pooling layer is deeply related to the receptive field size of each layer. Some studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref> show that the pooling layer contributes to the expressiveness and generalization performance of the network. However, unlike the CNNs, ViT does not use a pooling layer and uses spatial tokens of the same size in all layers.</p><p>First, we verify the advantages of pooling layers on CNNs. Our experiments show that the pooling layer improves the model capability and generalization performance of ResNet. To extend the advantages of the pooling layer to ViT, we propose a Pooling-based Vision Transformer (PiT). PiT is a transformer architecture combined with a pooling layer. It enables the spatial size reduction in the ViT structure as in ResNet. We also investigate the benefits of PiT compared to ViT and confirm that the pooling layers also improve the performance of ViT. Finally, to analyze the effect of the pooling layers in ViT, we measure the spatial interaction ratio of ViT similar to the receptive field size of a convolutional architecture. We show that the pooling layer Pooling Pooling (c) PiT-S <ref type="figure">Figure 1</ref>. Schematic illustration of dimension configurations of network architectures. We visualize ResNet50 <ref type="bibr" target="#b12">[13]</ref>, Vision Transformer (ViT) <ref type="bibr" target="#b8">[9]</ref>, and our Pooling-based Vision Transformer (PiT); (a) ResNet50 gradually downsamples the features from the input to the output; (b) ViT does not use pooling layers, so the feature dimension is maintained for all the layers; (c) PiT involves pooling layers into ViT.</p><p>has the effect of controlling the size of spatial interaction occurring in the self-attention layer, which is similar to the receptive field control of the convolutional architecture. We verify that PiT improves performances over ViT on various tasks. On ImageNet classification, PiT and outperforms ViT at various scales and training environments. Additionally, we have compared the performance of PiT with various convolutional architectures and have specified the scale at which the transformer architecture outperforms the CNN. We further measure the performance of PiT with a detection head on object detection. ViT-and PiT-based DETR <ref type="bibr" target="#b3">[4]</ref> are trained on the COCO 2017 dataset <ref type="bibr" target="#b24">[25]</ref> and the result shows that PiT is even better than ViT as a backbone architecture for a task other than image classification. Finally, we verify the performance of PiT in various environments through the robustness benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Dimension configuration of CNN</head><p>Convolutional neural networks use various types of pooling layers. We cover the variants in this section. In general, the term pooling is used for the max-pooling and averagepooling layers, but in this paper, the layer that increases the channel while reducing the spatial size is referred to as pooling. Pooling layers are found in AlexNet <ref type="bibr" target="#b20">[21]</ref>, which is early architecture of a convolutional neural network in computer vision. AlexNet uses three max-pooling layers. In max pooling layer, spatial size of the feature is reduced by half, and the channel size is increased by the convolution after the max-pooling. VGGnet <ref type="bibr" target="#b30">[31]</ref> uses 5 spatial resolutions using 5 max-pooling. In the pooling layer, the spatial size is reduced by half and the channel size is doubled. So, even when the spatial resolution was changed, the floating point operations (FLOPs) of convolution layers are maintained. GoogLeNet <ref type="bibr" target="#b31">[32]</ref> also used the pooling layer. In the paper <ref type="bibr" target="#b31">[32]</ref>, authors were concerned about the loss of spatial information due to the pooling layer, but argued that nevertheless the pooling layer is an essential part of a convolutional neural network. ResNet <ref type="bibr" target="#b12">[13]</ref> performed spatial size reduction using the convolution layer of stride 2 instead of max pooling. This is an improved pooling method and channel increase and spatial reduction are performed in a single layer. The convolution layer of stride 2 is also used as a pooling method in recent architectures (EfficietNet <ref type="bibr" target="#b32">[33]</ref>, MobileNet <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref>). The pooling layer used in this paper is also derived from the convolution layer of stride 2. Some papers focused on the channel increase due to the pooling layer. PyramidNet <ref type="bibr" target="#b10">[11]</ref> pointed out that the channel increase occurs only in the pooling layer and proposed a method to gradually increase the channel size in layers other than the pooling layer. ReXNet <ref type="bibr" target="#b11">[12]</ref> reported that the channel configuration of the network has a significant influence on the network performance. In summary, most of convolution networks use a dimension configuration based on a pooling layer. In addition, it is reported that the channel structure is important for performance of network. Our goal is to extend the effective channel configuration of CNN to ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-attention mechanism</head><p>Transformer architecture <ref type="bibr" target="#b34">[35]</ref> significantly increased the performance of the NLP task with self-attention mechanism. Funnel Transformer <ref type="bibr" target="#b6">[7]</ref> improves the transformer architecture with reducing tokens by a pooling layer and skipconnection. However, because of the basic difference between the architecture of NLP and computer vision, the method of applying pooling is different from our method. Some studies are conducted to utilize the transformer architecture to the backbone network for computer vision task. Non-local network <ref type="bibr" target="#b36">[37]</ref> adds a few self-attention layers to CNN backbone, and it shows that self-attention mechanism can be used in CNN. <ref type="bibr" target="#b28">[29]</ref> replaced 3 × 3 convolution of ResNet to local self-attention layer. <ref type="bibr" target="#b35">[36]</ref> used an attention layer for each spatial axis. <ref type="bibr" target="#b1">[2]</ref> enables self-attention of the entire spatial map by reducing the computation of the attention mechanism. Most of these methods replace 3x3 convolution with self-attention or adds a few self-attention layers. Therefore, the basic structure of ResNet is inherited, that is, it has the convolution of stride 2 as ResNet, resulting in a network having a dimension configuration of ResNet.  <ref type="figure">Figure 2</ref>. Effects of the pooling layer in ResNet50 <ref type="bibr" target="#b12">[13]</ref>. We perform an experiment to verify the effect of the pooling layer used in ResNet50. As shown in the figures, the pooling layer improves the model capability, generalization performance, and model performance of ResNet50.</p><p>Only the vision transformer uses a structure that uses the same spatial size in all layers. Although ViT did not follow the conventions of ResNet, it contains many valuable new components in the network architecture. In ViT, layer normalization is applied for each spatial token. Therefore, layer normalization of ViT is closer to positional normalization <ref type="bibr" target="#b21">[22]</ref> than a layer norm of convolutional neural network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>. Although it overlaps with the lambda network <ref type="bibr" target="#b1">[2]</ref>, it is not common to use global attention through all blocks of the network. The use of class tokens instead of global average pooling is also new, and it has been reported that separating token increases the efficiency of distillation <ref type="bibr" target="#b33">[34]</ref>. In addition, the layer configuration, the skipconnection position, and the normalization position of the Transformer are also different from ResNet. Therefore, our research can be seen as giving direction to a newly started architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting spatial pooling</head><p>In order to introduce spatial pooling to ViT, we investigate spatial pooling from various perspectives. First, we verify the benefits of pooling-based dimension configuration in ResNet architecture. Although the pooling layer has been widely used for most convolutional architectures, its effectiveness is rarely verified. Before utilizing the pooling layer to ViT, we measure the benefits of the pooling layer on ResNet. Based on the findings, we propose a Pooling-based Vision Transformer (PiT) that applies the pooling layers to ViT. We propose a pooling layer for transformer architecture and design ViT with pooling (PiT). With PiT models, we verify whether the pooling layer brings advantages to ViT as in ResNet. In addition, we analyze the spatial location used in the attention layer of ViT to investigate the effect of spatial pooling in ViT. Finally, we introduce PiT architectures corresponding to various scales of ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Effect of pooling on CNN</head><p>As shown in <ref type="figure">Figure 1</ref> (a), most convolutional neural networks have pooling layers which reduces the spatial dimension while increases the channel dimension. In ResNet50, a stem layer reduces the spatial size of an image to 56 × 56. Convolution layers with stride 2 reduce the spatial dimension by half and double the channel dimension. The pooling operation using a convolution layer with stride 2 is a frequently used method in recent architectures <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref>. This is referred to as stride convolution or learnable pooling, but in this paper, we refer to it as a pooling layer for simplicity. There are various ways to perform pooling, but it is common for a convolutional neural network to include a pooling layer, which makes a design principle of the convolutional architecture.</p><p>We conduct an experiment to analyze the performance difference according to the presence or absence of a pooling layer in a convolutional architecture. ResNet50, one of the most widely used networks in ImageNet, is used for the architecture and is trained over 100 epochs without complex training techniques. In the case of ResNet without pooling, we change the stem layer to reduce the feature to 14 × 14 and remove pooling layers to maintain feature dimensions like ViT. We measured the performance for several network sizes by changing the base channel size of ResNet.</p><p>First, we measured the relation between FLOPs and training loss with and without the pooling layers. As shown in <ref type="figure">Figure 2</ref> (a), ResNet with pooling layers shows lower training loss over the same computation costs (FLOPs). It implies that the pooling layers increase the capability of architecture. Next, we analyzed the relation between training and validation accuracy, which represents the generalization performance of architecture. As shown in <ref type="figure">Figure 2</ref> (b), ResNet with pooling layer achieves higher validation accuracy than Resnet without pooling layer. Therefore, the pooling layer is also helpful for the generalization performance of ResNet50. In summary, the pooling layers im- <ref type="figure">Figure 3</ref>. Effects of the pooling layer in vision transformer (ViT) <ref type="bibr" target="#b8">[9]</ref>. We compare our Pooling-based Vision Transformer (PiT) with original ViT at various aspects of network architecture. PiT outperforms ViT in capability, generalization performance, and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial tokens</head><p>Class token</p><formula xml:id="formula_0">( ×ℎ)× 1× ×ℎ× Reshape 2 × ℎ 2 ×2 Depth-wise Convolution Reshape</formula><p>Spatial tokens</p><formula xml:id="formula_1">( 2 × ℎ 2 )×2</formula><p>1×2 Fully-connected layer Class token </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pooling-based Vision Transformer (PiT)</head><p>Vision Transformer(ViT) performs network operations based on self-attention, not convolution operations. In the self-attention mechanism, the similarity between all locations is used for spatial interaction. <ref type="figure">Figure 1 (b)</ref> shows the dimension structure of this ViT. Similar to the stem layer of CNN, ViT divides the image by patch at the first embedding layer and embedding it to tokens. Basically, the structure does not include pooling and keeps the same number of spatial tokens overall layer of the network. Although the self-attention operation is not limited by spatial distance, the size of the spatial area participating in attention is affected by the spatial size of feature. Therefore, in order to adjust the size of the area covered by the layer like ResNet, a pooling layer is also required in ViT.</p><p>To utilize the advantages of the pooling layer to ViT, we propose a new architecture called Pooling-based Vision Transformer (PiT). First, we designed the pooling layer for ViT. Our pooling layer is shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Since ViT handles neuron responses in the form of 2D-matrix rather than 3D-tensor, the pooling layer should separate spatial tokens and reshape them into 3D-tensor with spatial structure. After reshaping, spatial size reduction and channel increase are performed by depth-wise convolution. And, the responses are reshaped into 2D-matrix for computation of transformer blocks. In ViT, there are parts that do not correspond to the spatial structure, such as a class token or distillation token <ref type="bibr" target="#b33">[34]</ref>. For these parts, the pooling layer uses an additional fully-connected layer to adjust the channel size to match the spatial tokens. Our design aims to perform the pooling role with minimal operation. As a result, a depthwise convolution layer conducts decreasing the spatial size and increasing the channel size with a small number of parameters. Our pooling layer enables pooling operation on ViT and is used for our PiT architecture as shown in <ref type="figure">Figure 1 (c)</ref>. PiT includes two pooling layers and uses three scales of spatial tokens similar to ResNet.</p><p>Using PiT architecture, we performed an experiment to verify the effect of the pooling layer in ViT. The experiment setting is the same as the ResNet experiment. <ref type="figure">Figure 3</ref> (a) represents the model capability of ViT with and without the pooling layer. At the same computation cost, ViT with pooling has a lower train loss than ViT without pooling. This result is similar to ResNet case. Using the pooling layers in ViT also improves the capability of architecture. The comparison between training accuracy and validation accuracy shows a significant difference. As shown in <ref type="figure">Figure 3 (b)</ref>, ViT without pooling does not improve validation accuracy even if training accuracy increases. On the other hand, in ViT with pooling, validation accuracy increases as training accuracy increases. The big difference in generalization performance causes the performance difference between ViT with pooling and ViT without pooling as shown in <ref type="figure">Figure 3</ref> (c). The phenomenon that ViT does not increase performance even when FLOPs increase in ImageNet is reported in ViT paper <ref type="bibr" target="#b8">[9]</ref>. In the training data of ImageNet scale, ViT shows poor generalization performance, and our proposed pooling layer alleviates this. So, we can say that the pooling layer is necessary for the generalization of ViT. Using the training trick is also a way to improve the generalization performance of ViT in ImageNet. The combination of training trick and pooling layer is covered in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatial interaction</head><p>We investigate the effect of the pooling in ViT by analyzing the self-attention of the vision transformer. The role of pooling in a convolutional architecture is to adjust the receptive field. When the spatial size is large, a portion of a convolutional kernel is relatively small. On the other hand, as the spatial size decreases, the portion of the convolution kernel in the entire image increases. Therefore, an area of the spatial interaction in the layer can be adjusted through pooling layers. In the case of ViT, spatial interaction is performed regardless of spatial distance by the self-attention layer.</p><p>However, the self-attention layer also has limitations in a number of interacting tokens, and therefore, the interaction area is determined according to the spatial size. We measured the spatial interaction area of ViT and PiT with pre-trained models on ImageNet. The criterion for spatial interaction is based on the score after the soft-max of the attention matrix. We used 1% and 10% as thresholds, counted the number of spatial locations where interactions above the threshold occurred, and calculated a spatial interaction ratio by dividing the number of interaction locations by the total size of spatial tokens. Transformer uses multi-head attention, so there are as many attention maps as the number of heads. We measured the ratio of location that was used more than once among several heads. <ref type="figure">Figure 5 (a)</ref> shows the average ratio of locations that involved attention more than 1%.</p><p>In the case of the ViT, the interactions are between 20%-40% on average, and since there is no pooling layer, the numerical value does not change significantly depending on the layer. PiT reduces the number of tokens while increasing the head through pooling. So, as shown in <ref type="figure">Figure 5</ref> (a), the early layers have a small interaction ratio, but the latter layer shows a nearly 100% interaction ratio. For comparison with ResNet, we changed the threshold to 10%, and the result is as shown in <ref type="figure">Figure 5 (b)</ref>. In the case of ResNet, 3x3 convolution means 3x3 spatial interactions. Therefore, we divide 3x3 by the spatial size and use it as an approximation to compare with the interaction ratio of attention. While the interaction ratio of ViT is similar across the layers, the interaction ratio of ResNet and PiT increases as it passes through the pooling layer. In summary, as the receptive field of ResNet increases through the pooling layer, the (a) Spatial interaction (over 1%) (b) Spatial interaction (over 10%) <ref type="figure">Figure 5</ref>. Spatial interaction ratio. We investigate the attention matrix of the self-attention layer and record the ratio of spatial locations that influenced more than 1% or 10%, which is called the interaction ratio. The figures show the average spatial interaction ratio of self-attention or convolution layer. PiT makes a transformer have interaction ratios similar to that of ResNet. spatial interaction area of the transformer becomes wider through the pooling layer. We believe that the control of the interaction area is closely related to the performance of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Architecture design</head><p>The architectures proposed in ViT paper <ref type="bibr" target="#b8">[9]</ref> aimed at datasets larger than ImageNet. These architectures (ViT-Large, ViT-Huge) have an extremely large scale than general ImageNet networks, so it is not easy to compare them with other networks. So, following the previous study <ref type="bibr" target="#b33">[34]</ref> of Vision Transformer on ImageNet, we design the PiT at a scale similar to the small-scale ViT architectures (ViT-Base, ViT-Small, ViT-Tiny). In the DeiT paper <ref type="bibr" target="#b33">[34]</ref>, ViT-Small and ViT-Tiny are named as DeiT-S and DeiT-Ti, but in order to avoid confusion due to the model name change, we use ViT for all models. Corresponding to the three scales of ViT (tiny, small, and base), we design four scales of PiT (tiny, extra small, small, and base). Detail architectures are described in <ref type="table" target="#tab_1">Table 1</ref> the model names: Tiny -Ti, eXtra Small -XS, Small -S, Base -B FLOPs and spatial size were measured based on 224 × 224 image. Since PiT uses a larger spatial size than ViT, we reduce the stride size of the embedding layer to 8, while patch-size is 16 as ViT. A total of two pooling layers are used for PiT, and the channel increase of the pooling layer was implemented by increasing the number of heads of multi-head attention. We design PiT to have the similar depth to ViT, and adjust the channels and the heads to have smaller FLOPs, parameter size, and GPU latency than those of ViT. We want to clarify that PiT is not designed by largescale parameter search such as NAS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b2">3]</ref>, so PiT can be further improved through a network architecture search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We verified the performance of PiT through various experiments. First, we compared PiT at various scales with ViT in various training environments of ImageNet training. And, we extended the ImageNet comparison to architectures other than Transformer. In particular, we focus on the comparison of the performance of ResNet and PiT, and investigate whether PiT can beat ResNet. We also applied PiT to an object detector based on DETR <ref type="bibr" target="#b3">[4]</ref>, and compared the performance as a backbone architecture for object detection. To analyze PiT in various views, we evaluated the performance of PiT on robustness benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet classification</head><p>We compared the performance of PiT models of <ref type="table" target="#tab_1">Table 1</ref> with corresponding ViT models. To clarify the computation time and size of the network, we measured FLOPs, the number of parameters, and GPU throughput (images / sec) of each network. The GPU throughput was measured on NVIDIA V100 single GPU with 128 batch-size. We trained the network using four representative training environments. The first is a vanilla setting that trains the network without complicated training techniques. The vanilla setting is the same setting as the experiments in <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 3</ref>, and it is the setting with the lowest performance due to the lack of techniques to help generalization performance. The second is a training setting using Cut-Mix <ref type="bibr" target="#b40">[41]</ref> data augmentation. CutMix can be used without any other technique, and it significantly improves the performance of the network. The third is the DeiT <ref type="bibr" target="#b33">[34]</ref> setting, which is a compilation of training techniques to train ViT on ImageNet. DeiT setting includes various training techniques and parameter tuning, and we used the same training setting through the open-source code. However, in the case of Repeated Augment <ref type="bibr" target="#b17">[18]</ref>, we confirmed that it had a negative effect in a small model, and it was used only for base models. The last is a DeiT setting with knowledge distillation. The distillation setting is reported as the best performance setting in DeiT <ref type="bibr" target="#b33">[34]</ref> paper. The network uses an additional distillation token and trained with distillation loss <ref type="bibr" target="#b16">[17]</ref> using RegNetY-16GF <ref type="bibr" target="#b27">[28]</ref> as a teacher network. We used AdamP <ref type="bibr" target="#b15">[16]</ref> optimizer for all settings, and the learning rate, weight decay, and warmup were set equal to DeiT <ref type="bibr" target="#b33">[34]</ref> setting. The cosine learning rate decay was used as the learning rate schedule. We train models over 100 epochs for Vanilla and CutMix settings, and 300 epochs for DeiT settings.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Comparing the PiT and ViT of the same name, the PiT has fewer FLOPs and faster speed than ViT. Nevertheless, PiT shows higher performance than ViT. In the case of vanilla and CutMix settings, where a few training techniques are applied, the performance of PiT is superior to the performance of ViT. Even in the case of a DeiT and distill settings, PiT shows comparable or better performance to ViT. Therefore, PiT can be seen as a better architecture than ViT in terms of performance and computation. The generalization performance issue of ViT in <ref type="figure">Figure 3</ref> can also be observed in this experiment. Like ViT-S in the Vanilla setting and ViT-B in the Cut-Mix setting, ViT often shows no increase in performance even when the model size increases. On the other hand, the performance of PiT increases according to the model size in all training settings. it seems that the generalization performance problem of ViT is alleviated by the pooling layers.</p><p>We also compared the performance of PiT with the convolutional architectures. In the previous experiment, we per-Architecture FLOPs # of params Throughput (imgs/sec) Vanilla +CutMix <ref type="bibr" target="#b40">[41]</ref> +DeiT <ref type="bibr" target="#b33">[34]</ref> +Distill <ref type="bibr" target="#b33">[34]</ref> ViT-Ti <ref type="bibr">[</ref>  <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42]</ref> 44.6M 757 81.6% ResNet50D <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref> 25.6M 1176 80.5% EfficientNet-B2 <ref type="bibr" target="#b32">[33]</ref> 9.2M 1333 80.1% EfficientNet-B3 <ref type="bibr" target="#b32">[33]</ref> 12.2M 806 81.6% RegNetY-4GF <ref type="bibr" target="#b27">[28]</ref> 20.6M 1136 79.4% ResNeSt50 <ref type="bibr" target="#b42">[43]</ref> 27.5M 877 81.1% ViT-S <ref type="bibr" target="#b33">[34]</ref> 22.1M 980 81.2% PiT-S 23.5M 1266 81.9%</p><p>ResNet152 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42]</ref> 60.2M 420 81.9% ResNet101D <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref> 44.6M 354 83.0% ResNet152D <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref> 60.2M 251 83.7% EfficientNet-B4 <ref type="bibr" target="#b32">[33]</ref> 19.3M 368 82.9% EfficientNet-B5 <ref type="bibr" target="#b32">[33]</ref> 30.4M 179 83.6% RegNetY-16GF <ref type="bibr" target="#b27">[28]</ref> 83.6M 352 80.4% ResNeSt101 <ref type="bibr" target="#b42">[43]</ref> 48.3M 398 83.0% ResNeSt200 <ref type="bibr" target="#b42">[43]</ref> 70.2M 144 83.9% ViT-B <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> 86.6M 303 83.4% PiT-B 73.8M 348 84.0%  <ref type="table">Table 4</ref>. COCO detection performance based on DETR <ref type="bibr" target="#b3">[4]</ref>. We evaluate the performance of PiT as a pretrained backbone for object detection.</p><p>fore, we performed the comparison based on the best performance reported for each architecture. But, it was limited to the model trained using only ImageNet images. When the paper that proposed the architecture and the paper that reported the best performance was different, we cite both papers. When the architecture is different, the comparison of FLOPs often fails to reflect the actual throughput. Therefore, we re-measured the GPU throughput and number of params on a single V100 GPU, and compared the top-1 accuracy for the performance index. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison result. In the case of the PiT-B scale, the transformer-based architecture (ViT-B, PiT-B) outperforms the convolutional architecture. Even in the PiT-S scale, PiT-S shows superior performance than convolutional architecture (ResNet50) or outperforms in throughput (EfficientNet-b3). However, in the case of PiT-Ti, the performance of convolutional architectures such as ResNet34 <ref type="bibr" target="#b12">[13]</ref>, MobileNetV3 <ref type="bibr" target="#b18">[19]</ref>, and EfficientNet-b0 <ref type="bibr" target="#b32">[33]</ref> outperforms ViT-Ti and PiT-Ti. Overall, the transformer architecture shows better performance than the convolutional architecture at the scale of ResNet50 or higher, but it is weak at a small scale. Creating a light-weight transformer architecture such as MobileNet is one of the future works of ViT research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object detection</head><p>We validate our backbones' performances through object detection on COCO dataset <ref type="bibr" target="#b24">[25]</ref>  is a transformer-based detector. We train each DETR with the different backbones including ResNet50, Vit-S, and our PiT-S. We follow the training setup of the original paper <ref type="bibr" target="#b3">[4]</ref>, but changed the image resolution to 600 × 400 and training epoch 150. To match spatial sizes of backbones' feature, we use dilated convolution <ref type="bibr" target="#b23">[24]</ref> in the pooling layer of PiT and the last block of ResNet. <ref type="table">Table 4</ref> shows the measured AP score on val2017. The detector based on PiT-S outperforms the detector with ViT-S. It shows that the pooling layer of PiT is effective not only for ImageNet classification but also for pretrained backbone for object detection. ViT detector and PiT detector have almost the same throughput. Since PiT-S originally has a higher throughput than ViT-S, it is possible to maintain the same throughput even when the computation of PiT is increased by the dilated convolution at the pooling layer. Although PiT detector cannot beat the performance of the ResNet50 detector, PiT detector has higher throughput, and the training setting was also not tuned for PiT detector. Additional investigation on the training settings for PiT detector would improve the performance of the PiT detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness benchmarks</head><p>In this subsection, we investigate the effectiveness of the proposed architecture in terms of robustness against input changes. We presume that the existing ViT design concept, which keeps the spatial dimension from the input layer to the last layer, has two conceptual limitations: Lack of background robustness and sensitivity to the local discriminative visual features. We, therefore, presume that PiT, our new design choice with the pooling mechanism, performs better than ViT for the background robustness benchmarks and the local discriminative sensitivity benchmarks.</p><p>We employ four different robustness benchmarks. Occlusion benchmark measures the ImageNet validation accuracy where the center 112 × 112 patch of the images is zero-ed out. This benchmark measures whether a model only focuses on a small discriminative visual feature or not. ImageNet-A <ref type="figure">(IN-A)</ref> is a dataset constructed by collecting the failure cases of ResNet50 from the web <ref type="bibr" target="#b14">[15]</ref> where the collected images contain unusual backgrounds or objects with very small size <ref type="bibr" target="#b22">[23]</ref>. From this benchmark, we can infer how a model is less sensitive to unusual backgrounds or object size changes. However, since IN-A is constructed by collecting images (queried by 200 ImageNet subclasses) where ResNet50 predicts a wrong label, this dataset can be biased towards ResNet50 features. We, therefore, employ background challenge (BGC) benchmark <ref type="bibr" target="#b39">[40]</ref> to explore the explicit background robustness. The BGC dataset consists of two parts, foregrounds, and backgrounds. This benchmark measures the model validation accuracy while keeping the foreground but adversarially changing the background from the other image. Since BGC dataset is built upon nine subclasses of ImageNet, the baseline random chance is 11.1%. Lastly, we tested adversarial attack robustness using the fast gradient sign method (FGSM) attack <ref type="bibr" target="#b9">[10]</ref>. <ref type="table" target="#tab_4">Table 5</ref> shows the results. First, we observe that PiT shows better performances than ViT in all robustness benchmarks, despite they show comparable performances in the standard ImageNet benchmark (80.8 vs. 79.8). It supports that our pooling design choice makes the model less sensitive to the backgrounds and the local discriminative features. Also, we found that the performance drops for occluded samples by ResNet50 are much dramatic than PiT; 80.8 → 74.6, 5% drops for PiT, 79.0 → 67.1, 15% drops for ResNet50. This implies that ResNet50 focuses more on the local discriminative areas, by the nature of convolutional operations. Interestingly, in <ref type="table" target="#tab_4">Table 5</ref>, ResNet50 outperforms vision transformer variants in the background challenge dataset (32.7 vs. 21.0). This implies that the self-attention mechanism unintentionally attends more backgrounds comparing to ResNet design choice. Overcoming this potential drawback of vision transformers will be an interesting research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have shown that the design principle widely used in CNNs -the spatial dimensional transformation performed by pooling or convolution with strides, is not considered in transformer-based architectures such as ViT; ultimately affects the model performance. We have first studied with ResNet and found that the transformation in respect of the spatial dimension increases the computational efficiency and the generalization ability. To leverage the benefits in ViT, we propose a PiT that incorporates a pooling layer into Vit, and PiT shows that these advantages can be well harmonized to ViT through extensive experiments. Consequently, while significantly improving the performance of the ViT architecture, we have shown that the pooling layer by considering spatial interaction ratio is essential to a self-attention-based architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Pooling layer of PiT architecture. PiT uses the pooling layer based on depth-wise convolution to achieve channel multiplication and spatial reduction with small parameters. prove the model capability and generalization performance of the architecture and consequently bring a significant improvement in validation accuracy as shown inFigure 2(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>. For convenience, we abbreviate Architecture configuration. The table shows spatial sizes, number of blocks, number of heads, channel size, and FLOPs of ViT and PiT. The structure of PiT is designed to be as similar as possible to ViT and to have less GPU latency.</figDesc><table><row><cell>Network</cell><cell>Spatial size</cell><cell># of blocks</cell><cell># of heads</cell><cell>Channel size</cell><cell>FLOPs</cell></row><row><cell cols="2">ViT-Ti [34] 14 x 14</cell><cell>12</cell><cell>3</cell><cell>192</cell><cell>1.3B</cell></row><row><cell></cell><cell>27 x 27</cell><cell>2</cell><cell>2</cell><cell>64</cell><cell></cell></row><row><cell>PiT-Ti</cell><cell>14 x 14</cell><cell>6</cell><cell>4</cell><cell>128</cell><cell>0.7B</cell></row><row><cell></cell><cell>7 x 7</cell><cell>4</cell><cell>8</cell><cell>256</cell><cell></cell></row><row><cell></cell><cell>27 x 27</cell><cell>2</cell><cell>2</cell><cell>96</cell><cell></cell></row><row><cell>PiT-XS</cell><cell>14 x 14</cell><cell>6</cell><cell>4</cell><cell>192</cell><cell>1.4B</cell></row><row><cell></cell><cell>7 x 7</cell><cell>4</cell><cell>8</cell><cell>384</cell><cell></cell></row><row><cell cols="2">ViT-S [34] 14 x 14</cell><cell>12</cell><cell>6</cell><cell>384</cell><cell>4.6B</cell></row><row><cell></cell><cell>27 x 27</cell><cell>2</cell><cell>3</cell><cell>144</cell><cell></cell></row><row><cell>PiT-S</cell><cell>14 x 14</cell><cell>6</cell><cell>6</cell><cell>288</cell><cell>2.9B</cell></row><row><cell></cell><cell>7 x 7</cell><cell>4</cell><cell>12</cell><cell>576</cell><cell></cell></row><row><cell>ViT-B [9]</cell><cell>14 x 14</cell><cell>12</cell><cell>12</cell><cell>768</cell><cell>17.6B</cell></row><row><cell></cell><cell>31 x 31</cell><cell>3</cell><cell>4</cell><cell>256</cell><cell></cell></row><row><cell>PiT-B</cell><cell>16 x 16</cell><cell>6</cell><cell>8</cell><cell>512</cell><cell>12.5B</cell></row><row><cell></cell><cell>8 x 8</cell><cell>4</cell><cell>16</cell><cell>1024</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>ImageNet performance comparison with ViT. We compare the performances of ViT and PiT with some training techniques on ImageNet dataset. PiT shows better performance with low computation compared to ViT.</figDesc><table><row><cell></cell><cell>34]</cell><cell>1.3 B</cell><cell>5.7 M</cell><cell>2564</cell><cell>68.7%</cell><cell>68.5%</cell><cell>72.2%</cell><cell>74.5%</cell></row><row><cell cols="2">PiT-Ti</cell><cell>0.7 B</cell><cell>4.9 M</cell><cell>3030</cell><cell>71.3%</cell><cell>72.6%</cell><cell>73.0%</cell><cell>74.6%</cell></row><row><cell cols="2">PiT-XS</cell><cell>1.4 B</cell><cell>10.6 M</cell><cell>2128</cell><cell>72.4%</cell><cell>76.8%</cell><cell>78.1%</cell><cell>79.1%</cell></row><row><cell cols="2">ViT-S [34]</cell><cell>4.6 B</cell><cell>22.1 M</cell><cell>980</cell><cell>68.7%</cell><cell>76.5%</cell><cell>79.8%</cell><cell>81.2%</cell></row><row><cell cols="2">PiT-S</cell><cell>2.9 B</cell><cell>23.5 M</cell><cell>1266</cell><cell>73.3%</cell><cell>79.0%</cell><cell>80.9%</cell><cell>81.9%</cell></row><row><cell cols="4">ViT-B [9] 17.6 B 86.6 M</cell><cell>303</cell><cell>69.3%</cell><cell>75.3%</cell><cell>81.8%</cell><cell>83.4%</cell></row><row><cell cols="4">PiT-B 12.5 B 73.8 M</cell><cell>348</cell><cell>76.1%</cell><cell>79.9%</cell><cell>82.0%</cell><cell>84.0%</cell></row><row><cell>Network</cell><cell cols="2"># of params</cell><cell>Throughput (imgs/sec)</cell><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet18 [13, 42]</cell><cell cols="2">11.7M</cell><cell>4545</cell><cell>72.5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV2 [30]</cell><cell></cell><cell>3.5M</cell><cell>3846</cell><cell>72.0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV3 [19]</cell><cell></cell><cell>5.5M</cell><cell>3846</cell><cell>75.2%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B0 [33]</cell><cell></cell><cell>5.3M</cell><cell>2857</cell><cell>77.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-Ti [34]</cell><cell></cell><cell>5.7M</cell><cell>2564</cell><cell>74.5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PiT-Ti</cell><cell></cell><cell>4.9M</cell><cell>3030</cell><cell>74.6%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet34 [13, 38]</cell><cell cols="2">21.8M</cell><cell>2631</cell><cell>75.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet34D [14, 38]</cell><cell cols="2">21.8M</cell><cell>2325</cell><cell>77.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B1 [33]</cell><cell></cell><cell>7.8M</cell><cell>1754</cell><cell>79.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PiT-XS</cell><cell cols="2">10.6M</cell><cell>2128</cell><cell>79.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet50 [13, 42]</cell><cell cols="2">25.6M</cell><cell>1266</cell><cell>80.2%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>ImageNet performance of various models.</figDesc><table><row><cell>We compare</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>ImageNet robustness benchmarks.</figDesc><table><row><cell>in DETR [4] which</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Model capability (b) Generalization performance (c) Model performance</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank NAVER AI Lab members for valuable discussion and advice. NAVER Smart Machine Learning (NSML) <ref type="bibr" target="#b19">[20]</ref> has been used for experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the expressive power of deep learning: A tensor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on learning theory</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="698" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive bias of deep convolutional networks through pooling geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Funnel-transformer: Filtering out sequential redundancy for efficient language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03236</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5927" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00992</idno>
		<title level="m">Rexnet: Diminishing representational bottleneck on convolutional neural network</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07174</idno>
		<title level="m">Natural adversarial examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyuwan</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">NSML: meet the mlaas platform with a real-world case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nako</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<idno>abs/1810.09957</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Positional normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rethinking natural adversarial examples for classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finetuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09994</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
