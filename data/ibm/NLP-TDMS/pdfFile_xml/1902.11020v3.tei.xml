<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DPOD: 6D Pose Object Detector and Refiner</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
							<email>sergey.zakharov@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technical University of Munich</orgName>
								<orgName type="institution" key="instit2">Siemens Corporate Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
							<email>ivan.shugurov@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technical University of Munich</orgName>
								<orgName type="institution" key="instit2">Siemens Corporate Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
							<email>slobodan.ilic@siemens.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technical University of Munich</orgName>
								<orgName type="institution" key="instit2">Siemens Corporate Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DPOD: 6D Pose Object Detector and Refiner</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable. Recent deep learning-based approaches, such as SSD6D [15], YOLO6D [33], AAE [31], PoseCNN [34] and PVNet [25], are the current top performers for this task in RGB images. Even though they all perform evaluation on LineMOD and OCCLUSION datasets, each of them focuses on different aspects of the 6D pose estimation pipeline. The majority is trained on real data [33, 34, 25, 14] while only SSD6D [15] and AAE [31] are trained on syn-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection has always been an important problem in computer vision and a large body of research has been dedicated to it in the past. This problem, like many other vision problems, witnessed a complete renaissance with the advent of deep learning. Detectors like R-CNN <ref type="bibr" target="#b7">[8]</ref>, and its follow-ups Fast-RCNN <ref type="bibr" target="#b6">[7]</ref>, Faster-RCNN <ref type="bibr" target="#b27">[28]</ref>, Mask-RCNN <ref type="bibr" target="#b8">[9]</ref>, then YOLO <ref type="bibr" target="#b26">[27]</ref> and SSD <ref type="bibr" target="#b19">[20]</ref> marked this research field with excellent performance. All these works localize objects of interest in images in terms of tight bounding boxes around them. However, in many applications, e.g., augmented reality, robotics, machine vision, etc., this is not enough and a full 6D pose is necessary. While this problem is easier to solve in depth images, in RGB images it is still quite challenging due to perspective ambiguities * These authors contributed equally to the work. <ref type="figure">Figure 1</ref>: Example output of the DPOD method: Given a single RGB image, we regress its ID mask and its 2D-3D correspondences. PnP+RANSAC is then applied to estimate the final pose. The green bounding box shows the ground truth pose, while the blue one corresponds to the estimated pose. The almost perfect overlap of the bounding boxes indicates that estimations are very accurate. and significant appearance changes of the object when seen from different viewpoints. thetic renderings. Some are presented without refinement, like YOLO6D <ref type="bibr" target="#b32">[33]</ref> and PoseCNN <ref type="bibr" target="#b33">[34]</ref>, while the others perform refinement. The most recent refiners are based on deep learning, e.g., DeepIM <ref type="bibr" target="#b17">[18]</ref> that acts on poses from the PoseCNN detector and the refiner of Manhardt et al. <ref type="bibr" target="#b20">[21]</ref> that uses SSD6D poses.</p><p>Inspired by the methods of Gueler et al. <ref type="bibr" target="#b0">[1]</ref> and Taylor et al. <ref type="bibr" target="#b31">[32]</ref>, which estimate dense correspondences between the human body model and the humans in the image, we propose a novel 3D object detector and pose estimator that also estimates dense 2D-3D correspondences. Unlike DensePose for humans, which requires a sophisticated annotation tool and enormous annotation efforts, our method is annotation-free and only requires creation of arbitrary UV texture maps of the objects, that we do automaticallymainly by spherical projections. The two key elements of our approach are: the pixel-wise prediction of the multiclass object ID masks and classification of correspondence maps that directly provide a relation between image pixels and 3D model vertices. In this way, we end up with a large number of pixel-wise correspondences, which allow for a much better pose estimation than, for example, 9 regressed virtual points of the object's bounding box as in YOLO6D.</p><p>In addition to this, we introduce a deep learning-based pose refinement network that takes initial poses estimated with our DPOD detector and enhances them. The proposed refinement approach builds on the successes of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>, but is shown to be faster, simpler to train, able to be trained both on synthetic and real data, and it outperforms the former solutions in terms of pose quality. We demonstrate that even our poses, which are already of high quality, can be further improved with our refiner.</p><p>We experimented by training our detector with only synthetic and only real images. In both cases, our unified method, named DPOD, composed of the dense pose detector and the refiner outperforms other related works. Dense correspondences not only allow for standard PnP and RANSAC to estimate accurate poses without refinement, but also pave the way for a successful pose refinement. For the models trained on real data, one iteration of refinement is enough to outperform all other reported results, even SSD6D with the depth-based ICP refinement.</p><p>In the remainder of the paper we first review related approaches, then introduce our approach, explaining data preparation, training, architectures and pose refinement. Finally, we present an exhaustive experimental validation and comparison with recent works, where we demonstrate the superiority of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Detecting 3D objects and estimating their 6D pose has been addressed in many works in the past, but the majority of them used depth or RGB-D cameras <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. Depth information disambiguates the object's scale that is the most critical in RGB images due to perspective projection. Therefore, using only RGB images for detection and 6D pose estimation is a quite challenging problem. Recent solutions are mainly based on deep learning and automatically learned features, while older ones use hand-crafted features or image information, e.g., gradients, or image pixel intensities directly.</p><p>Template matching approaches, e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref>, render synthetic image patches from different viewpoints distributed on a sphere around the 3D model of the object and store them as a database of templates. Then the input images are searched using this template database sequentially in a sliding window fashion. Efficient and robust template matching strategies have been presented for color, depth and RGB-D images. The most popular approach is arguably LineMOD <ref type="bibr" target="#b10">[11]</ref>, which also provided a first dataset with labeled poses. This dataset is still used as a benchmark for object detection and pose estimation. Another alternative to template matching approaches is the learning approaches that employ random forests <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Deep Learning 6D Pose Detectors. In the last two years deep learning approaches have shown that impressive results can be obtained for detection and pose estimation in RGB images. Here we review the following ones: SSD6D <ref type="bibr" target="#b14">[15]</ref>, YOLO6D <ref type="bibr" target="#b32">[33]</ref>, BB8 <ref type="bibr" target="#b25">[26]</ref>, iPose <ref type="bibr" target="#b13">[14]</ref>, AAE <ref type="bibr" target="#b30">[31]</ref>, PoseCNN <ref type="bibr" target="#b33">[34]</ref> and PVNet <ref type="bibr" target="#b24">[25]</ref>.</p><p>SSD6D <ref type="bibr" target="#b14">[15]</ref> extended the ideas of the 2D object detector <ref type="bibr" target="#b19">[20]</ref> by 6D pose estimation based on a discrete viewpoint classification rather than direct regression of rotations. The method is rather slow and poses predicted this way are quite inaccurate since they are only a rough discrete approximation of the real poses. The refinement is a must in order to produce presentable results. BB8 <ref type="bibr" target="#b25">[26]</ref> uses a three-stage approach. In the first two stages the coarse-to-fine segmentation is performed, the result of which is then fed to the third network trained to output projections of the object's bounding box points. Knowing 2D-3D correspondences, a 6D pose can be estimated with PnP. The main disadvantage of this pipeline is its multi-stage nature, resulting in very slow run times. Building on YOLO and BB8 ideas, YOLO6D <ref type="bibr" target="#b32">[33]</ref> proposed a novel deep learning architecture capable of efficient and precise object detection and pose estimation without refinement. As is the case with BB8, the key feature here is to perform the regression of reprojected bounding box corners in the image. The advantages of this parametrization are its relative compactness and that it does not introduce a pose ambiguity as opposed to a direct regression of the rotation. Moreover, in contrast to SSD6D, it does not suffer from pose discretization resulting in much more accurate pose estimates without refinement.</p><p>Among the methods that are specifically designed to be robust to occlusions we would like to highlight iPose <ref type="bibr" target="#b13">[14]</ref>, PoseCNN <ref type="bibr" target="#b33">[34]</ref>, and PVNet <ref type="bibr" target="#b24">[25]</ref>. iPose <ref type="bibr" target="#b13">[14]</ref> operates in 3 separate stages: segmentation, 3D coordinate regression and pose estimation. By contrast, our approach unifies the first two stages into the end-to-end network. Moreover, we do not regress 3D coordinates, but rather UV maps that turned out to be a much easier task for the network, resulting in less erroneous correspondences. PoseCNN <ref type="bibr" target="#b33">[34]</ref> also estimates object masks, but then separately estimates the translation of the object's centroid and regresses a quaternion for rotation. PVNet <ref type="bibr" target="#b24">[25]</ref> takes a different approach and designs a network which for every pixel in the image regresses an offset to some predefined keypoints. Instead of bounding box points, they vote for the points located on the object itself. This allows them to handle occlusions very well. AAE (Augmented Autoencoders) <ref type="bibr" target="#b30">[31]</ref> concentrates on pose estimation and training from synthetic models, while using already computed SSD detection bounding boxes as input.</p><p>Deep Learning 6D Pose Refiners. Deep learning-based 6D pose refinement has shown promising results in recent publications <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18]</ref>. Both refiners are conceptually very similar and are designed to output relative transformation between the real input image patch and the patch containing the object rendered with the predicted pose. Main differences are the used backbone architectures and loss functions. Both refinement algorithms rely on external object detection and pose estimation algorithms: for DeepIM <ref type="bibr" target="#b17">[18]</ref> it is PoseCNN, for <ref type="bibr" target="#b21">[22]</ref> it is SSD6D <ref type="bibr" target="#b14">[15]</ref>. The former relies on real data, whereas the latter focuses on training on synthetic images. We propose a network architecture which takes the best of above architectures and is independent of the type of training data used.</p><p>Our work differs from the above approaches by being a complete end-to-end pipeline integrating a detector and pose estimator based on dense correspondences. We demonstrate that we can train either from real or synthetic data and in both cases we outperform all related approaches by a large margin on the LineMOD and OCCLUSION datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section we first discuss the training data preparation steps, followed by the neural network architecture and loss functions used, as well as the pose estimation step from dense correspondences. Finally, we describe our deep learning model-based pose refiner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Preparation</head><p>Most recent RGB-based detectors can be divided in two groups based on the type of data they use for training: synthetic-based and real-based. The first group of methods, e.g., SSD6D <ref type="bibr" target="#b14">[15]</ref> and AAE <ref type="bibr" target="#b30">[31]</ref>, makes use of textured 3D models, usually provided with the public 6D pose de-tection datasets. The objects are rendered from different viewpoints, producing a synthetic training set. The methods of the second group on the other hand, e.g., BB8 <ref type="bibr" target="#b25">[26]</ref>, YOLO6D <ref type="bibr" target="#b32">[33]</ref>, PVNet <ref type="bibr" target="#b24">[25]</ref>, use the training split of the real dataset. They utilize ground truth poses provided with the dataset and compute object masks to crop the objects from real images producing a training set.</p><p>Both types of data generation have their pros and cons. When real images sufficiently covering the object are available, it is more advantageous to use them for training. The reason is that their close resemblance to the actual objects allows for faster convergence and better results. However, training on real images biases the detector to light conditions, poses, scales and occlusions present in the training set, which might lead to problems with generalization in new environments. When, however, no pose annotations are available, which can often be the case since acquiring pose annotations is an expensive process, we are left with 3D models of the objects. With synthetic renderings, one can produce a virtually infinite number of images from different viewpoints. Despite being advantageous in terms of the pose coverage, one has to deal with the domain gap problem severely hindering the performance if no additional data augmentation is applied. Potentially, one can benefit from the advantages of both data types by mixing real and synthetic data in the training set. Therefore, approaches which can be trained on both types of data are desirable. Since our pipeline is not data-specific, we show how to generate the training data for both scenarios.</p><p>Synthetic Training Data Generation. Given 3D models of the objects of interest, the first step is to render them from different poses sufficiently covering the object. The poses are sampled from the half-sphere above the object. Additionally, in-plane rotations of the camera around its viewing direction from -30 to 30 degrees are added. Then, for each of the camera poses, an object is rendered on a black background and both RGB and depth channels are stored.</p><p>Having the renderings at hand, we use a generated depth map as a mask to define a tight bounding box for each generated rendering. Cropping the image with this bounding box position, we store RGB patches, masks separating them from the background, and the camera poses. At this point, we have everything ready for the online augmentation stage, which is described in the later subsection. This step of data preparation is identical for the detector and for the refinement pipelines.</p><p>Real Training Data Generation. In this case, an available dataset with pose annotations is divided into nonoverlapping train and test subsets. Here, we follow the protocol defined by BB8 <ref type="bibr" target="#b25">[26]</ref> and YOLO6D <ref type="bibr" target="#b32">[33]</ref> and use 15% of data for training and the rest 85% for evaluation. Poses are selected such that the relative orientation between them  is larger than a certain threshold. This approach guarantees that selected poses cover the object from all sides. For training the detector, objects are cut out from the original image using the provided mask and then stored as patches for the online augmentation stage. Additional in-plane rotations are added to artificially simulate new poses. For training the refinement, objects are left as they are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Correspondence Mapping</head><p>To be able to learn dense 2D-3D correspondences, each model of the dataset is textured with a correspondence map (see <ref type="figure" target="#fig_1">Figure 3</ref>). A correspondence map is a 2-channel image with values ranging from 0 to 255. Objects are textured using either simple spherical or cylindrical projections. Once textured, we get a bijective mapping between the model's  <ref type="formula" target="#formula_0">(1)</ref>, we apply a 2 channel correspondence texture (2) to it. The resulting correspondence model <ref type="formula" target="#formula_2">(3)</ref> is then used to generate GT maps and estimate poses.</p><p>vertices and pixels on the correspondence map. This provides us with easy-to-read 2D-3D correspondences since given the pixel color, we can instantaneously estimate its position on the model surface by selecting the vertex with the same color value. For convenience, we call the copies of the original models textured with the correspondence map correspondence models. Given the predicted correspondence map, we estimate the object pose with respect to the camera using the pose estimation block, which is described later. Similar to the synthetic or real data generation steps, we render correspondence models under the same poses as for training data and store correspondence patches for each RGB patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Online Data Generation and Augmentation</head><p>Detection and Pose Estimation. The final stage of data preparation is the online data generation pipeline, which is responsible for providing full-sized RGB images ready for training. Generated patches (real or synthetic) are rendered on top of images from MS COCO dataset <ref type="bibr" target="#b18">[19]</ref> producing training images containing multiple objects. It is an important step, which ensures that the detector generalizes to different backgrounds and prevents it from overfitting to backgrounds seen during training. Moreover, it forces the network to learn the model's features needed for pose estimation rather than to learn contextual features which might not be present in images when the scene changes. This step is performed no matter whether the training is being done with synthetic or real patches. We additionally augment the RGB image by random changes in brightness, saturation, and contrast, and by adding Gaussian noise. Moreover, object ID masks and correspondence patches are also rendered on top of the black background in order to generate ground truth correspondence maps. An object ID mask is constructed by assigning a class ID number to each pixel that belongs to the object.</p><p>Pose Refinement. In the case of pose refinement, pairs of images containing the object in the current (searched) pose and in the predicted pose are provided to the network. The final stage of data preparation differs considerably depending on the type of data used. In case of synthetic data, images are generated by in-painting objects on random backgrounds in a current pose. A crucial part of the augmentation is to add random light sources for every image. If real images are used for training, no in-painting is performed. In any case, produced images are further augmented as discussed above. Then a random pose is sampled around the current pose simulating the predicted pose from the detector, which will be used as an original guess of the poses to be refined. It is crucial to choose the proper prior distribution from which distorted poses are sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dense Object Detection Pipeline</head><p>Our inference pipeline is divided into two blocks: the correspondence block and the pose block (see <ref type="figure" target="#fig_0">Figure 2</ref>). In this section, we provide their detailed description.</p><p>Correspondence Block. The correspondence block consists of an encoder-decoder convolutional neural network with three decoder heads which regress the ID mask and dense 2D-3D correspondence map from an RGB image of size 320×240×3. The encoder part is based on a 12-layer ResNet-like <ref type="bibr" target="#b9">[10]</ref> architecture featuring residual layers that allow for faster convergence. The decoders upsample the feature up to its original size using a stack of bilinear interpolations followed by convolutional layers. However, in principle the proposed method is agnostic to a particular choice of encoder-decoder architecture. Any other backbone architectures can be used without any need to change the conceptual principles of the method. For the ID mask head the output is a H×W ×O tensor, where H and W are the height and width of the original input image and O is the number of objects in the dataset plus one additional class for background. Similar to the ID mask head, the two correspondence heads regress tensors with the following dimensions H×W ×C, where C stands for the number of unique colors of the correspondence map, i.e., 256. Each channel of the output tensors stores the probability values for the class corresponding to the channel number. Once tensors are regressed, we store them as single channel images where each pixel stores the class with the maximal estimated probability, forming the ID mask, U and V channels of the correspondence image.</p><p>Formulating color regression problem as discrete color class classification problem proved to be useful for much faster convergence and for the superior quality of 2D-3D matches. Initial experiments on direct coordinate regression showed very poor results in terms of correspondence quality. The main reason for the problem was the infinite continuous solution space, i.e., [−1; 1] 3 , where 3 is the number of dimensions and [−1, 1] is the normalized coordinate range of a 3D model. Classification of the discretized 2D correspondences allowed for a huge boost of the output quality by dramatically decreasing the output space (now 256 2 , where 256 is the size of a single UV map dimension). Moreover, this parametrization also ensures that 3D points of the predicted correspondences always lie on the object surface.</p><p>The network parameters are optimized subject to the composite loss function:</p><formula xml:id="formula_0">L = αL m + βL u + γL v ,<label>(1)</label></formula><p>where L m is the mask loss, and L u and L v are the losses responsible for the quality of the U and V channels of the correspondence image. α, β, and γ are weight factors set to 1 in our case. Both L u and L v losses are defined as multi-class cross-entropy functions, whereas L m uses the weighted version of it.</p><p>Pose Block. The pose block is responsible for the pose prediction. Given the estimated ID mask, we can observe which objects were detected in the image and their 2D locations, whereas the correspondence map maps each 2D point to a coordinate on an actual 3D model. The 6D pose is then estimated using the Perspective-n-Point (PnP) <ref type="bibr" target="#b35">[36]</ref> pose estimation method that estimates the camera pose given correspondences and intrinsic parameters of the camera. Since we get a large set of correspondences for each model, RANSAC is used in conjunction with PnP to make camera pose prediction more robust to possible outliers. For the results presented in the evaluation section, for each pose we run 150 RANSAC iterations with the reprojection error threshold set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deep model-based pose refinement</head><p>The proposed pose refiner is a natural extension of refiners presented in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18]</ref> and relies on the strengths of both approaches. Similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref> we exploit an idea of using a network already pre-trained on ImageNet as a backbone architecture. Analogous to the detector, we used a ResNet-based architecture. Similar to <ref type="bibr" target="#b17">[18]</ref>, our loss function for pose estimation is the ADD measure with a more robust L 1 norm:</p><formula xml:id="formula_1">m = avg x∈Ms (Rx + t) − (Rx +t) 1 ,<label>(2)</label></formula><p>representing the vertex to vertex distance between the object in a ground truth pose and predicted pose. R, t denote the ground truth pose rotation and translation, whereasR andt denote the predicted transformation; M s is a set of points sampled from the CAD model. Points are resampled at every iteration. The number of sampled points was limited to ten thousand in order to ensure the efficiency of training iterations and reasonable memory consumption.</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref> we show a schematic representation of the refiner. In order to be able to benefit from the network weights pretrained on ImageNet, the network has two parallel input branches, each composed of the first five ResNet layers. These layers are initialized from the pre-trained network. One branch receives an input image patch (E 11 ), while the other (E 12 ) one extracts features from the rendering of the object in the predicted pose. Then features f r and f s from these two networks are subtracted and fed into the next ResNet block (E 2 ) producing the feature vector f . If the refinement is trained on synthetic data, it is essential to keep the first five layers unchanged and use them as the feature extractor as was shown in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22]</ref>. Freezing the branch that extracts features from object renderings is unnecessary as it always operates on synthetic data. The network ends with three separate output heads: one for regressing the rotation, one for regressing the translation in X and Y directions, and one for regressing the translation in Z direction. We opted for three separate heads as the scale of their outputs is different. Each head is implemented as two fully connected layers.</p><p>Rotation is always represented in the object coordinate system, which ensures that identically looking objects have the same rotation and that the network does not have to learn a more complicated transformation which arises if the world coordinate system is used. The first layer of the rotation re-  gression head takes the feature vector f produced by ResNet and adds four values, which are the quaternion representing an initial rotation. The second layer takes the output of the previous one, stacks with the initial quaternion and outputs the final rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement heads Feature extraction Input</head><p>The head responsible for the regression of X and Y translations operates in the coordinate system of the image rather than in the full 3D space, which significantly restricts the space of possible solutions. Similar to the rotation head, the XY regression head takes the initial 2D location of the object as input and refines it. Additionally, it takes the refined prediction of Z translation.</p><p>Weights of the fully connected layers are initialized in such a way that for the 0th iteration the network just outputs the input pose, and then during training learns how to refine those values. That significantly increases stability and speed of the training procedure as the network produces meaningful results from the very start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Training Details</head><p>Our pipeline is implemented using the Pytorch deep learning framework. All the experiments were conducted on an Intel Core i7-6900K CPU 3.20GHz with NVIDIA TITAN X (Pascal) GPU. To train our method, we used the ADAM solver with a constant learning rate of 3 × 10 −4 and weight decay of 3 × 10 −5 .</p><p>When training on synthetic data, the problem of domain adaptation becomes one of the main challenges. Training the network without any prior parameter initialization makes it impossible to generalize to the real data. The easy solution to this problem was proposed in several works, including <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>, where they freeze the first layers of the network trained on a large dataset of real images, e.g., Im-ageNet <ref type="bibr" target="#b5">[6]</ref> or MS COCO <ref type="bibr" target="#b18">[19]</ref>, for the object classification task. The common observation that the authors conclude is that these layers, learning low-level features, very quickly overfit to the perfect object renderings. We follow this setup, and freeze the first five layers of our encoder initialized with the weights of the same network pretrained on ImageNet. Last but not least, we found it crucial for the performance of the detector to use various light sources during the rendering of synthetic views to account for changing light conditions and shadows in the real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Evaluation</head><p>In this section we evaluate our algorithm in terms of its pose and detection performance, as well as its runtime, and compare it with the state of the art RGB detector solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Datasets</head><p>All experiments were conducted on LineMOD <ref type="bibr" target="#b11">[12]</ref> and OCCLUSION <ref type="bibr" target="#b1">[2]</ref> datasets, as they are the standard datasets <ref type="table">Table 1</ref>: Pose estimation performance: Comparison of our approach to the other RGB detectors on the LineMOD dataset. The table reports the percentages of correctly estimated poses w.r.t. the ADD score. Among the methods trained on synthetic data, our method shows the best results significantly surpassing the former state-of-the-art. The variant of our method trained on real data again demonstrates outstanding performance outperforming most of the competitors. Moreover, our new refinement pipeline improves the estimated poses even further and shows the best overall results. for evaluation of object detection and pose estimation methods. The LineMOD dataset consists of 13 sequences, each containing ground truth poses for a single object of interest in a cluttered environment. CAD models for all the objects are provided as well. The OCCLUSION dataset is an extension of LineMOD, suitable for testing how well detectors can deal with occlusions. Although it comprises only one sequence, all visible objects from the LineMOD dataset are supplied with their poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Evaluation Metrics</head><p>We evaluate the quality of 6DoF pose estimation following the procedure suggested at SSD6D <ref type="bibr" target="#b14">[15]</ref> also used in other papers. Analogously to other related papers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>, we measure the accuracy of pose estimation using the ADD score <ref type="bibr" target="#b11">[12]</ref>. ADD is defined as an average Euclidean distance between model vertices transformed with the predicted and the ground truth pose. More formally it is defined as follows:</p><formula xml:id="formula_2">m = avg x∈M (Rx + t) − (Rx +t) 2 ,<label>(3)</label></formula><p>where M is a set of vertices of a particular model, R and t are the rotation and translation of a ground truth transformation whereasR andt correspond to those of an estimated transformation. The ADD metric can be extended in order to handle symmetric objects as in <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_3">m = avg x2∈M min x1∈M (Rx 1 + t) − (Rx 2 +t) 2<label>(4)</label></formula><p>Instead of measuring distance from a predicted location of each particular model's vertex to its ground truth location, it suggests to take the closest vertex of the model transformed with the ground truth transformation. Conventionally, a pose is considered correct if ADD is smaller than the 10% of the model's diameter. The accuracy of pose estimation is reported as the percentage of correctly estimated poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Single Object Pose Estimation</head><p>Results of the pose estimation experiments on the LineMOD dataset are reported in <ref type="table">Table 1</ref>. We separately compared our method trained either on real data or on synthetic data. The table provides the comparison of deep learning-based refinement pipelines as well. The left-hand side of the table reports the accuracy of pose estimation as percentages of poses which are correct according to the ADD measure for the training done on synthetic data. If no refinement is used, our approach outperforms all other approaches by a significant margin on the majority of the objects. Moreover, the average percentage of correctly estimated poses (50%) is significantly higher than 28.65% of the second best approach. The accuracy gap is more prominent on small objects such as the ape and duck. The availability of a large number of 2D-3D correspondences ensures that the performance of our method is 5 times better than SSD6D's and almost 2 times better than AAE's. If deep learning-based refinement is used, we significantly outperform <ref type="bibr" target="#b21">[22]</ref> with 66.43% of correct poses against 34.1%.</p><p>If trained on real data, our method is the second best after <ref type="bibr" target="#b24">[25]</ref>. The right-hand side of <ref type="table">Table 1</ref> compares the proposed approach to the previous deep learning-based ones. If no refinement is used, the proposed approach outperforms PoseCNN and YOLO6D by a significant margin, while per- forming on par with PVNet on most of the objects. On average, we are better than PoseCNN by 31%, YOLO6D by 23.57%. Again, our approach uses RGB data exclusively and does not rely on depth data. <ref type="figure" target="#fig_4">Figure 5</ref> provides a visual comparison of ground truth poses versus predicted poses. Poses are visualized as projections of 3D bounding boxes of models in given poses on top of a test image. In comparison to deep learning-based refinement of <ref type="bibr" target="#b17">[18]</ref>, we perform on average better by 6.55% reaching 95.15% of correct poses. When DeepIM was applied to the poses predicted by the proposed approach, ADD improved to 91.8% which is better than the original 88.6% reported in their paper, but still worse than the result of our refiner.</p><p>In conclusion, the proposed detector achieves state-ofthe-art results surpassing other detectors by a large margin on synthetic data and performs either much better or comparable to the other detectors on real data. The proposed  refinement clearly outperforms all the competitors both on real and synthetic data. Pose quality varies from object to object, but in general poses are significantly better for larger objects since there are more 2D-3D correspondences available. On the other hand, simplicity of the proposed approach also makes it quick. On average our detector performs at 33 FPS. The runtime can be adjusted by changing the number of RANSAC iterations, as it is the bottleneck of the pipeline. One iteration of the refinement takes 5ms, excluding the rendering time, which heavily depends on the renderer used. Two refinement iterations suffice for synthetic data, one iteration-for real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Multiple Object Pose Estimation</head><p>Performance evaluation of the proposed detector in cases when the number of objects to detect increases and when severe occlusions are present was conducted on the OC-CLUSION dataset <ref type="bibr" target="#b1">[2]</ref>. Accuracy of object detection on the OCCLUSION dataset is conventionally reported in terms of mean average precision (mAP). The confidence score is computed based on the RANSAC inlier proportion as confidence, rendering the final score of 0.48, which is comparable the best result on this dataset (see <ref type="table" target="#tab_3">Table 3</ref>). <ref type="table" target="#tab_2">Table 2</ref> demonstrates ADD scores for various detectors on the OC-CLUSION dataset. Before the refinement, the proposed detector shows very competitive results in comparison to other detectors. After the refinement, the proposed approach performs substantially better and achieves the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper we proposed the Dense Pose Object Detector (DPOD) method that regresses multi-class object masks and dense 2D-3D correspondences between image pixels and corresponding 3D models. Unlike the best performing methods that regress projections of the object's bounding boxes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> or formulate pose estimation as a discrete pose classification problem <ref type="bibr" target="#b14">[15]</ref>, dense correspondences computed by our method allow for more robust and accurate 6D pose estimation. We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, by a large margin and performs similarly to <ref type="bibr" target="#b24">[25]</ref>. The proposed pose refinement approach also performs very well and allows for achieving a pose accuracy that surpasses all other related deep learningbased pose refinement approaches, while having a simpler and more lightweight backbone architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Implementation Details</head><p>The architecture of our detector is visualized in <ref type="figure">Figure 6</ref>. The refinement network utilizes the same backbone architecture. It is a standard ResNet-like (ResNet18 in PyTorch) model with a reduced number of layers and pooling operations in comparison to the original ResNet first presented in <ref type="bibr" target="#b9">[10]</ref>. Upsampling is implemented as bilinear interpolation rather than deconvolution in order to decrease the number of parameters and the required amount of computations. Each upsampling is followed by the concatenatination of the output feature map with the feature map from the previous level, and one convolutional layer. When the detector is trained on synthetic data, the first five layers are frozen in order to prevent overfitting to peculiarities of the rendered data. The architecture of the refinement network follows the same architectural idea, except for the absence of upsampling and presence of fully-connected layers at the end. Again, the first five layers are used in siamese-like fashion for extracting features from image crops and renderings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. RANSAC Iterations</head><p>The number of RANSAC iterations crucially influences the quality of predicted poses. We ended up using 150 iterations as it yielded the best trade off between quality and runtime. The larger amount of iterations generally did not improve the results significantly, but resulted in longer execution times (see <ref type="table" target="#tab_4">Table 4</ref>). Additionally, the ADD scores after one iteration of the proposed refinement are provided. They show that even 25 iterations of RANSAC are enough to beat the state-of-the-art results if the refinement is used. More iterations of RANSAC do not result in the considerable increase of pose quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Runtime analysis</head><p>In <ref type="table" target="#tab_7">Table 7</ref> we provide the runtimes of the proposed approach for all models of the LineMOD dataset. The total runtime consists of the time needed for PnP and approximately 13 ms for all the auxiliary tasks: the network's forward pass, post-processing of predicted segmentation, and computation of 2D-3D correspondences. <ref type="table" target="#tab_6">Table 6</ref> provides comparison of the runtime of our detector with all the main competitors mentioned in the paper. All the experiments <ref type="table">Table 5</ref>: Comparison of deep learning-based refinement methods: Our refinement approach shows the overall best ADD score with respect to the latest state-of-the art method DeepIM <ref type="bibr" target="#b17">[18]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Refinement</head><p>DeepIM <ref type="bibr" target="#b17">[18]</ref> presents an iterative refinement routine that takes an initial pose estimate from any external detector and iteratively improves it. An additional per-model evaluation is provided (see <ref type="table">Table 5</ref>) to have a fair comparison of DeepIM with our pose refinement. It compares the following ADD scores: 1) ADD reported in the original DeepIM paper <ref type="bibr" target="#b17">[18]</ref>, which used PoseCNN <ref type="bibr" target="#b33">[34]</ref> to predict initial poses, 2) ADD if DeepIM is applied to poses predicted by our detector, 3) ADD if poses predicted by the  <ref type="table" target="#tab_2">Benchvise  40  51  20  Cam  35  49  20  Can  30  44  23  Cat  20  33  30  Driller  26  40  25  Duck  4  16  63  Eggbox  9  23  43  Glue  5  17  59  Holepuncher  20  31  32  Iron  34  48  21  Lamp  40  54  19  Phone  31  45  22</ref> Average 23 <ref type="bibr">36 33</ref> proposed detector are refined with the proposed refinement.</p><p>It is important to mention that two iterations of DeepIM were made, as was suggested in the paper. The proposed refinement was run only for one iteration. The table clearly shows that better initial pose hypotheses allow for better results after refinement. It is also clear that our refinement clearly outperforms DeepIM on most of the objects, while performing only insignificantly worse on others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Correspondence Quality</head><p>In this section, we demonstrate the quality of the output correspondences. Namely, each classified correspondence point is mapped to 3D and compared to the ground truth 3D point. The ground truth 3D points are obtained in exactly the same way as predicted points, i.e., by matching a UV map rendered in the ground truth pose to model's vertices.</p><p>The results per object are shown in <ref type="table">Table 8</ref>. The table reports the quality of correspondences separately for real and synthetic data. For each model, mean absolute error, median absolute error, and standard deviation of absolute errors are reported in millimeters. Relatively large mean error is explained by outliers, some of which can be quite significant. Therefore, median is a better measure due to its robustness to outliers. The table shows that the median error is consistent across all the models. Additionally, it demonstrates that the median error for the detector trained on real data is noticeably lower than for the detector trained on synthetic data. This explains the superior performance of training on real data.   <ref type="table">8</ref>: Quantative correspondence quality: Correspondence quality for real and synthetic data estimated in terms of mean and median absolute errors, and standard deviation. <ref type="figure" target="#fig_5">Figure 7</ref> provides a visual comparison of predicted and ground truth UV maps and heat maps, which demonstrate where imprecisions take place. One can see that most imprecisions are concentrated on the outer boundaries of the object and, for objects with more complex geometry, on the edges of their structural elements, i.e. in places where rapid correspondence value changes occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Multiple Instance Detection</head><p>Our detector also works when multiple instances of the same object are presented, because we parse through the regions of the output mask when the network forward pass is complete. The only limitation comes into play when several objects of the same class overlap and form a single region. In this case, only one pose will be estimated instead of two.</p><p>To overcome this, an additional contour regression head can be added to the correspondence block (see <ref type="figure">Figure 8</ref>). Once regressed, the output contours are simply multiplied with the ID mask forming different regions, which, as a result, allows to distinguish between different regions of the same class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. UVW Mapping</head><p>While being computationally efficient, the UV mapping has a number of drawbacks. In the majority of cases, a simple spherical projection is sufficient to achieve a satisfactory quality of the mapping. However, certain cases can require a different treatment to minimize the stretching effect where one color can cover several model vertices due to discretization. This is especially a big problem for more complicated geometries, which in some cases might require a selection of another projection type or even a manual UV mapping for reaching the best results.</p><p>A straightforward solution to this is the UVW mapping based on normalized 3D coordinates of the model. Instead of 2-channel UV maps, we then have 3-channel UVW maps that are again discretized to the range [0, 255]. The only algorithmic adjustment that has to be done is an additional W-channel classification head. While decreasing the memory efficiency and increasing a computational complexity of the network (due to a higher-dimensional solution space, i.e., 256 3 instead of 256 2 in case of UV mapping), it has an advantage of providing better quality correspondences (especially for objects with complex geometries) and of being fully automatic (see <ref type="figure">Figure 9</ref> for visual comparison).</p><p>Our additional experimental ablations have shown an almost identical performance on the LineMOD and OC-CLUSION datasets, but slightly higher execution times and memory requirements. Nevertheless, despite the increased complexity, we believe that this extension would prove itself useful in many real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Additional Qualitative Results</head><p>In <ref type="figure" target="#fig_1">Figures 10 to 13</ref>, we show additional qualitative pose results on LineMOD and OCCLUSION datasets. Our method demonstrates very high quality poses and is robust to occlusions and illumination changes.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Pipeline description: Given an input RGB image, the correspondence block, featuring an encoder-decoder neural network, regresses the object ID mask and the correspondence map. The latter one provides us with explicit 2D-3D correspondences, whereas the ID mask estimates which correspondences should be taken for each detected object. The respective 6D poses are then efficiently computed by the pose block based on PnP+RANSAC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Correspondence model: Given a 3D model of interest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Refinement architecture: The network predicts a refined pose given an initial pose proposal. Crops of the real image and the rendering are fed into two parallel branches. The difference of the computed feature tensors is used to estimate the refined pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results: Poses predicted with the proposed approach on (a) the LineMOD dataset and (b) the OCCLUSION dataset. Green bounding boxes correspond to ground truth poses, bounding boxes of other colors to predicted poses. For both datasets predicted poses are very close to correct poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative correspondence quality: Comparison of ground truth (left), predicted (center) UV maps and heat maps (right) of absolute errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Contour regression: Additional contour regression head for multiple instance detection. UVW mapping: Visual comparison between UV and UVW mappings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Example results on the LineMOD dataset: ape, can (left), benchvise, cat (middle), cam, driller (right). Green bounding boxes correspond to ground truth poses, blue bounding boxes correspond to predicted poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Example results on the LineMOD dataset: duck, holepuncher (left), eggbox, iron (middle), glue, lamp, phone (right). Green bounding boxes correspond to ground truth poses, blue bounding boxes correspond to predicted poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Example results on the OCCLUSION dataset. Green bounding boxes correspond to ground truth poses, bounding boxes of other colors correspond to predicted poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Example results on the OCCLUSION dataset. Green bounding boxes correspond to ground truth poses, bounding boxes of other colors correspond to predicted poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Pose Block Output Correspondence block Output Input</head><label></label><figDesc></figDesc><table><row><cell>Correspondences</cell><cell></cell><cell>Cat</cell></row><row><cell></cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell></cell><cell>21</cell><cell>22</cell><cell>23</cell></row><row><cell></cell><cell>31</cell><cell>32</cell><cell>33</cell></row><row><cell></cell><cell>PnP + RANSAC</cell><cell cols="2">Eggbox</cell></row><row><cell></cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell></cell><cell>21</cell><cell>22</cell><cell>23</cell></row><row><cell></cell><cell>31</cell><cell>32</cell><cell>33</cell></row><row><cell>RGB</cell><cell></cell><cell cols="2">Camera</cell></row><row><cell></cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell></cell><cell>21</cell><cell>22</cell><cell>23</cell></row><row><cell></cell><cell>31</cell><cell>32</cell><cell>33</cell></row><row><cell>ID Mask</cell><cell>3D Models</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AAE [31] Ours SSD6D [22] Ours YOLO6D [33] PoseCNN [34] PVNet [25] Ours DeepIM [18] Ours</figDesc><table><row><cell>Train data</cell><cell cols="2">Synthetic</cell><cell></cell><cell cols="2">+ Refinement</cell><cell></cell><cell>Real</cell><cell></cell><cell></cell><cell cols="2">+ Refinement</cell></row><row><cell cols="2">Object SSD6D [15] Ape 2.6</cell><cell>3.96</cell><cell>37.22</cell><cell>-</cell><cell>55.23</cell><cell>21.62</cell><cell>-</cell><cell>43.62</cell><cell>53.28</cell><cell>77.0</cell><cell>87.73</cell></row><row><cell>Benchvise</cell><cell>15.1</cell><cell>20.92</cell><cell>66.76</cell><cell>-</cell><cell>72.69</cell><cell>81.80</cell><cell>-</cell><cell>99.90</cell><cell>95.34</cell><cell>97.5</cell><cell>98.45</cell></row><row><cell>Cam</cell><cell>6.1</cell><cell>30.47</cell><cell>24.22</cell><cell>-</cell><cell>34.76</cell><cell>36.57</cell><cell>-</cell><cell>86.86</cell><cell>90.36</cell><cell>93.5</cell><cell>96.07</cell></row><row><cell>Can</cell><cell>27.3</cell><cell>35.87</cell><cell>52.57</cell><cell>-</cell><cell>83.59</cell><cell>68.80</cell><cell>-</cell><cell>95.47</cell><cell>94.10</cell><cell>96.5</cell><cell>99.71</cell></row><row><cell>Cat</cell><cell>9.3</cell><cell>17.90</cell><cell>32.36</cell><cell>-</cell><cell>65.10</cell><cell>41.82</cell><cell>-</cell><cell>79.34</cell><cell>60.38</cell><cell>82.1</cell><cell>94.71</cell></row><row><cell>Driller</cell><cell>12.0</cell><cell>23.99</cell><cell>66.60</cell><cell>-</cell><cell>73.32</cell><cell>63.51</cell><cell>-</cell><cell>96.43</cell><cell>97.72</cell><cell>95.0</cell><cell>98.80</cell></row><row><cell>Duck</cell><cell>1.3</cell><cell>4.86</cell><cell>26.12</cell><cell>-</cell><cell>50.04</cell><cell>27.23</cell><cell>-</cell><cell>52.58</cell><cell>66.01</cell><cell>77.7</cell><cell>86.29</cell></row><row><cell>Eggbox</cell><cell>2.8</cell><cell>81.01</cell><cell>73.35</cell><cell>-</cell><cell>89.05</cell><cell>69.58</cell><cell>-</cell><cell>99.15</cell><cell>99.72</cell><cell>97.1</cell><cell>99.91</cell></row><row><cell>Glue</cell><cell>3.4</cell><cell>45.49</cell><cell>74.96</cell><cell>-</cell><cell>84.37</cell><cell>80.02</cell><cell>-</cell><cell>95.66</cell><cell>93.83</cell><cell>99.4</cell><cell>96.82</cell></row><row><cell>Holepuncher</cell><cell>3.1</cell><cell>17.60</cell><cell>24.50</cell><cell>-</cell><cell>35.35</cell><cell>42.63</cell><cell>-</cell><cell>81.92</cell><cell>65.83</cell><cell>52.8</cell><cell>86.87</cell></row><row><cell>Iron</cell><cell>14.6</cell><cell>32.03</cell><cell>85.02</cell><cell>-</cell><cell>98.78</cell><cell>74.97</cell><cell>-</cell><cell>98.88</cell><cell>99.80</cell><cell>98.3</cell><cell>100</cell></row><row><cell>Lamp</cell><cell>11.4</cell><cell>60.47</cell><cell>57.26</cell><cell>-</cell><cell>74.27</cell><cell>71.11</cell><cell>-</cell><cell>99.33</cell><cell>88.11</cell><cell>97.5</cell><cell>96.84</cell></row><row><cell>Phone</cell><cell>9.7</cell><cell>33.79</cell><cell>29.08</cell><cell>-</cell><cell>46.98</cell><cell>47.74</cell><cell>-</cell><cell>92.41</cell><cell>74.24</cell><cell>87.7</cell><cell>94.69</cell></row><row><cell>Mean</cell><cell>9.1</cell><cell>28.65</cell><cell>50</cell><cell>34.1</cell><cell>66.43</cell><cell>55.95</cell><cell>62.7</cell><cell>86.27</cell><cell>82.98</cell><cell>88.6</cell><cell>95.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Pose estimation for multiple objects: Comparison of our approach on real data to the other RGB detectors on the OCCLUSSION dataset. The table reports percentages of correctly estimated poses w.r.t. the ADD score.</figDesc><table><row><cell>Method</cell><cell>YOLO6D [33]</cell><cell>PoseCNN [34]</cell><cell>SSD6D + Ref [22]</cell><cell>HMap [24]</cell><cell>PVNet [25]</cell><cell>Ours Ours +Ref</cell></row><row><cell>Mean</cell><cell>6.42</cell><cell>24.9</cell><cell>27.5</cell><cell>30.4</cell><cell cols="2">40.77 32.79 47.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Detection performance for multiple objects:Comparison of the state-of-the-art mean average precision (mAP) scores on the OCCLUSION dataset.</figDesc><table><row><cell cols="5">Method SSD6D [15] YOLO6D [33] Brachmann [3] Ours</cell></row><row><cell>mAP</cell><cell>0.38</cell><cell>0.48</cell><cell>0.51</cell><cell>0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>RANSAC iterations test: The effect of the number of RANSAC iterations on the overall ADD score. ref 59.15 76.95 80.15 82.12 82.98 83.44 83.79 84.33 84.66 ADD w/ ref 80.45 92.59 93.88 94.79 95.15 95.31 95.39 95.38 95.39</figDesc><table><row><cell>RANSAC #</cell><cell>5</cell><cell>25</cell><cell cols="7">50 100 150 200 250 350 500</cell></row><row><cell cols="2">ADD w/o RANSAC ms 2</cell><cell>6</cell><cell>10</cell><cell>17</cell><cell>23</cell><cell>28</cell><cell>33</cell><cell>42</cell><cell>54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>/Object Ape Bench. Cam Can Cat Dril. Duck Eggb. Gl. Hol. Iron Lamp Ph. Avg. PoseCNN [34] + DeepIM [18] 77.0 97.5 93.5 96.5 82.1 95.0 77.7 97.1 99.4 52.8 98.3 97.5 87.7 88.6 Ours + DeepIM [18] 78.70 98.43 97.75 97.57 85.16 91.55 80.24 99.68 99.48 75.66 99.74 98.20 91.38 91.81 Ours + Our ref. 87.73 98.45 96.07 99.71 94.71 98.8 86.29 99.91 96.82 86.87 100 96.84 94.69 95.15</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Runtime comparison: Time-efficiency of our approach with respect to the other state-of-the-art approaches.</figDesc><table><row><cell>Method</cell><cell>Frames per second</cell><cell>Refinement</cell></row><row><cell>AAE [31]</cell><cell>4</cell><cell>200 ms/object</cell></row><row><cell>SSD6D [15]</cell><cell>10</cell><cell>24 ms/object</cell></row><row><cell>PVNet [25]</cell><cell>25</cell><cell>-</cell></row><row><cell>Ours</cell><cell>33</cell><cell>5 ms/object</cell></row><row><cell>YOLO6D [33]</cell><cell>50</cell><cell>-</cell></row><row><cell cols="3">were conducted on an Intel Core i7-6900K CPU 3.20GHz</cell></row><row><cell cols="2">with NVIDIA TITAN X (Pascal) GPU.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Runtime analysis: Runtime of the proposed approach for all models of the LineMOD dataset.</figDesc><table><row><cell>Model</cell><cell>PnP + RANSAC (ms)</cell><cell>Total (ms)</cell><cell>FPS</cell></row><row><cell>Ape</cell><cell>7</cell><cell>20</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Img 240x320x3 7x7 conv, 62, /2 Pool, /2 3x3 conv, 64 3x3 conv, 64 3x3 conv, 64 3x3 conv, 128, /2 3x3 conv, 128 3x3 conv, 128 3x3 conv, 128 3x3 conv, 256, /2 3x3 conv, 256 3x3 conv, 256 3x3 conv, 256 Up, 2 stack 3x3 conv, 128 Up, 2 stack 3x3 conv, 64 Up, 2 stack 3x3 conv, 64 Up, 2 3x3 conv, 64 3x3 conv, classes Up, 2 stack 3x3 conv, 128 Up, 2 stack 3x3 conv, 64 Up, 2 stack 3x3 conv, 64 Up, 2 3x3 conv, 64 3x3 conv, classes Up, 2 stack 3x3 conv, 128 Up, 2 stack 3x3 conv, 64 Up, 2 stack 3x3 conv, 64 Up, 2 3x3 conv, 64 3x3 conv, classes 3x3 conv, 64Figure 6: DPOD's network architecture: Encoderdecoder architecture based on ResNet.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Rıza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Real-time rgb-d camera pose estimation in novel scenes using a relocalisation cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rgb-d object pose estimation in unstructured environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="595" to="613" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On pre-trained image features and synthetic images for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01924</idno>
		<title level="m">The best of bothworlds: Learning geometry-based 6d object pose estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A scalable tree-based approach for joint object and pose recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global hypothesis generation for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discriminatively trained templates for 3d object detection: A real time scalable approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reyes</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2048" to="2055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view 6d object pose estimation and camera motion planning using rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juil</forename><surname>Sock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamidreza</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Seabra</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2228" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combined holistic and local patches for recovering 6d object pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixin</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2219" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
