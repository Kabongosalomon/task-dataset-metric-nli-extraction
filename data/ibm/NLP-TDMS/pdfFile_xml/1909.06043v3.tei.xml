<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lvaro</forename><surname>Parra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
							<email>tat-jun.chin@adelaide.edu.aunan.li@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep networks excel in learning patterns from large amounts of data. On the other hand, many geometric vision tasks are specified as optimization problems. To seamlessly combine deep learning and geometric vision, it is vital to perform learning and geometric optimization end-to-end. Towards this aim, we present BPnP, a novel network module that backpropagates gradients through a Perspective-n-Points (PnP) solver to guide parameter updates of a neural network. Based on implicit differentiation, we show that the gradients of a "self-contained" PnP solver can be derived accurately and efficiently, as if the optimizer block were a differentiable function. We validate BPnP by incorporating it in a deep model that can learn camera intrinsics, camera extrinsics (poses) and 3D structure from training datasets. Further, we develop an end-to-end trainable pipeline for object pose estimation, which achieves greater accuracy by combining feature-based heatmap losses with 2D-3D reprojection errors. Since our approach can be extended to other optimization problems, our work helps to pave the way to perform learnable geometric vision in a principled manner. Our PyTorch implementation of BPnP is available on</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of deep learning is due in large part to its ability to learn patterns from vast amounts of training data. Applications that have benefited from this ability include object detection and image segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>. Fundamentally, such problems can often be formulated as classification/regression problems, which facilitates suitable objective functions for backpropagation learning <ref type="bibr" target="#b28">[29]</ref>.</p><p>On the other hand, there are many important computer vision tasks that are traditionally formulated as geometric optimization problems, e.g., camera localization/pose estimation, 3D reconstruction, point set registration. A common property in these optimization problems is the minimization of a residual function (e.g., sum of squared reprojection errors) defined over geometric quantities (e.g., 6DOF camera poses), which are not immediately amenable to backpropagation learning. This limits the potential of geometric vision tasks to leverage large datasets.</p><p>A straightforward solution towards "learnable" geometric vision is to replace the "front end" modules (e.g., image feature detection and matching) using a deep learning alternative <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b44">45]</ref>. However, this does not allow the "back end" steps (e.g., searching for optimal geometric quantities) to influence the training of the neural network parameters.</p><p>On the other extreme, end-to-end methods have been devised <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b8">9]</ref> that bypass geometric optimization, by using fully connected layers to compute the geometric quantity (e.g., 6DOF camera pose) from a feature map derived from previous layers. However, it has been observed that these methods are equivalent to performing image retrieval <ref type="bibr" target="#b38">[39]</ref>, which raises questions on their ability to generalize. Also, such end-to-end methods do not explicitly exploit established methods from geometric vision <ref type="bibr" target="#b17">[18]</ref>, such as solvers for various well-defined tasks.</p><p>To benefit from the strengths of deep learning and geometry, it is vital to combine them in a mutually reinforcing manner. One approach is to incorporate a geometric optimization solver in a deep learning architecture, and allow the geometric solver to participate in guiding the updates of the neural network parameters, thereby realising end-toend learnable geometric vision. The key question is how to compute gradients from a "self-contained" optimizer.</p><p>A recent work towards the above goal is differentiable RANSAC <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, which was targeting at the camera localization task. A perspective-n-point (PnP) module was incorporated in a deep architecture, and the derivatives of the PnP solver are calculated using central differences <ref type="bibr" target="#b37">[38]</ref> to enable parameter updates in the rest of the pipeline. However, such an approach to compute gradients is inexact and time consuming because, in order to obtain each partial derivative, it requires solving PnP at values that lie to the left and right of the input.</p><p>Other approaches to derive gradients from an independent optimization block for backpropagation learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref> conduct implicit differentiation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">Chap. 8]</ref>. Briefly, in the context of end-to-end learning, the gradient of the opti-mization routine with respect to the input variables can be computed via partial derivatives of the stationary constraints of the optimization problem (more details in Sec. 3). The gradient can then be backpropagated to the previous layers for parameter updates. A number of motivating examples and applications were explored in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>. However, largerscale experiments in the context of specific geometric vision problems, and benchmarking against other end-to-end learning alternatives, were unavailable in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>. It is worth noting that implicit differentiation of optimization subroutines has been explored previously in several computer vision applications <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref> (also earlier in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">Chap. 5]</ref>).</p><p>Contributions Our main contribution is a novel network module called BPnP that incorporates a PnP solver. BPnP backpropagates the gradients through the PnP "layer" to guide the updates of the neural network weights, thereby achieving end-to-end learning using an established objective function (sum of squared 2D-3D reprojection errors) and solver from a geometric vision problem. Despite incorporating only a PnP solver, we show how BPnP can be used to learn effective deep feature representations for multiple geometric vision tasks (pose estimation, structurefrom-motion, camera calibration). We also compare our method against state-of-the-art methods for geometric vision tasks. Fundamentally, our method is based on implicit differentiation; thus our work can be seen as an application of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref> to geometric vision learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Backpropagating optimization problems As alluded to above, there are several works that incorporate optimizer blocks in deep neural network architectures, and perform differentiation of the optimization routines for backpropagation learning. A subset of these works address the chellange of incorporating RANSAC in an end-to-end trainable pipeline, such as DSAC <ref type="bibr" target="#b2">[3]</ref>, ESAC <ref type="bibr" target="#b4">[5]</ref>, and NG-DSAC <ref type="bibr" target="#b5">[6]</ref>. In fact, since these works aim to solve camera localization, they also incorporate a PnP solver in their pipeline. To backpropagate through the PnP solver, they use central differences to compute the partial derivatives. In effect, if the input dimension is n, it requires solving PnP 2n times in order to obtain the full Jacobian. Another group of methods applies implicit differentiation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>, which provides an exact and efficient solution for backpropagating through an optimization process. We will describe implicit differentiation in detail later.</p><p>Pose estimation from images A target application of our BPnP is pose estimation. Existing works on end-to-end pose estimation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref> usually employ fully connected layers to compute the target output (pose) using feature maps from previous layers. The output loss function is typically defined using pose metrics (e.g., chordal distance), which are backpropagated using standard differentiation. A recent analysis <ref type="bibr" target="#b38">[39]</ref> suggests that what is being performed by these end-to-end networks is akin to learning a set of base poses from the training images, computing a set of weights for the testing image, then predicting the pose as a weighted combination of the base poses. It was further shown that such methods were more related to image retrieval than intrinsically learning to predict pose, hence they may not outperform an image retrieval baseline <ref type="bibr" target="#b38">[39]</ref>.</p><p>Other pose estimation approaches that combine deep learning with geometric optimization (PnP solver) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b9">10]</ref> adopt a two-stage strategy: first learn to predict the 2D landmarks or fiducial points from the input image, then perform pose estimation by solving PnP on the 2D-3D correspondences. While the first stage can benefit from the regularities existing in a training dataset, the second stage (PnP solving) which encodes the fundamental geometric properties of the problem do not influence the learning in the first stage. Contrast this to our BPnP which seamlessly connects both stages, and allows the PnP optimizer to guide the weight updates in the first stage (in addition to standard keypoint or landmark regression losses).</p><p>Depth estimation and 3D reconstruction There exist many works that employ deep networks to learn to predict depth or 3D structure from input images in an end-to-end fashion. Some of these works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref> can only impose constraints on pairs of images, while others <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b48">49]</ref> learn the structure and the motion in different network branches and do not impose explicit geometric constraints. Also, many of such works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b50">51]</ref> require training datasets with ground truth depth labels, which can be expensive to obtain. The proposed BPnP may help to alleviate this shortcoming; as we will show in Sec. 4.2, a simple structure-from-motion (SfM) framework that utilizes BPnP can jointly optimize using multiple views (not just two), explicitly impose geometric constraints, and learn structure and motion in an unsupervised fashion without depth labels or ground truth 3D structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Backpropagating a PnP solver (BPnP)</head><p>Let g denote a PnP solver in the form of a "function"</p><formula xml:id="formula_0">y = g(x, z, K),<label>(1)</label></formula><p>which returns the 6DOF pose y of a camera with intrinsic matrix K ∈ R 3×3 from n 2D-3D correspondences</p><formula xml:id="formula_1">x = x T 1 x T 2 . . . x T n T ∈ R 2n×1 ,<label>(2)</label></formula><formula xml:id="formula_2">z = z T 1 z T 2 . . . z T n T ∈ R 3n×1 ,<label>(3)</label></formula><p>where (x i , z i ) is the i-th correspondence. Let π(·|y, K) be a projective transformation of 3D points onto the image plane with pose y and camera intrinsics K. Intrinsically, the "evaluation" of g requires solving the optimization problem</p><formula xml:id="formula_3">y = arg min y∈SE(3) n i=1 r i 2 2 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">r i = x i − π i<label>(5)</label></formula><p>is the reprojection error of the i-th correspondence and</p><formula xml:id="formula_5">π i = π(z i |y, K)<label>(6)</label></formula><p>is the projection of 3D point z i on the image plane. We introduce the shorthand</p><formula xml:id="formula_6">π := π T 1 , ..., π T n T ,<label>(7)</label></formula><p>thus <ref type="formula" target="#formula_3">(4)</ref> can be rewritten as</p><formula xml:id="formula_7">y = arg min y∈SE(3) x − π 2 2 .<label>(8)</label></formula><p>The choice of formulation (8) will be justified in Sec. 3.3. Our ultimate goal is to incorporate g in a learnable model, where x, z and K can be the (intermediate) outputs of a deep network. Moreover, the solver for (8) should be used to participate in the learning of the network parameters. To this end, we need to treat g as if it were a differentiable function, such that its "gradients" can be backpropagated to the rest of the network. In this section, we show how this can be achieved via implicit differentiation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Implicit Function Theorem (IFT)</head><formula xml:id="formula_8">Theorem 1 ([25]) Let f : R n+m → R m be a continuously differentiable function with input (a, b) ∈ R n × R m . If a point (a * , b * ) satisfies f (a * , b * ) = 0<label>(9)</label></formula><p>and the Jacobian matrix ∂f ∂b (a * , b * ) is invertible, then there exists an open set U ⊂ R n such that a * ∈ U and an unique continuously differentiable function g(a) : R n → R m such that b * = g(a * ) and f (a , g(a )) = 0 , ∀a ∈ U .</p><p>Moreover, for all a ∈ U , the Jacobian matrix ∂g ∂a (a ) is given by</p><formula xml:id="formula_10">∂g ∂a (a ) = − ∂f ∂b (a , g(a )) −1 ∂f ∂a (a , g(a )) . (11)</formula><p>The IFT allows computing the derivatives of a function g with respect to its input a without an explicit form of the function, but with a function f constraining a and g(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Constructing the constraint function f</head><p>To invoke the IFT for implicit differentiation, we first need to define the constraint function f (a, b) such that Eq. (9) is upheld. For our problem, we use all four variables x, y, z and K to construct f . But we treat f as a two variables function f (a, b), in which a takes values in {x, z, K} -depending on which partial derivative to obtain -and b = y (i.e., the output pose of g).</p><p>To uphold Eq. (9), we exploit the stationary constraint of the optimization process. Denote the objective function of the PnP solver g as</p><formula xml:id="formula_11">o(x, y, z, K) = n i=1 r i 2 2 .<label>(12)</label></formula><p>Since the output pose y of a PnP solver is a local optimum for the objective function, a stationary constraint can be established by taking the first order derivative of the objective with respect to y, i.e.,</p><formula xml:id="formula_12">∂o(x, y, z, K) ∂y y=g(x,z,K) = 0.<label>(13)</label></formula><p>Given an output pose from a PnP solver y = [y 1 , ..., y m ] T , we construct f based on Eq. <ref type="formula" target="#formula_0">(13)</ref>, which can be written as</p><formula xml:id="formula_13">f (x, y, z, K) = [f 1 , ..., f m ] T ,<label>(14)</label></formula><p>where for all j ∈ {1, ..., m},</p><formula xml:id="formula_14">f j = ∂o(x, y, z, K) ∂y j (15) = 2 n i=1 r i , ∂r i ∂y j (16) = n i=1 r i , c ij<label>(17)</label></formula><p>with</p><formula xml:id="formula_15">c ij = −2 ∂π i ∂y j .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Forward and backward pass</head><p>Our PnP formulation (8) for g essentially performs least squares (LS) estimation, which is not robust towards outliers (egregious errors in x, z and K). Alternatively, we could apply a more robust objective, such as incorporating an M-estimator <ref type="bibr" target="#b55">[56]</ref> or maximizing the number of inliers <ref type="bibr" target="#b14">[15]</ref>. However, our results suggest that LS is actually more appropriate, since its sensitivity to errors in the input measurements encourages the learning to quickly converge to parameters that do not yield outliers in x, z and K. In contrast, a robust objective would block the error signals of the outliers, causing the learning process to be unstable.</p><p>Given <ref type="bibr" target="#b7">(8)</ref>, the choice of the solver remains. To conduct implicit differentiation, we need not solve (8) exactly, since <ref type="formula" target="#formula_0">(13)</ref> is simply the stationary condition of (8), which is satisfied by any local minimum. To this end, we apply the Levenberg-Marquardt (LM) algorithm (as implemented in the SOLVEPNP ITERATIVE method in OpenCV <ref type="bibr" target="#b6">[7]</ref>), which guarantees local convergence. As an iterative algorithm, LM requires initialization y (0) in solving <ref type="bibr" target="#b7">(8)</ref>. We make explicit this dependence by rewriting (1) as y = g(x, z, K, y (0) ).</p><p>We obtain the initial pose y (0) with RANSAC if it is not provided.</p><p>In the backward pass, we first construct f as described in Sec. 3.2 to then obtain the Jacobians of g with respect to each of its inputs as</p><formula xml:id="formula_17">∂g ∂x = − ∂f ∂y −1 ∂f ∂x ,<label>(20)</label></formula><formula xml:id="formula_18">∂g ∂z = − ∂f ∂y −1 ∂f ∂z ,<label>(21)</label></formula><formula xml:id="formula_19">∂g ∂K = − ∂f ∂y −1 ∂f ∂K .<label>(22)</label></formula><p>Given the output gradient y, BPnP returns the input gradients</p><formula xml:id="formula_20">x = ∂g ∂x T y,<label>(23)</label></formula><formula xml:id="formula_21">z = ∂g ∂z T y,<label>(24)</label></formula><formula xml:id="formula_22">K = ∂g ∂K T y.<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation notes</head><p>The number of dimensions of y, i.e., m, is dependant on the parameterization of SO(3) within the pose. For example, m = 6 for the axis-angle representation, m = 7 for the quaternion representation, and m = 12 for the rotation matrix representation. Experimentally we found the axis-angle representation leads to the best result, possibly since then m = 6 is equal to the degrees of freedom.</p><p>We compute the partial derivatives in Eqs. <ref type="bibr" target="#b17">(18)</ref>, <ref type="bibr" target="#b19">(20)</ref>, <ref type="bibr" target="#b20">(21)</ref>, and (22) using the Pytorch autograd package <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">End-to-end learning with BPnP</head><p>BPnP enables important geometric vision tasks to be solved using deep networks and PnP optimization in an endto-end manner. Here, we explore BPnP for pose estimation, x ← h(I; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>y ← g(x, z, K, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>← l(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>θ ← θ − α ∂ ∂θ . (Backpropagate through PnP) 8: end while SfM and camera calibration, and report encouraging initial results. These results empirically validate the correctness of the Jacobians ∂g ∂x , ∂g ∂z and ∂g ∂K of the PnP solver g obtained using implicit differentiation in Sec. 3.2.</p><p>This section is intended mainly to be illustrative; in Sec. 5, we will develop a state-of-the-art object pose estimation method based on BPnP and also report more comprehensive experiments and benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pose estimation</head><p>Given a known sparse 3D object structure z and known camera intrinsics K, a function h (a deep network, e.g., CNN, with trainable parameters θ) maps input image I to a set of 2D image coordinates x corresponding to z, before g(x, z, K) is invoked to calculate the object pose y. Our goal is to train h to accomplish this task. Since the main purpose of this section is to validate BPnP, it is sufficient to consider a "fixed input" scenario where there is only one training image I with ground truth pose y * . Algorithm 1 describes the algorithm for this task. The loss function l(·) has the form l(x, y) = π(z|y, K) − π(z|y * , K) 2 2 + λR(x, y), <ref type="bibr" target="#b25">(26)</ref> which is the sum of squared errors between the projection of z using the ground truth pose y * and current pose y from PnP (which in turn depends on x), plus a regularization term</p><formula xml:id="formula_23">R(x, y) = x − π(z|y, K) 2 2 .<label>(27)</label></formula><p>The regularization ensures convergence of the estimated image coordinates x to the desired positions (note that the first error component does not impose constraints on x). A main distinguishing feature of Algorithm 1 is that one of the gradient flow of is calculated w.r.t. y = g(x, z, K) before the gradient of y is computed w.r.t. to x which is then backpropagated to update θ:  • h(I; θ) = θ, i.e., the parameters θ are directly output as the predicted 2D keypoint coordinates x; and</p><p>• h(I; θ) is a modified VGG-11 <ref type="bibr" target="#b42">[43]</ref> network that outputs the 2D keypoints x for I.</p><p>The experiments show that the loss is successfully minimized and the output pose y converges to the target pose y * -this is a clear indication of the validity of <ref type="bibr" target="#b19">(20)</ref>. The experiments also demonstrate the usefulness of the regularization term. While the output pose y will converge to y * with or without R(x, y), the output of h (the predicted keypoints x) can converge away from the desired positions π(z|y * , K) without regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SfM with calibrated cameras</head><p>Let {x (j) } N j=1 indicate a set of 2D image features corresponding to n 3D points z associated/tracked across N frames {I j } N j=1 . Following <ref type="formula" target="#formula_1">(2)</ref>, each x (j) is a vector of 2D coordinates; however, z may not be fully observed in I j , thus x (j) could contain fewer than n 2D coordinates. Let</p><formula xml:id="formula_24">z (j) = S(z|x (j) )<label>(29)</label></formula><p>indicate the selection of the 3D points z that are seen in I j .</p><p>Given {x (j) } N j=1 and the camera intrinsics for each frame (assumed to be constant K without loss of generality), we aim to estimate the 3D structure z ∈ R 3n×1 and camera poses {y (j) } N j=1 corresponding to the N frames. Our end-to-end method estimates the 3D structure  <ref type="figure">Figure 2</ref>. Same experiment as in <ref type="figure" target="#fig_0">Fig. 1</ref> except that h is a modified VGG-11 <ref type="bibr" target="#b42">[43]</ref> network which outputs the 2D keypoints x.</p><formula xml:id="formula_25">z = h(1 ⊗ ; θ)<label>(30)</label></formula><p>Algorithm 2 SfM with calibrated cameras.</p><p>1: y (j) ← Identity pose for j = 1, ..., N . 2: Randomly initialize θ 3: while loss has not converged do <ref type="bibr">4:</ref> z ← h(1 ⊗ ; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>z (j) ← S(z|x (j) ), for j = 1, . . . , N 6:</p><formula xml:id="formula_26">y (j) ← g(x (j) , z (j)</formula><p>, K, y (j) ), for j = 1, . . . , N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>← l({y (j) } N j=1 , z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>θ ← θ − α ∂ ∂θ . (Backpropagate through PnP) 9: end while using a deep network h (a modified VGG-11 <ref type="bibr" target="#b42">[43]</ref>) with the input fixed to a 1-tensor (more on this below); see Algorithm 2. Note that the algorithm makes use of the PnP subroutine to estimate each camera pose given the current z estimate. The loss function l(·) has the form</p><formula xml:id="formula_27">l({y (j) } N j=1 , z) = N j=1 x (j) − π(z (j) |y (j) , K) 2 2 ,<label>(31)</label></formula><p>which is simply the sum of squared reprojection errors across all frames. Again, a unique feature of our pipeline is the backpropagation of the loss through the PnP solver to update network parameters θ.</p><formula xml:id="formula_28">∂ ∂θ = N j=1 ∂l ∂z (j) ∂z (j) ∂θ + ∂l ∂y (j) ∂y (j) ∂z (j) ∂z (j) ∂θ<label>(32)</label></formula><p>The implicit differentiation of ∂y (j) ∂z (j) follows Eq. (21). <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates the results of Algorithm 2 on a synthetic dataset with n = 1000 points on a 3D object seen in N = 12 images (about half of the 3D points are seen in each image). Starting from a random initialization of θ The mesh of the object has n = 1000 points z * , which were projected to N = 12 different views to obtain {x (j) } N j=1 (about half of the 3D points are seen in each view). The function h is a modified VGG-11 network <ref type="bibr" target="#b42">[43]</ref> which outputs the 3D structure z from a fixed input of 1-tensor. We depict the output structure z at various steps. A movie of this reconstruction is provided in the supplementary material.</p><p>(which leads to a poor initial z), the method is able to successfully reduce the loss and recover the 3D structure and camera poses. <ref type="figure">Fig 4 shows</ref> the result from another dataset.</p><p>Effectively, our tests show that a generic deep model (VGG-11 with fixed input (30)) is able to "encode" the 3D structure z of the object in the network weights, even though the network is not designed using principles from multiple view geometry. Again, our aim in this section is mainly illustrative, and Algorithm 2 is not intended to replace established SfM algorithms, e.g., <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref>. However, the results again indicate the correctness of the steps in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Camera calibration</head><p>In the previous examples, the intrinsic matrix K is assumed known and only x and/or z are estimated. Here in our final example, given x and z (2D-3D correspondences), our aim is to estimate K of the form</p><formula xml:id="formula_29">K =   f x 0 c x 0 f y c y 0 0 1   ,<label>(33)</label></formula><p>where f x and f y define the focal length, and c x and c y locate the principal point of the image.</p><p>We assume [f x , f y , c x , c y ] T ∈ [0, 1000] <ref type="bibr" target="#b3">4</ref> . Under our BPnP approach, we train a simple neural network [f x , f y , c x , c y ] T ← h(θ).</p><formula xml:id="formula_30">[f x , f y , c x , c y ] T = h(θ) = 1000 sigmoid(θ)<label>(34)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>K</p><formula xml:id="formula_31">← [f x , 0, 0] T [0, f y , 0] T [c x , c y , 1] T . 6:</formula><p>y ← g(x, z, K, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>← l(K, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>θ ← θ − α ∂ ∂θ . (Backpropagate through PnP) 9: end while to learn the parameters from correspondences x and z, where θ ∈ R 4 . Algorithm 3 summarizes a BPnP approach to learn the parameters θ of h. The loss function is simply the sum of squared reprojection errors l(K, y) = x − π(z|y, K) <ref type="bibr" target="#b1">2</ref> 2 ,</p><p>which is backpropagated through the PnP solver via</p><formula xml:id="formula_33">∂ ∂θ = ∂l ∂K ∂K ∂θ + ∂l ∂y ∂g ∂K ∂K ∂θ .<label>(36)</label></formula><p>The implicit differentiation of ∂g ∂K follows Eq. <ref type="bibr" target="#b21">(22)</ref>. <ref type="figure" target="#fig_6">Fig. 5</ref> illustrates the result of Algorithm 3 using the ground truth correspondences x, z in <ref type="figure" target="#fig_0">Fig. 1</ref> as input. The correct intrinsic parameters are f * x = 800, f * y = 700, c * x = 400, c * y = 300, which the algorithm can clearly achieve as the loss converges to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Object pose estimation with BPnP</head><p>We apply BPnP to build a model that regresses the object pose directly from an input image, not through fully con-  nected layers, but through projective geometric optimization while remaining end-to-end trainable. Our model is unique in that it simultaneously learns from feature-based loss and geometric constraints in a seamless pipeline. The pipeline of the proposed model is depicted in <ref type="figure" target="#fig_7">Fig. 6</ref>. We use HRNet <ref type="bibr" target="#b43">[44]</ref> as the backbone to predict the landmark heatmaps Φ. We then use the Differentiable Spatial to Numerical Transform (DSNT) <ref type="bibr" target="#b32">[33]</ref> to convert heatmaps Φ to 2D landmark coordinates x. Finally, BPnP obtains the pose y from the 2D landmarks x and the 3D structural landmarks of the object z.</p><p>Let Φ * denote the ground truth heatmaps constructed with the ground truth 2D landmarks x * . We define a heatmap loss</p><formula xml:id="formula_34">h = MSE(Φ, Φ * ),<label>(37)</label></formula><p>where MSE(·, ·) is the mean squared error function; a pose loss</p><formula xml:id="formula_35">p = π(z | y, K) − x * 2 F + R(x, y);<label>(38)</label></formula><p>and a mixture loss</p><formula xml:id="formula_36">m = h + β π(z | y, K) − x * 2 F ,<label>(39)</label></formula><p>for training the model respectively. The regularization term R(x, y) is defined in Eq. <ref type="bibr" target="#b26">(27)</ref>. Note that in the mixture loss R(x, y) is unnecessary because the heatmap loss h acts as a regularization term. We set the balancing weight β to 0.0002 in the experiments. We apply our pipeline on the LINEMOD <ref type="bibr" target="#b19">[20]</ref> dataset. For each object we</p><p>• obtain a 3D model representation consisting of 15 landmarks by using the Farthest Point Sampling (FPS) <ref type="bibr" target="#b35">[36]</ref> over the original object mesh,</p><p>• randomly reserve 400 images as the test set and set the remaining (about 800, depending on the object) as the training set, and</p><p>• train a model to predict the 6DOF object pose from the input image.</p><p>We train each model with three different losses ( h , p , and m ), for 120 epochs each. To assist convergence, when training the model with p and m , we first train with h for the first 10 epochs leaving the remaining 110 epochs to train the target loss.</p><p>We evaluate our method with the following two metrics.</p><p>Average 3D distance of model points (ADD) <ref type="bibr" target="#b19">[20]</ref> This is the percentage of accurately predicted poses in the test set. We consider a predicted pose as accurate if the average distance between the 3D model points expressed in the predicted coordinate system and that expressed in the ground truth coordinate system is less than 10% of the model diameter. For symmetric objects we use the ADD-S <ref type="bibr" target="#b52">[53]</ref> metric instead which is based on the closest point distance.</p><p>2D projection <ref type="bibr" target="#b3">[4]</ref>. Mean distance between 2D keypoints projected with the estimated pose and those projected with ground truth pose. An estimated pose is considered correct if this distance is less than a threshold ψ. <ref type="table">Table 1</ref> summarizes the results of our experiments. In terms of the ADD(-S) metric, the model trained with h performs considerably better than the one with p . As expected, heatmaps can exploit richer spatial features than coordinates. However, the mixture loss achieves the highest accuracy, which suggests that heatmap loss benefits from additional correction signals from the pose loss.</p><p>In terms of the 2D projection metric, all methods perform similarly, with an average accuracy of at least 99%. To better distinguish the performances amongst different loss functions, we tighten the positive threshold ψ from the standard 5 pixels to 2 pixels. Consistent with the ADD(-S) result, the mixture loss outperformed pure heatmap loss on training an object pose estimation model. Visualization of a random subset from the test results is shown in <ref type="figure" target="#fig_8">Fig. 7</ref>.</p><p>We provide the result of the current state-of-the-art PVNet <ref type="bibr" target="#b35">[36]</ref>   <ref type="table">Table 1</ref>. Test accuracy on the LINEMOD dataset in terms of the ADD(-S) metric (columns 2-5) and the 2D projection metric with ψ = 5 pixels (columns 6-9) and ψ = 2 pixels (columns 10-12). Objects eggbox and glue are considered as symmetric objects and the ADD-S metric is used. we require the ground truth pose label for training, we did not use any data augmentation such as cropping, rotation or affine transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We present BPnP, a novel approach for performing backpropagation through a PnP solver. BPnP leverages on implicit differentiation to address computing the non-explicit gradient of this geometric optimization process. We validate our approach in three fundamental geometric optimization problems (pose estimation, structure from motion, and camera calibration). Furthermore, we developed an endto-end trainable object pose estimation pipeline with BPnP, which outperforms the current state-of-the-art. Our experiments show that exploiting 2D-3D geometry constraints improves the performance of a feature-based training scheme.</p><p>The proposed BPnP opens a door to vast possibilities for designing new models. We believe the ability to incorporate geometric optimization in end-to-end pipelines will further boost the learning power and promote innovations in various computer vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Pose estimation. 1: y ← Identity pose. 2: Randomly initialize θ 3: while loss has not converged do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The implicit differentiation of ∂g ∂x follows Eq.<ref type="bibr" target="#b19">(20)</ref>. Figs. 1 and 2 illustrate Algorithm 1 on a synthetic example with n = 8 landmarks, respectively for the cases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Sample run of Algorithm 1 on synthetic data with n = 8 landmarks and h(I; θ) = θ, where θ ∈ R 8×2 . The first and second row has λ = 1 and λ = 0 respectively. Left column: loss curve. Middle column: evolution of y presented as π(z|y, K). Right column: the evolution of predicted keypoints x. Red square markers represent the target locations π(z|y * , K).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>SfM result with Algorithm 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Algorithm 3</head><label>43</label><figDesc>SfM result with a different object which has n = 1000 points z * . All settings are the same as inFig. 3. A movie of this reconstruction is provided in the supplementary material. Camera calibration.1: y ← Identity pose.2:  Randomly initialize θ 3: while loss has not converged do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Camera calibration using Algorithm 3, based on the ground truth correspondences x, z in Fig. 1. Left: loss curve. Right: the intrinsic parameters which converge to the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The pipeline of our object pose estimation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Random sample of test results of the proposed model trained with the mixture loss m. The first row are the test images for predicting the pose of the central object. The second row shows the regressed alignment for each query object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ape 74.00 56.75 74.75 43.62 99.50 99.50 99.50 99.23 90.00 86.75 93.75 benchwise 98.50 98.00 99.00 99.90 99.25 98.75 99.25 99.81 86.00 82.75 86.00 cam 96.25 83.75 96.25 86.86 98.75 98.50 99.25 99.21 91.50 81.50 90.50 can 97.00 94.75 98.00 95.47 99.50 99.50 99.75 99.90 93.25 89.00 92.75 cat 93.00 85.25 94.25 79.34 99.50 99.50 99.50 99.30 96.75 95.25 96.75 driller 98.50 98.00 99.25 96.43 99.00 98.50 98.50 96.92 83.75 81.50 84.50 duck 76.25 49.25 78.50 52.58 99.00 99.25 99.00 98.02 88.50 84.00 91.50 eggbox 95.75 93.25 96.50 99.15 99.50 99.25 99.50 99.34 92.75 93.25 92.50 glue 87.50 76.25 90.00 95.66 99.50 99.50 99.50 98.45 93.50 90.00 94.00 holepuncher 89.50 80.25 91.50 81.92 99.75 99.50 99.75 100.00 92.25 90.25 91.75 iron 97.75 96.50 97.75 98.88 98.50 98.25 98.50 99.18 86.75 79.75 87.25 lamp 99.75 98.50 99.75 99.33 98.75 98.00 98.50 98.27 85.00 83.25 86.75 phone 95.75 96.00 97.00 92.41 99.25 99.00 99.25 99.42 91.50 88.00 92.25 average 92.27 85.12 93.27 86.27 99.21 99.00 99.21 99.00 90.12 86.56 90.79</figDesc><table><row><cell></cell><cell cols="2">ADD(-S)</cell><cell cols="3">2D projection with ψ = 5</cell><cell cols="3">2D projection with ψ = 2</cell></row><row><cell>Model</cell><cell>Ours</cell><cell>PVNet</cell><cell></cell><cell>Ours</cell><cell>PVNet</cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>h</cell><cell>p</cell><cell>m</cell><cell>h</cell><cell>p</cell><cell>m</cell><cell>h</cell><cell>p</cell><cell>m</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">as a reference. Overall, models trained with</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">h and m have higher average test accuracy than PVNet, in terms of both the ADD(-S) metric and the 2D projection metric. We remind the reader to be aware of several factors while comparing the performances: we use a different backbone from PVNet; our train-test numbers of images are about 800-400 while about 20200-1000 in PVNet. Because</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by ARC LP160100495 and the Australian Institute for Machine Learning. Nan Li is sponsored by NSF China (11601378) and Tencent-SZU Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optnet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Calculus. Cambridge University Press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Binmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dsac-differentiable ransac for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Expert sample consensus applied to camera re-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural-guided ransac: Learning where to sample model hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometry-aware learning of maps for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hybrid probabilistic model for camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Satellite pose estimation with deep landmark regression and nonlinear pose refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to solve nonlinear least squares for monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient computation of robust low-rank matrix approximations in the presence of missing data using the l1 norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Three-dimensional computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On differentiating parameterized argmin and argmax problems with application to bi-level optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">gvnn: Neural network library for geometric computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blösch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girschick</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modelling uncertainty in deep learning for camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">R</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parks</surname></persName>
		</author>
		<title level="m">The Implicit Function Theorem, History, Theory, and Applications</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhäuser</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep regression for monocular camera-based 6-dof global localization in outdoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tayyab</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Konstantinos G Derpanis, and Kostas Daniilidis. 6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An introduction to the calculus of finite differences. Van Nostrand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clarence</forename><surname>Hudson Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding the limitations of cnn-based absolute camera pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structure-from-Motion Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixelwise View Selection for Unstructured Multi-View Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning Gaussian conditional random fields for low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fully-trainable deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Imagebased localization using lstms for structured feature correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Walch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hilsenbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional neural networks for camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Parameter estimation techniques: a tutorial with application to conic fitting. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="59" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
