<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
							<email>muhammed.kocabas@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
							<email>eakbas@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estimation methods have been proposed due to lack of 3D data. Nevertheless, these methods, in addition to 2D groundtruth poses, require either additional supervision in various forms (e.g. unpaired 3D ground truth data, a small subset of labels) or the camera parameters in multiview settings. To address these problems, we present EpipolarPose, a self-supervised learning method for 3D human pose estimation, which does not need any 3D ground-truth data or camera extrinsics. During training, EpipolarPose estimates 2D poses from multi-view images, and then, utilizes epipolar geometry to obtain a 3D pose and camera geometry which are subsequently used to train a 3D pose estimator. We demonstrate the effectiveness of our approach on standard benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) where we set the new state-of-the-art among weakly/self-supervised methods. Furthermore, we propose a new performance measure Pose Structure Score (PSS) which is a scale invariant, structure aware measure to evaluate the structural plausibility of a pose with respect to its ground truth. Code and pretrained models are available at https://github.com/mkocabas/ EpipolarPose</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation in the wild is a challenging problem in computer vision. Although there are large-scale datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> for two-dimensional (2D) pose estimation, 3D datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> are either limited to laboratory settings or limited in size and diversity. Since collecting 3D human pose annotations in the wild is costly and 3D datasets are limited, researchers have resorted to weakly or self supervised approaches with the goal of obtaining an accurate 3D pose estimator by using minimal amount of additional * equal contribution supervision on top of the existing 2D pose datasets. Various methods have been developed to this end. These methods, in addition to ground-truth 2D poses, require either additional supervision in various forms (such as unpaired 3D ground truth data <ref type="bibr" target="#b40">[41]</ref>, a small subset of labels <ref type="bibr" target="#b30">[31]</ref>) or (extrinsic) camera parameters in multiview settings <ref type="bibr" target="#b29">[30]</ref>.</p><p>To the best of our knowledge, there is only one method <ref type="bibr" target="#b8">[9]</ref> which can produce a 3D pose estimator by using only 2D ground-truth. In this paper, we propose another such method. Our method, "EpiloparPose," uses 2D pose estimation and epipolar geometry to obtain 3D poses, which are subsequently used to train a 3D pose estimator. EpipolarPose works with an arbitrary number of cameras (must be at least 2) and it does not need any 3D supervision or the extrinsic camera parameters, however, it can utilize them if provided. On the Human3.6M <ref type="bibr" target="#b14">[15]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b22">[23]</ref> datasets, we set the new state-of-the-art in 3D pose estimation for weakly/self-supervised methods.</p><p>Human pose estimation allows for subsequent higher level reasoning, e.g. in autonomous systems (cars, industrial robots) and activity recognition. In such tasks, structural errors in pose might be more important than the localization error measured by the traditional evaluation metrics such as MPJPE (mean per joint position error) and PCK (percentage of correct keypoints). These metrics treat each joint independently, hence, fail to asses the whole pose as a structure. <ref type="figure" target="#fig_2">Figure 4</ref> shows that structurally very different poses yield the same MPJPE with respect to a reference pose. To address this issue, we propose a new performance measure, called the Pose Structure Score (PSS), which is sensitive to structural errors in pose. PSS computes a scale invariant performance score with the capability to score the structural plausibility of a pose with respect to its ground truth. Note that PSS is not a loss function, it is a performance measure that can be used along with MPJPE and PCK to account for structural errors made by a pose estimator.</p><p>To compute PSS, we first need to model the natural distribution of ground-truth poses. To this end, we use an unsupervised clustering method. Let p be the predicted pose for an image whose ground-truth is q. First, we find which cluster centers are closest to p and q. If both of them are closest to (i.e. assigned to) the same cluster center, then the pose structure score (PSS) of p is said to be 1, otherwise 0.</p><p>Contributions Our contributions are as follows:</p><p>• We present EpipolarPose, a method that can predict 3D human pose from a single-image. For training, Epipo-larPose does not require any 3D supervision nor camera extrinsics. It creates its own 3D supervision by utilizing epipolar geometry and 2D ground-truth poses.</p><p>• We set the new state-of-the-art among weakly/selfsupervised methods for 3D human pose estimation.</p><p>• We present Pose Structure Score (PSS), a new performance measure for 3D human pose estimation to better capture structural errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our method, EpipolarPose, is a single-view method during inference; and a multi-view, self-supervised method during training. Before discussing such methods in the literature, we first briefly review entirely single-view (during both training and inference) and entirely multi-view methods for completeness.</p><p>Single-view methods In many recent works, convolutional neural networks (CNN) are used to estimate the coordinates of the 3D joints directly from images <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23]</ref>. Li and Chan <ref type="bibr" target="#b18">[19]</ref> were the first to show that deep neural networks can achieve a reasonable accuracy in 3D human pose estimation from a single image. They used two deep regression networks and body part detection. Tekin et al. <ref type="bibr" target="#b37">[38]</ref> show that combining traditional CNNs for supervised learning with auto-encoders for structure learning can yield good results. Contrary to common regression practice, Pavlakos et al. <ref type="bibr" target="#b28">[29]</ref> were the first to consider 3D human pose estimation as a 3D keypoint localization problem in a voxel space. Recently, "integral pose regression" proposed by Sun et al. <ref type="bibr" target="#b35">[36]</ref> combined volumetric heat maps with a soft-argmax activation and obtained state-of-the-art results.</p><p>Additionally, there are two-stage approaches which decompose the 3D pose inference task into two independent stages: estimating 2D poses, and lifting them into 3D space <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b22">23]</ref>. Most recent methods in this category use state-of-the-art 2D pose estimators <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17</ref>] to obtain joint locations in the image plane. Martinez et al. <ref type="bibr" target="#b21">[22]</ref> use a simple deep neural network that can estimate 3D pose given the estimated 2D pose computed by a state-of-the-art 2D pose estimator. Pavlakos et al. <ref type="bibr" target="#b27">[28]</ref> proposed the idea of using ordinal depth relations among joints to bypass the need for full 3D supervision.</p><p>Methods in this category require either full 3D supervision or extra supervision (e.g. ordinal depth) in addition to full 3D supervision.</p><p>Multi-view methods Methods in this category require multi-view input both during testing and training. Early work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> used 2D pose estimations obtained from calibrated cameras to produce 3D pose by triangulation or pictorial structures model. More recently, many researchers <ref type="bibr" target="#b9">[10]</ref> used deep neural networks to model multi-view input with full 3D supervision.</p><p>Weakly/self-supervised methods Weak and self supervision based methods for human pose estimation have been explored by many <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref> due to lack of 3D annotations. Pavlakos et al. <ref type="bibr" target="#b29">[30]</ref> use a pictorial structures model to obtain a global pose configuration from the keypoint heatmaps of multi-view images. Nevertheless, their method needs full camera calibration and a keypoint detector producing 2D heatmaps.</p><p>Rhodin et al. <ref type="bibr" target="#b30">[31]</ref> utilize multi-view consistency constraints to supervise a network. They need a small amount of 3D ground-truth data to avoid degenerate solutions where poses collapse to a single location. Thus, lack of in-the-wild 3D ground-truth data is a limiting factor for this method <ref type="bibr" target="#b30">[31]</ref>.</p><p>Recently introduced deep inverse graphics networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44]</ref> have been applied to the human pose estimation problem <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref>. Tung et al. <ref type="bibr" target="#b40">[41]</ref> train a generative adversarial network which has a 3D pose generator trained with a reconstruction loss between projections of predicted 3D poses and input 2D joints and a discriminator trained to distinguish predicted 3D pose from a set of ground truth 3D poses. Following this work, Drover et al. <ref type="bibr" target="#b8">[9]</ref> eliminated the need for 3D ground-truth by modifying the discriminator to recognize plausible 2D projections.</p><p>To the best of our knowledge, EpipolarPose and Drover et al. 's method are the only ones that do not require any 3D supervision or camera extrinsics. While their method does not utilize image features, EpipolarPose makes use of both image features and epipolar geometry and produces much more accurate results (4.3 mm less error than Drover et al. 's method).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Models and Methods</head><p>The overall training pipeline of our proposed method, EpipolarPose, is given in <ref type="figure" target="#fig_1">Figure 2</ref>. The orange-background part shows the inference pipeline. For training of Epipolar-Pose, the setup is assumed to be as follows. There are n cameras (n ≥ 2 must hold) which simultaneously take the picture of the person in the scene. The cameras are given id numbers from 1 to n where consecutive cameras are close to each other (i.e. they have small baseline). The cameras produce images I 1 , I 2 , . . . I n . Then, the set of consecutive image pairs, {(I i , I i+1 )|i = 1, 2, . . . , n−1}, form the training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training</head><p>In the training pipeline of EpipolarPose ( <ref type="figure" target="#fig_1">Figure 2</ref>), there are two branches each starting with the same pose estimation network (a ResNet followed by a deconvolution network <ref type="bibr" target="#b35">[36]</ref>). These networks were pre-trained on the MPII Human Pose dataset (MPII) <ref type="bibr" target="#b1">[2]</ref>. During training, only the pose estimation network in the upper branch is trained; the other one is kept frozen.</p><p>EpipolarPose can be trained using more than 2 cameras but for the sake of simplicity, here we will describe the training pipeline for n = 2. For n = 2, each training example contains only one image pair. Images I i and I i+1 are fed into both the 3D (upper) branch and 2D (lower) branch pose estimation networks to obtain volumetric heatmapŝ H, H ∈ R w×h×d respectively, where w, h are the spatial size after deconvolution, d is the depth resolution defined as a hyperparameter. After applying soft argmax activation function ϕ(·) we get 3D poseV ∈ R J×3 and 2D pose U ∈ R J×2 outputs where J is the number of body joints. From a given volumetric heatmap, one can obtain both a 3D pose (by applying softargmax to all 3 dimensions) and a 2D pose (by applying softargmax to only x, y).</p><p>As an output of 2D pose branch, we want to obtain the 3D human pose V in the global coordinate frame. Let the 2D coordinate of the j th joint in the i th image be</p><formula xml:id="formula_0">U i,j = [x i,j , y i,j ] and its 3D coordinate be [X j , Y j , Z j ],</formula><p>we can describe the relation between them assuming a pinhole image projection model</p><formula xml:id="formula_1">  xi,j yi,j wi,j   = K [R|RT ]     Xj Yj Zj 1     , K =   fx 0 cx 0 fy cy 0 0 1   , T =   Tx Ty Tz   ,<label>(1)</label></formula><p>where w i,j is the depth of the j th joint in the i th camera's image with respect to the camera reference frame, K encodes the camera intrinsic parameters (e.g., focal length f x and f y , principal point c x and x y ), R and T are camera extrinsic parameters of rotation and translation, respectively. We omit camera distortion for simplicity. When camera extrinsic parameters are not available, which is usually the case in dynamic capture environments, we can use body joints as calibration targets. We assume the first camera as the center of the coordinate system, which means R of the first camera is identity. For corresponding joints in U i and U i+1 , in the image plane, we find the fundamental matrix F satisfying U i,j F U i+1,j = 0 for ∀j using the RANSAC algorithm. From F , we calculate the essential matrix E by E = K T F K. By decomposing E with SVD, we obtain 4 possible solutions to R. We decide on the correct one by verifying possible pose hypotheses by doing cheirality check. The cheirality check basically means that the triangulated 3D points should have positive depth <ref type="bibr" target="#b25">[26]</ref>. We omit the scale during training, since our model uses nor-malized poses as ground truth.</p><p>Finally, to obtain a 3D pose V for corresponding synchronized 2D images, we utilize triangulation (i.e. epipolar geometry) as follows. For all joints in (I i , I i+1 ) that are not occluded in either image, triangulate a 3D point [X j , Y j , Z j ] using polynomial triangulation <ref type="bibr" target="#b11">[12]</ref>. For settings including more than 2 cameras, we calculate the vector-median to find the median 3D position.</p><p>To calculate the loss between 3D pose in camera framê V predicted by the upper (3D) branch, we project V onto corresponding camera space, then minimize smooth</p><formula xml:id="formula_2">L1 (V − V ) to train the 3D branch where smooth L1 (x) = 0.5x 2 if |x| &lt; 1 |x| − 0.5 otherwise (2)</formula><p>Why do we need a frozen 2D pose estimator? In the training pipeline of EpipolarPose, there are two branches each of which is starting with a pose estimator. While the estimator in the upper branch is trainable, the other one in the lower branch is frozen. The job of the lower branch estimator is to produce 2D poses. One might question the necessity of the frozen estimator since we could obtain 2D poses from the trainable upper branch as well. When we tried to do so, our method produced degenerate solutions where all keypoints collapse to a single location. In fact, other multi-view methods faced the same problem <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>. Rhodin et al. <ref type="bibr" target="#b30">[31]</ref> solved this problem by using a small set of ground-truth examples, however, obtaining such groundtruth may not be feasible in most of the in the wild settings.</p><p>Another solution proposed recently <ref type="bibr" target="#b36">[37]</ref> is to minimize angular distance between estimated relative rotationR (computed via Procrustes alignment of the two sets of keypoints) and the ground truth R. Nevertheless, it is hard to obtain ground truth R in dynamic capture setups. To overcome these shortcomings, we utilize a frozen 2D pose detector during training time only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference</head><p>Inference involves the orange-background part in <ref type="figure" target="#fig_1">Figure  2</ref>. The input is just a single image and the output is the estimated 3D poseV obtained by a soft-argmax activation, ϕ(·), on 3D volumetric heatmapĤ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Refinement, an optional post-training</head><p>In the literature there are several techniques <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39]</ref> to lift detected 2D keypoints into 3D joints. These methods are capable of learning generalized 2D→3D mapping which can be obtained from motion capture (MoCap) data by simulating random camera projections. Integrating a refinement unit (RU) to our self supervised model can further improve the pose estimation accuracy. In this way, one can train EpipolarPose on his/her own data which consists of Refinement unit f ConvNet <ref type="figure">Figure 3</ref>. Overall inference pipeline with a refinement unit which is an optional stage to refine the predictions of the model trained with self supervision. The f function denotes the inference function (orange-background part in <ref type="figure" target="#fig_1">Figure 2</ref>) of EpipolarPose.  multiple view footages without any labels and integrate it with RU to further improve the results. To make this possible, we modify the input layer of RU to accept noisy 3D detections from EpipolarPose and make it learn a refinement strategy. (See <ref type="figure">Figure 3</ref>)</p><p>The overall RU architecture is inspired by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11]</ref>. It has 2 computation blocks which have certain linear layers followed by Batch Normalization <ref type="bibr" target="#b13">[14]</ref>, Leaky ReLU <ref type="bibr" target="#b20">[21]</ref> activation and Dropout layers to map 3D noisy inputs to more reliable 3D pose predictions. To facilitate information flow between layers, we add residual connections <ref type="bibr" target="#b12">[13]</ref> and apply intermediate loss to expedite the intermediate layers' access to supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pose Structure Score</head><p>As we discussed in Section 1, traditional distance-based evaluation metrics (such as MPJPE, PCK) treat each joint independently, hence, fail to asses the whole pose as a structure. In <ref type="figure" target="#fig_2">Figure 4</ref>, we present example poses that have the same MPJPE but are structurally very different, with respect to a reference pose.</p><p>We propose a new performance measure, called the Pose Structure Score (PSS), which is sensitive to structural errors in pose. PSS computes a scale invariant performance score with the capability to assess the structural plausibility of a pose with respect to its ground truth. Note that PSS is not a loss function, it is a performance score that can be used along with MPJPE and PCK to account for structural errors made by the pose estimator. PSS is an indicator about the deviation from the ground truth pose that has the potential to cause a wrong inference in a subsequent task requiring semantically meaningful poses, e.g. action recognition, human-robot interaction.</p><p>How to compute PSS? The computation of PSS requires a reference distribution of ground-truth poses. Given a ground-truth set composed of n poses q i , i ∈ {1, · · · , n}, we normalize each pose vector asq i = qi ||qi|| . Then, we compute k cluster centers µ j , j ∈ {1, · · · , k} using kmeans clustering. Then, to compute the PSS of a predicted pose p against its ground-truth pose q, we use</p><formula xml:id="formula_3">PSS(p, q) = δ C(p), C(q) where (3) C(p) = arg min k ||p − µ k || 2 2 , δ(i, j) = 1 i = j 0 i = j<label>(4)</label></formula><p>The mPSS, mean-PSS, of a set of poses is the average over their individual scores as computed in Eq. (3). <ref type="figure" target="#fig_3">Figure 5</ref> shows the t-SNE <ref type="bibr" target="#b41">[42]</ref> graph of poses and clusters. <ref type="figure" target="#fig_4">Figure 6</ref> depicts the cluster centers which represent canonical poses.</p><p>In our experiments, we chose the number of pose clusters as 50 and 100. We denoted the corresponding PSS results with mPSS@50 and mPSS@100 expressions. Note that mPSS gives the percentage of structurally correct poses, therefore higher scores are better. To test the stability of our clustering, we ran k-means 100 times each with random initializations. Then, for each pair of runs, we established pairwise correspondences between clusters. For each correspondence, we computed intersection over union (IOU). Average IOU over all pairings and correspondences turned out to be 0.78. Additionally, the mPSS of different pose estimation models vary ±0.1% when we use different kmeans outputs as reference. These analyses prove the stability of PSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We use the Integral Pose <ref type="bibr" target="#b35">[36]</ref> architecture for both 2D and 3D branches with a ResNet-50 <ref type="bibr" target="#b12">[13]</ref> backend. Input image and output heatmap sizes are 256 × 256 and J × 64 × 64 × 64, respectively where J is  the number of joints. We initialize all models used in experiments after training on the MPII <ref type="bibr" target="#b1">[2]</ref>.</p><p>During training, we use mini-batches of size 32, each one containing I i , I i+1 image pairs. If more than two cameras are available, we include the views from all cameras in a mini-batch. We train the network for 140 epochs using Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with a learning rate of 10 −3 multiplied with 0.1 at steps 90 and 120. Training data is augmented by random rotations of ±30 • and scaled by a factor between 0.8 and 1.2. Additionally, we utilize synthetic occlusions <ref type="bibr" target="#b33">[34]</ref> to make the network robust to occluded joints. For the sake of simplicity, we run the 2D branch once to produce triangulated 3D targets and train the 3D branch using cached labels. We implemented the whole pipeline using PyTorch <ref type="bibr" target="#b26">[27]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We first conduct experiments on the Hu-man3.6M (H36M) large scale 3D human pose estimation benchmark <ref type="bibr" target="#b14">[15]</ref>. It is one of the largest datasets for 3D human pose estimation with 3.6 million images featuring 11 actors performing 15 daily activities, such as eating, sitting, walking and taking a photo, from 4 camera views. We mainly use this dataset for both quantitative and qualitative evaluation.</p><p>We follow the standard protocol on H36M and use the subjects 1, 5, 6, 7, 8 for training and the subjects 9, 11 for evaluation. Evaluation is performed on every 64 th frame of the test set. We include average errors for each method.</p><p>To demonstrate further applicability of our method, we use MPI-INF-3DHP (3DHP) <ref type="bibr" target="#b22">[23]</ref> which is a recent dataset that includes both indoor and outdoor scenes. We follow the standard protocol: The five chest-height cameras and the provided 17 joints (compatible with H36M) are used for training. For evaluation, we use the official test set which includes challenging outdoor scenes. We report the results in terms of PCK and NPCK to be consistent with <ref type="bibr" target="#b30">[31]</ref>. Note that we do not utilize any kind of background augmentation to boost the performance for outdoor test scenes.</p><p>Metrics. We evaluate pose accuracy in terms of MPJPE (mean per joint position error), PMPJPE (procrustes aligned mean per joint position error), PCK (percentage of correct keypoints), and PSS at scales @50 and @100. To compare our model with <ref type="bibr" target="#b30">[31]</ref>, we measured the normalized metrics NMPJPE and NPCK, please refer to <ref type="bibr" target="#b30">[31]</ref> for further details. Note that PSS, by default, uses normalized poses during evaluation. In the presented results "n/a" means "not applicable" where it's not possible to measure respective metric with provided information, "-" means "not available". For instance, it's not possible to measure MPJPE or PCK when R, the camera rotation matrix, is not available. For some of the previous methods with open source code, we indicate their respective PSS scores. We hope, in the future, PSS will be adapted as an additional performance measure, thus more results will become available for complete comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Can we rely on the labels from multi view images? <ref type="table" target="#tab_1">Table 1</ref> summarizes triangulation results from different 2D keypoint sources on the H36M dataset. Note that we use training subjects to obtain these results, since our goal is to find out the performance of triangulation on the training data. Overall, the quality of estimated keypoints is crucial to attain better results. If we have the ground truth 2D keypoints and camera geometry, triangulation gives 4.3 mm error and 99% PSS which is near perfect. Lack of camera geometry reduces the PMPJE and mPSS@50 by a small amount of 13 mm and 1%, respectively. A pose detector trained on the 2D labels of H36M improves the MPIIpretrained one up to 17 mm and 5%. Note that, it is expected to have slightly worse performance when evaluating the MPII-pretrained detector on the H36M validation set. Data in H36M was captured with markers, and therefore, have high accuracy and consistency in 2D annotations across subject and scenes; on the other hand, the annotations in MPII were done by humans and some of the keypoints are localized differently. For instance, shoulders and hips are closer to edges of the body in the MPII dataset.</p><p>Compared to Pavlakos et al. 's <ref type="bibr" target="#b29">[30]</ref> results, our triangulation using an MPII-pretrained detector is 11mm better in terms of MPJPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to State-of-the-art</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we present the results of our model with different supervision types in comparison with recent state-ofthe-art methods. We present the fully supervised (FS) version of our model to provide a baseline. Our own implementation of "Integral Pose" architecture <ref type="bibr" target="#b35">[36]</ref> produced a slightly different result than reported. The difference between our result (52mm) and the reported one <ref type="bibr" target="#b35">[36]</ref> (49mm) can be attributed to the authors' 2D-3D mixed training which we refrained from doing in order to decouple 3D pose estimation stage from 2D.</p><p>Our self supervised (SS) model performs quite well compared to the recent fully 3D supervised methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> which require abundant labeled data to learn. Obtaining comparable results to state-of-the-art methods without using any 3D ground truth examples is a promising step for such a nontrivial task.</p><p>Refinement Unit (RU) which is an optional extension to our SS network is helpful for achieving better results. Adding RU further improves the performance of our SS model by 20% . To measure the representation capacity of the outputs from our SS model, we compare its result with Martinez et al. 's work <ref type="bibr" target="#b21">[22]</ref>. Since the RU architecture is identical to Martinez et al., we selected their model trained with 2D keypoints from an MPII-pretrained pose detector for a fair comparison. This results show that 3D depth in- formation learned by our SS training method provides helpful cues to improve the performance of 2D-3D lifting approaches.</p><p>In <ref type="table" target="#tab_4">Table 4</ref> top, we show the FS training results on the 3DHP dataset as a baseline. We further use that information to analyze the differences between FS and SS training. <ref type="table" target="#tab_3">Table 3</ref> outlines the performance of weakly/self supervised methods in the literature along with ours on the H36M dataset. The top part includes the methods not requiring paired 3D supervision. Since Tung et al. <ref type="bibr" target="#b40">[41]</ref> use unpaired 3D ground truth labels that are easier to obtain, we place them here. Our SS model (with or without R) outperforms all previous methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30]</ref> by a large margin in MPJPE metric. We observe a large difference (21mm) between training with ground truth 2D triangulations and MPIIpretrained ones. This gap indicates us that the 2D keypoint estimation quality is crucial for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weakly/Self Supervised Methods</head><p>To better understand the source of performance gain in ours and Rhodin et al., we can analyze the gap between the models trained with full supervision (FS) and subject 1 of H36M and 3DHP only (S1). In our method, the difference between FS and S1 training is 12 and 9mm, while Rhodin et al. 's difference is 15 and 18mm for H36M and 3DHP, respectively (lower is better). It shows us that our learning strategy is better at closing the gap. Even though Rhodin et al. uses S1 for training, our SS method outperforms it on H36M dataset. In the case of S1 training, there is an explicit improvement (14mm, 4mm for H36M and 3DHP) with our approach. Also, SS training with our method on 3DHP has comparable results to Rhodin et al. ' S1.</p><p>Finally, the bottom part in <ref type="table" target="#tab_3">Table 3</ref> gives a fair comparison of our model against Drover et al. 's since they report results only with 14 joints. Our method yields 4mm less error than their approach.    <ref type="bibr" target="#b40">[41]</ref> uses unpaired 3D supervision which is easier to get. 3DInterp denotes the results of <ref type="bibr" target="#b43">[44]</ref> implemented by <ref type="bibr" target="#b40">[41]</ref>. 2D GT denotes training with triangulations obtained from ground truth 2D labels.) Middle: Methods requiring a small set of ground truth data. (S1 denotes using ground truth labels of H36M subject #1 during training.) Bottom: Comparison to Drover et al. <ref type="bibr" target="#b8">[9]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have shown that even without any 3D ground truth data and the knowledge of camera extrinsics, multi view images can be leveraged to obtain self supervision. At the core of our approach, there is Epipo-larPose which can utilize 2D poses from multi-view images using epipolar geometry to self-supervise a 3D pose estimator. EpipolarPose achieved state-of-the-art results in Human3.6M and MPI-INF-3D-HP benchmarks among weakly/self-supervised methods. In addition, we discussed the weaknesses of localization based metrics i.e. MPJPE and PCK for human pose estimation task and therefore proposed a new performance measure Pose Structure Score (PSS) to score the structural plausibility of a pose with respect to its ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>EpipolarPose uses 2D pose estimation and epipolar geometry to obtain 3D poses which are subsequently used to train a 3D pose estimator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of EpipolarPose during training. The orange-background part in the upper branch denotes the inference pipeline. During training, EpipolarPose is multi-view: a pair of images (Ii, Ii+1) simultaneously taken by two consecutive cameras is fed into the CNN pose estimators. It is also selfsupervised: the 3D pose (V ) generated by the lower branch using triangulation (i.e. epipolar geometry) is used as a training signal for the CNN in the upper branch. During inference (the orangebackground part), EpipolarPose is a monocular method: it takes a single image (Ii) as input and estimates the corresponding 3D pose (Vi). (ϕ: soft argmax function, T: triangulation, L: smooth L1 loss.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Left: reference poses from Human3.6M dataset. Middle: manually modified poses to obtain similar MPJPE with poses on the right, yet structured differently from reference poses. Right: poses obtained by adding random gaussian noise to each body joint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>t-SNE graph of human poses after clustering. Here we choose k = 10 for visualization purposes. Each color represents a cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Cluster centers which represents the canonical poses in Human3.6M (k = 50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on H36M dataset. Provided 3D poses are from different camera views for better visualization. Last row depicts a failure case. (FS: fully supervised training, SS: self supervised training)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Triangulation results on H36M. Effects of different 2D keypoint sources on triangulation performance. GT 2D denotes the usage of ground truth 2D labels. H36M 2D and MPII 2D shows the pose estimation models trained on those datasets.</figDesc><table><row><cell>Methods</cell><cell cols="5">MPJPE NMPJPE PMPJPE mPSS@50 mPSS@100</cell></row><row><cell>Pavlakos et al. [30]</cell><cell>56.89</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GT 2D</cell><cell>4.38</cell><cell>2.87</cell><cell>2.13</cell><cell>98.93</cell><cell>97.16</cell></row><row><cell>GT 2D (w/o R)</cell><cell>n/a</cell><cell>22.46</cell><cell>15.06</cell><cell>98.83</cell><cell>96.03</cell></row><row><cell>H36M 2D</cell><cell>28.37</cell><cell>26.28</cell><cell>25.19</cell><cell>95.08</cell><cell>94.2</cell></row><row><cell>MPII 2D</cell><cell>45.86</cell><cell>37.79</cell><cell>36.83</cell><cell>90.06</cell><cell>85.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>H36M results. Top: Comparison of results between our methods trained with different settings and the state-of-the-art fully supervised methods. (FS: fully supervised, SS: self supervised) Bottom: Effect of adding refinement unit (RU) over SS. (* uses the 2D keypoints from an MPII pre trained model as input, hence is comparable to our SS+RU model.)   Supervised training on all subjects of H36M</figDesc><table><row><cell>Methods</cell><cell cols="5">MPJPE NMPJPE PMPJPE mPSS@50 mPSS@100</cell></row><row><cell>Nie et al. [45] (ICCV'17)</cell><cell>97.5</cell><cell>-</cell><cell>79.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Sanzari et al. [33] (ECCV'16)</cell><cell>93.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tome et al. [40] (CVPR'17)</cell><cell>88.4</cell><cell>-</cell><cell>-</cell><cell>73.0</cell><cell>58.8</cell></row><row><cell>Rogez et al. [32] (CVPR'17)</cell><cell>87.7</cell><cell>-</cell><cell>71.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Pavlakos et al. [29] (CVPR'17)</cell><cell>71.9</cell><cell>-</cell><cell>-</cell><cell>74.05</cell><cell>53.93</cell></row><row><cell>Rhodin et al. [31] (CVPR'18)</cell><cell>66.8</cell><cell>63.3</cell><cell>51.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Martinez et al. [22] (ICCV'17)</cell><cell>62.9</cell><cell>-</cell><cell>47.7</cell><cell>78.12</cell><cell>73.26</cell></row><row><cell>Pavlakos et al. [28] (CVPR'18)</cell><cell>56.2</cell><cell>-</cell><cell>-</cell><cell>80.03</cell><cell>69.18</cell></row><row><cell>Sun et al. [36] (ECCV'18)</cell><cell>49.6</cell><cell>-</cell><cell>40.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours FS</cell><cell>51.83</cell><cell>51.58</cell><cell>45.04</cell><cell>84.44</cell><cell>78.67</cell></row><row><cell>Ours SS</cell><cell>76.60</cell><cell>75.25</cell><cell>67.45</cell><cell>73.09</cell><cell>64.03</cell></row><row><cell>Ours SS (w/o R)</cell><cell>n/a</cell><cell>77.75</cell><cell>70.67</cell><cell>70.67</cell><cell>62.05</cell></row><row><cell cols="5">Integrating Refinement Unit with SS trained network on H36M</cell><cell></cell></row><row><cell>Methods</cell><cell cols="5">MPJPE NMPJPE PMPJPE mPSS@50 mPSS@100</cell></row><row><cell>Martinez et al. [22] (ICCV'2017)*</cell><cell>67.5</cell><cell>-</cell><cell>52.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours SS + RU</cell><cell>60.56</cell><cell>60.04</cell><cell>47.48</cell><cell>80.42</cell><cell>75.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>H36M weakly/self supervised results. Top: Methods that can be trained without 3D ground truth labels.(Tung et al.  </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>that evaluated using 14 joints (14j) Training without ground truth data Methods MPJPE NMPJPE PMPJPE mPSS@50 mPSS@100 Pavlakos et al. 3DHP results. Top: Fully supervised training results. Middle: Self supervised learning using only subject 1. Bottom: Self supervised training without any ground truth examples.</figDesc><table><row><cell>[30] (CVPR'2017)</cell><cell></cell><cell>118.41</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Tung et al.-3DInterp [41] (ICCV'2017)</cell><cell>98.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tung et al. [41] (ICCV'2017)</cell><cell></cell><cell>97.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours SS</cell><cell></cell><cell>76.60</cell><cell>75.25</cell><cell>67.45</cell><cell>73.09</cell><cell>64.03</cell></row><row><cell>Ours SS (w/o R)</cell><cell></cell><cell>n/a</cell><cell>77.75</cell><cell>70.67</cell><cell>70.67</cell><cell>62.05</cell></row><row><cell>Ours SS (2D GT)</cell><cell></cell><cell>55.08</cell><cell>54.90</cell><cell>47.91</cell><cell>83.9</cell><cell>78.69</cell></row><row><cell></cell><cell cols="4">Training with only Subject 1 of H36M</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell cols="5">MPJPE NMPJPE PMPJPE mPSS@50 mPSS@100</cell></row><row><cell>Rhodin et al. [31] S1</cell><cell></cell><cell>n/a</cell><cell>78.2</cell><cell>64.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Rhodin et al. [31] S1 (w/o R)</cell><cell></cell><cell>n/a</cell><cell>80.1</cell><cell>65.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours S1</cell><cell></cell><cell>65.35</cell><cell>64.76</cell><cell>57.22</cell><cell>81.91</cell><cell>75.2</cell></row><row><cell>Ours S1 (w/o R)</cell><cell></cell><cell>n/a</cell><cell>66.98</cell><cell>60.16</cell><cell>77.65</cell><cell>72.4</cell></row><row><cell></cell><cell cols="3">Evaluation using 14 joints</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell cols="5">MPJPE NMPJPE PMPJPE mPSS@50 mPSS@100</cell></row><row><cell cols="2">Drover et al. [9](14j) (ECCVW'2018)</cell><cell>-</cell><cell>-</cell><cell>64.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours SS (14j)</cell><cell></cell><cell>69.94</cell><cell>67.90</cell><cell>60.24</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell></cell><cell cols="2">Supervised training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">MPJPE NMPJPE PCK NPCK mPSS@50 mPSS@100</cell></row><row><cell>Mehta et al. [23]</cell><cell>-</cell><cell>-</cell><cell>72.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Rhodin et al. [31] FS</cell><cell>n/a</cell><cell>101.5</cell><cell>n/a</cell><cell>78.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours FS</cell><cell>108.99</cell><cell>106.38</cell><cell>77.5</cell><cell>78.1</cell><cell>87.15</cell><cell>82.21</cell></row><row><cell cols="4">Training with only Subject 1 of 3DHP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">MPJPE NMPJPE PCK NPCK mPSS@50 mPSS@100</cell></row><row><cell>Rhodin et al. [31] S1</cell><cell>n/a</cell><cell>119.8</cell><cell>n/a</cell><cell>73.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Rhodin et al. [31] S1 (w/o R)</cell><cell>n/a</cell><cell>121.8</cell><cell>n/a</cell><cell>72.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours S1</cell><cell>n/a</cell><cell>115.37</cell><cell>n/a</cell><cell>74.4</cell><cell>75.64</cell><cell>73.15</cell></row><row><cell>Ours S1 (w/o R)</cell><cell>n/a</cell><cell>119.86</cell><cell>n/a</cell><cell>73.5</cell><cell>73.41</cell><cell>70.97</cell></row><row><cell cols="4">Training without ground truth data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">MPJPE NMPJPE PCK NPCK mPSS@50 mPSS@100</cell></row><row><cell>Ours SS</cell><cell>126.79</cell><cell>125.65</cell><cell>64.7</cell><cell>71.9</cell><cell>70.94</cell><cell>67.58</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3D pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3D pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A study of parts-based object class detection using complete graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bergtholdt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Can 3d pose be learned from 2d projections alone? European Conference on Computer Vision Workshops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MARCOnI-ConvNet-based MARker-less motion capture in outdoor and indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Edilson De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Triangulation. Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improve cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3DVision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An efficient solution to the five-point relative pose problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valsamis</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiora</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How robust is 3d human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS Workshop -Robotic Co-workers 4</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structured prediction of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to fuse 2D and 3D image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosta</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
