<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single image dehazing for a variety of haze scenarios using back projected pyramid network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology (ISM)</orgName>
								<address>
									<postCode>826004</postCode>
									<settlement>Dhanbad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Bhave</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology (ISM)</orgName>
								<address>
									<postCode>826004</postCode>
									<settlement>Dhanbad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><forename type="middle">K</forename><surname>Prasad</surname></persName>
							<email>dilip.prasad@uit.no</email>
							<affiliation key="aff1">
								<orgName type="department">UiT The</orgName>
								<orgName type="institution">Arctic University of Norway</orgName>
								<address>
									<postCode>9019</postCode>
									<settlement>Troms</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single image dehazing for a variety of haze scenarios using back projected pyramid network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Single image dehazing</term>
					<term>Generative adversarial network</term>
					<term>Back projection</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to dehaze single hazy images, especially using a small training dataset is quite challenging. We propose a novel generative adversarial network architecture for this problem, namely back projected pyramid network (BPPNet), that gives good performance for a variety of challenging haze conditions, including dense haze and inhomogeneous haze. Our architecture incorporates learning of multiple levels of complexities while retaining spatial context through iterative blocks of UNets and structural information of multiple scales through a novel pyramidal convolution block. These blocks together for the generator and are amenable to learning through back projection. We have shown that our network can be trained without over-fitting using as few as 20 image pairs of hazy and non-hazy images. We report the state of the art performances on NTIRE 2018 homogeneous haze datasets for indoor and outdoor images, NTIRE 2019 denseHaze dataset, and NTIRE 2020 non-homogeneous haze dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The quality of images of scenes in our daily life is greatly affected by the particles suspended in the environment, such as due to dust, smoke, mist, fog, smog, etc. Bad weather also contributes to this. Beside significantly higher and nonuniform noise in the images, the usual effects are reduced visibility, reduced sharpness, and contrast of the objects within the visibility and obscuring of other objects. Therefore, performing computer vision tasks like object detection, object recognition, tracking and segmentation becomes complicated for such images. Therefore, the true potential of computer vision empowered automated and remote surveillance systems such as drones and robots cannot be realized under hazy conditions. Thus, it is of interest to enhance the quality of images taken under homogeneous and non-homogeneous hazy conditions and recover the details of the scene. Haze removal or dehazing algorithms address this problem.</p><p>There has been a significant activity in the topic of dehazing in recent years. New algorithms ranging from physics-based solvers, image processing based algo- rithms, as well as deep learning-based approaches, are being proposed. Furthermore, newer challenges are being undertaken, including dehazing in the presence of dense haze, non-homogenous haze, and using a single RGB image of a scene. It is being recognized that deep learning architectures provide better performance than the other approaches for diverse and challenging dehazing scenarios if suitably designed large datasets are available. However, dehazing images through deep learning on a small dataset using a single RGB image is quite challenging and of significant practical interest. For example, in the situation of fire management or natural disaster management, a suitable dehazing model characteristic of the situation needs to be learned quickly using a small number of images in haze and corresponding pre-disaster images.</p><p>We propose a novel deep learning architecture that is amenable to reliable learning of dehazing model using a small dataset. Our novel generative adversarial network (GAN) architecture includes iterative blocks of UNets to model haze features of different complexities and a pyramid convolution block to preserve and restore spatial features of different scales. The key contributions of this paper are as follows:</p><p>-A novel technique named pyramid convolution is introduced for dehazing to obtain spatial features of multiple scales structural information. -We have used iterative UNet block for image dehazing tasks to make the generator learn different and complex features of haze without the loss of local and global structural information or without making the network too deep to result into loss of spatial features. -The model used is end-to-end trainable with hazy image as input and hazefree image as the desired output. Therefore the conventional practice of using the atmospheric scattering model is obviated, and the problems encountered in inverse reconstruction are circumvented. It also makes the approach more versatile and applicable to haze scenarios where the conventional simplified atmospheric model may not apply.</p><p>-Extensive experimentation is done on four contemporary challenging datasets, namely I-Haze and O-Haze datasets of NTIRE 2018 challenge, Dense-haze dataset of NTIRE 2019 challenge, and non-homogeneous dehazing dataset of NTIRE 2020 challenge.</p><p>The outline of the paper is as follows. Section 2 presents related work, and section 3 introduces our architecture and learning approach. Section 4 presents numerical experiments and results. Section 5 includes an ablation study on the proposed method. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since this paper's focus is single image dehazing, we exclude a discussion on studies that required multiple images, for example, exploiting polarization, to perform dehazing. Single image dehazing is an ill posed problem because the number of measurements is not sufficient for learning the haze model, and the non-linearity of the haze model implies higher sensitivity to noise. Single image based dehazing exploits polarization-independent atmospheric scattering model proposed by Koschmieder <ref type="bibr" target="#b15">[16]</ref> and its characteristics such as dark channel, color attenuation and haze-free priors. According to this model, the hazy image is specified by the atmospheric light (generally assumed uniform), the albedo of the objects in the scene, and the transmission map of the hazy medium. More details can be found in <ref type="bibr" target="#b15">[16]</ref> and its subsequent citations,including recent ones <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>. We have to predict the unknown transmission map and global atmospheric light. In the past, many methods have been proposed for this task. The methods can be divided into two categories, namely (i) Traditional handcrafted prior based methods and (ii) Learning based methods.</p><p>Traditional handcrafted prior based methods: Fattal <ref type="bibr" target="#b8">[9]</ref> proposed a physically grounded method by estimating the albedo of the scene. Tan <ref type="bibr" target="#b21">[22]</ref> proposed the use of the Markov random field to maximize the local contrast of the image. He et al. <ref type="bibr" target="#b11">[12]</ref> proposed a dark channel prior for the estimation of the transmission map. Fattal <ref type="bibr" target="#b9">[10]</ref> proposed a color-line method based on the observation that small image patches typically exhibit a one-dimensional distribution in the RGB color space. Traditional handcrafted prior methods give good results for certain cases but are not robust for all the cases.</p><p>Learning based approaches: In recent years, many learning based methods have been proposed for single image dehazing that encash the success of deep learning in image processing tasks, availability of large datasets, and better computation resources. Some examples are briefly mentioned here. Cai et al. <ref type="bibr" target="#b5">[6]</ref> proposed an end-to-end CNN based deep architecture to estimate the transmission map. Ren et al. <ref type="bibr" target="#b19">[20]</ref> proposed a multi-scale deep architecture, which also estimates the transmission map from the haze image. Zhang et.al. in <ref type="bibr" target="#b24">[25]</ref> proposed a deep network architecture that estimates the transmission map and the atmospheric light. These estimates are then used together with the atmospheric scattering model to generate the haze-free image. Our approach in context: In contrast to these approaches, our approach is an end-to-end learning based approach in which the learnt model directly predicts the haze-free image without needing to reconstruct the transmission map and the atmospheric light, or using the atmospheric scattering model. It is therefore more versatile to be trained for situations where the atmospheric scattering model of <ref type="bibr" target="#b15">[16]</ref> may not apply or may be too simple. Example includes non-uniform haze. It also circumvents the numerical errors and artifacts associated with the use of inverse approaches of reconstructing the haze-free image from the transmission map and atmospheric light.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>In this section we present our model, namely back projected pyramid network (BPPNet). The overall architecture is based on generative adversarial network <ref type="bibr" target="#b10">[11]</ref>, where a generator generates a haze-free image from a hazy image, and a discriminator tells whether the image provided to it is real or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generator</head><p>The architecture of the generator is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It comprises of two blocks in series, namely (i) iterative UNet block, (ii) pyramid convolution block, which we describe next.</p><p>Iterative UNet block (IUB): This block consists of multiple UNet <ref type="bibr" target="#b20">[21]</ref> units connected in series i.e. the output of one UNet (architecture in the supplementary) is fed as the input to the next UNet. In addition, the output of each UNet is passed to a concatenator, which concatenates the 3 channel output of all the UNets, providing an intermediate 12 channel feature map. The equations describing the working of IUB are the following. </p><formula xml:id="formula_0">I 1 = UNET 1 (I haze ); I i = UNET i (I i−1 ) for i &gt; 1,<label>(1)</label></formula><p>where I i is the output of ith UNet unit, I haze is the input hazy image after being transformed to YCbCr space, and the outputÎ IUB of IUB is given aŝ</p><formula xml:id="formula_1">I IUB = I 1 ⊕ I 2 ⊕ . . . I M<label>(2)</label></formula><p>where ⊕ indicates concatenation operator and M is the number of UNet unit. We have used M = 4. An ablation study on the value of M is presented later in section 5. Here, we discuss the need of more than one UNet. In principle, a single UNet may be able to support dehazing to some extent. However, it may not be able to extract complex features and generate an output with fine details. One way to tackle this problem is to increase the number of layers in the encoder block so that more complex features can be learned. But the layers in the encoder block are arranged in feed forward fashion, and the height and the width of layers decreases upon moving further. This causes loss in spatial information and reduces the possibility of extraction of spatial features of high complexity. Therefore, we take an alternate approach of creating sequence of the multiple UNets. The sequence of UNets may be interpreted as a sequence of multiple encoder-decoder pairs with skip connections. The encoder in each UNet extracts the features from input tensor in the downsample feature map and decoder uses those features and projects them into an upsample latent space with same height and width as input tensor. In this way, each generator helps in learning increasingly complex features of haze while the decoder helps in retaining the spatial information in the image. Lastly, the concatenation step ensures that complexity of all the levels are available for subsequent reconstruction. We illustrate the effect of using multiple UNets in <ref type="figure" target="#fig_2">Fig. 3</ref>. Histogram equalized 3-channel output of each UNet is shown as an RGB image. It is seen that the spatial context is preserved, and at the same time haze introduced blur of different complexities are present in the outputs of different images. The haze in the last UNet output is flatter across the image and shows large scale blurs while the haze in the first UNet is local and introduces small scale blurs. Therefore, most dehazing is accomplished in UNet1, although the subsequent UNets pick the dehazing components that the previous UNets did not. <ref type="figure" target="#fig_2">Fig. 3</ref> also explains our choice of only four UNet blocks even though more blocks could be used in principle. We explain our choice in two parts. First, there is a trade-off involved between accuracy and speed when choosing the number of UNet blocks. Second, as seen in the histograms in <ref type="figure" target="#fig_2">Fig. 3</ref>, the dynamic range of channels decreases with every subsequent UNet block, thereby indicating the reduction in the usable information content. The standard deviation of the intensity values in the 3 channels after UNet4 is ∼12.2. Adding more blocks would further reduce this value, and therefore not provide significantly exploitable data for dehazing.</p><p>Pyramid convolution (PyCon) block: Although the iterative UNet block does provide global and local structural information, the output lacks the global and local structural information for different sized objects. An underlying reason is that the structural information from different scales are not directly used to generate the output. To overcome this issue, we have used a novel pyramid convolution technique. Earlier pyramid pooling has been used in <ref type="bibr" target="#b20">[21]</ref> to leverage the global structural information. However, since the pooling layers are not learnable, we instead employ learnable convolution layers that can easily outperform the pooling layers in leveraging the information. We employ many convolution layers of different kernel sizes in parallel on the input map (the 12 channel output of iterative UNet block). Corresponding to different kernel sizes used for convolution, different output maps are generated with structural information of different spatial scales. The kernel sizes are chosen as 3, 5, 7, 11, 17, 25, 35, and 45, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Odd sized kernels are used since pixels in the intermediate feature map are assumed to be symmetrical around output pixel. We observed introduction of distortion over layers upon using even-sized kernels, indicating the importance of exploiting the symmetry of the features around the output pixel. Zero padding is used to ensure that the features at the edges are not lost. After the generation of feature map from corresponding kernels, all the generated maps are concatenated to make an output feature map of 128 channels, which is subsequently used for the final image construction by applying a convolution layer of kernel size 3×3 with zero padding. In this manner, the local to global information is directly used for the final image reconstruction.</p><p>The effect of using pyramid convolution is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. In the zoom-ins shown in the middle panel, the arrows indicate some features of the size of the convolution filter used for generating that particular feature map. The illustrated 3 channels are superimposed as a hypothetical RGB image in the bottom left of <ref type="figure" target="#fig_3">Fig. 4</ref> to demonstrate the types of details present in a subset of the output feature map. Since we have used 8 convolution filters that operate on 12 channel input, we generate a total of 128 channels in the output feature map with a large variety of spatial features of multiple scales learned and restored. Therefore, the result image shown in the bottom panel has spatial features closely matching the ground truth, resulting in a low difference map (shown in the bottom right).</p><p>One may consider using the 12 channel output of the iterative UNet block for generating the dehazed image directly, without employing the PyCon block. To indicate the importance of including the PyCon block, we include an ablation study in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discriminator</head><p>We have used a patch discriminator to determine whether a particular patch is realistic or not. The patches overlap in order to eliminate the problem of low performance on the edges. We have used 4×4 convolution layers in discriminator. After every convolution layer, we have added an activation layer with an activation leaky rectified linear unit (Leaky ReLu) except the last layer where the activation function is sigmoid. The size of the convolution kernel used is 4×4, and the output map size is 62×62 for an input of size 512×512. The architecture of our discriminator is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss functions</head><p>Most dehazing models use the mean squares error (MSE) as the loss function <ref type="bibr" target="#b25">[26]</ref>. However, MSE is known to be only weakly correlated with human perception of image quality <ref type="bibr" target="#b12">[13]</ref>. Hence, we employ additional loss functions that are closer to human perception. To this end, we have used a combination of MSE (L 2 loss), adversarial loss L adv , content loss L con , and structural similarity loss L SSIM . We define the remaining loss functions below.</p><p>The adversarial loss for the generator L adv and the discriminator L dis is defined as:</p><formula xml:id="formula_2">L adv = log(D(I pred )) ,<label>(3)</label></formula><p>L dis = log(D(I GT )) + log(1 − D(I pred )) ,</p><p>where (I haze , I GT ) are the supervised pair of the hazy image and the corresponding ground truth. D(I) is the discriminators estimate of the probability that data instance I provided to it is indeed real. Similarly, G(I) is the generator output for the input instance I. Further, I pred = G(I haze ). The notation &lt;&gt; indicates the expected value over all the supervised pairs. The MSE, also referred to as the L 2 loss, is defined as the average norm 2 distance between I GT and I pred :</p><formula xml:id="formula_4">L 2 = I GT − I pred<label>(5)</label></formula><p>Our content loss is the VGG based perceptual loss <ref type="bibr" target="#b13">[14]</ref>, defined as:</p><formula xml:id="formula_5">L con = i 1 N i || φ i (I GT ) − φ i (I pred ) || ,<label>(6)</label></formula><p>where N i is the number of elements in the i th activation of VGG-19 and φ i represents i th activation of VGG-19. The structural similarity loss L SSIM over reconstructed image I pred and ground truth I GT is defined as:</p><formula xml:id="formula_6">L SSIM = 1 − SSIM(I GT , I pred ) ,<label>(7)</label></formula><p>where SSIM(I, I ) is the SSIM <ref type="bibr" target="#b23">[24]</ref> between the images I and I . We note that although the losses L 2 and L SSIM directly compare the predicted and the ground truth images, the nature of comparison is quite different across them. L 2 is insensitive to the structural details but retains the comparison of the general energy and dynamic range of the two images being compared. L SSIM on the other hand compared the structural content in the images with less sensitivity to the contrast. Therefore, including these two loss functions provide complementary aspects of comparison between the predicted and the ground truth images. The overall generator loss L G and discriminator loss L D are given as</p><formula xml:id="formula_7">L G = A 1 L adv + A 2 L con + A 3 L 2 + A 4 L SSIM (8) L D = B 1 L D adv .<label>(9)</label></formula><p>We have heuristically chosen the values of the constant weights in the above equation as A 1 = 0.7, A 2 = 0.5, A 3 = 1.0, A 4 = 1.0, and B 1 = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We have trained and tested our model on the following four datasets, namely NTIRE 2018 image dehazing indoor dataset (abbreviated as I-Haze), NTIRE 2018 image dehazing outdoor dataset (O-Haze), Dense-Haze dataset of NTIRE 2019, and NTIRE 2020 dataset for non-homogeneous dehazing challenge (NTIRE 2020).</p><p>I-Haze <ref type="bibr" target="#b3">[4]</ref> and O-Haze <ref type="bibr" target="#b1">[2]</ref>: These datasets contains 25 and 35 hazy images (size 2833×4657 pixels) respectively for training. Both datasets contain 5 hazy images for validation along with their corresponding ground truth images. For both of these datasets, the training was done on training data and validation images were used for testing because although 5 hazy images for testing were given but their ground truths were not available to make the quantitative comparison.</p><p>Dense-Haze <ref type="bibr" target="#b0">[1]</ref>: This dataset contains 45 hazy images (size 1200×1600 pixels) for training and 5 hazy images for validation and 5 more for testing with their corresponding ground truth images. We have done training on training data and tested our model with test data.</p><p>NTIRE 2020 <ref type="bibr" target="#b7">[8]</ref>: This dataset contains 45 hazy images (size 1200×1600 pixels) for training with their corresponding ground truth. It is the first dataset of non-homogeneous haze in our knowledge. As ground truth for validation was not given, hence we used only 40 image pairs for training and calculated our results on the rest of the 5 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>The optimizer used for the training was Adam <ref type="bibr" target="#b14">[15]</ref> with the initial learning rate for 0.001 and 0.001 for generator and discriminator respectively. We have randomly cropped large square patches from the training images. The crop size was 1024×1024 for NTIRE 2020 and Dense-Haze. Leveraging on the even large sizes of images in I-Haze and O-Haze datasets, we created four crops each of two different sizes, 1024×1024 and 2048×2048. We then resized all the patches to 512×512 using bicubic interpolation. These patches are randomly cropped for each epoch i.e. these patches are not same for every epoch. This has created an effectively larger dataset out of the small dataset available for training for each of the considered datasets. For datasets named I-Haze, O-Haze and NTIRE 2020, we converted these randomly cropped resize patches from RGB space to YCbCr space and then used them for training. For Dense-Haze dataset we directly used RGB patches for training. We decreased the learning rate of the generator whenever the loss became stable. We stopped training when the learning rate of the generator reached 0.00001 and the loss stabilized. We also tried to decrease the learning rate of discriminator but found that doing so did not give the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Here, we present our results and their comparison with the results of other models available in the literature. We note that we converted the test input image size to 512×512 for all the datasets for generating our results in view of our hardware (GPU) memory constraints. Quantitative evaluation is performed using the SSIM metric and the peak signal to noise ratio (PSNR). The metrics are computed in the RGB space even if the training was done in YCbCr space. The quantitative results are compared with earlier state-of-the-art in <ref type="table" target="#tab_0">Table 1</ref>. The metrics for the other methods are reproduced from <ref type="bibr" target="#b25">[26]</ref> for the I-Haze and O-Haze dataset. The benchmark for Dense-haze was provided in <ref type="bibr" target="#b0">[1]</ref>. We further include the results of Morales at al. <ref type="bibr" target="#b18">[19]</ref> for comparison.</p><p>I-Haze: The average PSNR and SSIM of our method for this dataset on validation data were 22.56 and 0.8994 respectively. It is evident from <ref type="table" target="#tab_0">Table 1</ref> that our model outperforms the state-of-the-art in both SSIM and PSNR index by a good margin. Qualitative comparison results on the test data are shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. It is evident that only CVPRW'18 <ref type="bibr" target="#b25">[26]</ref> competes with our method in the quality of dehazed image and match with the ground truth.</p><p>O-Haze: The average PSNR and SSIM for this dataset on validation data were 24.27 and 0.8919 respectively on validation data, see <ref type="table" target="#tab_0">Table 1</ref>. Our model clearly outperforms all the state-of-the-art in both PSNR and SSIM index by a large margin. For SSIM, the closest performing method was CVPRW'18 <ref type="bibr" target="#b25">[26]</ref> with SSIM of 0.7205, which is significantly lower than ours i.e 0.8919. It is notable from the results of all the methods that this dataset is more challenging than I-Haze. Nonetheless, our method provides comparable performance over both I-Haze and O-Haze datasets. The qualitative comparison of results on the test data are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. Similar to the I-Haze dataset, only CVPRW'18 <ref type="bibr" target="#b25">[26]</ref> and our method generate dehazed images of good quality.</p><p>As compared to I-Haze results in <ref type="figure" target="#fig_5">Fig. 6</ref>, it is more strongly evident in <ref type="figure" target="#fig_6">Fig.  7</ref> that the color cast of our method is slightly mismatched with the ground truth, where CVPRW'18 performs better than our method. However, CVPRW'18 shows poorer structural continuity than our method, as evident more strongly in <ref type="figure" target="#fig_5">Fig.  6</ref>.</p><p>Dense-Haze: From <ref type="table" target="#tab_0">Table 1</ref>, it is evident that this dataset is significantly more challenging that the I-Haze and O-Haze datasets. All methods perform quite poorer for this dataset, as compared to the numbers reported for I-Haze and O-Haze dataset. Even though the performance of our method is also poorer for this dataset as compared to the other datasets, its SSIM and PSNR values are significantly better than the other 8 methods whose results are included in <ref type="table" target="#tab_0">Table 1</ref> for this dataset. Qualitative comparison with select methods is shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. The results clearly illustrate the challenge of this dataset as the features and details of the scene are almost imperceptible through the dense haze. Only our method is capable of dehazing the image effectively and bringing forth the details of the scene. Nonetheless, the color cast is non-uniformly compensated and different from the ground truth in regions.</p><p>NTIRE 2020: As the ground truth for test data is not given, we randomly chose 5 images for testing and used the rest of the 40 image pair for training. The average SSIM and PSNR are 0.8726 and 19.40 respectively. This SSIM value is better than the best SSIM observed in the competition and informed to the participants in a personal email after the test phase. The qualitative results are shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. The observations are generally similar to the observations for the Dense-Haze dataset. Our results are qualitatively quite close to the ground truth and show the ability of our method to recover the details lost in haze, despite the non-homogeneity of the haze. Second, we observe a little bit of mismatch in the color reproduction and in-homogeneity in the color cast, which needs further work. We expect that the problem of color cast inhomogeneity may be related to the inhomogeneity in the haze itself, which may have been present in the Dense-Haze data as well but may not have been perceptible due to the generally high density of haze. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation study</head><p>We conduct ablation study using I-Haze and O-Haze datasets. We consider the ablation associated with the architectural elements in section 5.1, loss components in section 5.2, and the image space used in training in section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture ablation</head><p>Here, we consider ablation study relating to the number of UNet units used in the iterative UNet block and the absence or presence of the pyramid convolution block. The results are shown in <ref type="table">Table 2</ref>. It is evident that decreasing or increasing the number of UNet blocks degrades the performance and the use of M = 4 UNet blocks is optimal for the architecture. This is in agreement in the observations derived from <ref type="figure" target="#fig_2">Fig. 3</ref>. Similarly, dropping the PyCon block also degrades the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss ablation</head><p>We proposed in section 3.3 to use four types of loss functions for the training of the generator. Here, we consider the effect of dropping one loss function at a time. The results are presented in the bottom panel of <ref type="table">Table 2</ref>. It is seen than dropping any of the loss function results into significant deterioration of performance. This indicates the different and complementary roles each of these loss functions is playing. Our observation of the qualitative results, discussed in section 4, we might need to introduce another loss function related to the preservation of the color cast or color constancy. <ref type="table">Table 2</ref>. The results of ablation study are presented here. The reference indicates the use of 4 UNet blocks, inclusion of PyCon block with layer configuration as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. All loss functions discussed in section 3.3 are used and the entire architecture uses YCbCr space, such as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>(a) Ablation study on architectural units </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Use of RGB versus YCbCr space</head><p>If we used RGB space instead of YCbCr space for training, we observe a degraded performance in terms of SSIM as reported in section 2(b). However, we note that this observation is not consistent over all the datasets. Specifically, we noted that for Dense-Haze, the YCbCr conversion gave little poorer results than RGB based training. Hence, we have used RGB patches for training on Dense-Haze dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The presented single image dehazing method is an end-to-end trainable architecture that is applicable in diverse situations such as indoor, outdoor, dense, and non-homogeneous haze even though training datasets used are small in each of these cases. It beats the state-of-the-art results in terms of SSIM and PSNR for all the three datasets whose results are available. Qualitative results for indoor images indicate preservation of colors in the reconstructed image in the I-Haze dataset while a poorer color reconstruction is observed in the results of other datasets. In the future, we will improve our model to inherently include color preservation and seamless color cast as well. Source code, results, and trained model are shared at our project page ( https://github.com/ayu-22/BPPNet-Back-Projected-Pyramid-Network ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A compact representation of our novel generator and its important features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of our generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The effect of the successive UNet units is illustrated. Images are histogram equalized for better visualization. The histograms of the channels becomes narrower after passing more number of UNet units, indicating that adding more UNet units may cease to create more value after a certain limit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Feature maps corresponding to one of the channels of 3×3, 17×17, and 45×45 convolution layer respectively. The figure shows that smaller kernel size generates smaller scale features such as edges while large kernel size generates large scale features such as big patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Architecture of the discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative comparison of various benchmark with our model on I-Haze dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative comparison of various methods with our model on O-Haze dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results for Dense-Haze and NTIRE 2020 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of various state of the art methods with our model on I-Haze, O-Haze and Dense-Haze datasets.Our model does the dehazing task in realtime at an average running time of 0.0311 s i.e. 31.1 ms per image.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">I-Haze dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Metric Input CVPR'09 TIP'15 ECCV'16 CVPR'16 ICCV'17 CVPRW'18 Our</cell></row><row><cell></cell><cell>[12]</cell><cell>[27]</cell><cell>[20]</cell><cell>[5]</cell><cell>[17]</cell><cell>[26]</cell><cell>model</cell></row><row><cell cols="4">SSIM 0.7302 0.7516 0.6065 0.7545</cell><cell>0.6537</cell><cell>0.7323</cell><cell cols="2">0.8705 0.8994</cell></row><row><cell>PSNR 13.80</cell><cell>14.43</cell><cell>12.24</cell><cell>15.22</cell><cell>14.12</cell><cell>13.98</cell><cell>22.53</cell><cell>22.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">O-Haze dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Metric Input CVPR'09 TIP'15 ECCV'16 CVPR'16 ICCV'17 CVPRW'18 Our</cell></row><row><cell></cell><cell>[12]</cell><cell>[27]</cell><cell>[20]</cell><cell>[5]</cell><cell>[17]</cell><cell>[26]</cell><cell>model</cell></row><row><cell cols="4">SSIM 0.5907 0.6532 0.5965 0.6495</cell><cell>0.5849</cell><cell>0.5385</cell><cell cols="2">0.7205 0.8919</cell></row><row><cell>PSNR 13.56</cell><cell>16.78</cell><cell>16.08</cell><cell>17.56</cell><cell>15.98</cell><cell>15.03</cell><cell>24.24</cell><cell>24.27</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Dense-Haze dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Metric CVPR Meng Fattal Cai</cell><cell cols="4">Ancuti CVPR ECCV Morales Our</cell></row><row><cell cols="8">'09 [12] et. al [18] [10] et. al [6] et. al [3] '16 [5] '16 [20] et. al [19] model</cell></row><row><cell>SSIM 0.398</cell><cell cols="3">0.352 0.326 0.374</cell><cell cols="2">0.306 0.358 0.369</cell><cell cols="2">0.569 0.613</cell></row><row><cell>PSNR 14.56</cell><cell cols="3">14.62 12.11 11.36</cell><cell cols="2">13.67 13.18 12.52</cell><cell cols="2">16.37 17.01</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense-Haze: a benchmark for image dehazing with dense-haze and haze-free images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1014" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">O-HAZE: a dehazing benchmark with real hazy and haze-free outdoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Night-time dehazing by fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2256" to="2260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">I-HAZE: a dehazing benchmark with real hazy and haze-free indoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Vleeschouwer</surname></persName>
		</author>
		<editor>Blanc-Talon, J., Helbert, D., Philips, W., Popescu, D., Scheunders, P.</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="620" to="631" />
		</imprint>
	</monogr>
	<note>Advanced Concepts for Intelligent Vision Systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-scale adaptive dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ancuti</surname></persName>
		</author>
		<ptr target="https://competitions.codalab.org/competitions/22236" />
		<title level="m">NTIRE 2020 NonHomogeneous Dehazing Challenge</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visibility restoration of single hazy images captured in real-world weather conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1814" to="1824" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koschmieder</surname></persName>
		</author>
		<title level="m">Theorie der horizontalen Sichtweite. Keim &amp; Nemnich</title>
		<imprint>
			<date type="published" when="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature forwarding for efficient single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinghoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Physical-based optimization for non-physical image dehazing methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vazquez-Corral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmío</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Express</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="9327" to="9339" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-scale single image dehazing using perceptual pyramid deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="902" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
