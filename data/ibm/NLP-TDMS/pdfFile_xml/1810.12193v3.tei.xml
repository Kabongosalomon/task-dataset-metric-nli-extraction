<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramidal Person Re-IDentification via Multi-Loss Dynamic Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtu</forename><surname>Lab Tencent</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><surname>Rongrong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Southern</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic</orgName>
								<address>
									<addrLine>Enigineering Xidian University Xi&apos;an</addrLine>
									<postCode>710071</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Information Science and Engineering</orgName>
								<orgName type="department" key="dep2">Peng Cheng Laboratory</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramidal Person Re-IDentification via Multi-Loss Dynamic Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing Re-IDentification (Re-ID) methods are highly dependent on precise bounding boxes that enable images to be aligned with each other. However, due to the challenging practical scenarios, current detection models often produce inaccurate bounding boxes, which inevitably degenerate the performance of existing Re-ID algorithms. In this paper, we propose a novel coarse-to-fine pyramid model to relax the need of bounding boxes, which not only incorporates local and global information, but also integrates the gradual cues between them. The pyramid model is able to match at different scales and then search for the correct image of the same identity, even when the image pairs are not aligned. In addition, in order to learn discriminative identity representation, we explore a dynamic training scheme to seamlessly unify two losses and extract appropriate shared information between them. Experimental results clearly demonstrate that the proposed method achieves the state-of-the-art results on three datasets. Especially, our approach exceeds the current best method by 9.5% on the most challenging CUHK03 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-IDentification (Re-ID) aims to associate the images of the same person captured at different physical sites, facilitating cross-camera tracking techniques used in vision-based smart retail and security surveillance. In general, person Re-ID is considered to be the next high-level task after a pedestrian detection system, so the basic as- <ref type="figure">Figure 1</ref>. The examples of part-based matching at different scales, when bounding boxes are not aligned or parts of human body have been occluded. The red bounding box indicates that most cues in the two parts are varied. We can see that, in a finely partitioned way, a handful of horizontal stripes (left) cannot be well-matched due to different cues, while those stripes (right) in a more global view have more similar cues. sumption of Re-ID is that the detection model can provide precise and highly-aligned bounding boxes. Despite the recent great progress, there are limited room for the performance improvement of existing methods due to the problems with part-based models and the difficulties in training.</p><p>Drawbacks of part-based models: As it is well-known, part-based models can generally achieve promising performance in many computer vision tasks, because these models are potentially robust to some unavoidable challenges such as occlusion and partial variations. Actually, the performance of person Re-ID in the real-world applications is severely affected by these challenges. Thus, the recent proposed Part-based Convolutional Baseline (PCB) <ref type="bibr" target="#b24">[24]</ref> can achieve the state-of-the-art results. PCB is simple but very effective and even can outperform other learned part models. Nevertheless, in PCB, directly partitioning the feature map of backbone networks into a fixed number of parts strictly limits the capacities of further improving the performance. It has at least two major drawbacks, but not limited to: 1) The overall performance seriously depends on that a powerful and robust pedestrian detection model outputs a precise bounding box otherwise the parts cannot be well-aligned. However, in most cases of challenging scenes, current detection models are insufficient to do that. 2) The global information, which is also a very significant cue for recognition and identification, is completely ignored in this model whilst global features are normally robust to the subtle view changes and internal variations. Several examples are illustrated in <ref type="figure">Fig. 1</ref> to show that the parts of diverse scales are equivalently important for matching.</p><p>Difficulties of multi-loss training: Recent studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> demonstrate that multi-task learning has the capabilities to achieve advanced performance by extracting appropriate shared information between tasks. Without loss of generality, the terms "loss" and "task" will be used alternatively. In fact, many existing Re-ID methods <ref type="bibr" target="#b26">[26]</ref> also benefit from the multi-loss scheme to improve the performance. Generally, most multi-task methods choose to weight the losses using balancing parameters which are fixed during entire training process. 1) The performance highly relies on an appropriate parameter but choosing an appropriate parameter is undoubtedly a labor-intensive and tricky work. 2) The difficulties of different tasks actually change when the models are updated gradually, resulting in really varied appropriate parameters for different iterations. 3) More importantly, sampling strategies for different losses are generally diverse due to the specific considerations. For example, hard sample sampling for triplet loss would suppress the role of another task of identification loss.</p><p>To address above problems, in this paper, we specifically propose a novel coarse-to-fine pyramidal model based on the feature map extracted by a backbone network for person re-identification. First, the pyramid is actually a set of 3-dimensional sub-maps with a specific coarse-to-fine architecture, in which each member captures the discriminative information of different spatial scales. Then, a convolutional layer is used to reduce the dimension of features for each separated branch in the pyramid. Third, for each branch, the identification loss of a softmax function is independently applied to a fully connected layer which considers the features as the input. Furthermore, the features of all branches will be concatenated to form an identity representation, for which a triplet loss is defined to learn more discriminative features. To smoothly integrate the two losses, a dynamic training scheme with two sampling strategies is explored to optimize the parameters of deep neural networks. Finally, the learned identity representation will be used for person image matching, retrieval and re-identification.</p><p>In summary, the contribution of this paper is three-fold: 1) To relax the assumption of requiring a strong detection model, we propose a novel coarse-to-fine pyramid model that not only incorporates local and global information, but also integrates the gradual cues between them. 2) To maximally take advantage of different losses, we explore a dynamic training scheme to seamlessly unify two losses and extract appropriate shared information between them for learning discriminative identity representation.</p><p>3) The proposed method achieves the state-of-the-art results on the three datasets and most significantly, our approach exceeds the current best method by 9.5% on dataset CUHK03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most existing Re-ID methods either consider the local parts of person images or explore the global information. Some methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">26]</ref> aware of that integrating the local and global features can improve the performance but the information between them is also ignored. We observe that those cues in the transition process are significant as well.</p><p>Part-based algorithms: By performing bilinear pooling in a more local way, an embedding can be learned, in which each pooling is confined to a predefined region <ref type="bibr" target="#b25">[25]</ref>. Inspired by attention models, in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">21]</ref>, the attention-based deep neural networks are proposed to capture multiple attentions and select multi-scale attentive features. Similarly, Zhao et al. <ref type="bibr" target="#b31">[31]</ref> explore a deep neural network method to jointly model body part extraction and representation computation, and learn model parameters. Based on a L2 distance, <ref type="bibr" target="#b12">[13]</ref> formulates a method for joint learning of local and global feature selection losses particularly designed person Re-ID. In <ref type="bibr" target="#b22">[22]</ref>, a pose-driven deep convolutional model, which leverages the human part cues to alleviate the pose variations, is designed to learn feature extraction and matching models. Furthermore, both the fine and coarse pose information of the person <ref type="bibr" target="#b19">[19]</ref> are incorporated to learn a discriminative embedding. A part loss is proposed in <ref type="bibr" target="#b29">[29]</ref>, which automatically detects human body parts and computes the person classification loss on each part separately. Chen et al. <ref type="bibr" target="#b3">[4]</ref> develop a CNN-based appearance model to jointly learn scale-specific features and maximize multiscale feature fusion selections. Several part regions are first detected and then deep neural networks are designed for representation learning on both the local and global regions <ref type="bibr" target="#b27">[27]</ref>. Part-based Convolutional Baseline (PCB) <ref type="bibr" target="#b24">[24]</ref> outputs a convolutional descriptor consisting of several part-level features and then a refined part pooling method is used to re-assign outliers in the parts. Based on PCB, Multiple Granularity Network (MGN) <ref type="bibr" target="#b26">[26]</ref> explores a branch for global features and two branches for local representations for person re-identificaiton.</p><p>Non part-based methods: Recently, a completely synthetic dataset <ref type="bibr" target="#b0">[1]</ref> and some adversarially occluded samples <ref type="bibr" target="#b7">[8]</ref> are constructed to train the re-identification model. In  <ref type="figure">Figure 2</ref>. The architecture of our proposed pyramidal model for person re-identification. For better layout, only the spatial profile of the member branch in the pyramid is shown, which is originally 3-dimensional tensors. We assume that the original feature map is divided into 6 basic sub-maps, while other number of sub-maps can be used as well. The branch always consists of several consecutive basic sub-maps and the basic operation for each branch will be given in <ref type="figure">Fig. 3</ref>.</p><p>[23], singular vector decomposition is used to iteratively integrate the orthogonality constraint in CNN training for image retrieval. A pedestrian alignment network <ref type="bibr" target="#b37">[37]</ref> is built to learn discriminative embedding and pedestrian alignment without extra annotations. Geng et al. <ref type="bibr" target="#b5">[6]</ref> propose a number of deep transfer learning models to address the data sparsity problem and transfer knowledge from auxiliary datasets. <ref type="bibr" target="#b6">[7]</ref> shows that a plain CNN with a triplet loss can outperform most recent published methods by a large margin. Learning binary representation for fast matching <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b32">32]</ref> is also a promising direction of object re-identification. In <ref type="bibr" target="#b20">[20]</ref>, A group-shuffling random walk network is proposed to refine the probe-to-gallery affinities based on gallery-to-gallery affinities. The "local similarity" metrics for image pairs are learned with considering dependencies from all the images in a group, forming "group similarities" in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Multi-task learning: Self-paced learning <ref type="bibr" target="#b9">[10]</ref> and focal loss <ref type="bibr" target="#b14">[15]</ref> both train models by diversely weighting the samples in different learning stages. Inspired by this, in <ref type="bibr" target="#b10">[11]</ref>, a task-oriented regularizer is designed to jointly prioritize both tasks and instances. The multiple loss functions <ref type="bibr" target="#b8">[9]</ref> are weighted by considering the uncertainty of tasks in both classification and regression settings. Moreover, a routing network consisting of two components is introduced to dynamically select different functions in response to the input <ref type="bibr" target="#b18">[18]</ref>. While, Chen et al. <ref type="bibr" target="#b4">[5]</ref> propose a gradient normalization algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We learn the embedding for person Re-ID by simultaneously minimizing a list-wise metric loss and a classification loss with two types of sampling strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Coarse-to-Fine Pyramidal Model</head><p>In this section, we propose a novel coarse-to-fine pyramidal model which can moderately relax the requirement of detection model and smoothly incorporate the global information, simultaneously. It is worth noting that, not only local and global information is integrated but the gradual transition process between them is also incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pyramidal Branches</head><p>Given a set of images X = {I 1 , · · · , I N } containing persons captured by cameras in surveillance systems where N is the number of images, the task of person Re-IDentification (Re-ID) is to associate the images of the same person at different times and locations. Our model is built on a feature map M extracted by a backbone network BN . Thus, we have a 3-dimensional tensor M = BN (I) of the size C × H × W , where C is the number of encoded channels and W and H are the spatial width and height of the tensor, respectively.</p><p>First, we divide the feature map into n number of parts according to the spatial height axis and thus each basic part has the size of C × (H/n) × W . Suppose that H can be divisible by n. Thus, our pyramidal model is constructed according to the following rules: 1) In the bottom level (l = 1) of the pyramid, there are n number of branches in which one corresponds to a basic part. 2) The branches in higher level has one more adjacent basic part than that of previous lower level. 3) The sliding step for all levels is set to one. It means the number of branches in the current level is just one less than that of previous level. 4) In the top level (l = n) of the pyramid, there is only one branch which is just the original feature map M . Therefore, we assume that P{l, k} is the kth sub-map in the lth level of the pyramid model P defined as:</p><formula xml:id="formula_0">P = {M (1 : C, st : ed, 1 : W ) :</formula><p>(1) st = (k − 1) * H/n + 1, ed = (k − 1) * H/n + l * H/n, l = 1, · · · , n, k = 1, · · · , n − l + 1}, where 1 : C means that all elements from index 1 to index C are selected. Obviously, P is a set of 3-dimensional submaps with a specific coarse-to-fine architecture, in which each member captures the discriminative information of different spatial scales. Moreover, the pyramidal model contains both the global feature map M ∈ P and the part-based model: PCB. While, it is easy to know that there are totally n l=1 l number of components in P and the lth level has n − l + 1 number of components, where the level index is in a fine-to-coarse fashion. The details of the proposed architecture are shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Basic Operations</head><p>For each branch P{l, k} in pyramid P, first, a global maximum pooling (GMP) and a global average pooling (GAP) is separately executed to capture the statistical properties of different channels in the sub-maps. Then, the two statistical variables are added to form a vector with the same size of encoded channels. Third, a convolutional layer followed by a batch normalization and a ReLU activation is explored to reduce the dimension and produce a feature vector x(l, k) ∈ R D for the re-identification task. Simply, we denote x(l, k) = BO(P{l, k}). Fourth, to make the feature vector capable of sufficient discriminativity, a softmax based IDentification loss (ID loss) will be used for a fully connected layer which considers the feature map as the input. At the same time, a triplet loss will be imposed on a vector x = (x(1, 1) T , · · · , x(n, 1) T ) T which concatenates all the feature vectors of different branches in the pyramid P. The basic operations for each branch will be executed independently with respect to different components in the pyramid. Finally, all the parameters will be learned by simultaneously minimizing the two losses in a dynamic way. A branch example consisting of two consecutive basic parts is illustrated in <ref type="figure">Fig. 3</ref>.</p><p>If assume that f (·) refers to all the operations in the embedding including BN , P and BOs, we can denote the feature as x = f (I) simply. In the inference stage, the reidentification task will be achieved by ranking the distances {d(x, x i ) : x = f (I), I i ∈ G} between a query I and a gallery G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Loss Dynamic Training</head><p>Recent studies demonstrate that multi-task learning has the capabilities to achieve advanced performance by extracting appropriate shared information between tasks. The potential reason is that multiple tasks can benefit from each other by exploring the relatedness, leading to boosted generalization performance.  <ref type="figure">Figure 3</ref>. The illustration of basic operations for a branch consisting of two consecutive basic sub-maps, including a global maximum pooling, a global average pooling, a convolutional filter, a batch normalization, a ReLu activation and a linear fully connected layer. These operations for different branches will be executed independently and the features of all branches will be finally concatenated for the triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Two Tasks</head><p>To learn the discriminative features, we adopt two related tasks but emphasizing different aspects to learn the parameters of the embedding f , including an identification loss and a triplet loss. The first one is point-wise classification loss while the second one is for list-wise metric learning.</p><p>Identification loss: Generally, the identification loss is the same as the classification loss defined as:</p><formula xml:id="formula_1">L id = 1 N id i k,l S((W kl c ) T x i (l, k)) (2) = 1 N id i k,l − log (W kl c ) T x i (l, k) j (W kl j ) T x i (l, k) ,</formula><p>where N id is the number of used images, c denotes the corresponding identity of the input image I i , S is the softmax function and W kl c is the weight matrix of the fully connected layer for cth identity in the (l, k) branch.</p><p>Triplet loss: Given a triplet of samples (I, I p , I n ) where I and I p are of the same identity whilst I and I n are the images for different identities, the aim of embedding is to learn a new feature space in which the distance between the sample pair I and I p will be smaller than that between the pair I and I p . Intuitively, a triplet loss can be defined as:</p><formula xml:id="formula_2">L tp = 1 N tp I,Ip,In [d(f (I), f (I p )) (3) −d(f (I), f (I n )) + δ] + ,</formula><p>where δ is a margin hyper-parameter to control the distance differences, N tp is the number of available triplets and [·] + = max(·, 0) is the hinge loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dynamic Training</head><p>The above two tasks are not novel and popularly used in various applications. While, how to integrate them is still  an open problem. Actually, from the general perspective, most multi-task methods normally weight the tasks using balancing parameters and treat some tasks as the regularization items. In the learning stage, the balancing parameters are fixed during entire training process. 1) The performance strictly lies in an appropriate parameter but choosing an appropriate parameter is undoubtedly a labor-intensive and tricky work. 2) The difficulty of different tasks actually changes when the models are updated gradually, resulting in really varied appropriate parameters for different iterations. Furthermore, from the view of the re-identification task, to some extent, the two tasks are also conflicting when they are directly combined. On the one hand, effective triplets are rare, if the general random mini-batch sampling is used, making that the triplet loss contributes little in the learning procedure. This is because the number of identities is large but the number of images for each identity is small. On the other hand, to avoid the problem, we propose an IDbalanced sampling strategy to make sure triplets do exist in the mini-batches. However, this strategy suppresses the identification loss since fewer identities can be used in each mini-batch. Due to the sampling bias, it is possible that some images cannot be used all the time. Therefore, directly arithmetic weighting the losses would be very simple but obviously result in many difficulties in optimization.</p><p>Sampling: To solve the problem, alternatively, we choose to dynamically minimize the two losses incorporating two sampling methods accordingly: random sampling and ID-balanced hard triplet sampling. Random sampling is easy to be implemented while ID-balanced hard triplet sampling is implemented according to the following steps. To build the effective triplets, we randomly select 8 number of identities for each mini-batch, in which 8 images of each identity are randomly chosen. Hence, this strategy definitely enables to use the hard positive/negative mining based on the largest intra-class (identity) distance and the smallest inter-class distance. However, the samples for different identities are unbalanced and those whose number of images is less than 8 will never be used.</p><p>Dynamic weighting: For each loss, we define a performance measure to estimate the likelihood of a loss reduction. Suppose L t τ be the average loss in the current training iteration τ for the task t ∈ {id, tp}. Thus, we can calculate k t τ to be an exponential moving average according to:</p><formula xml:id="formula_3">k t τ = αL t τ + (1 − α)k t τ −1 ,<label>(4)</label></formula><p>where α ∈ [0, 1] is a discount factor and k t −1 = L t 0 . Based on the quantity k t τ , we defined a probability to describe the likelihood of a loss reduction as:</p><formula xml:id="formula_4">p t τ = min{k t τ , k t τ −1 } k t τ −1 .<label>(5)</label></formula><p>In case of loss increasing occasionally, the function min is used to normalize p t τ to be 1. Obviously, p t τ = 1 means the current optimization step didn't reduce the loss yet. The larger the value, the greater the probability that the optimization of the task t steps into a local minimum. Similar to the Focal Loss which down-weights easier samples and concentrates on hard samples, we define a measure (F L(·)) to weight the losses:</p><formula xml:id="formula_5">F L(p t τ , γ) = −(1 − p t τ ) γ log(p t τ ),<label>(6)</label></formula><p>where γ is used to control the focusing intensity. F L(p t τ , γ) is designed to weight tasks and choose the desire loss to be optimized. Thus, the overall objective can be rewritten as:</p><formula xml:id="formula_6">L = t∈{id,tp} F L(p t τ , γ)L t τ .<label>(7)</label></formula><p>Due to the different sampling strategies, we optimize the ID loss in Eq. 2 with randomly selected mini-batches, when F L(p id τ , γ) dominates the two tasks (F L(p tp τ , γ)/F L(p id τ , γ) &lt; δ). Thus, we start our dynamic optimization system from simply minimizing the ID loss. Actually, F L(p id τ , γ) always dominates in the early optimization since each step can greatly reduce the ID loss. Moreover, because the model is currently in an immature status, all samples are equally difficult so that hard sampling-based triplet loss cannot play essential role for our optimization. This is similar to the scheme of self-paced (curriculum learning) leaning <ref type="bibr" target="#b9">[10]</ref> in which easier samples are first trained and hard samples are considered later while here dynamically optimizing the two tasks plays the same role. In this case, the both losses L t τ : t ∈ {id, tp} in the objective Eq. 7 will be calculated.</p><p>When F L(p tp τ , γ) dominates in the optimization, the overall objective 7 considering both Eq. 2 and 3 will be directly optimized because ID-balanced sampling will not influence the use of ID loss. This optimization successfully avoids the tortuous parameter tuning and seamlessly incorporates the ideas of both ID-balanced hard triplet sampling and curriculum learning to further improve the performance. The flowchart of dynamic training is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref> and details of training are given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Loss Dynamic Training</head><p>Input: Dataset X, pretrained backbone network BN and hyperparameters (n, α, δ, γ, D). Output: The embedding function x = f (I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization:</head><p>Initiate the network parameters except for the backbone. Set p id τ = 0 and p tp τ = 1. </p><formula xml:id="formula_7">for τ = 1, · · · , Nτ Calculate F L(p tp τ , γ) and F L(p id τ , γ). if F L(p tp τ , γ)/F L(p id τ , γ) &lt; δ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>To verify the proposed method, we test it on three popular used person re-identification datasets: Market-1501 <ref type="bibr" target="#b34">[34]</ref>, DukeMTMC-reID <ref type="bibr" target="#b17">[17]</ref> and CUHK03 <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Implementation details: All images are resized into a resolution of 384 × 128 which is the same as that of PCB. The ResNet model with the pretrained parameters on Ima-geNet is considered as the backbone network in our system. For the feature map, the number of encoded channels is 2048 while the feature will be reduced to a 128-dimensional vector using a convolutoinal layer. We set the number of basic parts to 6 so there are 21 branches in the pyramid according to construction rules. The margin in the triplet loss is 1.4 in all our experiments. We select a mini-batch of 64 images for each iteration. Stochastic gradient descent (SGD) with two sampling strategies is used in our optimization, where the momentum and the weight decay factor is set to 0.9 and 0.0005, respectively. Totally, the proposed model will be trained 120 epochs. As for the learning rate, the initial learning rate is set to 0.01 while the learning rate will be dropped by half every 10 epochs from epoch 60 to epoch 90. While, for the dynamic training, we set the parameters δ = 0.16, α = 0.25 and γ = 2, according to the suggestions in <ref type="bibr" target="#b14">[15]</ref>. All the experiments in this paper will follow the same setting.</p><p>Evaluation metrics: To compare the re-identification performance of the proposed method with the existing advanced methods, we adopt the Cumulative Matching Characteristics (CMC) at rank-1, rank-5 and rank-10, and mean Average Precision (mAP) on all the datasets. It is worth not-ing that all our results are obtained in a single-query setting and, more importantly, re-ranking algorithm is not used to improve the mAP in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Market-1501: In this dataset, 32, 668 images of 1, 501 identities with annotated bounding boxes detected using the pedestrian detector of Deformable Part Model (DPM) are collected. View overlapping exists among different cameras, including 5 high-resolution cameras, and a lowresolution camera. Following the setting of PCB, we divide the dataset into a training set with 12, 936 images of 751 persons and a testing set of 750 persons containing 3, 368 query images and 19, 732 gallery images.</p><p>DukeMTMC-reID: Following the protocol <ref type="bibr" target="#b36">[36]</ref> of the Market-1501 dataset, this dataset is a subset of the DukeMTMC dataset specifically collected for person reidentification. In this dataset, 1, 404 identities appears in more than two cameras while 408 (distractor) identities appears in only one camera. We divide the dataset into a training set of 16, 522 images with 702 identities and a testing set which consists of 2, 228 query images of the other 702 identities and 17, 661 gallery images of 702 identities plus 408 distractor identities.</p><p>CUHK03: We follow the new protocol <ref type="bibr" target="#b38">[38]</ref> similar to that of Market-1501, which splits the CUHK03 dataset into training set of 767 identities and testing set of 700 identities. From each camera, one image is selected as the query for each identity and the rest of images are used to construct the gallery set. This dataset has two ways of annotating bounding box including labelled by human or detected by a detector. The labelled dataset includes 7, 368 training, 1, 400 query and 5, 328 gallery images while detected dataset consists of 7, 365 training, 1, 400 query and 5, 332 gallery images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head><p>In this section, we compare the proposed method called "Pyramid-ours" with 26 state-of-the-art methods, most of which is proposed in the last year, on the three datasets including Market-1501, DukeMTMC-reID and CUHK03. For the comparison of each dataset, we detail the following.</p><p>Market-1501: For this dataset, we divide the compared methods into two groups: including part-based and global methods, and the comparisons are given in Tab. 1. The results clearly show that local-based methods generally get better evaluation scores than that of these methods extracting global features only. The PCB is a convolutional baseline that motivates our approach, but we have improved performance by 10.8% and 3.4% on metrics mAP and rank 1, respectively. MGN considers multiple branches as well but ignores the gradual cues between global and local information. Our method achieves the same result with MGN on  <ref type="table">Table 1</ref>. Comparison results (%) on Market-1501 dataset at 4 evaluation metrics: mAP, rank 1, rank 5 and rank 10 where the bold font denotes the best method. "*" denotes that the method needs auxiliary part labels. We divide other compared methods into two groups: the methods exploring part-based features and the methods extracting global features. Our proposed pyramid model achieves the best results on all the 4 evaluation metrics.</p><p>Method mAP rank 1</p><p>Pyramid-ours 79.0 89.0 MGN <ref type="bibr" target="#b26">[26]</ref> 78.4 88.7 SVDNet <ref type="bibr" target="#b23">[23]</ref> 56.8 76.7 AOS <ref type="bibr" target="#b7">[8]</ref> 62.1 79.2 HA-CNN <ref type="bibr" target="#b13">[14]</ref> 63.8 80.5 GSRW <ref type="bibr" target="#b20">[20]</ref> 66.4 80.7 DuATM <ref type="bibr" target="#b21">[21]</ref> 64.6 81.8 PCB+RPP <ref type="bibr" target="#b24">[24]</ref> 69.2 83.3 PSE+ECN <ref type="bibr" target="#b19">[19]</ref> 75.7 84.5 DNN-CRF <ref type="bibr" target="#b2">[3]</ref> 69.5 84.9 GP-reid <ref type="bibr" target="#b28">[28]</ref> 72.8 85.2  <ref type="table">Table 3</ref>. Comparison results (%) on CUHK03 dataset using the new protocol in <ref type="bibr" target="#b38">[38]</ref>. For the labelled set, the results of our model at rank 5 and rank 10 are 91.0% and 94.4%, respectively, while they are 90.7% and 94.5% for the detected dataset. This dataset is the most difficult one by comparing the average performance of all methods. The proposed pyramid model outperforms all the other state-of-the-art methods with large margins. metric rank 1 but exceeds it 1.3% on metric mAP. In comparison, the performances of other algorithms are similar with each other on metric rank 10 but all of them are much worse on metrics mAP and rank 1 than ours. DukeMTMC-reID: From Tab. 2, we can see that our method also achieves the best results on this dataset at both metrics mAP and rank 1. Among the compared methods, MGN is the closest method to our method score, but still below 0.6% mAP score. PSE+ECN which is a method using a pose-sensitive embedding and the re-ranking procedure also performs worse than ours (75.7% vs. 79.0%). Similar to the comparison on Market-1501 dataset, our pyramid model exceeds PCB+RPP 9.8% and 5.7% at metrics mAP and rank 1, respectively. We provide the achievements of our method at metrics rank 5 (94.7% ) and rank 10 (96.3% ) for comparison in the future.</p><p>CUHK03: This dataset is the most challenging dataset under the new protocol and the bounding boxes are annotated using two ways. While, from Tab. 3, we can see that our proposed approach has achieved the most outstanding results for these two annotation ways. On this datasst, the pyramid model outperforms all other methods at least 8.8% and 10.9%, respectively.</p><p>Furthermore, on Market-1501 dataset, we compare our model with PCB using the same sampling strategy and some retrieved examples are shown in <ref type="figure">Fig. 5</ref>. We can see that the PCB cannot respond well to the challenge of inaccurate bounding boxes. Taking the first query as an example, our model is able to find three images of the same identity in the top 11 results whilst PCB could not search anyone. While, from the second query, we can see that the lower-body parts (blue eclipse) of retrieved images match to the upper-body part of query, due to the imprecise detection.</p><p>In summary, our proposed pyramid using the novel multi-loss dynamic training can always be superior to all other existing advanced methods, no matter which evaluation metric is used. Through the comparative experiment on the three datasets, it is easy to know that CUHK03 dataset with the new protocol would be the most challenging one because all methods make worse results on it. However, our method can consistently outperform all other algorithms by a large margin. Therefore, we can conclude that our method particularly specializes in challenging problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Component Analysis</head><p>To further investigate every component in the pyramid model, we conduct comprehensive ablation studies on the performance of different sub-models. The results at the metrics: mAP, rank 1, rank 5 and rank 10 are shown in Tab. 4 and each result is obtained with only one setting changed and the rest being the same as the default.</p><p>First, we merely use the part of branches in the pyramid to test the function. In the term "Pyramid-000001", the left Query Top 11 retrieved images <ref type="figure">Figure 5</ref>. Examples of retrieved images by two methods: Pyramidours and PCB, in case of the imprecise detection. For each query, the images in first row are returned by our proposed method while the images in second row are searched by PCB. The green/red rectangles indicate that images have the same/different identities as the query and the blue eclipse marks the similar contexts. number denotes whether the branches in low level are used or not while the rightest number is for the global branch. For example, "Pyramid-000001" means only the global branch in the highest level of the pyramid is used. From this table, we can obtain: 1) The local branches in the lower levels play more important roles than that of the global branches (84.9% vs. 82.1%).</p><p>2) The more branches we use, the better the performance.</p><p>3) The only global branch plus the proposed dynamic training strategy can achieve better results than that of PCA+RPP. It clearly shows the dynamic training strategy is able to improve the capacities of the model. Second, the features of different dimensions are also analyzed. Compared to the default dimension 128, the features of dimension 64 and 256 both achieve worse results. It shows that the redundant information plays negative influence to the performance while too short feature cannot provide sufficient discriminative cues. In summary, the performance is relatively (≤ 1.3%) stable with the change of the feature dimension. While, in the case of resource-limited application, 64-dimensional feature is a more acceptable choice.</p><p>Finally, we fix the dynamic balance parameter to 0 and alternately execute the two sampling strategies to train the identification loss. It means that the triplet loss will never be used in this experiment. In one step, mini-batch is selected using random sampling while ID-balance hard sampling is adopted in the next step. We can see that the overall performance is a little bit lower than that of the default setting of our proposed model but still much higher than that of  <ref type="table">Table 4</ref>. Results (%) of sub-models on Market-1501 dataset. In the term "Pyramid-000001", '0' means the corresponding level of pyramid is not used while '1' means that is used. "Feature-64" denotes the dimension of features for each branch is set 64. "No triplet loss" refers to that only identification loss is optimized.</p><p>PCB-RPP. It demonstrates the new pyramid model and the dynamic sampling strategy contribute most for the performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we construct a coarse-to-fine pyramid model for person re-identification via a novel dynamic training scheme. Our model relaxes the requirement of detection models and thus achieves advanced results on benchmark. Specially, our model outperforms the existing best method by a large margin on CUHK03 dataset which is the most challenging dataset under the new protocol of <ref type="bibr" target="#b38">[38]</ref>. It is worth noting that all our results are achieved in a singlequery setting without using any re-ranking algorithms. In the future, it will be interesting to jointly learn the detection and re-identification models in an integrated framework. The two tasks are highly related and Re-ID models can be improved by means of attention maps in the detection models. Moreover, the middle layer features in the backbone can be incorporated into the proposed pyramid model as well to further improve the Re-ID performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Dynamic training for two related tasks with two sampling strategies (i.e., ID-balanced hard triplet and randomized).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Perform random mini-batch sampling. Forward though BN and construct the pyramid P. Apply batch-normalization and calculate x = fτ (I).</figDesc><table><row><cell>Optimize the objective in Eq. 2.</cell></row><row><cell>else</cell></row><row><cell>Perform ID-balanced hard triplet sampling.</cell></row><row><cell>Forward though BN and construct the pyramid P.</cell></row><row><cell>Apply batch-normalization and calculate x = fτ (I).</cell></row><row><cell>Optimize the objective in Eq. 7.</cell></row><row><cell>end if</cell></row><row><cell>Backpropagate the gradients and update the parameters.</cell></row><row><cell>end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison results (%) on DukeMTMC-reID dataset. The results of our proposed pyramid model at rank 5 and rank 10 are 94.7% and 96.3%, respectively, which achieve the state-ofthe-art.</figDesc><table><row><cell>Method</cell><cell cols="4">Labelled mAP rank 1 mAP rank 1 Detected</cell></row><row><cell>Pyramid-ours</cell><cell>76.9</cell><cell>78.9</cell><cell>74.8</cell><cell>78.9</cell></row><row><cell>MGN [26]</cell><cell>67.4</cell><cell>68.0</cell><cell>66.0</cell><cell>68.0</cell></row><row><cell>PCB+RPP [24]</cell><cell>-</cell><cell>-</cell><cell>57.5</cell><cell>63.7</cell></row><row><cell>MLFN [2]</cell><cell>49.2</cell><cell>54.7</cell><cell>47.8</cell><cell>52.8</cell></row><row><cell>HA-CNN [14]</cell><cell>41.0</cell><cell>44.4</cell><cell>38.6</cell><cell>41.7</cell></row><row><cell>SVDNet [23]</cell><cell>37.8</cell><cell>40.9</cell><cell>37.3</cell><cell>41.5</cell></row><row><cell>PAN [37]</cell><cell>35.0</cell><cell>36.9</cell><cell>34</cell><cell>36.3</cell></row><row><cell>IDE [35]</cell><cell>21.0</cell><cell>22.2</cell><cell>19.7</cell><cell>21.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Our work was supported in part by the Science and Technology Innovation Committee Foundation of Shenzhen </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Aleksander Rognhaugen, and Theoharis Theoharis. Looking beyond appearances: Synthetic training data for deep cnns in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">Barros</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03153v1</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person reidentification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyue</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737v4</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarially occluded samples for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selfpaced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-paced multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person reidentification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu Shengand Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai Yiand Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Routing networks adaptive selection of non-linear functions for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep group-shuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multiregion bilinear convolutional neural networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05300v5</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01438v3</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glad: Global-local-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11042v1</idno>
		<title level="m">Towards good practices on building effective cnn baseline model for person reidentification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00798v2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast vehicle identification via ranked semantic sampling based embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning cross-view binary identities for fast person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Person re-identification: Pastand present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984v1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
