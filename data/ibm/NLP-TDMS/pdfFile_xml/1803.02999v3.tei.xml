<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On First-Order Meta-Learning Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
							<email>jachiam@openai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Schulman</forename><surname>Openai</surname></persName>
						</author>
						<title level="a" type="main">On First-Order Meta-Learning Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only firstorder derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While machine learning systems have surpassed humans at many tasks, they generally need far more data to reach the same level of performance. For example, Schmidt et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b14">15]</ref> showed that human subjects can recognize new object categories based on a few example images. Lake et al. <ref type="bibr" target="#b11">[12]</ref> noted that on the Atari game of Frostbite, human novices were able to make significant progress on the game after 15 minutes, but double-dueling-DQN <ref type="bibr" target="#b18">[19]</ref> required more than 1000 times more experience to attain the same score.</p><p>It is not completely fair to compare humans to algorithms learning from scratch, since humans enter the task with a large amount of prior knowledge, encoded in their brains and DNA. Rather than learning from scratch, they are fine-tuning and recombining a set of pre-existing skills. The work cited above, by Tenenbaum and collaborators, argues that humans' fast-learning abilities can be explained as Bayesian inference, and that the key to developing algorithms with human-level learning speed is to make our algorithms more Bayesian. However, in practice, it is challenging to develop (from first principles) Bayesian machine learning algorithms that make use of deep neural networks and are computationally feasible.</p><p>Meta-learning has emerged recently as an approach for learning from small amounts of data. Rather than trying to emulate Bayesian inference (which may be computationally intractable), meta-learning seeks to directly optimize a fast-learning algorithm, using a dataset of tasks. Specifically, we assume access to a distribution over tasks, where each task is, for example, a classification problem. From this distribution, we sample a training set and a test set of tasks. Our algorithm is fed the training set, and it must produce an agent that has good average performance on the test set. Since each task corresponds to a learning problem, performing well on a task corresponds to learning quickly.</p><p>A variety of different approaches to meta-learning have been proposed, each with its own pros and cons. In one approach, the learning algorithm is encoded in the weights of a recurrent network, but gradient descent is not performed at test time. This approach was proposed by Hochreiter et al. <ref type="bibr" target="#b7">[8]</ref> who used LSTMs for next-step prediction and has been followed up by a burst of recent work, for example, Santoro et al. <ref type="bibr" target="#b15">[16]</ref> on few-shot classification, and Duan et al. <ref type="bibr" target="#b2">[3]</ref> for the POMDP setting.</p><p>A second approach is to learn the initialization of a network, which is then fine-tuned at test time on the new task. A classic example of this approach is pretraining using a large dataset (such as ImageNet <ref type="bibr" target="#b1">[2]</ref>) and fine-tuning on a smaller dataset (such as a dataset of different species of bird <ref type="bibr">[20]</ref>). However, this classic pre-training approach has no guarantee of learning an initialization that is good for fine-tuning, and ad-hoc tricks are required for good performance. More recently, Finn et al. <ref type="bibr" target="#b3">[4]</ref> proposed an algorithm called MAML, which directly optimizes performance with respect to this initialization-differentiating through the fine-tuning process. In this approach, the learner falls back on a sensible gradient-based learning algorithm even when it receives out-of-sample data, thus allowing it to generalize better than the RNN-based approaches <ref type="bibr" target="#b4">[5]</ref>. On the other hand, since MAML needs to differentiate through the optimization process, it's not a good match for problems where we need to perform a large number of gradient steps at test time. The authors also proposed a variant called first-order MAML (FOMAML), which is defined by ignoring the second derivative terms, avoiding this problem but at the expense of losing some gradient information. Surprisingly, though, they found that FOMAML worked nearly as well as MAML on the Mini-ImageNet dataset <ref type="bibr" target="#b17">[18]</ref>. (This result was foreshadowed by prior work in meta-learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> that ignored second derivatives when differentiating through gradient descent, without ill effect.) In this work, we expand on that insight and explore the potential of meta-learning algorithms based on first-order gradient information, motivated by the potential applicability to problems where it's too cumbersome to apply techniques that rely on higher-order gradients (like full MAML).</p><p>We make the following contributions:</p><p>• We point out that first-order MAML <ref type="bibr" target="#b3">[4]</ref> is simpler to implement than was widely recognized prior to this article.</p><p>• We introduce Reptile, an algorithm closely related to FOMAML, which is equally simple to implement. Reptile is so similar to joint training (i.e., training to minimize loss on the expecation over training tasks) that it is especially surprising that it works as a meta-learning algorithm. Unlike FOMAML, Reptile doesn't need a training-test split for each task, which may make it a more natural choice in certain settings. It is also related to the older idea of fast weights / slow weights <ref type="bibr" target="#b6">[7]</ref>.</p><p>• We provide a theoretical analysis that applies to both first-order MAML and Reptile, showing that they both optimize for within-task generalization.</p><p>• On the basis of empirical evaluation on the Mini-ImageNet <ref type="bibr" target="#b17">[18]</ref> and Omniglot <ref type="bibr" target="#b10">[11]</ref> datasets, we provide some insights for best practices in implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Meta-Learning an Initialization</head><p>We consider the optimization problem of MAML <ref type="bibr" target="#b3">[4]</ref>: find an initial set of parameters, φ, such that for a randomly sampled task τ with corresponding loss L τ , the learner will have low loss after k updates. That is:</p><formula xml:id="formula_0">minimize φ E τ L τ U k τ (φ) ,<label>(1)</label></formula><p>where U k τ is the operator that updates φ k times using data sampled from τ . In few-shot learning, U corresponds to performing gradient descent or Adam <ref type="bibr" target="#b9">[10]</ref> on batches of data sampled from τ .</p><p>MAML solves a version of Equation (1) that makes on additional assumption: for a given task τ , the inner-loop optimization uses training samples A, whereas the loss is computed using test samples B. This way, MAML optimizes for generalization, akin to cross-validation. Omitting the superscript k, we notate this as</p><formula xml:id="formula_1">minimize φ E τ [L τ,B (U τ,A (φ))] ,<label>(2)</label></formula><p>MAML works by optimizing this loss through stochastic gradient descent, i.e., computing</p><formula xml:id="formula_2">g MAML = ∂ ∂φ L τ,B (U τ,A (φ)) (3) = U τ,A (φ)L τ,B ( φ), where φ = U τ,A (φ)<label>(4)</label></formula><p>In Equation (4), U τ,A (φ) is the Jacobian matrix of the update operation U τ,A . U τ,A corresponds to adding a sequence of gradient vectors to the initial vector, i.e., U τ,A (φ) = φ + g 1 + g 2 + · · · + g k . (In Adam, the gradients are also rescaled elementwise, but that does not change the conclusions.) Firstorder MAML (FOMAML) treats these gradients as constants, thus, it replaces Jacobian U τ,A (φ) by the identity operation. Hence, the gradient used by FOMAML in the outer-loop optimization is g FOMAML = L τ,B ( φ). Therefore, FOMAML can be implemented in a particularly simple way: (1) sample task τ ; (2) apply the update operator, yielding φ = U τ,A (φ); (3) compute the gradient at φ, g FOMAML = L τ,B ( φ); and finally (4) plug g FOMAML into the outer-loop optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reptile</head><p>In this section, we describe a new first-order gradient-based meta-learning algorithm called Reptile. Like MAML, Reptile learns an initialization for the parameters of a neural network model, such that when we optimize these parameters at test time, learning is fast-i.e., the model generalizes from a small number of examples from the test task. The Reptile algorithm is as follows:</p><formula xml:id="formula_3">Algorithm 1 Reptile (serial version)</formula><p>Initialize φ, the vector of initial parameters for iteration = 1, 2, . . . do Sample task τ , corresponding to loss L τ on weight vectors φ</p><formula xml:id="formula_4">Compute φ = U k τ (φ), denoting k steps of SGD or Adam Update φ ← φ + ( φ − φ) end for</formula><p>In the last step, instead of simply updating φ in the direction φ − φ, we can treat (φ − φ) as a gradient and plug it into an adaptive algorithm such as Adam <ref type="bibr" target="#b9">[10]</ref>. (Actually, as we will discuss in Section 5.1, it is most natural to define the Reptile gradient as (φ − φ)/α, where α is the stepsize used by the SGD operation.) We can also define a parallel or batch version of the algorithm that evaluates on n tasks each iteration and updates the initialization to</p><formula xml:id="formula_5">φ ← φ + 1 n n i=1 ( φ i − φ)<label>(5)</label></formula><p>where φ i = U k τ i (φ); the updated parameters on the i th task. This algorithm looks remarkably similar to joint training on the expected loss E τ [L τ ]. Indeed, if we define U to be a single step of gradient descent (k = 1), then this algorithm corresponds to stochastic gradient descent on the expected loss:</p><formula xml:id="formula_6">g Reptile,k=1 = E τ [φ − U τ (φ)] /α (6) = E τ [∇ φ L τ (φ)]<label>(7)</label></formula><p>However, if we perform multiple gradient updates in the partial minimization (k &gt; 1), then the expected update E τ U k τ (φ) does not correspond to taking a gradient step on the expected loss E τ [L τ ]. Instead, the update includes important terms coming from second-and-higher derivatives of L τ , as we will analyze in Section 5.1. Hence, Reptile converges to a solution that's very different from the minimizer of the expected loss E τ [L τ ].</p><p>Other than the stepsize parameter and task sampling, the batched version of Reptile is the same as the SimuParallelSGD algorithm <ref type="bibr">[21]</ref>. SimuParallelSGD is a method for communicationefficient distributed optimization, where workers perform gradient updates locally and infrequently average their parameters, rather than the standard approach of averaging gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Case Study: One-Dimensional Sine Wave Regression</head><p>As a simple case study, let's consider the 1D sine wave regression problem, which is slightly modified from Finn et al. <ref type="bibr" target="#b3">[4]</ref>. This problem is instructive since by design, joint training can't learn a very useful initialization; however, meta-learning methods can.</p><p>• The task τ = (a, b) is defined by the amplitude a and phase φ of a sine wave function f τ (x) = a sin(x + b). The task distribution by sampling a ∼ U ([0.1, 5.0]) and b ∼ U ([0, 2π]).</p><formula xml:id="formula_7">• Sample p points x 1 , x 2 , . . . , x p ∼ U ([−5, 5])</formula><p>• Learner sees (x 1 , y 1 ), (x 2 , y 2 ), . . . , (x p , y p ) and predicts the whole function f (x)</p><p>• Loss is 2 error on the whole interval [−5, 5]</p><formula xml:id="formula_8">L τ (f ) = 5 −5 dx f (x) − f τ (x) 2<label>(8)</label></formula><p>We calculate this integral using 50 equally-spaced points x.</p><p>First note that the average function is zero everywhere, i.e., E τ [f τ (x)] = 0, due to the random phase b. Therefore, it is useless to train on the expected loss E τ [L τ ], as this loss is minimized by the zero function f (x) = 0. On the other hand, MAML and Reptile give us an initialization that outputs approximately f (x) = 0 before training on a task τ , but the internal feature representations of the network are such that after training on the sampled datapoints (x 1 , y 1 ), (x 2 , y 2 ), . . . , (x p , y p ), it closely approximates the target function f τ . This learning progress is shown in the figures below. <ref type="figure" target="#fig_0">Figure 1</ref> shows that after Reptile training, the network can quickly converge to a sampled sine wave and infer the values away from the sampled points. As points of comparison, we also show the behaviors of MAML and a randomly-initialized network on the same task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we provide two alternative explanations of why Reptile works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Leading Order Expansion of the Update</head><p>Here, we will use a Taylor series expansion to approximate the update performed by Reptile and MAML. We will show that both algorithms contain the same leading-order terms: the first term minimizes the expected loss (joint training), the second and more interesting term maximizes within-task generalization. Specifically, it maximizes the inner product between the gradients on different minibatches from the same task. If gradients from different batches have positive inner product, then taking a gradient step on one batch improves performance on the other batch.</p><p>Unlike in the discussion and analysis of MAML, we won't consider a training set and test set from each task; instead, we'll just assume that each task gives us a sequence of k loss functions L 1 , L 2 , . . . , L k ; for example, classification loss on different minibatches. We will use the following definitions:</p><formula xml:id="formula_9">g i = L i (φ i ) (gradient obtained during SGD) (9) φ i+1 = φ i − αg i (sequence of parameter vectors)<label>(10)</label></formula><formula xml:id="formula_10">g i = L i (φ 1 ) (gradient at initial point)<label>(11)</label></formula><formula xml:id="formula_11">H i = L i (φ 1 ) (Hessian at initial point)<label>(12)</label></formula><p>For each of these definitions, i ∈ [1, k].</p><p>First, let's calculate the SGD gradients to O(α 2 ) as follows.</p><formula xml:id="formula_12">g i = L i (φ i ) = L i (φ 1 ) + L i (φ 1 )(φ i − φ 1 ) + O( φ i − φ 1 2 ) =O(α 2 ) (Taylor's theorem) (13) = g i + H i (φ i − φ 1 ) + O(α 2 ) (using definition of g i , H i ) (14) = g i − αH i i−1 j=1 g j + O(α 2 ) (using φ i − φ 1 = −α i−1 j=1 g j ) (15) = g i − αH i i−1 j=1 g j + O(α 2 ) (using g j = g j + O(α))<label>(16)</label></formula><p>Next, we will approximate the MAML gradient. Define U i as the operator that updates the parameter vector on minibatch i:</p><formula xml:id="formula_13">U i (φ) = φ − αL i (φ). g MAML = ∂ ∂φ 1 L k (φ k ) (17) = ∂ ∂φ 1 L k (U k−1 (U k−2 (. . . (U 1 (φ 1 ))))) (18) = U 1 (φ 1 ) · · · U k−1 (φ k−1 )L k (φ k ) (repeatedly applying the chain rule) (19) = I − αL 1 (φ 1 ) · · · I − αL k−1 (φ k−1 ) L k (φ k ) (using U i (φ) = I − αL i (φ)) (20) =   k−1 j=1 (I − αL j (φ j ))   g k (product notation, definition of g k )<label>(21)</label></formula><p>Next, let's expand to leading order</p><formula xml:id="formula_14">g MAML =   k−1 j=1 (I − αH j )     g k − αH k k−1 j=1 g j   + O(α 2 )<label>(22)</label></formula><p>(replacing L j (φ j ) with H j , and replacing g k using Equation <ref type="formula" target="#formula_0">(16)</ref></p><formula xml:id="formula_15">) =   I − α k−1 j=1 H j     g k − αH k k−1 j=1 g j   + O(α 2 ) (23) = g k − α k−1 j=1 H j g k − αH k k−1 j=1 g j + O(α 2 )<label>(24)</label></formula><p>For simplicity of exposition, let's consider the k = 2 case, and later we'll provide the general formulas.</p><formula xml:id="formula_16">g MAML = g 2 − αH 2 g 1 − αH 1 g 2 + O(α 2 ) (25) g FOMAML = g 2 = g 2 − αH 2 g 1 + O(α 2 )<label>(26)</label></formula><formula xml:id="formula_17">g Reptile = g 1 + g 2 = g 1 + g 2 − αH 2 g 1 + O(α 2 )<label>(27)</label></formula><p>As we will show in the next paragraph, the terms like H 2 g 1 serve to maximize the inner products between the gradients computed on different minibatches, while lone gradient terms like g 1 take us to the minimum of the joint training problem.</p><p>When we take the expectation of g FOMAML , g Reptile , and g MAML under minibatch sampling, we are left with only two kinds of terms which we will call AvgGrad and AvgGradInner. In the equations below E τ,1,2 [. . . ] means that we are taking the expectation over the task τ and the two minibatches defining L 1 and L 2 , respectively.</p><p>• AvgGrad is defined as gradient of expected loss.</p><formula xml:id="formula_18">AvgGrad = E τ,1 [g 1 ]<label>(28)</label></formula><p>(−AvgGrad) is the direction that brings φ towards the minimum of the "joint training" problem; the expected loss over tasks.</p><p>• The more interesting term is AvgGradInner, defined as follows:</p><formula xml:id="formula_19">AvgGradInner = E τ,1,2 H 2 g 1 (29) = E τ,1,2 H 1 g 2 (interchanging indices 1, 2) (30) = 1 2 E τ,1,2 H 2 g 1 + H 1 g 2 (averaging last two equations) (31) = 1 2 E τ,1,2 ∂ ∂φ 1 (g 1 · g 2 )<label>(32)</label></formula><p>Thus, (−AvgGradInner) is the direction that increases the inner product between gradients of different minibatches for a given task, improving generalization.</p><p>Recalling our gradient expressions, we get the following expressions for the meta-gradients, for SGD with k = 2:</p><formula xml:id="formula_20">E [g MAML ] = (1)AvgGrad − (2α)AvgGradInner + O(α 2 ) (33) E [g FOMAML ] = (1)AvgGrad − (α)AvgGradInner + O(α 2 ) (34) E [g Reptile ] = (2)AvgGrad − (α)AvgGradInner + O(α 2 )<label>(35)</label></formula><p>In practice, all three gradient expressions first bring us towards the minimum of the expected loss over tasks, then the higher-order AvgGradInner term enables fast learning by maximizing the inner product between gradients within a given task. Finally, we can extend these calculations to the general k ≥ 2 case:</p><formula xml:id="formula_21">g MAML = g k − αH k k−1 j=1 g j − α k−1 j=1 H j g k + O(α 2 ) (36) E [g MAML ] = (1)AvgGrad − (2(k − 1)α)AvgGradInner (37) g FOMAML = g k = g k − αH k k−1 j=1 g j + O(α 2 ) (38) E [g FOMAML ] = (1)AvgGrad − ((k − 1)α)AvgGradInner (39) g Reptile = −(φ k+1 − φ 1 )/α = k i=1 g i = k i=1 g i − α k i=1 i−1 j=1 H i g j + O(α 2 ) (40) E [g Reptile ] = (k)AvgGrad − 1 2 k(k − 1)α AvgGradInner<label>(41)</label></formula><p>As in the k = 2, the ratio of coefficients of the AvgGradInner term and the AvgGrad term goes MAML &gt; FOMAML &gt; Reptile. However, in all cases, this ratio increases linearly with both the stepsize α and the number of iterations k. Note that the Taylor series approximation only holds for small αk.  <ref type="figure">Figure 2</ref>: The above illustration shows the sequence of iterates obtained by moving alternately towards two optimal solution manifolds W 1 and W 2 and converging to the point that minimizes the average squared distance. One might object to this picture on the grounds that we converge to the same point regardless of whether we perform one step or multiple steps of gradient descent. That statement is true, however, note that minimizing the expected distance objective E τ [D(φ, W τ )] is different than minimizing the expected loss</p><formula xml:id="formula_22">objective E τ [L τ (f φ )].</formula><p>In particular, there is a high-dimensional manifold of minimizers of the expected loss L τ (e.g., in the sine wave case, many neural network parameters give the zero function f (φ) = 0), but the minimizer of the expected distance objective is typically a single point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Finding a Point Near All Solution Manifolds</head><p>Here, we argue that Reptile converges towards a solution φ that is close (in Euclidean distance) to each task τ 's manifold of optimal solutions. This is a informal argument and should be taken much less seriously than the preceding Taylor series analysis. Let φ denote the network initialization, and let W τ denote the set of optimal parameters for task τ . We want to find φ such that the distance D(φ, W τ ) is small for all tasks.</p><formula xml:id="formula_23">minimize φ E τ 1 2 D(φ, W τ ) 2<label>(42)</label></formula><p>We will show that Reptile corresponds to performing SGD on that objective. Given a non-pathological set S ⊂ R d , then for almost all points φ ∈ R d the gradient of the squared distance D(φ, S) 2 is 2(φ − P S (φ)), where P S (φ) is the projection (closest point) of φ onto S. Thus,</p><formula xml:id="formula_24">∇ φ E τ 1 2 D(φ, W τ ) 2 = E τ 1 2 ∇ φ D(φ, W τ ) 2 (43) = E τ [φ − P Wτ (φ)] , where P Wτ (φ) = arg min p∈Wτ D(p, φ)<label>(44)</label></formula><p>Each iteration of Reptile corresponds to sampling a task τ and performing a stochastic gradient update</p><formula xml:id="formula_25">φ ← φ − ∇ φ 1 2 D(φ, W τ ) 2 (45) = φ − (φ − P Wτ (φ)) (46) = (1 − )φ + P Wτ (φ).<label>(47)</label></formula><p>In practice, we can't exactly compute P Wτ (φ), which is defined as a minimizer of L τ . However, we can partially minimize this loss using gradient descent. Hence, in Reptile we replace W * τ (φ) by the result of running k steps of gradient descent on L τ starting with initialization φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Few-Shot Classification</head><p>We evaluate our method on two popular few-shot classification tasks: Omniglot <ref type="bibr" target="#b10">[11]</ref> and Mini-ImageNet <ref type="bibr" target="#b17">[18]</ref>. These datasets make it easy to compare our method to other few-shot learning approaches like MAML.</p><p>In few-shot classification tasks, we have a meta-dataset D containing many classes C, where each class is itself a set of example instances {c 1 , c 2 , ..., c n }. If we are doing K-shot, N -way classification, then we sample tasks by selecting N classes from C and then selecting K + 1 examples for each class. We split these examples into a training set and a test set, where the test set contains a single example for each class. The model gets to see the entire training set, and then it must classify a randomly chosen sample from the test set. For example, if you trained a model for 5-shot, 5-way classification, then you would show it 25 examples (5 per class) and ask it to classify a 26 th example.</p><p>In addition to the above setup, we also experimented with the transductive setting, where the model classifies the entire test set at once. In our transductive experiments, information was shared between the test samples via batch normalization <ref type="bibr" target="#b8">[9]</ref>. In our non-transductive experiments, batch normalization statistics were computed using all of the training samples and a single test sample. We note that Finn et al. <ref type="bibr" target="#b3">[4]</ref> use transduction for evaluating MAML.</p><p>For our experiments, we used the same CNN architectures and data preprocessing as Finn et al. <ref type="bibr" target="#b3">[4]</ref>. We used the Adam optimizer <ref type="bibr" target="#b9">[10]</ref> in the inner loop, and vanilla SGD in the outer loop, throughout our experiments. For Adam we set β 1 = 0 because we found that momentum reduced performance across the board. <ref type="bibr" target="#b0">1</ref> During training, we never reset or interpolated Adam's rolling moment data; instead, we let it update automatically at every inner-loop training step. However, we did backup and reset the Adam statistics when evaluating on the test set to avoid information leakage.</p><p>The results on Omniglot and Mini-ImageNet are shown in <ref type="table" target="#tab_0">Tables 1 and 2</ref>. While MAML, FOMAML, and Reptile have very similar performance on all of these tasks, Reptile does slightly better than the alternatives on Mini-ImageNet and slightly worse on Omniglot. It also seems that transduction gives a performance boost in all cases, suggesting that further research should pay close attention to its use of batch normalization during testing.   <ref type="bibr" target="#b3">[4]</ref>. 1 st -order MAML results were generated by the code for <ref type="bibr" target="#b3">[4]</ref> with the same hyper-parameters as MAML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparing Different Inner-Loop Gradient Combinations</head><p>For this experiment, we used four non-overlapping mini-batches in each inner-loop, yielding gradients g 1 , g 2 , g 3 , and g 4 . We then compared learning performance when using different linear combinations of the g i 's for the outer loop update. Note that two-step Reptile corresponds to g 1 + g 2 , and two-step FOMAML corresponds to g 2 .</p><p>To make it easier to get an apples-to-apples comparison between different linear combinations, we simplified our experimental setup in several ways. First, we used vanilla SGD in the inner-and outer-loops. Second, we did not use meta-batches. Third, we restricted our experiments to 5-shot, 5-way Omniglot. With these simplifications, we did not have to worry as much about the effects of hyper-parameters or optimizers. <ref type="figure">Figure 3</ref> shows the learning curves for various inner-loop gradient combinations. For gradient combinations with more than one term, we ran both a sum and an average of the inner gradients to correct for the effective step size increase. Accuracy g 1 1 2 * (g 1 + g 2 ) g 1 + g 2 g 2 1 3 * (g 1 + g 2 + g 3 ) g 1 + g 2 + g 3 g 3 1 4 * (g 1 + g 2 + g 3 + g 4 ) g 1 + g 2 + g 3 + g 4 g 4 <ref type="figure">Figure 3</ref>: Different inner-loop gradient combinations on 5-shot 5-way Omniglot.</p><p>As expected, using only the first gradient g 1 is quite ineffective, since it amounts to optimizing the expected loss over all tasks. Surprisingly, two-step Reptile is noticeably worse than two-step FOMAML, which might be explained by the fact that two-step Reptile puts less weight on AvgGradInner relative to AvgGrad (Equations (34) and (35)). Most importantly, though, all the methods improve as the number of mini-batches increases. This improvement is more significant when using a sum of all gradients (Reptile) rather than using just the final gradient (FOMAML). This also suggests that Reptile can benefit from taking many inner loop steps, which is consistent with the optimal hyper-parameters found for Section 6.1.  The experiments in this section look at the difference between shared-tail FOMAML, where the final inner-loop mini-batch comes from the same set of data as the earlier inner-loop batches, to separate-tail FOMAML, where the final mini-batch comes from a disjoint set of data. Viewing FOMAML as an approximation to MAML, separate-tail FOMAML can be seen as the more correct approach (and was used by Finn et al. <ref type="bibr" target="#b3">[4]</ref>), since the training-time optimization resembles the test-time optimization (where the test set doesn't overlap with the training set). Indeed, we find that separate-tail FOMAML is significantly better than shared-tail FOMAML. As we will show, shared-tail FOMAML degrades in performance when the data used to compute the meta-gradient (g FOMAML = g k ) overlaps significantly with the earlier batches; however, Reptile and separate-tail MAML maintain performance and are not very sensitive to the inner-loop hyperparameters. <ref type="figure" target="#fig_4">Figure 4a</ref> shows that when minibatches are selected by cycling through the training data (shared-tail, cycle), shared-tail FOMAML performs well up to four inner-loop iterations, but drops in performance starting at five iterations, where the final minibatch (used to compute g FOMAML = g k ) overlaps with the earlier ones. When we use random sampling instead (shared-tail, replacement), shared-tail FOMAML degrades more gradually. We hypothesize that this is because some samples still appear in the final batch that were not in the previous batches. The effect is stochastic, so it makes sense that the curve is smoother. <ref type="figure" target="#fig_4">Figure 4b</ref> shows a similar phenomenon, but here we fixed the inner-loop to four iterations and instead varied the batch size. For batch sizes greater than 25, the final inner-loop batch for shared-tail FOMAML necessarily contains samples from the previous batches. Similar to <ref type="figure" target="#fig_4">Figure 4a</ref>, here we observe that shared-tail FOMAML with random sampling degrades more gradually than shared-tail FOMAML with cycling.</p><p>In both of these parameter sweeps, separate-tail FOMAML and Reptile do not degrade in performance as the number of inner-loop iterations or batch size changes.</p><p>There are several possible explanations for above findings. For example, one might hypothesize that shared-tail FOMAML is only worse in these experiments because its effective step size is much lower than that of separate-tail FOMAML. However, <ref type="figure" target="#fig_4">Figure 4c</ref> suggests that this is not the case: performance was equally poor for every choice of step size in a thorough sweep. A different hypothesis is that shared-tail FOMAML performs poorly because, after a few inner-loop steps on a sample, the gradient of the loss for that sample does not contain very much useful information about the sample. In other words, the first few SGD steps might bring the model close to a local optimum, and then further SGD steps might simply bounce around this local optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Meta-learning algorithms that perform gradient descent at test time are appealing because of their simplicity and generalization properties <ref type="bibr" target="#b4">[5]</ref>. The effectiveness of fine-tuning (e.g. from models trained on ImageNet <ref type="bibr" target="#b1">[2]</ref>) gives us additional faith in these approaches. This paper proposed a new algorithm called Reptile, whose training process is only subtlely different from joint training and only uses first-order gradient information (like first-order MAML).</p><p>We gave two theoretical explanations for why Reptile works. First, by approximating the update with a Taylor series, we showed that SGD automatically gives us the same kind of second-order term that MAML computes. This term adjusts the initial weights to maximize the dot product between the gradients of different minibatches on the same task-i.e., it encourages the gradients to generalize between minibatches of the same task. We also provided a second informal argument, which is that Reptile finds a point that is close (in Euclidean distance) to all of the optimal solution manifolds of the training tasks.</p><p>While this paper studies the meta-learning setting, the Taylor series analysis in Section 5.1 may have some bearing on stochastic gradient descent in general. It suggests that when doing stochastic gradient descent, we are automatically performing a MAML-like update that maximizes the generalization between different minibatches. This observation partly explains why fine tuning (e.g., from ImageNet to a smaller dataset [20]) works well. This hypothesis would suggest that joint training plus fine tuning will continue to be a strong baseline for meta-learning in various machine learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Work</head><p>We see several promising directions for future work:</p><p>• Understanding to what extent SGD automatically optimizes for generalization, and whether this effect can be amplified in the non-meta-learning setting.</p><p>• Applying Reptile in the reinforcement learning setting. So far, we have obtained negative results, since joint training is a strong baseline, so some modifications to Reptile might be necessary.</p><p>• Exploring whether Reptile's few-shot learning performance can be improved by deeper architectures for the classifier.</p><p>• Exploring whether regularization can improve few-shot learning performance, as currently there is a large gap between training and testing error.</p><p>• Evaluating Reptile on the task of few-shot density modeling <ref type="bibr" target="#b13">[14]</ref>.</p><p>[20] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-based R-CNNs for fine-grained category detection. In European conference on computer vision, pages 834-849. Springer, 2014.</p><p>[21] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In Advances in neural information processing systems, pages 2595-2603, 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyper-parameters</head><p>For all experiments, we linearly annealed the outer step size to 0. We ran each experiment with three different random seeds, and computed the confidence intervals using the standard deviation across the runs. Initially, we tried optimizing the Reptile hyper-parameters using CMA-ES <ref type="bibr" target="#b5">[6]</ref>. However, we found that most hyper-parameters had little effect on the resulting performance. After seeing this result, we simplified all of the hyper-parameters and shared hyper-parameters between experiments when it made sense.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Demonstration of MAML and Reptile on a toy few-shot regression problem, where we train on 10 sampled points of a sine wave, performing 32 gradient steps on an MLP with layers 1 → 64 → 64 → 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-tail, cycling) FOMAML (shared-tail, replacement) FOMAML (shared-tail, cycling) (a) Final test performance vs. number of inner-loop iterations. -tail, cycling) FOMAML (shared-tail, replacement) FOMAML (shared-tail, cycling) (b) Final test performance vs. inner-loop batch size. Final test performance vs. outer-loop step size for sharedtail FOMAML with batch size 100 (full batches).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The results of hyper-parameter sweeps on 5-shot 5-way Omniglot.6.3 Overlap Between Inner-Loop Mini-BatchesBoth Reptile and FOMAML use stochastic optimization in their inner-loops. Small changes to this optimization procedure can lead to large changes in final performance. This section explores the sensitivity of Reptile and FOMAML to the inner loop hyperparameters, and also shows that FOMAML's performance significantly drops if mini-batches are selected the wrong way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6 : 3</head><label>63</label><figDesc>Hyper-parameters Section 6.3. All outer step sizes were linearly annealed to zero during training. Parameter Figure 4b Figure 4a Figure 4c Inner learning rate 3 × 10 −3 3 × 10 −3 3 × 10 −Inner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on Mini-ImageNet. Both MAML and 1 st -order MAML results are from<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell>1-shot 5-way</cell><cell>5-shot 5-way</cell><cell></cell></row><row><cell cols="2">MAML + Transduction</cell><cell cols="2">48.70 ± 1.84% 63.11 ± 0.92%</cell><cell></cell></row><row><cell cols="4">1 st -order MAML + Transduction 48.07 ± 1.75% 63.15 ± 0.91%</cell><cell></cell></row><row><cell>Reptile</cell><cell></cell><cell cols="2">47.07 ± 0.26% 62.74 ± 0.37%</cell><cell></cell></row><row><cell cols="2">Reptile + Transduction</cell><cell cols="2">49.97 ± 0.32% 65.99 ± 0.58%</cell><cell></cell></row><row><cell>Algorithm</cell><cell>1-shot 5-way</cell><cell cols="3">5-shot 5-way 1-shot 20-way 5-shot 20-way</cell></row><row><cell>MAML + Transduction</cell><cell>98.7 ± 0.4%</cell><cell>99.9 ± 0.1%</cell><cell>95.8 ± 0.3%</cell><cell>98.9 ± 0.2%</cell></row><row><cell>1 st -order MAML + Transduction</cell><cell>98.3 ± 0.5%</cell><cell>99.2 ± 0.2%</cell><cell>89.4 ± 0.5%</cell><cell>97.9 ± 0.1%</cell></row><row><cell>Reptile</cell><cell cols="4">95.39 ± 0.09% 98.90 ± 0.10% 88.14 ± 0.15% 96.65 ± 0.33%</cell></row><row><cell>Reptile + Transduction</cell><cell cols="4">97.68 ± 0.04% 99.48 ± 0.06% 89.43 ± 0.14% 97.12 ± 0.32%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on Omniglot. MAML results are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Reptile hyper-parameters for the Omniglot comparison between all algorithms.</figDesc><table><row><cell>Parameter</cell><cell cols="2">5-way 20-way</cell></row><row><cell>Adam learning rate</cell><cell cols="2">0.001 0.0005</cell></row><row><cell>Inner batch size</cell><cell>10</cell><cell>20</cell></row><row><cell>Inner iterations</cell><cell>5</cell><cell>10</cell></row><row><cell>Training shots</cell><cell>10</cell><cell>10</cell></row><row><cell>Outer step size</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Outer iterations</cell><cell>100K</cell><cell>200K</cell></row><row><cell>Meta-batch size</cell><cell>5</cell><cell>5</cell></row><row><cell>Eval. inner iterations</cell><cell>50</cell><cell>50</cell></row><row><cell>Eval. inner batch</cell><cell>5</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Reptile hyper-parameters for the Mini-ImageNet comparison between all algorithms.</figDesc><table><row><cell>Parameter</cell><cell cols="2">1-shot 5-shot</cell></row><row><cell>Adam learning rate</cell><cell cols="2">0.001 0.001</cell></row><row><cell>Inner batch size</cell><cell>10</cell><cell>10</cell></row><row><cell>Inner iterations</cell><cell>8</cell><cell>8</cell></row><row><cell>Training shots</cell><cell>15</cell><cell>15</cell></row><row><cell>Outer step size</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Outer iterations</cell><cell cols="2">100K 100K</cell></row><row><cell>Meta-batch size</cell><cell>5</cell><cell>5</cell></row><row><cell>Eval. inner batch size</cell><cell>5</cell><cell>15</cell></row><row><cell>Eval. inner iterations</cell><cell>50</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters for Section 6.2. All outer step sizes were linearly annealed to zero during training.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Inner learning rate</cell><cell>3 × 10 −3</cell></row><row><cell>Inner batch size</cell><cell>25</cell></row><row><cell>Outer step size</cell><cell>0.25</cell></row><row><cell>Outer iterations</cell><cell>40K</cell></row><row><cell>Eval. inner batch size</cell><cell>25</cell></row><row><cell>Eval. inner iterations</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This finding also matches our analysis from Section 5.1, which suggests that Reptile works because sequential steps come from different mini-batches. With momentum, a mini-batch has influence over the next few steps, reducing this effect.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">RL 2 : Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11622</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The CMA evolution strategy: a comparing review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards a new evolutionary computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="75" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference of the Cognitive Science Society</title>
		<meeting>the ninth annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter R Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Cognitive Science Society (CogSci)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rezende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10304</idno>
		<title level="m">Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One-shot learning with a hierarchical nonparametric bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metalearning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Meaning and compositionality as statistical induction of categories and constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lauren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<title level="m">Dueling network architectures for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
