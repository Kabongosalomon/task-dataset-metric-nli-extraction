<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognition of Instrument-Tissue Interactions in Endoscopic Videos via Action Triplets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinedu</forename><forename type="middle">Innocent</forename><surname>Nwoye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>IHU</postCode>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristians</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">University Hospital of Strasbourg, IRCAD, IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>IHU</postCode>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Mascagni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>IHU</postCode>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Fondazione Policlinico Universitario Agostino Gemelli IRCCS</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Mutter</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">University Hospital of Strasbourg, IRCAD, IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Marescaux</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">University Hospital of Strasbourg, IRCAD, IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>IHU</postCode>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recognition of Instrument-Tissue Interactions in Endoscopic Videos via Action Triplets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Surgical activity recognition · Action triplet · Tool-tissue interaction · Deep learning · Endoscopic video · CholecT40</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognition of surgical activity is an essential component to develop context-aware decision support for the operating room. In this work, we tackle the recognition of fine-grained activities, modeled as action triplets 〈instrument, verb, target〉 representing the tool activity. To this end, we introduce a new laparoscopic dataset, CholecT40, consisting of 40 videos from the public dataset Cholec80 in which all frames have been annotated using 128 triplet classes. Furthermore, we present an approach to recognize these triplets directly from the video data. It relies on a module called class activation guide, which uses the instrument activation maps to guide the verb and target recognition. To model the recognition of multiple triplets in the same frame, we also propose a trainable 3D interaction space, which captures the associations between the triplet components. Finally, we demonstrate the significance of these contributions via several ablation studies and comparisons to baselines on CholecT40.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recognition of the surgical workflow has been identified as a key research area in surgical data science <ref type="bibr" target="#b13">[14]</ref>, as this recognition enables the development of intra-and post-operative context-aware decision support tools fostering both surgical safety and efficiency. Pioneering work in surgical workflow recognition has mostly focused on phase recognition from endoscopic video <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref> and from ceiling mounted cameras <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref>, gesture recognition from robotic data (kinematic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>, video <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>, system events <ref type="bibr" target="#b14">[15]</ref>) and event recognition, such as the presence of smoke or bleeding <ref type="bibr" target="#b12">[13]</ref>.</p><p>1. Grasper retract gallbladder 2. Hook dissect cystic duct 5. Grasper grasp specimen bag <ref type="figure">Fig. 1</ref>. Examples of action triplets from the CholecT40 dataset. The three images show four different triplets. The localization is not part of the dataset, but a representation of the weakly-supervised output of our recognition model.</p><p>In this paper, we focus on recognizing fine-grained activities representing the instrument-tissue interactions in endoscopic videos. These interactions are modeled as triplets instrument, verb, target . Triplets represent the used instrument, the performed action, and the anatomy acted upon, as proposed in existing surgical ontologies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref>. The target anatomy, while more challenging to annotate, adds substantial semantics to the recognized action/instrument. Triplet information has already been used to recognize phases <ref type="bibr" target="#b9">[10]</ref>, however, to the best of our knowledge, this is the first work aiming at recognizing triplets directly from the video data. The fine-grained nature of the triplets also makes this recognition task very challenging. For comparison, the action recognition task introduced within the Endovis challenge at MICCAI 2019 targeted the recognition of 4 verbs only (grasp, hold, cut, clip).</p><p>To perform this work, we present a new dataset, called CholecT40, containing 135K action triplets annotated on 40 cholecystectomy videos from the public Cholec80 dataset <ref type="bibr" target="#b21">[22]</ref>. The triplets belong to 128 action triplet classes, composed of 6 instruments, 8 verbs, and 19 target classes. Examples of such action triplets are 〈grasper, retract, gallbladder〉, 〈scissor, cut, cystic duct〉, 〈hook, coagulate, liver bed〉 (see also <ref type="figure">Fig. 1</ref>).</p><p>To design our recognition model, we build a multitask learning (MTL) network with three branches for the instrument, verb and target recognition. We also observe that triplets are instrument-centric: an action is only performed if an instrument is present. Indeed, clinically an action can only occur if a hand is manipulating the instrument. We therefore introduce a new module, called class activation guide (CAG), which uses the weak localization information from the instrument activation maps to guide the recognition of the verbs and targets. The idea is similar to <ref type="bibr" target="#b7">[8]</ref>, which uses the human's ROI produced by Faster-RCNN to inform the model on the likely location of the target. Other related works from the computer vision community <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> rely heavily on the overlap of the subject-object bounding boxes to learn the interactions. However, in addition to the fact that our work target triplets, our approach differs in that it does not rely on any spatial annotations in the dataset, which are expensive to generate.</p><p>Since instrument, verb, and target are multi-label classes, another challenge is to model their associations within the triplets. As will be shown in the experiments, naively assigning an ID to each triplet and classifying the IDs is not effective, due to the large amount of combinatorial possibilities. In <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> mentioned above, human is considered to be the only possible subject of interaction. Hence, in those works data association requires only bipartite matching to match verbs to objects. This is solvable by using the outer product of the detected object's logits and detected verb's logits to form a 2D matrix of interaction at test time <ref type="bibr" target="#b19">[20]</ref>. Data association's complexity increases however with a triplet. Solving a triplet relationship is a tripartite graph matching problem, which is an NP-hard optimisation problem. In this work, inspired by <ref type="bibr" target="#b19">[20]</ref>, we therefore propose a 3D interaction space to recognize the triplets. Unlike <ref type="bibr" target="#b19">[20]</ref>, where the data association is not learned, our interaction space learns the triplet relationships.</p><p>In summary, the contributions of this work are as follows:</p><p>1. We propose the first approach to recognize surgical actions as triplets of 〈instrument, verb, target〉 directly from surgical videos. 2. We present a large endoscopic action triplet dataset, CholecT40, for this task. 3. We develop a novel deep learning model that uses weak localization information from tool prediction to guide verb and target detection. 4. We introduce a trainable 3D interaction space to learn the relationships within the triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cholecystectomy Action Triplet Dataset</head><p>To encourage progress towards the recognition of instrument-tissue interactions in laparoscopic surgery, we generated a dataset consisting of 40 videos from Cholec80 <ref type="bibr" target="#b21">[22]</ref> annotated with action triplet information. We call this dataset CholecT40. The cholecystectomy recordings were first annotated by a surgeon using the software Surgery Workflow Toolbox-Annotate from the B-com institute. For each identified action, the surgeon sets times for the start and end frames, then labels the instrument, the verb and the target. Any change in the triplet configuration marks the end of the current action and the beginning of a different one. This first step was followed by a mediation on the annotations and a class grouping carried out by another clinician. The resulting action triplets span 128 classes encompassing 6 instruments, 8 verbs, and 19 target classes. For our experiments, we downsample the videos to 1 fps yielding a total of 83.2K frames annotated with 135K action-triplet instances. <ref type="table" target="#tab_0">Table 1</ref> shows the frequency of occurrence of the instruments, verbs and targets in the dataset. When a tool is idle, the verb and the target are both set to null. Additional statistics on the co-occurence distribution of the triplets are presented in the supplementary material. The video dataset is randomly split into training <ref type="bibr">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>To recognize the instrument-tissue interactions in the CholecT40 dataset, we build a new deep learning model, called Tripnet, by following a multitask learning (MTL) strategy. The principal novelty of this model is its use of the instrument's class activation guide and 3D interaction space to learn the relationships between the components of the action triplets.</p><p>Multitask Learning: Recently, multitask deep learning models have shown that correlated tasks can share deep learning layers and features to improve performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>. Following this observation, we build a MTL network with three branches for the instrument (I), verb (V), and target (T) recognition tasks. The instrument branch is a two layers convolutional network trained for instrument classification. It uses global max pooling (GMP) to learn the class activation maps (CAM) of the instruments for their weak localization, as suggested in <ref type="bibr" target="#b17">[18]</ref>. Similarly, the verb and the target branches learn the verb and target classifications using each two convolutional layers and one fully-connected (FC)-layer. All the three branches share the same ResNet-18 backbone for feature extraction.</p><p>Class Activation Guide: The pose of the instruments is indicative of their interactions with the tissues. However, there is no bounding box annotation in the dataset that could be used to learn how to crop the action's locations, as done in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. We therefore hypothesize that the instrument's CAM from the instrument branch, learnable in a weakly supervised manner, has sufficient information to direct the verb and target detection branches towards the likely region of interest of the actions. For convenience, we regroup the three branches of the MTL into two subnets: the instrument subnet and the verb-target subnet, as illustrated in <ref type="figure" target="#fig_0">Fig. 2a</ref>. The verb-target subnet is then transformed to a class (a) activation guide (CAG) unit, as shown in <ref type="figure" target="#fig_0">Fig. 2b</ref>. It receives the instrument's CAM as additional input. This CAM input is then concatenated with the verb and target features, concurrently, to guide and condition the model search space of the verb and target on the instrument appearance cue.</p><formula xml:id="formula_0">i 1 v 1 v 2 i 2 t 2 t 1 A 1 (c) 0 1 . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Interaction Space:</head><p>Recognizing the correct action triplets involves associating the right (I, V, T ) components using the raw output vectors, also called logits, of the instrument (I), verb (V ) and target (T ) branches. In the existing work <ref type="bibr" target="#b19">[20]</ref>, where the data association problem involves only the object-verb pair, the outer product of their logits is used to form a 2D matrix of component interaction at test time. In a similar manner, we propose a 3D interaction space for associating the triplets, as shown in <ref type="figure" target="#fig_0">Fig. 2c</ref>. Unlike in <ref type="bibr" target="#b19">[20]</ref>, where the data association is not learned by the trained model, we model a trainable interaction space. Given the m-logits, n-logits and p-logits for the I,V,T respectively, we learn the triplets y using a 3D projection function Ψ as follows:</p><formula xml:id="formula_1">y = Ψ (αI, βV, γT ),<label>(1)</label></formula><p>where α, β, γ, are the learnable weight vectors for projecting I, V and T to the 3D space and Ψ is an outer product operation. This gives an m × n × p grid of logits with the three axes representing the three components of the triplets.</p><p>For all i ∈ I, v ∈ V, t ∈ T the 3D point y i,v,t represents a possible triplet. A 3D point with a probability above a threshold is considered a valid triplet.</p><p>In practice, there are more 3D points in the space than valid triplets in the CholecT40 dataset. Therefore, we mask out the invalid points, obtained using the training set, at both train and test times.</p><p>Proposed Model: The proposed network is called Tripnet and shown in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>: it is an integration of the CAG unit and of the 3D interaction space within the MTL model. The whole model is trained end-to-end using a warm-up parameter which allows the instrument subnet to learn some semantics for a few epochs before guiding the verb-target subnet with instrument cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Implementation Details: We perform our experiments on CholecT40. During training, we employ three types of data augmentation (rotation, horizontal flipping and patch masking) with no image preprocessing. The model is trained on images resized to 256 × 448 × 3. All the individual tasks are trained for multilabel classification using the weighted sigmoid cross-entropy with logits as loss function, regularized by an L 2 norm with 1e −5 weight decay. The class weights are calculated as in <ref type="bibr" target="#b17">[18]</ref>. The Resnet-18 backbone is pretrained on Imagenet. All the experimented models are trained using learning rates with exponential decay and initialized with the values 1e −3 , 1e −4 , 1e −5 for the subnets, backbone, and 3D interaction space, respectively. The learning rates and other hyperparameters are tuned from the validation set using grid search. Our network is implemented using TensorFlow and trained on GeForce GTX 1080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks and Metrics:</head><p>To evaluate the capacity of a model to recognize correctly a triplet and its components, we use two types of metrics:</p><p>1. Instrument detection performance: This measures the average precision (AP) of detecting the correct instruments, as the area under the precision-recall curve per instrument(AP I ). 2. Triplet recognition performance: This measures the AP of recognizing the instrument-tissue interactions by looking at different sets of triplet components. We use three metrics: the instrument-verb (AP IV ), instrument-target (AP IT ), and instrument-verb-target (AP IV T ) metrics. All the listed components need to be correct during the AP computation. AP IV T evaluate the recognition of the complete triplets.</p><p>Baselines: We build two baseline models. The naive CNN baseline is a ResNet-18 backbone with two additional 3x3 convolutional layers and a fully connected (FC) classification layer with N units, where N corresponds to the number of triplet classes (N = 128). The naive model learns the action-triplets using their IDs without any consideration of the components that constitute the triplets. We therefore also include an MTL baseline built with the I, V and T branches described in Section 3. The outputs of the three branches are concatenated and fed to an FC-layer to learn the triplets. For fair comparison, the two baselines share the same backbone as Tripnet.  <ref type="table">Table 3</ref>. Action triplet recognition performance for instrument-verb (APIV ), instrument-target (APIT ) and instrument-verb-target (APIV T ) components.</p><p>The triplet recognition performance is presented in <ref type="table">Table 3</ref>. The naive CNN model has again the worst performance for the AP IV , AP IT and AP IV T metrics, as expected from the previous results. The MTL baseline model, on the other hand, performs only slightly above the naive model despite its high instrument detection performance in <ref type="table" target="#tab_1">Table 2</ref>. This is because the MTL baseline model, after learning the components of the triplets, dilutes this semantic information by concatenating and feeding the output to an FC-layer. However, Tripnet improves over the MTL baseline by leveraging the instrument cue from the CAG unit. It also learns better triplet association by increasing the AP IV T by 12.5% on average. Tripnet outperformed all the baselines in instrument-tissue interaction recognition by a minimum of 15.6%. In general, it can be observed that it is easier to learn the instrument-verb components than the instrument-target components. This is likely due to the fact that (a) a verb has a more direct association to the instrument creating the action (b) the dataset contains many more target classes than verb classes (c) many anatomical structures in the abdomen are usually discriminated with difficulty by non-medical experts.</p><p>While the action recognition performance appears to be low, it follows the same pattern as other models in the computer vision literature on action datasets of even lesser complexity. For instance, on the HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr">[</ref>  <ref type="table">Table 4</ref>. Ablation study for the CAG unit and 3D interaction space in Tripnet model. <ref type="table">Table 4</ref> presents an ablation study of the novel components of the Tripnet model. The CAG unit improves the AP IV and AP IT by approximately 2.0% and 1.0%, respectively, justifying the need for using instrument cues in the verb and target recognition. We also observe that learning the instrument-tissue interactions is better with a trainable 3D projection than with either the untrained 3D space or with an FC-layer. This results in a large 6.0% improvement of the AP IV T . We record the best performance in all four metrics by combining the CAG unit and the trained 3D interaction space. The two units complement each other and improve the results across all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies:</head><p>Qualitative results: To better appreciate the performance of the proposed model in understanding instrument-tissue interactions, we overlay the predictions on several surgical images in <ref type="figure">Fig. 4</ref>  majority of incorrect predictions are due to one incorrect triplet component. Instruments are usually correctly predicted and localized. As can be seen in the complete statistics provided in the supplementary material, it is however not straightforward to predict the verb/target directly from the instrument due to the multiple possible associations. More qualitative results are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we tackle the task of recognizing action triplets directly from surgical videos. Our overarching goal is to detect the instruments and learn their interactions with the tissues during laparoscopic procedures. To this aim, we present a new dataset, which consists of 135k action triplets over 40 videos. For recognition, we propose a novel model that relies on instrument class activation maps to learn the verbs and targets. We also introduce a trainable 3D interaction space for learning the 〈instrument, verb, target〉 associations within the triplets. Experiments show that our model outperforms the baselines by a substantial margin in all the metrics, hereby demonstrating the effectiveness of the proposed approach.   <ref type="table" target="#tab_0">-772  gallbladder  48720  731 25750  57  -73  cystic plate  1451  478 2959  32  54  199  cystic artery  38  190 2639  558  953  cystic duct  786  215 6710  670  1572  70  cystic pedicle  112  90  48  4  58  240  liver  10919  2399  356  90  -669  adhesion  1  73  9  154  -clip  137  ----fluid  7  ----1943  specimen bag  5685  79  ---29  omentum  4413  521 3553  110  -218  peritoneum  298  -286  57  -gut  709  19  6  --11  hepatic pedicle  10  46  4  --tissue sampling  72  9  -7  -fallciform ligament  81  33  ---suture  1  --9</ref> -- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>===== Supplementary Material =====</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Proposed model: (a) Tripnet for action triplet recognition, (b) class activation guide (CAG) unit for spatially guided detection, (c) 3D interaction space for triplet association.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative results: triplet prediction and weak localization of the regions of action (best seen in color ). Predicted and ground-truth triplets are displayed below each image: black = ground-truth, green = correct prediction, red = incorrect prediction. A missed triplet is marked as false negative and a false detection is marked as false positive. The color of the text corresponds to the color of the associated bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>25 videos, 50.6K frames, 82.4K triplets), validation (5 videos, 10.2K frames, 15.9K triplets) and testing (10 videos, 22.5K frames, 37.1K triplets) sets. Dataset statistics showing the frequency of occurrence of the instruments, verbs and targets. Target ids 0...18 correspond to null, abdominal wall/cavity, gallbladder, cystic plate, cystic artery, cystic duct, cystic pedicle, liver, adhesion, clip, fluid, specimen bag, omentum, peritoneum, gut, hepatic pedicle, tissue sampling, falciform ligament, suture.</figDesc><table><row><cell cols="2">Instrument</cell><cell>Verb</cell><cell></cell><cell></cell><cell cols="2">Target</cell><cell></cell></row><row><cell cols="2">Name Count</cell><cell>Name</cell><cell>Count</cell><cell cols="4">ID Count ID Count ID Count</cell></row><row><cell cols="2">grasper 76196</cell><cell>null</cell><cell>5807</cell><cell>0</cell><cell>5807 8</cell><cell>236 16</cell><cell>88</cell></row><row><cell>bipolar</cell><cell>5616</cell><cell>place/pack</cell><cell>273</cell><cell>1</cell><cell>1169 9</cell><cell>137 17</cell><cell>114</cell></row><row><cell>hook</cell><cell>44413</cell><cell cols="2">grasp/retract 74720</cell><cell>2</cell><cell>75331 10</cell><cell>1950 18</cell><cell>9</cell></row><row><cell cols="2">scissors 1856</cell><cell>clip</cell><cell>2578</cell><cell>3</cell><cell>5173 11</cell><cell>5793</cell><cell></cell></row><row><cell>clipper</cell><cell>2851</cell><cell>dissect</cell><cell>42851</cell><cell>4</cell><cell>4378 12</cell><cell>8815</cell><cell></cell></row><row><cell cols="2">irrigator 4522</cell><cell>cut</cell><cell>1544</cell><cell>5</cell><cell>10023 13</cell><cell>641</cell><cell></cell></row><row><cell></cell><cell></cell><cell>coagulate</cell><cell>4306</cell><cell>6</cell><cell>552 14</cell><cell>745</cell><cell></cell></row><row><cell></cell><cell></cell><cell>clean</cell><cell>3375</cell><cell>7</cell><cell>14433 15</cell><cell>60</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Instrument detection performance of (API ) across all triplets. The IDs 0...5 correspond to grasper, bipolar, hook, scissors, clipper and irrigator, respectively.Table 2presents the AP results for the instrument detection across all triplets. The results show that the naive model does not understand the triplet components. This comes from the fact that it is designed to learn the triplets using their IDs: two different triplets sharing the same instrument or verb still have different IDs. On the other hand, the MTL and Tripnet networks, which both model the triplet components, show competing performance on instrument detection. Moreover, Tripnet outperforms the MTL baseline by 15.1% mean AP. This can be attributed to its use of CAG unit and 3D interaction space to learn better semantic information about the instrument behaviors.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Instrument</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Mean</cell></row><row><cell cols="8">Naive CNN 75.3 04.3 64.6 02.1 05.5 06.0 27.5</cell></row><row><cell>MTL</cell><cell cols="7">96.1 91.9 97.2 55.7 30.3 76.8 74.6</cell></row><row><cell>Tripnet</cell><cell cols="7">96.3 91.6 97.2 79.9 90.5 77.9 89.7</cell></row><row><cell cols="2">Quantitative Results: Model</cell><cell cols="5">APIV APIT APIV T Mean</cell><cell></cell></row><row><cell cols="2">Naive CNN</cell><cell>7.54</cell><cell>6.89</cell><cell cols="2">5.88</cell><cell>6.77</cell><cell></cell></row><row><cell>MTL</cell><cell></cell><cell cols="2">14.02 7.15</cell><cell cols="2">6.43</cell><cell>9.20</cell><cell></cell></row><row><cell cols="2">Tripnet</cell><cell cols="5">35.45 19.94 18.95 24.78</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>8] achieves 10.8%,<ref type="bibr" target="#b18">[19]</ref> achieves 14.2% and<ref type="bibr" target="#b22">[23]</ref> achieves 15.1% action recognition AP, also known as AP role . In fact, the current state-of-the-art performance on HICO-DET dataset is 21.2% as reported on the leaderboard server. Similarly, the winner of the MICCAI 2019 subchallenge on action recognition, involving only four verb classes, scores 23.3% F1-score. This shows the challenging nature of fine-grained action recognition.</figDesc><table><row><cell>FC 3D(untrained) 3D(trained) CAG</cell><cell cols="3">API APIV APIT APIV T</cell></row><row><cell></cell><cell>74.6 14.02</cell><cell>7.15</cell><cell>6.43</cell></row><row><cell></cell><cell>89.3 14.28</cell><cell>6.99</cell><cell>6.03</cell></row><row><cell></cell><cell>89.7 16.72</cell><cell>7.62</cell><cell>6.32</cell></row><row><cell></cell><cell cols="2">89.5 20.63 12.08</cell><cell>12.06</cell></row><row><cell></cell><cell cols="3">89.7 35.45 19.94 18.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. The qualitative results show that Tripnet does not only improve the performance of the baseline models, but also localizes accurately the regions of interest of the actions. It is observed that the</figDesc><table><row><cell>grasper, grasp/retract, gallbladder</cell><cell>hook, dissect, gallbladder</cell><cell>grasper, grasp/retract, gallbladder</cell><cell>grasper, grasp/retract, gallbladder</cell></row><row><cell>grasper, grasp/retract, gallbladder</cell><cell>hook, dissect, gallbladder</cell><cell>grasper, grasp/retract, gallbladder</cell><cell>grasper, grasp/retract, gallbladder</cell></row><row><cell>clipper, clip, cystic artery</cell><cell></cell><cell>irrigator, clean, fluid</cell><cell>bipolar, coagulate, cystic artery</cell></row><row><cell>clipper, clip, cystic artery</cell><cell></cell><cell>irrigator, clean, fluid</cell><cell>bipolar, coagulate, cystic artery</cell></row><row><cell>scissors, dissect, cystic plate</cell><cell>irrigator, clean, fluid</cell><cell>bipolar, coagulate, cystic plate</cell><cell>clipper, clip, cystic artery</cell></row><row><cell>scissors, dissect, cystic plate</cell><cell>irrigator, dissect, gallbladder</cell><cell>bipolar, dissect, gallbladder</cell><cell>clipper, clip, cystic duct</cell></row><row><cell>grasper, grasp/retract, gallbladder</cell><cell>grasper, grasp/retract, liver</cell><cell>grasper, grasp/retract, liver</cell><cell>grasper, grasp/retract, gallbladder</cell></row><row><cell>grasper dissect, peritoneum</cell><cell>grasper, dissect, gallbladder</cell><cell>false negative</cell><cell>false negative</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Appendix I : Co-occurence Distribution of the Triplets</figDesc><table><row><cell>Verb</cell><cell cols="6">Instrument grasper bipolar hook scissors clipper irrigator</cell></row><row><cell>null</cell><cell>2722</cell><cell cols="2">372 2093</cell><cell>108</cell><cell>214</cell><cell>298</cell></row><row><cell>place/pack</cell><cell>273</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>grasp/retract</cell><cell>72394</cell><cell cols="2">589 1006</cell><cell>45</cell><cell>59</cell><cell>627</cell></row><row><cell>clip</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2578</cell><cell>-</cell></row><row><cell>dissect</cell><cell>767</cell><cell cols="2">892 40772</cell><cell>151</cell><cell>-</cell><cell>269</cell></row><row><cell>cut</cell><cell>-</cell><cell>-</cell><cell>8</cell><cell>1536</cell><cell>-</cell><cell>-</cell></row><row><cell>coagulation</cell><cell>-</cell><cell>3756</cell><cell>534</cell><cell>16</cell><cell>-</cell><cell>-</cell></row><row><cell>clean</cell><cell>40</cell><cell>7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3328</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Dataset statistics showing the instrument-verb occurrence frequency.</figDesc><table><row><cell>Target</cell><cell cols="6">Instrument grasper bipolar hook scissors clipper irrigator</cell></row><row><cell>null</cell><cell>2722</cell><cell cols="2">372 2093</cell><cell>108</cell><cell>214</cell><cell>298</cell></row><row><cell>abdominal wall/cavity</cell><cell>36</cell><cell>361</cell><cell>-</cell><cell>-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Dataset statistics showing the instrument-target occurrence frequency. ‡ Accepted at International Conference on Medical Image Computing and Computer-Assisted Intervention MICCAI 2020.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was supported by French state funds managed within the Investissements d'Avenir program by BPI France (project CONDOR) and by the ANR (references ANR-11-LABX-0004 and ANR-16-CE33-0009). The authors would also like to thank the IHU and IRCAD research teams for their help with the data annotation during the CONDOR project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling and segmentation of surgical workflow from laparoscopic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feußner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video based activity recognition in trauma resuscitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Burd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
	<note>ieee winter conference on applications of computer vision (WACV)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic data-driven real-time segmentation and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dergachyova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huaulmé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Morandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1081" to="1089" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmenting and classifying activities in robot-assisted surgery with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waldram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing surgical activities with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal coherence-based self-supervised learning for laparoscopic workflow analysis. In: OR 2.0 Context-Aware Operating Theaters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clinical Image-Based Procedures, and Skin Image Analysis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting and recognizing humanobject interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-task recurrent convolutional network with correlation loss for surgical video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101572</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lapontospm: an ontology for laparoscopic surgeries and its application to surgical phase recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Julliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Müller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gibaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1427" to="1434" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time automatic surgical phase recognition in laparoscopic sigmoidectomy using the convolutional neural network-based deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kitaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takeshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Owada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamanashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical Endoscopy</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Episode classification for the analysis of tissue/instrument interaction with multiple visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<biblScope unit="page" from="230" to="237" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smoke detection in endoscopic surgery videos: a first step towards retrieval of semantic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Medical Robotics and Computer Assisted Surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="94" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Surgical data science: Enabling next-generation surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical Action Triplet Recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Nature Biomedical Engineering</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">System events: readily accessible features for surgical phase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1201" to="1209" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multitask learning of temporal connectionism in convolutional networks using a joint distribution loss function to simultaneously identify tools and phase in surgical videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sathish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acquisition of process descriptions from surgical interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Strauß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meixensberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Burgert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database and Expert Systems Applications</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="602" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised convolutional lstm approach for tool tracking in laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1059" to="1067" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-driven spatio-temporal rgbd feature encoding for action recognition in operating rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Alkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="737" to="747" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Endonet: A deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to detect humanobject interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Surgical activity recognition in robot-assisted radical prostatectomy using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jarc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepphase: surgical phase recognition in cataracts videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giataganas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
