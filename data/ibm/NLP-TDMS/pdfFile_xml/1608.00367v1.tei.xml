<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accelerating the Super-Resolution Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Accelerating the Super-Resolution Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image super-resolution (SR) aims at recovering a high-resolution (HR) image from a given low-resolution (LR) one. Recent SR algorithms are mostly learning-based (or patch-based) methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> that learn a mapping between the LR and HR image spaces. Among them, the Super-Resolution Convolutional Neural Network (SRCNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> has drawn considerable attention due to its simple network structure and excellent restoration quality. Though SRCNN is already faster than most previous learning-based methods, the processing speed on large images is still unsatisfactory. For example, to upsample an 240 × 240 image by a factor of 3, the speed of the original SR-CNN <ref type="bibr" target="#b0">[1]</ref> is about 1.32 fps, which is far from real-time (24 fps). To approach real-time, we should accelerate SRCNN for at least 17 times while keeping the previous performance. This sounds implausible at the first glance, as accelerating by simply reducing the parameters will severely impact the performance. However, when we delve into the network structure, we find two inherent limitations that restrict its running speed.</p><p>First, as a pre-processing step, the original LR image needs to be upsampled to the desired size using bicubic interpolation to form the input. Thus the computation complexity of SRCNN grows quadratically with the spatial size of the HR image (not the  <ref type="figure">Fig. 1</ref>. The proposed FSRCNN networks achieve better super-resolution quality than existing methods, and are tens of times faster. Especially, the FSRCNN-s can run in real-time (&gt; 24 fps) on a generic CPU. The chart is based on the Set14 <ref type="bibr" target="#b8">[9]</ref> results summarized in <ref type="table" target="#tab_3">Tables 3 and 4.</ref> original LR image). For the upscaling factor n, the computational cost of convolution with the interpolated LR image will be n 2 times of that for the original LR one. This is also the restriction for most learning-based SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. If the network was learned directly from the original LR image, the acceleration would be significant, i.e., about n 2 times faster.</p><p>The second restriction lies on the costly non-linear mapping step. In SRCNN, input image patches are projected on a high-dimensional LR feature space, then followed by a complex mapping to another high-dimensional HR feature space. Dong et al. <ref type="bibr" target="#b1">[2]</ref> show that the mapping accuracy can be substantially improved by adopting a wider mapping layer, but at the cost of the running time. For example, the large SRCNN (SRCNN-Ex) <ref type="bibr" target="#b1">[2]</ref> has 57,184 parameters, which are six times larger than that for SRCNN <ref type="bibr">(8,032 parameters)</ref>. Then the question is how to shrink the network scale while still keeping the previous accuracy.</p><p>According to the above observations, we investigate a more concise and efficient network structure for fast and accurate image SR. To solve the first problem, we adopt a deconvolution layer to replace the bicubic interpolation. To further ease the computational burden, we place the deconvolution layer 1 at the end of the network, then the computational complexity is only proportional to the spatial size of the original LR image. It is worth noting that the deconvolution layer is not equal to a simple substitute of the conventional interpolation kernel like in FCN <ref type="bibr" target="#b12">[13]</ref>, or 'unpooling+convolution' like <ref type="bibr" target="#b13">[14]</ref>. Instead, it consists of diverse automatically learned upsampling kernels (see <ref type="figure" target="#fig_2">Figure 3</ref>) that work jointly to generate the final HR output, and replacing these deconvolution filters with uniform interpolation kernels will result in a drastic PSNR drop (e.g., at least 0.9 dB on the Set5 dataset <ref type="bibr" target="#b14">[15]</ref> for ×3).</p><p>For the second problem, we add a shrinking and an expanding layer at the beginning and the end of the mapping layer separately to restrict mapping in a low-dimensional feature space. Furthermore, we decompose a single wide mapping layer into several layers with a fixed filter size 3 × 3. The overall shape of the new structure looks like an hourglass, which is symmetrical on the whole, thick at the ends and thin in the middle. Experiments show that the proposed model, named as Fast Super-Resolution Convolutional Neural Networks (FSRCNN) 2 , achieves a speed-up of more than 40× with even superior performance than the SRCNN-Ex. In this work, we also present a small FS-RCNN network (FSRCNN-s) that achieves similar restoration quality as SRCNN, but is 17.36 times faster and can run in real time (24 fps) with a generic CPU. As shown in <ref type="figure">Figure 1</ref>, the FSRCNN networks are much faster than contemporary SR models yet achieving superior performance.</p><p>Apart from the notable improvement in speed, the FSRCNN also has another appealing property that could facilitate fast training and testing across different upscaling factors. Specifically, in FSRCNN, all convolution layers (except the deconvolution layer) can be shared by networks of different upscaling factors. During training, with a well-trained network, we only need to fine-tune the deconvolution layer for another upscaling factor with almost no loss of mapping accuracy. During testing, we only need to do convolution operations once, and upsample an image to different scales using the corresponding deconvolution layer.</p><p>Our contributions are three-fold: 1) We formulate a compact hourglass-shape CNN structure for fast image super-resolution. With the collaboration of a set of deconvolution filters, the network can learn an end-to-end mapping between the original LR and HR images with no pre-processing.</p><p>2) The proposed model achieves a speed up of at least 40× than the SRCNN-Ex <ref type="bibr" target="#b1">[2]</ref> while still keeping its exceptional performance. One of its small-size version can run in real-time (&gt;24 fps) on a generic CPU with better restoration quality than SRCNN <ref type="bibr" target="#b0">[1]</ref>. 3) We transfer the convolution layers of the proposed networks for fast training and testing across different upscaling factors, with no loss of restoration quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning for SR: Recently, the deep learning techniques have been successfully applied on SR. The pioneer work is termed as the Super-Resolution Convolutional Neural Network (SRCNN) proposed by Dong et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Motivated by SRCNN, some problems such as face hallucination <ref type="bibr" target="#b15">[16]</ref> and depth map super-resolution <ref type="bibr" target="#b16">[17]</ref> have achieved state-of-the-art results. Deeper structures have also been explored in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>. Different from the conventional learning-based methods, SRCNN directly learns an end-to-end mapping between LR and HR images, leading to a fast and accurate inference. The inherent relationship between SRCNN and the sparse-codingbased methods ensures its good performance. Based on the same assumption, Wang et al. <ref type="bibr" target="#b7">[8]</ref> further replace the mapping layer by a set of sparse coding sub-networks and propose a sparse coding based network (SCN). With the domain expertise of the conventional sparse-coding-based method, it outperforms SRCNN with a smaller model size. However, as it strictly mimics the sparse-coding solver, it is very hard to shrink the sparse coding sub-network with no loss of mapping accuracy. Furthermore, all these networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> need to process the bicubic-upscaled LR images. The proposed FS-RCNN does not only perform on the original LR image, but also contains a simpler but more efficient mapping layer. Furthermore, the previous methods have to train a totally different network for a specific upscaling factor, while the FSRCNN only requires a different deconvolution layer. This also provides us a faster way to upscale an image to several different sizes. CNNs acceleration: A number of studies have investigated the acceleration of CNN. Denton et al. <ref type="bibr" target="#b19">[20]</ref> first investigate the redundancy within the CNNs designed for object detection. Then Zhang et al. <ref type="bibr" target="#b20">[21]</ref> make attempts to accelerate very deep CNNs for image classfication. They also take the non-linear units into account and reduce the accumulated error by asymmetric reconstruction. Our model also aims at accelerating CNNs but in a different manner. First, they focus on approximating the existing well-trained models, while we reformulate the previous model and achieves better performance. Second, the above methods are all designed for high-level vision problems (e.g., image classification and object detection), while ours are for the low-level vision task. As the deep models for SR contain no fully-connected layers, the approximation of convolution filters will severely impact the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fast Super-Resolution by CNN</head><p>We first briefly describe the network structure of SRCNN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and then we detail how we reformulate the network layer by layer. The differences between FSRCNN and SRCNN are presented at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SRCNN</head><p>SRCNN aims at learning an end-to-end mapping function F between the bicubicinterpolated LR image Y and the HR image X. The network contains all convolution layers, thus the size of the output is the same as that of the input image. As depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, the overall structure consists of three parts that are analogous to the main steps of the sparse-coding-based methods <ref type="bibr" target="#b9">[10]</ref>. The patch extraction and representation part refers to the first layer, which extracts patches from the input and represents each patch as a high-dimensional feature vector. The non-linear mapping part refers to the middle layer, which maps the feature vectors non-linearly to another set of feature vectors, or namely HR features. Then the last reconstruction part aggregates these features to form the final output image.</p><p>The computation complexity of the network can be calculated as follows,</p><formula xml:id="formula_0">O{(f 2 1 n 1 + n 1 f 2 2 n 2 + n 2 f 2 3 )S HR },<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">{f i } 3 i=1 and {n i } 3 i=1</formula><p>are the filter size and filter number of the three layers, respectively. S HR is the size of the HR image. We observe that the complexity is proportional to the size of the HR image, and the middle layer contributes most to the network parameters. In the next section, we present the FSRCNN by giving special attention to these two facets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FSRCNN</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, FSRCNN can be decomposed into five parts -feature extraction, shrinking, mapping, expanding and deconvolution. The first four parts are convolution layers, while the last one is a deconvolution layer. For better understanding, we denote a convolution layer as Conv(f i , n i , c i ), and a deconvolution layer as</p><formula xml:id="formula_2">DeConv(f i , n i , c i ),</formula><p>where the variables f i , n i , c i represent the filter size, the number of filters and the number of channels, respectively. As the whole network contains tens of variables (i.e., {f i , n i , c i } 6 i=1 ), it is impossible for us to investigate each of them. Thus we assign a reasonable value to the insensitive variables in advance, and leave the sensitive variables unset. We call a variable sensitive when a slight change of the variable could significantly influence the performance. These sensitive variables always represent some important influential factors in SR, which will be shown in the following descriptions. Feature extraction: This part is similar to the first part of SRCNN, but different on the input image. FSRCNN performs feature extraction on the original LR image without interpolation. To distinguish from SRCNN, we denote the small LR input as Y s . By doing convolution with the first set of filters, each patch of the input (1-pixel overlapping) is represented as a high-dimensional feature vector.</p><p>We refer to SRCNN on the choice of parameters -f 1 , n 1 , c 1 . In SRCNN, the filter size of the first layer is set to be 9. Note that these filters are performed on the upscaled image Y . As most pixels in Y are interpolated from Y s , a 5 × 5 patch in Y s could cover almost all information of a 9 × 9 patch in Y . Therefore, we can adopt a smaller filter size f 1 = 5 with little information loss. For the number of channels, we follow SRCNN to set c 1 = 1. Then we only need to determine the filter number n 1 . From another perspective, n 1 can be regarded as the number of LR feature dimension, denoted as dthe first sensitive variable. Finally, the first layer can be represented as Conv <ref type="bibr">(5, d, 1)</ref>. Shrinking: In SRCNN, the mapping step generally follows the feature extraction step, then the high-dimensional LR features are mapped directly to the HR feature space. However, as the LR feature dimension d is usually very large, the computation complexity of the mapping step is pretty high. This phenomenon is also observed in some deep models for high-level vision tasks. Authors in <ref type="bibr" target="#b21">[22]</ref> apply 1 × 1 layers to save the computational cost.</p><p>With the same consideration, we add a shrinking layer after the feature extraction layer to reduce the LR feature dimension d. We fix the filter size to be f 2 = 1, then the filters perform like a linear combination within the LR features. By adopting a smaller filter number n 2 = s &lt;&lt; d, the LR feature dimension is reduced from d to s. Here s is the second sensitive variable that determines the level of shrinking, and the second layer can be represented as Conv <ref type="bibr">(1, s, d)</ref>. This strategy greatly reduces the number of parameters (detailed computation in Section 3.3). Non-linear mapping: The non-linear mapping step is the most important part that affects the SR performance, and the most influencing factors are the width (i.e., the number of filters in a layer) and depth (i.e., the number of layers) of the mapping layer. As indicated in SRCNN <ref type="bibr" target="#b1">[2]</ref>, a 5 × 5 layer achieves much better results than a 1 × 1 layer. But they are lack of experiments on very deep networks. The above experiences help us to formulate a more efficient mapping layer for FSRCNN. First, as a trade-off between the performance and network scale, we adopt a medium filter size f 3 = 3. Then, to maintain the same good performance as SRCNN, we use multiple 3 × 3 layers to replace a single wide one. The number of mapping layers is another sensitive variable (denoted as m), which determines both the mapping accuracy and complexity. To be consistent, all mapping layers contain the same number of filters n 3 = s. Then the non-linear mapping part can be represented as m × Conv(3, s, s). Expanding: The expanding layer acts like an inverse process of the shrinking layer. The shrinking operation reduces the number of LR feature dimension for the sake of the computational efficiency. However, if we generate the HR image directly from these low-dimensional features, the final restoration quality will be poor. Therefore, we add an expanding layer after the mapping part to expand the HR feature dimension. To maintain consistency with the shrinking layer, we also adopt 1 × 1 filters, the number of which is the same as that for the LR feature extraction layer. As opposed to the shrinking layer Conv(1, s, d), the expanding layer is Conv <ref type="bibr">(1, d, s)</ref>. Experiments show that without the expanding layer, the performance decreases up to 0.3 dB on the Set5 test set <ref type="bibr" target="#b14">[15]</ref>. Deconvolution: The last part is a deconvolution layer, which upsamples and aggregates the previous features with a set of deconvolution filters. The deconvolution can be regarded as an inverse operation of the convolution. For convolution, the filter is convolved with the image with a stride k, and the output is 1/k times of the input. Contrarily, if we exchange the position of the input and output, the output will be k times of the input, as depicted in <ref type="figure">Figure 4</ref>. We take advantage of this property to set the stride k = n, which is the desired upscaling factor. Then the output is directly the reconstructed HR image. When we determine the filter size of the deconvolution filters, we can look at the network from another perspective. Interestingly, the reversed network is like a downscaling operator that accepts an HR image and outputs the LR one. Then the deconvolution layer becomes a convolution layer with a stride n. As it extracts features from the HR image, we should adopt 9 × 9 filters that are consistent with the first layer of SRCNN. Similarly, if we reverse back, the deconvolution filters should also have a spatial size f 5 = 9. Experiments also demonstrate this assumption. <ref type="figure" target="#fig_2">Figure 3</ref> shows the learned deconvolution filters, the patterns of which are very similar to that of the first-layer filters in SRCNN. Lastly, we can represent the deconvolution layer as DeConv <ref type="figure">(9, 1, d)</ref>.</p><p>Different from inserting traditional interpolation kernels (e.g., bicubic or bilinear) in-network <ref type="bibr" target="#b12">[13]</ref> or having 'unpooling+convolution' <ref type="bibr" target="#b13">[14]</ref>, the deconvolution layer learns a set of upsampling kernel for the input feature maps. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, these kernels are diverse and meaningful. If we force these kernels to be identical, the parameters will be used inefficiently (equal to sum up the input feature maps as one), and the performance will drop at least 0.9 dB on the Set5. PReLU: For the activation function after each convolution layer, we suggest the use of the Parametric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b22">[23]</ref> instead of the commonly-used Rectified Linear Unit (ReLU). They are different on the coefficient of the negative part. For ReLU and PReLU, we can define a general activation function as f (x i ) = max(x i , 0) + a i min(0, x i ), where x i is the input signal of the activation f on the i-th channel, and a i is the coefficient of the negative part. The parameter a i is fixed to be zero for ReLU, but is learnable for PReLU. We choose PReLU mainly to avoid the "dead features" <ref type="bibr" target="#b10">[11]</ref> caused by zero gradients in ReLU. Then we can make full use of all parameters to test the maximum capacity of different network designs. Experiments show that the performance of the PReLU-activated networks is more stable, and can be seen as the up-bound of that for the ReLU-activated networks.</p><p>Overall structure: We can connect the above five parts to form a complete FSRCNN network as Conv(5, d, 1)−P ReLU −Conv(1, s, d)−P ReLU −m×Conv(3, s, s)− P ReLU − Conv(1, d, s) − P ReLU − DeConv <ref type="figure">(9, 1, d)</ref>. On the whole, there are three sensitive variables (i.e., the LR feature dimension d, the number of shrinking filters s, and the mapping depth m) governing the performance and speed. For simplicity, we represent a FSRCNN network as <ref type="figure">F SRCN N (d, s, m)</ref>. The computational complexity can be calculated as We exclude the parameters of PReLU , which introduce negligible computational cost. Interestingly, the new structure looks like an hourglass, which is symmetrical on the whole, thick at the ends, and thin in the middle. The three sensitive variables are just the controlling parameters for the appearance of the hourglass. Experiments show that this hourglass design is very effective for image super-resolution.</p><formula xml:id="formula_3">O{(25d + sd + 9ms 2 + ds + 81d)S LR } = O{(9ms 2 + 2sd + 106d)S LR }. (2)</formula><p>Cost function: Following SRCNN, we adopt the mean square error (MSE) as the cost function. The optimization objective is represented as</p><formula xml:id="formula_4">min θ 1 n n i=1 ||F (Y i s ; θ) − X i || 2 2 ,<label>(3)</label></formula><p>where Y i s and X i are the i-th LR and HR sub-image pair in the training data, and F (Y i s ; θ) is the network output for Y i s with parameters θ. All parameters are optimized using stochastic gradient descent with the standard backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Differences against SRCNN: From SRCNN to FSRCNN</head><p>To better understand how we accelerate SRCNN, we transform the SRCNN-Ex to another FSRCNN (56,12,4) within three steps, and show how much acceleration and PSNR gain are obtained by each step. We use a representative upscaling factor n = 3. The network configurations of SRCNN, FSRCNN and the two transition states are shown in <ref type="table" target="#tab_0">Table 1</ref>. We also show their performance (average PSNR on Set5) trained on the 91-image dataset <ref type="bibr" target="#b9">[10]</ref>.</p><p>First, we replace the last convolution layer of SRCNN-Ex with a deconvolution layer, then the whole network will perform on the original LR image and the computation complexity is proportional to S LR instead of S HR . This step will enlarge the network scale but achieve a speedup of 8.7× (i.e., 57184/58976 × 3 2 ). As the learned deconvolution kernels are better than a single bicubic kernel, the performance increases roughly by 0.12 dB. Second, the single mapping layer is replaced with the combination of a shrinking layer, 4 mapping layers and an expanding layer. Overall, there are 5 more layers, but the parameters are decreased from 58,976 to 17,088. Also, the acceleration after this step is the most prominent -30.1×. It is widely observed that depth is the key factor that affects the performance. Here, we use four "narrow" layers to replace a single "wide" layer, thus achieving better results (33.01 dB) with much less parameters. Lastly, we adopt smaller filter sizes and less filters (e.g., from Conv(9, 64, 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution layers</head><formula xml:id="formula_5">Stride = 3 Stride = 2</formula><p>For factor 2 For factor 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deconvolution layer</head><p>The convolution filters can be shared for different upscaling factors <ref type="figure">Fig. 4</ref>. The FSRCNN consists of convolution layers and a deconvolution layer. The convolution layers can be shared for different upscaling factors. A specific deconvolution layer is trained for different upscaling factors.</p><p>to Conv(5, 56, 1)), and obtain a final speedup of 41.3×. As we remove some redundant parameters, the network is trained more efficiently and achieves another 0.05 dB improvement.</p><p>It is worth noting that this acceleration is NOT at the cost of performance degradation. Contrarily, the FSRCNN (56,12,4) outperforms SRCNN-Ex by a large margin (e.g., 0.23dB on the Set5 dataset). The main reasons of high performance have been presented in the above analysis. This is the main difference between our method and other CNN acceleration works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Nevertheless, with the guarantee of good performance, it is easier to cooperate with other acceleration methods to get a faster model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SR for Different Upscaling Factors</head><p>Another advantage of FSRCNN over the previous learning-based methods is that FSR-CNN could achieve fast training and testing across different upscaling factors. Specifically, we find that all convolution layers on the whole act like a complex feature extractor of the LR image, and only the last deconvolution layer contains the information of the upscaling factor. This is also proved by experiments, of which the convolution filters are almost the same for different upscaling factors <ref type="bibr" target="#b2">3</ref> . With this property, we can transfer the convolution filters for fast training and testing.</p><p>In practice, we train a model for an upscaling factor in advance. Then during training, we only fine-tune the deconvolution layer for another upscaling factor and leave the convolution layers unchanged. The fine-tuning is fast, and the performance is as good as training from scratch (see <ref type="bibr">Section 4.4)</ref>. During testing, we perform the convolution operations once, and upsample an image to different sizes with the corresponding deconvolution layer. If we need to apply several upscaling factors simultaneously, this property can lead to much faster testing (as illustrated in <ref type="figure">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Training dataset. The 91-image dataset is widely used as the training set in learningbased SR methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1]</ref>. As deep models generally benefit from big data, studies have found that 91 images are not enough to push a deep model to the best performance. Yang et al. <ref type="bibr" target="#b23">[24]</ref> and Schulter et al. <ref type="bibr" target="#b6">[7]</ref> use the BSD500 dataset <ref type="bibr" target="#b24">[25]</ref>. However, images in the BSD500 are in JPEG format, which are not optimal for the SR task. Therefore, we contribute a new General-100 dataset that contains 100 bmp-format images (with no compression) <ref type="bibr" target="#b3">4</ref> . The size of the newly introduced 100 images ranges from 710 × 704 (large) to 131 × 112 (small). They are all of good quality with clear edges but fewer smooth regions (e.g., sky and ocean), thus are very suitable for the SR training. In the following experiments, apart from using the 91-image dataset for training, we will also evaluate the applicability of the joint set of the General-100 dataset and the 91image dataset to train our networks. To make full use of the dataset, we also adopt data augmentation as in <ref type="bibr" target="#b7">[8]</ref>. We augment the data in two ways. 1) Scaling: each image is downscaled with the factor 0.9, 0,8, 0.7 and 0.6. 2) Rotation: each image is rotated with the degree of 90, 180 and 270. Then we will have 5 × 4 − 1 = 19 times more images for training. Test and validation dataset. Following SRCNN and SCN, we use the Set5 <ref type="bibr" target="#b14">[15]</ref>, Set14 <ref type="bibr" target="#b8">[9]</ref> and BSD200 <ref type="bibr" target="#b24">[25]</ref> dataset for testing. Another 20 images from the validation set of the BSD500 dataset are selected for validation. Training samples. To prepare the training data, we first downsample the original training images by the desired scaling factor n to form the LR images. Then we crop the LR training images into a set of f sub × f sub -pixel sub-images with a stride k. The corresponding HR sub-images (with size (nf sub ) 2 ) are also cropped from the ground truth images. These LR/HR sub-image pairs are the primary training data.</p><p>For the issue of padding, we empirically find that padding the input or output maps does little effect on the final performance. Thus we adopt zero padding in all layers according to the filter size. In this way, there is no need to change the sub-image size for different network designs. Another issue affecting the sub-image size is the deconvolution layer. As we train our models with the Caffe package <ref type="bibr" target="#b26">[27]</ref>, its deconvolution filters will generate the output with size (nf sub − n + 1) 2 instead of (nf sub ) 2 . So we also crop (n − 1)-pixel borders on the HR sub-images. Finally, for ×2, ×3 and ×4, we set the size of LR/HR sub-images to be 10 2 /19 2 , 7 2 /19 2 and 6 2 /21 2 , respectively. Training strategy. For fair comparison with the state-of-the-arts (Sec. 4.5), we adopt the 91-image dataset for training. In addition, we also explore a two-step training strategy. First, we train a network from scratch with the 91-image dataset. Then, when the training is saturated, we add the General-100 dataset for fine-tuning. With this strategy, the training converges much earlier than training with the two datasets from the beginning.</p><p>When training with the 91-image dataset, the learning rate of the convolution layers is set to be 10 −3 and that of the deconvolution layer is 10 −4 . Then during fine-tuning, the learning rate of all layers is reduced by half. For initialization, the weights of the convolution filters are initialized with the method designed for PReLU in <ref type="bibr" target="#b22">[23]</ref>. As we do not have activation functions at the end, the deconvolution filters are initialized by the same way as in SRCNN (i.e., drawing randomly from a Gaussian distribution with zero mean and standard deviation 0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Investigation of Different Settings</head><p>To test the property of the FSRCNN structure, we design a set of controlling experiments with different values of the three sensitive variables -the LR feature dimension d, the number of shrinking filters s, and the mapping depth m. Specifically, we choose d = 48, 56, s = 12, 16 and m = 2, 3, 4, thus we conduct a total of 2 × 2 × 3 = 12 experiments with different combinations.</p><p>The average PSNR values on the Set5 dataset of these experiments are shown in <ref type="table" target="#tab_1">Table 2</ref>. We analyze the results in two directions, i.e., horizontally and vertically in the table. First, we fix d, s and examine the influence of m. Obviously, m = 4 leads to better results than m = 2 and m = 3. This trend can also be observed from the convergence curves shown in <ref type="figure">Figure 5(a)</ref>. Second, we fix m and examine the influence of d and s. In general, a better result usually requires more parameters (e.g., a larger d or s), but more parameters do not always guarantee a better result. This trend is also reflected in <ref type="figure">Figure 5</ref>(b), where we see the three largest networks converge together. From all the results, we find the best trade-off between performance and parameters -FSRCNN (56,12,4), which achieves one of the highest results with a moderate number of parameters.</p><p>It is worth noticing that the smallest network FSRCNN (48,12,2) achieves an average PSNR of 32.87 dB, which is already higher than that of SRCNN-Ex (32.75 dB) reported in <ref type="bibr" target="#b1">[2]</ref>. The FSRCNN (48,12,2) contains only 8,832 parameters, then the acceleration compared with SRCNN-Ex is 57184/8832 × 9 = 58.3 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Towards Real-Time SR with FSRCNN</head><p>Now we want to find a more concise FSRCNN network that could realize real-time SR while still keep good performance. First, we calculate how many parameters can meet the minimum requirement of real-time implementation (24 fps). As mentioned in the introduction, the speed of SRCNN to upsample an image to the size 760 × 760 is 1.32 fps. The upscaling factor is 3, and SRCNN has 8032 parameters. Then according to <ref type="bibr">Equation</ref>   <ref type="table" target="#tab_3">Table 3</ref> and 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments for Different Upscaling Factors</head><p>Unlike existing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> that need to train a network from scratch for a different scaling factor, the proposed FSRCNN enjoys the flexibility of learning and testing across upscaling factors through transferring the convolution filters (Sec. 3.4). We demonstrate this flexibility in this section. We choose the FSRCNN (56,12,4) as the default network. As we have obtained a well-trained model under the upscaling factor 3 (in Section 4.2), we then train the network for ×2 on the basis of that for ×3. To be specific, the parameters of all convolution filters in the well-trained model are transferred to the network of ×2. During training, we only fine-tune the deconvolution layer on the 91-image and General-100 datasets of ×2. For comparison, we train another network also for ×2 but from scratch. The convergence curves of these two networks are shown in <ref type="figure">Figure 6</ref>. Obviously, with the transferred parameters, the network converges very fast (only a few hours) with the same good performance as that training form scratch. In the following experiments, we only train the networks from scratch for ×3, and fine-tune the corresponding deconvolution layers for ×2 and ×4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with State-of-the-Arts</head><p>Compare using the same training set. First, we compare our method with four stateof-the-art learning-based SR algorithms that rely on external databases, namely the super-resolution forest (SRF) <ref type="bibr" target="#b6">[7]</ref>, SRCNN <ref type="bibr" target="#b0">[1]</ref>, SRCNN-Ex <ref type="bibr" target="#b1">[2]</ref> and the sparse coding based network (SCN) <ref type="bibr" target="#b7">[8]</ref>. The implementations of these methods are all based on their released source code. As they are written in different programming languages, the comparison of their test time may not be fair, but still reflects the main trend. To have a fair comparison on restoration quality, all models are trained on the augmented 91-image dataset, so the results are slightly different from that in the corresponding paper. We select two representative FSRCNN networks -FSRCNN (short <ref type="figure" target="#fig_1">for FSRCNN (56,12,4)</ref>), and FSRCNN-s (short <ref type="figure" target="#fig_1">for FSRCNN (32,5,1)</ref>). The inference time is tested with the C++ implementation on an Intel i7 CPU 4.0 GHz. The quantitative results (PSNR and test time) for different upscaling factors are listed in <ref type="table" target="#tab_3">Table 3</ref>. We first look at the test time, which is the main focus of our work. The proposed FSRCNN is undoubtedly the fastest method that is at least 40 times faster than SRCNN-Ex, SRF and SCN (with the upscaling factor 3), while the fastest FSRCNN-s can achieve real-time performance (&gt; 24 fps) on almost all the test images. Moreover, the FSRCNN still outperforms the previous methods on the PSNR values especially for ×2 and ×3. We also notice that the FSRCNN achieves slightly lower PSNR than SCN on factor 4. This is mainly because that the SCN adopts two models of ×2 to upsample an image by ×4. We have also tried this strategy and achieved comparable results. However, as we pay more attention to speed, we still present the results of a single network. Compare using different training sets (following the literature). To follow the literature, we also compare the best PSNR results that are reported in the corresponding paper, as shown in <ref type="table" target="#tab_4">Table 4</ref>. We also add another two competitive methods -KK <ref type="bibr" target="#b27">[28]</ref> and A+ <ref type="bibr" target="#b4">[5]</ref> for comparison. Note that these results are obtained using different datasets, and our models are trained on the 91-image and General-100 datasets. From <ref type="table" target="#tab_4">Table 4</ref>, we can see that the proposed FSRCNN still outperforms other methods on most upscaling factors and datasets. We have also done comprehensive comparisons in terms of SSIM and IFC <ref type="bibr" target="#b28">[29]</ref> in <ref type="table" target="#tab_5">Table 5</ref> and 6, where we observe the same trend. The reconstructed images of FSRCNN (shown in <ref type="figure" target="#fig_3">Figure 7 and 8)</ref>, more examples can be found on the project page) are sharper and clearer than other results. In another aspect, the restoration quality of small models (FSRCNN-s and SRCNN) is slightly worse than large models (SRCNN-Ex, SCN and FSRCNN). In <ref type="figure" target="#fig_3">Figure 7</ref>, we could observe some "jaggies" or ringing effects in the results of FSRCNN-s and SRCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>While observing the limitations of current deep learning based SR models, we explore a more efficient network structure to achieve high running speed without the loss of restoration quality. We approach this goal by re-designing the SRCNN structure, and achieves a final acceleration of more than 40 times. Extensive experiments suggest that the proposed method yields satisfactory SR performance, while superior in terms of run time. The proposed model can be adapted for real-time video SR, and motivate fast deep models for other low-level vision tasks.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FSRCNNFig. 2 .</head><label>2</label><figDesc>This figure shows the network structures of the SRCNN and FSRCNN. The proposed FSRCNN is different from SRCNN mainly in three aspects. First, FSRCNN adopts the original low-resolution image as input without bicubic interpolation. A deconvolution layer is introduced at the end of the network to perform upsampling. Second, The non-linear mapping step in SRCNN is replaced by three steps in FSRCNN, namely the shrinking, mapping, and expanding step. Third, FSRCNN adopts smaller filter sizes and a deeper network structure. These improvements provide FSRCNN with better performance but lower computational cost than SRCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The learned deconvolution layer (56 channels) for the upscaling factor 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>The "lenna" image from the Set14 dataset with an upscaling factor 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>The "butterfly" image from the Set5 dataset with an upscaling factor 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The transitions from SRCNN to FSRCNN. SRCNN-Ex Transition State 1 Transition State 2 FSRCNN (56,12,4) Mid part Conv(5,32,64) Conv(5,32,64) 4Conv(3,12,12) 4Conv<ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b11">12)</ref> </figDesc><table><row><cell cols="2">First part Conv(9,64,1)</cell><cell>Conv(9,64,1)</cell><cell>Conv(9,64,1)</cell><cell>Conv(5,56,1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Conv(1,12,64)-</cell><cell>Conv(1,12,56)-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-Conv(1,64,12)</cell><cell>-Conv(1,56,12)</cell></row><row><cell cols="5">Last part Conv(5,1,32) DeConv(9,1,32) DeConv(9,1,64) DeConv(9,1,56)</cell></row><row><cell>Input size</cell><cell>S HR</cell><cell>S LR</cell><cell>S LR</cell><cell>S LR</cell></row><row><cell>Parameters</cell><cell>57184</cell><cell>58976</cell><cell>17088</cell><cell>12464</cell></row><row><cell>Speedup</cell><cell>1×</cell><cell>8.7×</cell><cell>30.1×</cell><cell>41.3×</cell></row><row><cell cols="2">PSNR (Set5) 32.83 dB</cell><cell>32.95 dB</cell><cell>33.01 dB</cell><cell>33.06 dB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The comparison of PSNR (Set5) and parameters of different settings. = 48, s = 12 32.87 (8832) 32.88 (10128) 33.08 (11424) d = 56, s = 12 33.00 (9872) 32.97 (11168) 33.16 (12464) d = 48, s = 16 32.95 (11232) 33.10 (13536) 33.18 (15840) d = 56, s = 16 33.01 (12336) 33.12 (14640) 33.17 (16944)</figDesc><table><row><cell>Settings</cell><cell>m = 2</cell><cell>m = 3</cell><cell>m = 4</cell></row><row><cell>d</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 and 2, the desired FSRCNN network should have at most 8032 × 1.32/24 ×Fig. 5. Convergence curves of different network designs.</figDesc><table><row><cell>Average)test)PSNR)(dB)</cell><cell>28.3 28.4 28.5 28.6</cell><cell cols="3">Number)of)backprops 6 8 (a) We fix d=56, s=16 and change m 2 4 10 FSRCNN)(56,16,4) × 10 8 FSRCNN)(56,16,3) FSRCNN)(56,16,2) 12</cell><cell>Average)test)PSNR)(dB)</cell><cell>28.3 28.4 28.5 28.6 28.2</cell><cell>2 (b) We fix m=4 and change d, s 4 6 8 10 12 FSRCNN)(56,16,4) 14 FSRCNN)(48,16,4) FSRCNN)(56,12,4) FSRCNN)(48,12,4) Number)of)backprops × 10 8</cell></row><row><cell></cell><cell></cell><cell>AveragedtestdPSNRdldBy</cell><cell>31.6 31.8 31.4</cell><cell></cell><cell></cell><cell>fine-tuningdthedlastdlayer trainingdfromdscratch</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Numberdofdbackprops</cell><cell>× 10 8</cell></row><row><cell></cell><cell></cell><cell cols="5">Fig. 6. Convergence curves for different training strategies.</cell></row><row><cell cols="7">3 2 ≈ 3976 parameters. To achieve this goal, we find an appropriate configuration -</cell></row><row><cell cols="7">FSRCNN (32,5,1) that contains 3937 parameters. With our C++ test code, the speed of</cell></row><row><cell cols="7">FSRCNN (32,5,1) reaches 24.7 fps, satisfying the real-time requirement. Furthermore,</cell></row><row><cell cols="7">the FSRCNN (32,5,1) even outperforms SRCNN (9-1-5) [1] (see</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The results of PSNR (dB) and test time (sec) on three test datasets. All models are trained on the 91-image dataset. Time PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Set5 2 33.66 -36.84 2.1 36.33 0.18 36.67 1.3 36.76 0.94 36.53 0.024 36.94 0.068 Set14 2 30.23 -32.46 3.9 32.15 0.39 32.35 2.8 32.48 1.7 32.22 0.061 32.54 0.</figDesc><table><row><cell cols="3">test upscaling Bicubic</cell><cell cols="3">SRF [7] SRCNN [1] SRCNN-Ex [2] SCN [8]</cell><cell>FSRCNN-s</cell><cell>FSRCNN</cell></row><row><cell cols="6">dataset factor PSNR 16</cell></row><row><cell>BSD200</cell><cell>2</cell><cell cols="2">29.70 -31.57 3.1 31.34 0.23 31.53</cell><cell>1.7</cell><cell>31.63 1.1 31.44 0.033 31.73 0.098</cell></row><row><cell>Set5</cell><cell>3</cell><cell cols="2">30.39 -32.73 1.7 32.45 0.18 32.83</cell><cell>1.3</cell><cell>33.04 1.8 32.55 0.010 33.06 0.027</cell></row><row><cell>Set14</cell><cell>3</cell><cell cols="2">27.54 -29.21 2.5 29.01 0.39 29.26</cell><cell>2.8</cell><cell>29.37 3.6 29.08 0.023 29.37 0.061</cell></row><row><cell>BSD200</cell><cell>3</cell><cell cols="2">27.26 -28.40 2.0 28.27 0.23 28.47</cell><cell>1.7</cell><cell>28.54 2.4 28.32 0.013 28.55 0.035</cell></row><row><cell>Set5</cell><cell>4</cell><cell cols="2">28.42 -30.35 1.5 30.15 0.18 30.45</cell><cell>1.3</cell><cell>30.82 1.2 30.04 0.0052 30.55 0.015</cell></row><row><cell>Set14</cell><cell>4</cell><cell cols="2">26.00 -27.41 2.1 27.21 0.39 27.44</cell><cell>2.8</cell><cell>27.62 2.3 27.12 0.0099 27.50 0.029</cell></row><row><cell>BSD200</cell><cell>4</cell><cell cols="2">25.97 -26.85 1.7 26.72 0.23 26.88</cell><cell>1.7</cell><cell>27.02 1.4 26.73 0.0072 26.92 0.019</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The results of PSNR (dB) on three test datasets. We present the best results reported in the corresponding paper. The proposed FSCNN and FSRCNN-s are trained on both 91-image and General-100 dataset. More comparisons with other methods on PSNR, SSIM and IFC<ref type="bibr" target="#b28">[29]</ref> can be found in the supplementary file.</figDesc><table><row><cell cols="9">test upscaling Bicubic KK [28] A+ [5] SRF [7] SRCNN [1] SRCNN-Ex [2] SCN [8] FSRCNN-s FSRCNN</cell></row><row><cell cols="4">dataset factor PSNR PSNR PSNR PSNR</cell><cell>PSNR</cell><cell>PSNR</cell><cell>PSNR</cell><cell>PSNR</cell><cell>PSNR</cell></row><row><cell>Set5</cell><cell>2</cell><cell cols="2">33.66 36.20 36.55 36.89</cell><cell>36.34</cell><cell>36.66</cell><cell>36.93</cell><cell>36.58</cell><cell>37.00</cell></row><row><cell>Set14</cell><cell>2</cell><cell cols="2">30.23 32.11 32.28 32.52</cell><cell>32.18</cell><cell>32.45</cell><cell>32.56</cell><cell>32.28</cell><cell>32.63</cell></row><row><cell>BSD200</cell><cell>2</cell><cell cols="2">29.70 31.30 31.44 31.66</cell><cell>31.38</cell><cell>31.63</cell><cell>31.63</cell><cell>31.48</cell><cell>31.80</cell></row><row><cell>Set5</cell><cell>3</cell><cell cols="2">30.39 32.28 32.59 32.72</cell><cell>32.39</cell><cell>32.75</cell><cell>33.10</cell><cell>32.61</cell><cell>33.16</cell></row><row><cell>Set14</cell><cell>3</cell><cell cols="2">27.54 28.94 29.13 29.23</cell><cell>29.00</cell><cell>29.30</cell><cell>29.41</cell><cell>29.13</cell><cell>29.43</cell></row><row><cell>BSD200</cell><cell>3</cell><cell cols="2">27.26 28.19 28.36 28.45</cell><cell>28.28</cell><cell>28.48</cell><cell>28.54</cell><cell>28.32</cell><cell>28.60</cell></row><row><cell>Set5</cell><cell>4</cell><cell cols="2">28.42 30.03 30.28 30.35</cell><cell>30.09</cell><cell>30.49</cell><cell>30.86</cell><cell>30.11</cell><cell>30.71</cell></row><row><cell>Set14</cell><cell>4</cell><cell cols="2">26.00 27.14 27.32 27.41</cell><cell>27.20</cell><cell>27.50</cell><cell>27.64</cell><cell>27.19</cell><cell>27.59</cell></row><row><cell>BSD200</cell><cell>4</cell><cell cols="2">25.97 26.68 26.83 26.89</cell><cell>26.73</cell><cell>26.92</cell><cell>27.02</cell><cell>26.75</cell><cell>26.98</cell></row><row><cell cols="2">Originalx/xPSNR</cell><cell></cell><cell>Bicubicx/x31.68xdB</cell><cell></cell><cell>SRFx/x33.53xdB</cell><cell></cell><cell cols="2">SRCNNx/x33.39xdB</cell></row><row><cell cols="3">SRCNN-Exx/x33.67xdB</cell><cell>SCNx/x33.61xdB</cell><cell cols="3">FSRCNN-sx/x33.43xdB</cell><cell cols="2">FSRCNNx/x33.85xdB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The results of PSNR (dB), SSIM and IFC<ref type="bibr" target="#b28">[29]</ref> on the Set5<ref type="bibr" target="#b29">[30]</ref>, Set14<ref type="bibr" target="#b8">[9]</ref> and BSD200<ref type="bibr" target="#b24">[25]</ref> datasets. SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC Set5 2 33.66/0.9299/6.10 36.20/0.9511/6.87 35.83/0.9499/8.09 36.55/0.9544/8.48 36.87/0.9556/8.63 Set14 2 30.23/0.8687/6.09 32.11/0.9026/6.83 31.80/0.9004/7.81 32.28/0.9056/8.11 32.51/0.9074/8.22 BSD200 2 29.70/0.8625/5.70 31.30/0.9000/6.26 31.02/0.8968/7.27 31.44/0.9031/7.49 31.65/0.9053/7.60 Set5 3 30.39/0.9299/6.10 32.28/0.9033/4.14 31.92/0.8968/4.52 32.59/0.9088/4.84 32.71/0.9098/4.90 Set14 3 27.54/0.7736/3.41 28.94/0.8132/3.83 28.65/0.8093/4.23 29.13/0.8188/4.45 29.23/0.8206/4.49 BSD200 3 27.26/0.7638/3.19 28.19/0.8016/3.49 28.02/0.7981/3.91 28.36/0.8078/4.07 28.45/0.8095/4.11 Set5 4 28.42/0.8104/2.35 30.03/0.8541/2.81 29.69/0.8419/3.02 30.28/0.8603/3.26 30.35/0.8600/3.26 Set14 4 26.00/0.7019/2.23 27.14/0.7419/2.57 26.85/0.7353/2.78 27.32/0.7471/2.74 27.41/0.7497/2.94 BSD200 4 25.97/0.6949/2.04 26.68/0.7282/2.22 26.56/0.7253/2.51 26.83/0.7359/2.62 26.89/0.7368/2.62</figDesc><table><row><cell>test upscaling</cell><cell>Bicubic</cell><cell>KK [28]</cell><cell>ANR [4]</cell><cell>A+ [4]</cell><cell>SRF [7]</cell></row><row><cell cols="2">dataset factor PSNR/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The results of PSNR (dB), SSIM and IFC<ref type="bibr" target="#b28">[29]</ref> on the Set5<ref type="bibr" target="#b29">[30]</ref>, Set14<ref type="bibr" target="#b8">[9]</ref> and BSD200<ref type="bibr" target="#b24">[25]</ref> datasets. SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC PSNR/SSIM/IFC Set5 2 36.34/0.9521/7.54 36.66/0.9542/8.05 36.76/0.9545/7.32 36.58/0.9532/7.75 37.00/0.9558/8.06 Set14 2 32.18/0.9039/7.22 32.45/0.9067/7.76 32.48/0.9067/7.00 32.28/0.9052/7.47 32.63/0.9088/7.71 BSD200 2 31.38/0.9287/6.80 31.63/0.9044/7.26 31.63/0.9048/6.45 31.48/0.9027/7.01 31.80/0.9074/7.25 Set5 3 32.39/0.9033/4.25 32.75/0.9090/4.58 33.04/0.9136/4.37 32.54/0.9055/4.56 33.16/0.9140/4.88 Set14 3 29.00/0.8145/3.96 29.30/0.8215/4.26 29.37/0.8226/3.99 29.08/0.8167/4.24 29.43/0.8242/4.47 BSD200 3 28.28/0.8038/3.67 28.48/0.8102/3.92 28.54/0.8119/3.59 28.32/0.8058/3.96 28.60/0.8137/4.11 Set5 4 30.09/0.8530/2.86 30.49/0.8628/3.01 30.82/0.8728/3.07 30.11/0.8499/2.76 30.71/0.8657/3.01 Set14 4 27.20/0.7413/2.60 27.50/0.7513/2.74 27.62/0.7571/2.71 27.19/0.7423/2.55 27.59/0.7535/2.70 BSD200 4 26.73/0.7291/2.37 26.92/0.7376/2.46 27.02/0.7434/2.38 26.75/0.7312/2.32 26.98/0.7398/2.41</figDesc><table><row><cell>test upscaling</cell><cell cols="2">SRCNN [1]</cell><cell>SRCNN-Ex [2]</cell><cell>SCN [8]</cell><cell>FSRCNN-s</cell><cell>FSRCNN</cell></row><row><cell cols="4">dataset factor PSNR/Bicubic3/324.043dB Original3/3PSNR</cell><cell cols="2">SRF3/327.963dB</cell><cell>SRCNN3/327.583dB</cell></row><row><cell cols="2">SRCNN-Ex3/327.953dB</cell><cell cols="2">SCN3/328.573dB</cell><cell cols="2">FSRCNN-s3/327.733dB</cell><cell>FSRCNN3/328.683dB</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We follow<ref type="bibr" target="#b10">[11]</ref> to adopt the terminology 'deconvolution'. We note that it carries very different meaning in classic image processing, see<ref type="bibr" target="#b11">[12]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The implementation is available on the project page http://mmlab.ie.cuhk.edu.hk/ projects/FSRCNN.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that in SRCNN and SCN, the convolution filters differ a lot for different upscaling factors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We follow<ref type="bibr" target="#b25">[26]</ref> to introduce only 100 images in a new super-resolution dataset. A larger dataset with more training images will be released on the project page.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is partially supported by SenseTime Group Limited.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="page" from="184" to="199" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast direct super-resolution by simple functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<biblScope unit="page" from="1920" to="1927" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep network cascade for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="page" from="49" to="64" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with superresolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeply improved sparse coding for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="370" to="378" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curves and Surfaces</title>
		<imprint>
			<biblScope unit="page" from="711" to="730" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1790" to="1798" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="1538" to="1546" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L A</forename><surname>Morel</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep cascaded bi-network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Depth map super resolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deeply-recursive convolutional network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1269" to="1277" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Single-image super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed selfexemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ACM MM</title>
		<imprint>
			<biblScope unit="page" from="675" to="678" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
