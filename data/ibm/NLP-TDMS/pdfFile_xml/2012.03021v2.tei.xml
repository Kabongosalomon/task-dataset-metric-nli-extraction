<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Depth estimation from 4D light field videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Kinoshita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kagoshima University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Ono</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kagoshima University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Depth estimation from 4D light field videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Light field video</term>
					<term>Light field dataset</term>
					<term>Depth estimation</term>
					<term>Deep neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth (disparity) estimation from 4D Light Field (LF) images has been a research topic for the last couple of years. Most studies have focused on depth estimation from static 4D LF images while not considering temporal information, i.e., LF videos. This paper proposes an end-to-end neural network architecture for depth estimation from 4D LF videos. This study also constructs a medium-scale synthetic 4D LF video dataset that can be used for training deep learning-based methods. Experimental results using synthetic and real-world 4D LF videos show that temporal information contributes to the improvement of depth estimation accuracy in noisy regions. Dataset and code is available at: https://mediaeng-lfv.github.io/LFV_Disparity_Estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Computational photography has attracted much attention because post-image processing after shooting can synthesize photos that cannot be taken with a normal camera. Light field imaging, one of the major computational photography techniques, captures the amount of light rays from several different directions. 4D light field images (LFIs), which are represented by two spatial and two angular parameters, can be captured by camera arrays, gantries consisting of a single moving camera, and specially designed plenoptic cameras 1 such as Lytro and Raytrix. The spatial-angular information recorded in 4D LFIs has been exploited to improve the performance of many computer vision applications, such as digital refocusing, 1 image segmentation, material classification, super-resolution, and, most recently, depth estimation . <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> Depth estimation from 4D LFIs has been a research topic for the last few years. Due to plenoptic cameras' structure, the baseline between sub-aperture images is very narrow and there exists a trade-off between the spatial and the angular resolution. This makes depth estimation from 4D LFIs captured by a plenoptic camera challenging. Recently, deep learning-based methods <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref> using epipolar plane images (EPIs) <ref type="figure" target="#fig_1">(Fig. 1a</ref>), which consist of two-dimensional angular-spatial slices, achieved high performance in the accuracy metrics of the HCI 4D Light Field Benchmark * , 5 which is often used as a benchmark for evaluating 4D LFIs depth estimation methods. However, previous studies have focused on depth estimation from static 4D LFIs while no studies  have considered the temporal information, i.e., 4D light field videos (LFVs). The success in monocular depth estimation considering the temporal correlations and consistency among consecutive video frames 6 suggests that the temporal information can also help 4D LF-based depth estimation. Therefore, this paper proposes a depth estimation neural network using 4D LFV as input. The proposed method estimates depth maps of scenes by extracting the spatial-angular information from the EPIs volume at each frame and by collecting the temporal information using Convolutional Long Short-Term Memory (CLSTM).</p><p>The contributions of this study are summarized as follows:</p><p>• This study developed a medium-scale synthetic 4D LFV dataset that can be used for training deep learningbased methods from the open-source movie "Sintel". 7</p><p>• To the best of our knowledge, this is the first study on depth estimation from the spatial-angular-temporal information recorded in a 4D LFV.</p><p>• Experimental results using synthetic and real-world 4D LFVs showed that the temporal information helps to improve 4D LF-based depth estimation performance.</p><p>In this study, the target value estimated by the proposed method is the disparity <ref type="figure" target="#fig_1">(Fig. 1b</ref>). Since the disparity and depth can be transformed into each other, this paper uses these terms indistinguishably where it is not necessary to make the distinction between them to facilitate the reader's understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS 2.1 4D LFIs depth estimation methods</head><p>Over the past few years, many deep learning-based depth estimation methods from 4D LFIs have been proposed with great success. <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> Such methods improve both accuracy and runtime compared to conventional optimization-based methods. 2, 10 Heber et al. proposed a method that predicts the slope of lines in EPI using convolutional neural network (CNN) combined with by the high-order regularization. <ref type="bibr" target="#b7">8</ref> They also succeeded in producing high-quality results at low computational cost by extracting geometric information from a LFI using a U-shaped encoder-decoder architecture 9 . Shin et al. <ref type="bibr" target="#b2">3</ref> proposed a fully convolutional neural network with fast and accurate performance in depth estimation. They introduced a multi-stream architecture consisting of multiple inputs of sub-aperture image stacks along four angular directions (horizontal, vertical, left-and right-diagonal). Faluvégi et al. <ref type="bibr" target="#b3">4</ref> showed that, by applying 3D convolution in each stream, the performance was maintained even when only two angular directions (horizontal, vertical) were used instead of the four directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Monocular depth estimation methods</head><p>With the success of CNN in image recognition, many deep learning-based monocular depth estimation methods have been proposed, and some of them estimate depth values from videos by considering the temporal information . <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref> Karsch et al. <ref type="bibr" target="#b10">11</ref> proposed a method to produce temporally consistent depth maps using local motion cues and optical flow. Zhou et al. <ref type="bibr" target="#b11">12</ref> proposed a method that simultaneously estimates depth and camera pose using bundle adjustment and recovers the details of the estimated depth map using a super-resolution network. Zhang et al. <ref type="bibr" target="#b5">6</ref> proposed a video depth estimation framework that captures spatial features as well as temporal correlations between consecutive video frames using CLSTM. In this framework, the trained deep CNN extracts the spatial features from video frames and the subsequent CLSTM collects temporal correlations to estimate depth values. Their framework can handle various kinds of spatial feature extraction network, and thus can be applied to other video depth estimation methods with minimum modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE PROPOSED METHOD</head><p>This paper proposes a neural network architecture for depth estimation from 4D LFV, which builds on top of the findings of the previous studies. <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10</ref> The proposed architecture uses a temporal sequence of two image stacks of horizontal and vertical slices of 4D LFV frames as input. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the proposed architecture consisting of two modules: (1) the first module for the spatial-angular feature extraction from two LF slices (horizontal and vertical) using two-stream 3D CNN, and (2) the second module for depth estimation by collecting the temporal information of the spatial-angular features using CLSTM.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-stream 3D CNN for extracting the spatial-angular features</head><p>The proposed method adopts a two-stream structure to extract meaningful representations of the horizontal and vertical directions from the sub-aperture images sliced in their direction. Focusing on Faluvegi et al. <ref type="bibr" target="#b3">4</ref> 's model, which benefits from having fewer parameters to be trained, the proposed method employs 3D CNN for spatial-angular feature extraction. Here, the viewpoint (angle) index is the third spatial dimension for the 3D convolution, since the slope of lines in EPIs <ref type="figure" target="#fig_1">(Fig. 1a</ref>) represents depth. In the following, we describe an example of a 4D LF with an angular resolution of 9 × 9.</p><p>Each of the two streams that process the horizontal and vertical sub-aperture image stacks first applies preprocesses consisting of two steps. First, zero-mean normalization is applied to the input data in batches, resulting in better emphasizing the differences in the lines of EPIs and making it easier for the network to learn. Next, a spatial padding of 4 × 4 is applied so that the spatial dimension of the feature maps can be maintained when applying 3D convolution.</p><p>Each stream consists of four consecutive 3D convolutional layers with kernel size 3 × 3 × 3 and stride size one. The number of feature maps to be output is 32 in the first convolutional layer and 64 in the remaining three layers. The four 3D convolutional layers reduce the dimension of feature maps corresponding to the number of views from nine to one while keeping the spatial dimension the same size as the input.</p><p>The two feature map sets obtained from the two streams are concatenated (totally 128 maps) and input into the next five 2D convolutional layers in order to produce higher level representation. The proposed method uses 2D convolutional layers with kernel size 3 × 3 and zero padding to maintain the feature map size. The number of feature maps gradually decreases to 64, 32, 32, 32, and 16 as passing through the five convolutional layers.</p><p>All of the above 2D and 3D convolutional layers use rectified linear unit as the activation function, and all kernel weights are initialized with Glorot initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CLSTM for collecting the temporal information</head><p>As the input frames are continuous in the temporal dimension, taking the temporal information of these frames into consideration may help to estimate the depth. Following the previous work, 6 the proposed method employs CLSTM shown in <ref type="figure" target="#fig_3">Fig. 3</ref> for collecting the temporal information of the spatial-angular features extracted by CNN (Sec. 3.1). In this study, the number of feature maps in the hidden state f t is set to eight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss function</head><p>This study employs the loss function proposed by Hu et al. <ref type="bibr" target="#b12">13</ref> to train the network. While Hu et al. adopted the logarithm of the difference between the estimated and ground-truth depth value, this study uses the difference directly as a loss since the proposed method estimates disparity that is inversely proportional to depth. The loss function L consists of three terms, designed to penalize inaccurate disparity estimations, the errors around edges, and small structural errors such as high-frequency undulation of a surface, respectively:</p><formula xml:id="formula_0">L = l disparity + λl grad + µl normal l disparity = 1 n n i=1 d i − g i 1 l grad = 1 n n i=1 ( ∇ x (d i ) − ∇ x (g i ) 1 + ∇ y (d i ) − ∇ y (g i ) 1 ) l normal = 1 n n i=1 1 − η d i · η g i η d i · η d i η g i · η g i ,<label>(1)</label></formula><p>where λ and µ are weighting coefficients. n denotes the number of pixels; d i and g i are the estimated and ground-truth disparity of pixel i, respectively. ∇ x and ∇ y represent the spatial derivative along the x-axis and y-axis respectively.</p><formula xml:id="formula_1">η d i = [−∇ x (d i ), −∇ y (d i )</formula><p>, 1] and · denotes inner product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">4D LIGHT FIELD VIDEO DATASET</head><p>This study developed the Sintel 4D LFV dataset from the open-source movie "Sintel" 7 ( <ref type="figure" target="#fig_1">Fig. 1</ref>) because it was difficult to accurately evaluate the effectiveness of deep learning-based 4D LFVs depth estimation methods with existing 4D LFV datasets <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15</ref> due to small number of samples or the lack of ground-truth disparity values.</p><p>The generated dataset consists of 23 synthetic 4D LFVs with 1, 204 × 436 pixels, 9 × 9 views, and 20-50 frames, and includes the ground-truth disparity values in the central view, so that it can be used for training deep learning-based methods.</p><p>Each scene was rendered with a "clean" pass after modifying the production file of "Sintel" with reference to the MPI Sintel dataset. <ref type="bibr" target="#b15">16</ref> A "clean" pass includes complex illumination and reflectance properties including specular reflections, such as smooth shading and specular reflections, while bokeh, motion blur, and semitransparent objects are excluded. The 4D LFVs were captured by moving the camera with a baseline of 0.01[m] towards a common focus plane while keeping the optical axes parallel. The ground-truth disparity value was obtained by transforming the depth value obtained in Blender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>The performance of the proposed method is evaluated using the Sintel 4D LFV dataset and the real-world 4D LFVs. 14 By comparing with the conventional method 4 as the baseline that does not consider the temporal information, we examined whether the temporal information contributes to improve the depth estimation performance. The baseline method has a structure that replaces the CLSTM module of the proposed method with a 2D convolutional layer producing a single feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>In the experiments, 23 scenes in the Sintel 4D LFV dataset created in Sec. 4 were divided into three subsets, i.e., 16 scenes that were used for training, three scenes for validation and four scenes for test. In total, we extracted about 1.4M patch samples with a spatial resolution of 32 × 32 and a frame length of five using a spatial stride of 16 and a temporal stride of one. By the scene-wise splitting, about 1M, 130K and 300K samples were allocated to the training, validation and test data, respectively.</p><p>Parameters were configured as follows: The upper limit of epoch and the batch size were set to 20 and 64, respectively. Adam optimizer 17 was employed. The learning rate started at 0.0005 and was decreased by a factor   of 0.1 after 10 and 15 epochs. The weights λ and µ of loss function L were set as λ = 1 and µ = 1. It took about four days for the proposed network to be trained on a computer with a NVIDIA GTX 1080Ti.</p><p>The results were measured using mean square error (MSE) and bad pixel ratio (BadPix), which are common in 4D LFIs depth estimation. <ref type="bibr" target="#b4">5</ref> The definition of BadPix is the percentage of pixels whose absolute errors exceed the specified threshold, i.e., d i −g i 1 &gt; t, where t is the threshold. Three thresholds are often used for calculating the bad pixel ratios: 0.01, 0.03, and 0.07. These values were calculated based on 1, 024 × 432 depth maps that were reconstructed by combining the depth estimation results for test patches. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of the proposed method and the baseline method 4 that does not consider the temporal information on the four test scenes. In most of the test scenes, the proposed method achieved better performance than the baseline by considering the temporal information. Focusing on the results of thebigfight2, the proposed method was slightly inferior in MSE but superior in BadPix, indicating that the proposed method estimated better disparity values in regions that were difficult to estimate for the baseline method. <ref type="figure" target="#fig_4">Fig. 4</ref> shows example estimation results for ambushfight5 scene, which included noisy textured rock surfaces and two characters fighting. As can be seen from <ref type="figure" target="#fig_4">Fig. 4</ref>, by using the temporal information the proposed method can estimate a better disparity map with a smoother surface in noisy regions than the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>On the other hand, some regions were difficult to estimate the depth even for the proposed method, e.g. the occipital region in the image center. This region corresponded to textureless regions defined by Shin et al., <ref type="bibr" target="#b2">3</ref> i.e., the mean absolute difference between a center pixel and other pixels in the patch was less than 0.02.</p><p>In addition to the synthetic data, this study also tested the proposed method using real-world 4D LFVs. 14 The real-world data captured by a light field camera is challenging as they contain severe image noise due to the inherent structural problem in the camera. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the disparity estimation results using the model trained on the Sintel 4D LFV dataset. We observed that the proposed method provided more natural disparity estimation than the baseline method. The results also showed that the synthetic 4D LFV dataset created in this study was sufficiently realistic to train deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>This paper proposes an end-to-end neural network architecture for depth estimation from 4D LFV. The proposed method employs two-stream CNN and CLSTM to process the spatial-angular-temporal information recorded in a 4D LFV. This study also constructs a synthetic 4D LFV dataset from the open-source movie "Sintel", 7 which can be used for training deep learning-based methods. Experimental results showed that the temporal information contributed to the performance of 4D LF-based depth estimation and that depth estimation was possible even in noisy real-world data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>E</head><label></label><figDesc>-mail: {sc115015, ono}@ibe.kagoshima-u.ac.jp * https://lightfield-analysis.uni-konstanz.de/ 4D LFI and corresponding horizontal and vertical EPIs. (b) The ground-truth depth (disparity) map corresponding to central view of 4D LFI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Exsample of the 4D LFV dataset developed in this study. arXiv:2012.03021v2 [cs.CV] 8 Dec 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The proposed architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>CLSTM structure of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The estimation results of the proposed method and the baseline method.<ref type="bibr" target="#b3">4</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The results of real-world data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of the propsed method and the baseline method 4 on the test scenes of the Sintel 4D LFV dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Center view</cell><cell>Baseline</cell><cell>Proposed</cell></row><row><cell></cell><cell></cell><cell>ambushfight5</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MSE*100 BadPix(0.07) BadPix(0.03) BadPix(0.01)</cell></row><row><cell>Baseline 4</cell><cell>0.2590</cell><cell>11.005</cell><cell>30.3701</cell><cell>65.4103</cell></row><row><cell>Proposed</cell><cell>0.2167</cell><cell>8.3404</cell><cell>22.8762</cell><cell>62.0493</cell></row><row><cell></cell><cell></cell><cell>bamboo3</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MSE*100 BadPix(0.07) BadPix(0.03) BadPix(0.01)</cell></row><row><cell>Baseline 4</cell><cell>0.2895</cell><cell>12.1862</cell><cell>29.2804</cell><cell>57.9944</cell></row><row><cell>Proposed</cell><cell>0.2159</cell><cell>8.9475</cell><cell>21.8162</cell><cell>53.2985</cell></row><row><cell></cell><cell></cell><cell>shaman2</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MSE*100 BadPix(0.07) BadPix(0.03) BadPix(0.01)</cell></row><row><cell>Baseline 4</cell><cell>3.391</cell><cell>37.9157</cell><cell>53.4889</cell><cell>75.6842</cell></row><row><cell>Proposed</cell><cell>2.4421</cell><cell>32.7585</cell><cell>50.6706</cell><cell>74.7733</cell></row><row><cell></cell><cell></cell><cell>thebigfight2</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MSE*100 BadPix(0.07) BadPix(0.03) BadPix(0.01)</cell></row><row><cell>Baseline 4</cell><cell>0.0305</cell><cell>1.0964</cell><cell>4.4142</cell><cell>18.6974</cell></row><row><cell>Proposed</cell><cell>0.0367</cell><cell>1.0688</cell><cell>3.6084</cell><cell>17.7493</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Light field photography with a hand-held plenoptic camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brédif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth from combining defocus and correspondence using light-field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="673" to="680" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Epinet: A fully-convolutional neural network using epipolar geometry for depth from light field images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="4748" to="4757" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A 3d convolutional neural network for light field depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Á</forename><surname>Faluvégi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bolseé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Dȃdârlat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Munteanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dataset and evaluation methodology for depth estimation on 4d light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploiting temporal consistency for real-time video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1725" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sintel. Durian Open Movie Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roosendaal</surname></persName>
		</author>
		<ptr target="https://durian.blender.org/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Producer)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional networks for shape from light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3746" to="3754" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural epi-volume networks for shape from light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2252" to="2260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust depth estimation for light field via spinning parallelogram operator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="148" to="159" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised learning of monocular depth estimation with bundle adjustment, super-resolution and clip loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03368</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1043" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Light field video capture using a learning-based hybrid imaging system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A feature-based approach for light field video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1204" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
