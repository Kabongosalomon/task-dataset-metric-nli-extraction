<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP MULTI-FRAME MVDR FILTERING FOR SINGLE-MICROPHONE SPEECH ENHANCEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Tammen</surname></persName>
							<email>marvin.tammen@uni-oldenburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Physics and Acoustics and Cluster of Excellence Hearing4all</orgName>
								<orgName type="institution">University of Oldenburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Doclo</surname></persName>
							<email>simon.doclo@uni-oldenburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Physics and Acoustics and Cluster of Excellence Hearing4all</orgName>
								<orgName type="institution">University of Oldenburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEEP MULTI-FRAME MVDR FILTERING FOR SINGLE-MICROPHONE SPEECH ENHANCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Single-Microphone Speech Enhancement</term>
					<term>Multi-Frame Filtering</term>
					<term>Temporal Convolutional Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-frame algorithms for single-microphone speech enhancement, e.g., the multi-frame minimum variance distortionless response (MFMVDR) filter, are able to exploit speech correlation across adjacent time frames in the short-time Fourier transform (STFT) domain. Provided that accurate estimates of the required speech interframe correlation vector and the noise correlation matrix are available, it has been shown that the MFMVDR filter yields a substantial noise reduction while hardly introducing any speech distortion. Aiming at merging the speech enhancement potential of the MFMVDR filter and the estimation capability of temporal convolutional networks (TCNs), in this paper we propose to embed the MFMVDR filter within a deep learning framework. The TCNs are trained to map the noisy speech STFT coefficients to the required quantities by minimizing the scale-invariant signal-to-distortion ratio loss function at the MFMVDR filter output. Experimental results show that the proposed deep MFMVDR filter achieves a competitive speech enhancement performance on the Deep Noise Suppression Challenge dataset. In particular, the results show that estimating the parameters of an MFMVDR filter yields a higher performance in terms of PESQ and STOI than directly estimating the multi-frame filter or single-frame masks and than Conv-TasNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In many hands-free speech communication systems such as hearing aids, mobile phones and smart speakers, ambient noise may degrade the speech quality and intelligibility of the recorded microphone signals. To alleviate this issue, several single-and multi-microphone speech enhancement algorithms have been proposed <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Singlemicrophone speech enhancement algorithms typically 1) transform the noisy time-domain signal to a domain that is better suited for speech enhancement, e.g., the short-time Fourier transform (STFT) domain, 2) apply a (real-or complex-valued) gain/mask to the transform-domain coefficients to obtain an estimate of the clean speech, and 3) transform the modified coefficients back to the time-domain. For such single-frame algorithms, many traditional model-based approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> as well as supervised learningbased approaches <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> have been proposed. A disadvantage of single-frame algorithms is that attenuation of the noise component may be accompanied by some distortion of the speech component in the enhanced signal. In contrast to single-frame algorithms, multi-frame algorithms have been proposed which apply a complex-valued filter to the noisy speech STFT coefficients <ref type="bibr" target="#b2">[3]</ref>. Also for multi-frame algorithms both model-based approaches such as the multi-frame minimum variance distortionless response (MFMVDR) filter <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> as well as supervised learning-based approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed. The MFMVDR filter has been derived to explicitly take speech correlations across adjacent time frames into account and requires an estimate of the noise correlation matrix and the so-called speech interframe correlation (IFC) vector in each time-frequency bin. Although it has been shown that the MFMVDR filter can yield a good noise reduction performance and little speech distortion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, its performance is very sensitive to estimation errors of the required quantities, in particular the speech IFC vector <ref type="bibr" target="#b14">[15]</ref>.</p><p>To estimate the speech IFC vector from the noisy speech STFT coefficients, several model-based approaches have been proposed. In <ref type="bibr" target="#b13">[14]</ref> a maximum likelihood approach has been derived, assuming that the speech and noise IFC vectors follow multi-variate Gaussian distributions. This approach requires an estimate of the a-priori signal-to-noise ratio (SNR), which can be estimated, e.g., using the decision-directed approach <ref type="bibr" target="#b5">[6]</ref> or using a supervised learning-based approach <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b15">[16]</ref> a subspace estimator has been proposed, which is based on a low-rank speech model. However, simulation results have shown that estimating the required quantities from the noisy speech STFT coefficients using these model-based approaches typically results in a large performance degradation compared to the oracle MFMVDR filter.</p><p>In this paper we propose to embed the MFMVDR filter within a deep learning framework as shown in <ref type="figure">Fig. 1</ref>. More in particular, we propose to train temporal convolutional networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref> to map the noisy speech STFT coefficients to the required quantities, i.e., the noise correlation matrix and the a-priori SNR, by minimizing the scale-invariant signal-to-distortion ratio loss function <ref type="bibr" target="#b20">[21]</ref> at the MFMVDR filter output. Experimental results using the INTERSPEECH 2020 Deep Noise Suppression (DNS) Challenge dataset <ref type="bibr" target="#b21">[22]</ref> show that the proposed deep MFMVDR filter outperforms complex-valued masking as well as directly estimating the multi-frame filter without exploiting the MFMVDR structure and Conv-TasNet <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SIGNAL MODEL</head><p>We consider an acoustic scenario with a single microphone recording one speech source and additive ambient noise. In the STFT-domain, the noisy microphone signal is given by  <ref type="bibr" target="#b13">(14)</ref> filter (4) correlation matrices <ref type="bibr" target="#b11">(12)</ref>, <ref type="bibr" target="#b12">(13)</ref> f l y f l n Φ y,l , Φ n,l ξ l γ x,l w l X l speech IFC vector (11) <ref type="figure">Fig. 1</ref>. Block diagram of the training process of the proposed deep MFMVDR filter. The speech enhancement-related loss function is used to update the weights of the temporal convolutional networks estimating the noisy speech and noise correlation matrices Φ y,l and Φ n,l as well as the a-priori SNR ξ l .</p><formula xml:id="formula_0">Y k,l = X k,l + N k,l ,<label>(1)</label></formula><p>where Y k,l , X k,l , and N k,l denote the noisy speech component, the speech component, and the noise component, respectively, at the kth frequency bin and the l-th time frame. Since all frequency bins are assumed to be independent, the index k will be omitted in the remainder of this paper.</p><p>In single-frame speech enhancement algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, the speech component X l is typically estimated by applying a (realor complex-valued) gain/mask W l to the noisy speech STFT coefficients, i.e.,</p><formula xml:id="formula_1">X l = W l Y l .<label>(2)</label></formula><p>In multi-frame speech enhancement algorithms <ref type="bibr" target="#b2">[3]</ref>, the N -dimensional noisy speech vector y l is defined as</p><formula xml:id="formula_2">y l = [Y l , Y l−1 , . . . , Y l−N +1 ] T ,<label>(3)</label></formula><p>where • T denotes the transpose operator. Using (1), the noisy speech vector y l can be written as y l = x l + n l , where the speech vector x l and the noise vector n l are defined similarly as y l in (3). The speech component X l is then estimated by applying a (complexvalued) finite impulse response filter w l with N taps to the noisy speech STFT coefficients, i.e.,</p><formula xml:id="formula_3">X l = w H l y l ,<label>(4)</label></formula><p>where • H denotes the Hermitian operator. Assuming that the speech and noise components are uncorrelated, the N × N -dimensional noisy speech correlation matrix Φ y,l = E y l y H l , with E{•} the expectation operator, can be written as</p><formula xml:id="formula_4">Φ y,l = Φ x,l + Φ n,l ,<label>(5)</label></formula><p>with the speech and noise correlation matrices Φ x,l = E x l x H l and Φ n,l = E n l n H l . In <ref type="bibr" target="#b12">[13]</ref>, it has been proposed to exploit the speech correlation across adjacent time frames by decomposing the speech vector into a temporally correlated and a temporally uncorrelated part, i.e.,</p><formula xml:id="formula_5">x l = γ x,l X l correlated + x l uncorrelated ,<label>(6)</label></formula><p>where the (highly time-varying) speech IFC vector γ x,l describes the correlation between the current and previous time frames w.r.t. the speech STFT coefficient X l , i.e.,</p><formula xml:id="formula_6">γ x,l = E {x l X * l } E |X l | 2 = Φ x,l e e T Φ x,l e ,<label>(7)</label></formula><p>where • * denotes the complex-conjugate operator, e = [1, 0, . . . , 0] T is an N -dimensional selection vector, and e T Φ x,l e = E |X l | 2 = φ X,l corresponds to the speech power spectral density (PSD). Using (5) and <ref type="formula" target="#formula_6">(7)</ref>, the speech IFC vector γ x,l can be written as</p><formula xml:id="formula_7">γ x,l = 1 + ξ l ξ l Φ y,l e e T Φ y,l e − 1 ξ l Φ n,l e e T Φ n,l e γ n,l<label>(8)</label></formula><p>where ξ l = φ X,l φ N,l denotes the a-priori SNR, with φ N,l = E |N l | 2 = e T Φ n,l e the noise PSD, and γ n,l denotes the noise IFC vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DEEP MULTI-FRAME MVDR FILTER</head><p>In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> the MFMVDR filter for single-microphone speech enhancement has been proposed, which aims at minimizing the output noise PSD while not distorting the correlated speech component γ x,l X l , i.e.,</p><formula xml:id="formula_8">min w l ∈ C N w H l Φ n,l w l , s.t. w H l γ x,l = 1.<label>(9)</label></formula><p>Solving this constrained optimization problem yields the MFMVDR filter vector:</p><formula xml:id="formula_9">w MFMVDR,l = Φ −1 n,l γ x,l γ H x,l Φ −1 n,l γ x,l<label>(10)</label></formula><p>To implement the MFMVDR filter, estimates of the noise correlation matrix Φ n,l as well as the speech IFC vector γ x,l are required. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> it has been shown that the speech enhancement performance of the MFMVDR filter strongly depends on how well these quantities can be estimated from the noisy speech STFT coefficients. Previously proposed model-based approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> simply use an estimate of the noisy speech correlation matrix Φ y,l instead of the noise correlation matrix Φ n,l (leading to the multi-frame minimum power distortionless response filter) and estimate the speech IFC vector γ x,l based on (8) by using the decision-directed approach <ref type="bibr" target="#b5">[6]</ref> to estimate the a-priori SNR ξ l and assuming the noise IFC vector γ n,l to be constant for all time-frequency points.</p><p>In this paper we propose to estimate the required quantities for the MFMVDR filter in (10) from the noisy speech STFT coefficients using a supervised learning-based approach by minimizing a speech enhancement-related loss function at the MFMVDR filter output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Speech IFC Vector</head><p>Due to its highly time-varying nature, the speech IFC vector γ x,l is difficult to estimate accurately. Since preliminary experiments have shown that using (8) instead of directly estimating γ x,l with a DNN yields a higher speech enhancement performance, we propose to estimate γ x,l using the estimated correlation matrices Φ y,l and Φ n,l as well as the estimated a-priori SNR ξ l , i.e.,</p><formula xml:id="formula_10">γ x,l = 1 + ξ l ξ l Φ y,l e e T Φ y,l e − 1 ξ l Φ n,l e e T Φ n,l e<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Correlation Matrices</head><p>Since the N × N -dimensional correlation matrices Φ y,l and Φ n,l can be assumed to be Hermitian positive semidefinite (PSD), each matrix consists of a total of N 2 real-valued coefficients, denoted by h y,l and h n,l . As illustrated in <ref type="figure">Fig. 1</ref>, we propose to estimate these coefficients using separate DNNs f l y and f l n in state l, i.e.,</p><formula xml:id="formula_11">h y,l = f l y (y c,l ) h n,l = f l n (y c,l ) ,<label>(12)</label></formula><p>where • denotes an estimate of •, and y c,l = [ Y l , Y l ] T denotes the vector of the real and imaginary parts of the noisy speech STFT coefficient Y l . Since the coefficients h y,l and h n,l are unbounded, a linear activation is used for f l y and f l n . The Hermitian PSD correlation matrix estimates are obtained as</p><formula xml:id="formula_12">Φ y,l = H y,l H H y,l , H y,l = Hermitian h y,l Φ n,l = H n,l H H n,l , H n,l = Hermitian h n,l<label>(13)</label></formula><p>where Hermitian {h} assembles an N × N -dimensional Hermitian matrix from the (real-valued) N 2 -dimensional vector h, and the matrix multiplication ensures that the correlation matrix estimates are Hermitian PSD. It should be noted that no correlation matrix labels are used in the training process -instead, the DNNs are trained to minimize the speech enhancement-related loss function at the MFMVDR filter output (see Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A-Priori SNR</head><p>Similarly to the approach described in the previous section, we propose to use a DNN f l ξ to map noisy speech features to an a-priori SNR estimate ξ l , i.e.,</p><formula xml:id="formula_13">ξ l = f l ξ (log 10 |Y l |)<label>(14)</label></formula><p>Since ξ l ≥ 0, a softmax activation is used for f l ξ . Similarly as for the correlation matrices, the DNN is trained to output an a-priori SNR estimate ξ l such that the speech enhancement-related loss function at the MFMVDR filter output is minimized (see Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>In this section, the speech enhancement performance of the proposed deep MFMVDR filter is compared to several baseline deep learningbased speech enhancement algorithms (see Section 4.1). In Sections 4.2 -4.4 we discuss the used dataset, DNN architecture, and algorithm settings. In Section 4.5 we present the results in terms of the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b22">[23]</ref> and short-time objective intelligibility (STOI) <ref type="bibr" target="#b23">[24]</ref> improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline Algorithms</head><p>As baseline single-microphone speech enhancement algorithms, we consider three deep learning-based algorithms:</p><p>1. Masking: in order to investigate the possible benefit of multiframe filtering, i.e., N &gt; 1, we also consider the complexvalued gain/mask in (2), i.e., N = 1: <ref type="bibr" target="#b14">(15)</ref> where the bounds for the real and imaginary parts are motivated by <ref type="bibr" target="#b24">[25]</ref>.</p><formula xml:id="formula_14">W M,l = f l M (y c,l ) ∈ C; W M,l , W M,l ∈ [−2, 2],</formula><p>2. Direct filtering: in order to investigate the possible benefit of using the MFMVDR filter structure in (10), we also consider directly estimating the complex-valued coefficients of the Ndimensional multi-frame filter w l in (4), similarly to <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_15">w F,l = f l F (y c,l ) ∈ C N ; w F,l , w F,l ∈ [−1, 1],<label>(16)</label></formula><p>where the bounds for the real and imaginary parts are motivated by <ref type="bibr" target="#b16">[17]</ref>.</p><p>3. Conv-TasNet <ref type="bibr" target="#b10">[11]</ref>: instead of considering the STFT-domain as the transform-domain for speech enhancement, Conv-TasNet uses learnable transformations and applies a realvalued mask in the transform-domain. For a fair comparison with the other considered algorithms, we considered the causal version of Conv-TasNet <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset</head><p>All considered algorithms were trained and evaluated on the DNS Challenge dataset <ref type="bibr" target="#b21">[22]</ref>. In total, this dataset contains more than 500 h of speech from 2150 speakers and 180 h of noise from 150 different noise classes at a sampling frequency of 16 kHz. For training and validation, we randomly selected a subset of 45000 utterances of length 4 s, with SNRs uniformly sampled from [0, 20] dB. Using a validation split of 20 %, this resulted in 40 h for training and 10 h for validation, respectively. Evaluation was performed on the DNS Challenge synthetic test set without reverberation. This test set is disjoint from the training and validation set and includes 20 speakers, 12 VoIP-relevant noise sources, and SNRs uniformly sampled from [0, 25] dB, in total consisting of 150 utterances of length 10 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">DNN Architecture</head><p>As the DNN architecture for all estimators, we used temporal convolutional networks (TCNs) <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b0">1</ref> , which have been demonstrated to exhibit strong temporal and spectral modeling capabilities <ref type="bibr" target="#b10">[11]</ref>. Without performing extensive hyperparameter optimization, we fixed the hyperparameters of all TCN modules (except for the Conv-TasNet baseline, for which we used the hyperparameters proposed in <ref type="bibr" target="#b10">[11]</ref>) to 2 stacks of 4 layers each, with a kernel size of 3, resulting in a temporal receptive field of 128 ms. Aiming at a fair comparison, the number of hidden dimensions was varied to obtain a similar total number of trainable weights for all considered algorithms (cf. <ref type="table" target="#tab_1">Table 1</ref>). Note that this hyperparameter was varied as opposed to the number of stacks/layers or the kernel size, since increasing the temporal receptive field might give an unfair advantage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Algorithm Settings</head><p>In order to be able to exploit speech correlation, an STFT with high temporal resolution, i.e., a frame length of 8 ms and a frame shift of 2 ms, was employed for all STFT-based algorithms, similarly as in <ref type="bibr" target="#b12">[13]</ref>. A Hann window was used both as analysis and synthesis window. The multi-frame algorithms (i.e., the proposed deep MFMVDR filter and direct filtering) use a filter length of N = 5, such that speech correlation within 16 ms can be exploited.</p><p>To improve the numerical stability of the matrix inversion in <ref type="formula" target="#formula_0">(10)</ref>, Tikhonov regularization with a regularization constant δ = 10 −3 was used <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Finally, a minimum gain of -17 dB was included in all algorithms except Conv-TasNet. The TCNs were trained for 50 epochs using the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with an initial learning rate of 3 * 10 −4 . The learning rate was halved after the validation loss did not decrease for 3 consecutive epochs, and training was stopped early in case the validation loss did not decrease for 10 consecutive epochs. The gradient norms were clipped to 5, and the batch size was set to 6 to maximize the usage of graphics card memory. As loss function, we used the negative scale-invariant signal-to-distortion ratio (SI-SDR) <ref type="bibr" target="#b20">[21]</ref>, i.e., SI-SDR = 10 log 10 |αx| 2</p><formula xml:id="formula_16">|αx − x| 2 , α = x Tx ||x|| 2 ,<label>(17)</label></formula><p>wherex and x denote the speech signal and the estimated speech signal in the time-domain. All algorithms were implemented in PyTorch 1.6.0, and training and evaluation were performed on an NVIDIA GeForce ® RTX 2080 Ti graphics card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results</head><p>For all considered algorithms, <ref type="table" target="#tab_2">Table 2</ref> shows the average improvement in terms of PESQ and STOI w.r.t. the noisy microphone signals using the speech signal as reference signal. As can be observed, all considered algorithms yield a significant PESQ and STOI improvement, where the proposed deep MFMVDR filter outperforms all other algorithms. A minor performance improvement can be observed between direct filtering (N = 5) and masking (N = 1), hinting at the potential of exploiting multiple frames. A much larger improvement can be observed between deep MFMVDR filtering and direct filtering, showing that exploiting the MFMVDR filter structure and guiding the TCN training to estimate the required quantities (correlation matrices and a-priori SNR) instead of directly estimating the filter coefficients is advantageous. Exemplary audio examples for all considered algorithms are available online 2 . As a measure for computational complexity, <ref type="table" target="#tab_2">Table 2</ref> also shows the average real-time factor (RTF) for all considered algorithms, defined as RTF = (processing time)/(signal length), processed using 4 cores of an Intel ® Xeon ® CPU clocked at 2.6 GHz. Although the proposed deep MFMVDR filter results in a larger computational 2 https://uol.de/en/mediphysics-acoustics/sigproc/research/audio-demos complexity than directly estimating the filter coefficients, its computational complexity is similar to that of Conv-TasNet. A PyTorch implementation of the deep MFMVDR filter is available online 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper we proposed a supervised learning-based approach to estimate the required parameters of an MFMVDR filter for singlemicrophone speech enhancement. Because the MFMVDR filter requires accurate estimates of the noisy speech and noise correlation matrices as well as the speech IFC vector, we proposed to utilize the temporal and spectral modeling capabilities of TCNs for this estimation task. The TCNs are trained to map the noisy speech STFT coefficients to the required parameters by minimizing the SI-SDR loss function at the output of the MFMVDR filter. Experiments on the DNS Challenge dataset demonstrate the benefits of (i) using a multi-frame algorithm as compared to a single-frame algorithm, and (ii) guiding the TCN training by using the MFMVDR filter structure instead of directly estimating the filter coefficients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -Project ID 390895286 -EXC 2177/1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>TCN module hyperparameters.</figDesc><table><row><cell>algorithm</cell><cell cols="2">hidden dimension trainable weights</cell></row><row><cell>masking</cell><cell>226</cell><cell>5.0 M</cell></row><row><cell>direct filtering</cell><cell>225</cell><cell>5.1 M</cell></row><row><cell>Conv-TasNet</cell><cell>128</cell><cell>5.0 M</cell></row><row><cell cols="2">deep MFMVDR 128</cell><cell>5.3 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>algorithm∆PESQ / MOS ∆STOI real-time factor PESQ and STOI improvement as well as real-time factors obtained on the DNS Challenge synthetic test set without reverberation, averaged over all utterances.</figDesc><table><row><cell>masking</cell><cell>0.65</cell><cell>0.037</cell><cell>0.068</cell></row><row><cell>direct filtering</cell><cell>0.67</cell><cell>0.038</cell><cell>0.070</cell></row><row><cell>Conv-TasNet</cell><cell>0.67</cell><cell>0.041</cell><cell>0.194</cell></row><row><cell cols="2">deep MFMVDR 0.76</cell><cell>0.042</cell><cell>0.176</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We used the implementation provided by the authors of<ref type="bibr" target="#b10">[11]</ref>, available at https://github.com/naplab/Conv-TasNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://uol.de/en/mediphysics-acoustics/sigproc/research/code-examples</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Digital speech transmission: enhancement, coding and error concealment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>John Wiley</publisher>
			<pubPlace>Chichester, England; Hoboken, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">DFT-Domain Based Single-Microphone Noise Reduction for Speech Enhancement: A Survey of the State of the Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note>Speech Enhancement in the STFT Domain</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multichannel signal enhancement algorithms for assisted listening devices: Exploiting spatial diversity using multiple microphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Nordholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="30" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Audio Source Separation and Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum mean-square error short-time spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="1984-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Phase Processing for Single-Channel Speech Enhancement: History and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krawczyk-Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Regression Approach to Speech Enhancement Based on Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On Training Targets for Supervised Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech Intelligibility Potential of General and Specialized Deep Neural Network Based Speech Enhancement Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2017-01" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="153" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gated Residual Networks With Dilated Convolutions for Monaural Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Multi-Frame Approach to the Frequency-Domain Single-Channel Noise Reduction Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1256" to="1269" />
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of Subband Speech Correlations for Noise Reduction via MVDR Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1355" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sensitivity analysis of the multiframe MVDR filter for single-microphone speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Signal Processing Conference</title>
		<meeting>European Signal essing Conference<address><addrLine>Kos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="603" to="607" />
		</imprint>
		<respStmt>
			<orgName>EU-SIPCO</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Subspace-Based Speech Correlation Vector Estimation for Single-Microphone Multi-Frame MVDR Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="856" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Filtering: Signal Extraction and Reconstruction Using Complex Time-Frequency Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03889</idno>
		<title level="m">Neural Spatio-Temporal Beamformer for Target Speech Separation</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DNN-Based Speech Presence Probability Estimation for Multi-Frame Single-Microphone Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tammen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="191" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SDR -Half-baked or Well Done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13981</idno>
		<title level="m">The INTER-SPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ), an objective method for end-to-end speech quality assessment of narrowband telephone networks and speech codecs P.862</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Telecommunications Union (ITU-T) Recommendation</title>
		<imprint>
			<date type="published" when="2001-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Algorithm for Intelligibility Prediction of Time-Frequency Weighted Noisy Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phasebook and Friends: Leveraging Discrete Representations for Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="370" to="382" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
