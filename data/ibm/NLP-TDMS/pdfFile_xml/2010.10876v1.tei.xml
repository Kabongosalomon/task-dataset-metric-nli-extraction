<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PROBABILISTIC NUMERIC CONVOLUTIONAL NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Bondesan</surname></persName>
							<email>rbondesa@qti.qualcomm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<email>mwelling@qti.qualcomm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Qualcomm AI Research †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PROBABILISTIC NUMERIC CONVOLUTIONAL NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Continuous input signals like images and time series that are irregularly sampled or have missing values are challenging for existing deep learning methods. Coherently defined feature representations must depend on the values in unobserved regions of the input. Drawing from the work in probabilistic numerics, we propose Probabilistic Numeric Convolutional Neural Networks which represent features as Gaussian processes (GPs), providing a probabilistic description of discretization error. We then define a convolutional layer as the evolution of a PDE defined on this GP, followed by a nonlinearity. This approach also naturally admits steerable equivariant convolutions under e.g. the rotation group. In experiments we show that our approach yields a 3× reduction of error from the previous state of the art on the SuperPixel-MNIST dataset and competitive performance on the medical time series dataset PhysioNet2012.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Standard convolutional neural networks are defined on a regular input grid. For continuous signals like time series and images, these elements correspond to regular samples of an underlying function f defined on a continuous domain. In this case, the standard convolutional layer of a neural network is a numerical approximation of a continuous convolution operator A.</p><p>Coherently defined networks on continuous functions should only depend on the input function f , and not on spurious shortcut features <ref type="bibr" target="#b19">(Geirhos et al., 2020)</ref> such as the sampling locations or sampling density, which enable overfitting and reduce robustness to changes in the sampling procedure. Each application of A in a standard neural network incurs some discretization error which is determined by the sampling resolution. In some sense, this error is unavoidable because the features f ( ) at the layers depend on the values of the input function f at regions that have not been observed.</p><p>For input signals which are sampled at a low resolution, or even sampled irregularly such as with the sporadic measurements of patient vitals data in ICUs or dispersed sensors for measuring ocean currents, this discretization error cannot be neglected. Simply filling in the missing data with zeros or imputing the values is not sufficient since many different imputations are possible, each of which can affect the outcomes of the network.</p><p>Probabilistic numerics is an emergent field that studies discretization errors in numerical algorithms using probability theory <ref type="bibr" target="#b6">Cockayne et al. (2019)</ref>. Here we build upon these ideas to quantify the dependence of the network on the regions in the input which are unknown, and integrate this uncertainty into the computation of the network. To do so, we replace the discretely evaluated feature maps {f ( ) (x i )} N i=1 with Gaussian processes: distributions over the continuous function f ( ) that track the most likely values as well as the uncertainty. On this Gaussian process feature representation, we need not resort to discretizing the convolution operator A as in a standard convnet, but instead we can apply the continuous convolution operator directly. If a given feature is a Gaussian process, then applying linear operators yields a new Gaussian process with transformed mean and covariance functions. The dependence of Af on regions of f which are not known translates into the uncertainty represented in the transformed covariance function, the analogue of the discretization error in a CNN, which is now tracked explicitly. We call the resulting model Probalistic Numeric Convolutional Neural Network (PNCNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Over the years there have been many successful convolutional approaches for ungridded data such as <ref type="bibr">GCN (Kipf and Welling, 2016)</ref>, <ref type="bibr">PointNet (Qi et al., 2017)</ref>, Transformer <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>, Deep Sets <ref type="bibr" target="#b54">(Zaheer et al., 2017)</ref>, SplineCNN <ref type="bibr" target="#b13">(Fey et al., 2018)</ref>, PCNN <ref type="bibr" target="#b0">(Atzmon et al., 2018)</ref>, PointConv <ref type="bibr" target="#b53">(Wu et al., 2019)</ref>, KPConv <ref type="bibr" target="#b44">(Thomas et al., 2019)</ref> and many others <ref type="bibr" target="#b11">(de Haan et al., 2020;</ref><ref type="bibr" target="#b14">Finzi et al., 2020;</ref><ref type="bibr" target="#b37">Schütt et al., 2017;</ref><ref type="bibr" target="#b49">Wang et al., 2018)</ref>. However, the target domains of sets, graphs, and point clouds are intrinsically discrete and for continuous data each of these methods fail to take full advantage of the assumption that the underlying signal is continuous. Furthermore, none of these approaches reason about the underlying signal probabilistically.</p><p>In a separate line of work there are several approaches tackling irregularly spaced time series with RNNs <ref type="bibr" target="#b4">(Che et al., 2018)</ref>, Neural ODEs <ref type="bibr" target="#b35">(Rubanova et al., 2019)</ref>, imputation to a regular grid <ref type="bibr" target="#b27">(Li and Marlin, 2016;</ref><ref type="bibr" target="#b17">Futoma et al., 2017;</ref><ref type="bibr" target="#b40">Shukla and Marlin, 2019;</ref><ref type="bibr" target="#b15">Fortuin et al., 2020)</ref>, set functions <ref type="bibr" target="#b22">(Horn et al., 2019)</ref> and attention <ref type="bibr" target="#b39">(Shukla and Marlin, 2020)</ref>. Additionally there are several works exploring reconstruction of images from incomplete observations for downstream classification <ref type="bibr" target="#b23">(Huijben et al., 2019;</ref><ref type="bibr" target="#b28">Li and Marlin, 2020)</ref>.</p><p>Most similar to our method are the end-to-end Gaussian process adapter <ref type="bibr" target="#b27">(Li and Marlin, 2016)</ref> and the multi-task Gaussian process RNN classifier <ref type="bibr" target="#b17">(Futoma et al., 2017)</ref>. In these two works, a Gaussian process is fit to an irregularly spaced time series and sampled imputations from this process are fed into a separate RNN classifier. Unlike our approach where the classifier operates directly on a continuous and probabilistic signal, in these works the classifier operates on a deterministic signal on a regular grid and cannot reason probabilistically about discretization errors.</p><p>Finally, while superficially similar to Deep GPs <ref type="bibr" target="#b10">(Damianou and Lawrence, 2013)</ref> or Deep Differential Gaussian Process Flows <ref type="bibr" target="#b20">(Hegde et al., 2018)</ref>, our PNCNNs tackle fundamentally different kinds of problems like image classification 1 , and our GPs represent epistemic uncertainty over the values of the feature maps rather than the parameters of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Numerics:</head><p>We draw inspiration for our approach from the community of probabilistic numerics where the error in numerical algorithms are modeled probabilistically, and typically with a Gaussian process. In this framework, only a finite number of input function calls can be made, and therefore the numerical algorithm can be viewed as an autonomous agent which has epistemic uncertainty over the values of the input. A well known example is Bayesian Monte Carlo where a Gaussian process is used to model the error in the numerical estimation of an integral and optimally select a rule for its computation <ref type="bibr" target="#b30">(Minka, 2000;</ref><ref type="bibr" target="#b33">Rasmussen and Ghahramani, 2003)</ref>. Probabilistic numerics has been applied widely to numerical problems such as the inversion of a matrix <ref type="bibr" target="#b21">(Hennig, 2015)</ref>, the solution of an ODE <ref type="bibr" target="#b36">(Schober et al., 2019)</ref>, a meshless solution to boundary value PDEs <ref type="bibr" target="#b5">(Cockayne et al., 2016)</ref>, and other numerical problems <ref type="bibr" target="#b6">(Cockayne et al., 2019)</ref>. To our knowledge, we are the first to construct a probabilistic numeric method for convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian Processes:</head><p>We are interested in operating on the continuous function f (x) underlying the input, but in practice we have access only to a collection of the values of that function sampled on a finite number of points {x i } N i=1 . Classical interpolation theory reconstructs f deterministically by assuming a certain structure of the signal in the frequency domain. Gaussian processes give a way of modeling our beliefs about values that have not been observed <ref type="bibr" target="#b34">(Rasmussen et al., 2006)</ref>, as reviewed in appendix A. These beliefs are encoded into a prior covariance k of the GP f ∼ GP(0, k) and updated upon seeing data with Bayesian inference. Explicitly, given a set of sampling locations</p><formula xml:id="formula_0">x = {x i } N i=1 and noisy observations y = {y i } N i=1 sampled y i ∼ N (f (x i ), σ 2 i ), using Bayes rule Probabilistic Probabilistic Probabilistic</formula><p>Input Data <ref type="figure">Figure 1</ref>: The PNCNN operating on SuperPixel-MNIST images shown on the left. The mean and elementwise uncertainty of the Gaussian process feature maps are shown as they are transformed through the network by the convolution layers. Observation points shown as green dots in σ(x).</p><p>one can compute the posterior distribution f |y, x ∼ GP(µ p , k p ), which captures our epistemic uncertainty about the values between observations. The posterior mean and covariance are given by</p><formula xml:id="formula_1">µ p (x) = k(x) [K + S] −1 y , k p (x, x ) = k(x, x ) − k(x) [K + S] −1 k(x ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">K ij = k(x i , x j ), k(x) i = k(x, x i ) and S = diag(σ 2 i ).</formula><p>Below we shall choose the RBF kernel 2 as prior covariance, due to its convenient analytical properties:</p><formula xml:id="formula_3">k RBF (x, x ) = aN (x; x , l 2 I) = a 2πl 2 − d 2 exp(− 1 2l 2 ||x − x || 2 ).</formula><p>In typical applications of GPs to machine learning tasks such as regression, the function f that we want to predict is already the regression model. In contrast, here we use GPs as a way of representing our beliefs and epistemic uncertainty about the values of both the input function and the intermediate feature maps of a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROBABILISTIC NUMERIC CONVOLUTIONAL NEURAL NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">OVERVIEW</head><p>Given an input signal f : X → R c , we define a network with layers that act directly on this continuous input signal. We define our neural network recursively from the input f (0) = f , as a series of L continuous convolutions A ( ) with pointwise ReLU nonlinearities and weight matrices which mix only channels (known as 1 × 1 convolutions) M ∈ R c×c :</p><formula xml:id="formula_4">f ( +1) = M ( ) ReLU[A ( ) f ( ) ],<label>(2)</label></formula><p>and a final global average pooling layer P which acts channel-wise as natural generalization of the discrete case:</p><formula xml:id="formula_5">P(f (L) ) α = f (L)</formula><p>α (x)dx for each α = 1, 2, . . . , c. Denoting the space of functions on X with c channels by H c , the convolution operators A ( ) are linear operators from H c to H c +1 . Like in ordinary convolutional neural networks, the layers build up increasingly more expressive spatial features and depend on the parameters in A ( ) and M ( ) . Unlike ordinary convolutional networks, these layers are well defined operations on the underlying continuous signal.</p><p>While it is clear that such a network can be defined abstractly, the exact values of the function f (L) cannot be computed as the operators depend on unknown values of the input. However, by adapting a probabilistic description we can formulate our ignorance of f (0) with a Gaussian process and see how the uncertainties propagate through the layers of the network, yielding a probabilistic output. Before delving into details, we outline the key components of equation 2 that make this possible.</p><p>Continuous Convolutional Layers: Crucially, we consider continuous convolution operators A that can be applied to input Gaussian process f ∼ GP(µ p , k p ) in closed form. The output is another Gaussian process with a transformed mean and covariance Af ∼ GP(Aµ p , Ak p A ) where A acts to the left on the primed argument of k p (x, x ). 3 In section 4.2 we show how to parametrize these continuous convolutions in terms of the flow of a PDE and show how they can be applied to the RBF kernel exactly in closed form.</p><p>Probabilistic ReLUs: Applying the ReLU nonlinearity to the GP yields a new non Gaussian stochastic process h ( ) = ReLU[A ( ) f ( ) ], and we show in section 4.5 that the mean and covariance of this process has a closed form solution which can be computed.</p><p>Channel Mixing and Central Limit Theorem: The activations h ( ) are not Gaussian; however, for a large number of weakly dependent channels we argue that f ( +1) = M ( ) h ( ) is approximately distributed as a Gaussian Process in section 4.5.</p><p>Measurement and Projection to RBF Gaussian Process: While f ( +1) is approximately a Gaussian process, the mean and covariance functions have a complicated form. Instead of using these functions directly, we take measurements of the mean and variance of this process and feed them in as noisy observations to a fresh RBF kernel GP, allowing us to repeat the process and build up multiple layers without increasing complexity.</p><p>The Gaussian process feature maps in the final layer f (L) are aggregated spatially by the integral pooling P which can also be applied in closed form (see appendix D), to yield a Gaussian output. Assembling these components, we implement the end to end trainable Probabilistic Numeric Convolutional Neural Network which integrates a probabilistic description of missing data and discretization error inherent to continuous signals. The layers of the network are shown in figure 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CONTINUOUS CONVOLUTIONAL LAYERS</head><p>On a discrete domain such as the lattice X = Z d , all translation equivariant linear operators A are convolutions, a fact which we review in appendix B. In general, these convolutions can be written in terms of a linear combination of powers of the generators of the translation group: the shift operators τ i , i = 1, . . . , d shift all elements by one unit along the i-th axis of the grid. For a one dimensional grid, one can always write A = k W k τ k where the weight matrices W k ∈ R c×c act only on the channels and the shift operator τ acts on functions on the lattice. In d dimensions, A = k1,...,k d W k1,...,k d τ k1 1 · · · τ k d d for some set of integer coefficients k 1 , . . . , k d . For example when d = 2, we can take k 1 , k 2 ∈ {−1, 0, 1} to fill out a 3 × 3 neighborhood.</p><p>On the continuous domain X = R d we similarly parametrize convolutions with A = k W k e D k , where D k is given by powers of the partial derivatives ∂ i , i = 1, . . . , d which generate infinitesimal translations along the i-th axes. Setting d = 1 for simplicity, we can indeed verify by Taylor expansion that the operator exponential τ a = e a∂ applied to a function g(x) is a translation: e a∂ g(x) = g(x) + ag (x) + 1 2 a 2 g (x) + · · · = g(x + a). Exponentials of operators can be defined similarly in terms of formal Taylor expansions or more conveniently in terms of the solution to a PDE:</p><formula xml:id="formula_6">∂ t g(t, x) = (Dg)(t, x) , g(0, x) = g(x) ,<label>(3)</label></formula><p>at time t = 1: e D g(x) = g(t = 1, x).</p><p>Following the discussion in the discrete case, translation invariance of D k imposes that it is expressed in terms of powers of the generators. Collecting the derivatives into the gradient ∇, we can write the general form of D k as α k + β k ∇ + 1 2 ∇ Σ k ∇ + ... for any constants α k , vectors β k , matrices Σ k etc. For simplicity, we truncate the series at second order to get</p><formula xml:id="formula_7">D k = β k ∇ + 1 2 ∇ Σ k ∇ ,<label>(4)</label></formula><p>where we omit the constants α k that can be absorbed into the definition of W k . For this choice of D, the PDE in equation 3 is nothing but the diffusion equation with drift β k and diffusion Σ k . When discussing rotational equivariance in section 4.4, we also consider a more general form of D.</p><p>The diffusion layer can also be viewed in another way as the infinitesimal generator of an Ito diffusion (a stochastic process). Given an Ito process with constant drift and diffusion dX t = βdt + Σ 1/2 dB t where B t is a d dimensional Brownian motion, the time evolution operator can be written via the Feynman-Kac formula as e tD f (x) = E[f (X t )] where X 0 = x. In other words, the operator layer A = e tD is the expectation under a parametrized Neural Stochastic Differential equation <ref type="bibr" target="#b46">Tzen and Raginsky, 2019)</ref> that is homogeneous and therefore shift invariant. The flow of this SDE depends on the drift and diffusion parameters β and Σ.</p><p>To recap, we define our convolution operator through the general form A = k W k e D k where the weight matrices W k ∈ R c×c mix only channels and e D k is the forward evolution by one unit of time of the diffusion equation with drift β k and diffusion Σ k containing learnable parameters</p><formula xml:id="formula_8">{(W k , β k , Σ k )} K k=1 .</formula><p>The translation equivariance of A follows directly from the fact that the generators commute ∀k, i : [D k , ∇ i ] = 0 and therefore [A, τ i ] = 0. In appendix B we show that our definition of A reduces to the usual one in the discrete case and is thus a principled generalization to the continuous domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EXACT APPLICATION ON RBF GPS</head><p>Although the application of the linear operator A = k W k e D k involves the time evolution of a PDE, owing to properties of the RBF kernel we fortuitously can apply the operator to an input GP in closed form! Gaussian processes are closed under linear transformations: given f ∼ GP(µ p , k p ), we need only compute the action of A on the mean and covariance:</p><formula xml:id="formula_9">Af ∼ GP(Aµ p , Ak p A ), where A is the adjoint w.r.t. the L 2 (X ) inner product. The application of time evolution e D k is a convolution with a Green's function G k , so Af = k W k e D k f = k W k G k * f . As we derive in appendix C, the Green's function for D k = β k ∇ + (1/2)∇ Σ k ∇, is nothing but the multivariate Gaussian density G k (x) = N (x; −β k , Σ k ): Af = k W k e D k f = k W k G k * f = k W k N (−β k , Σ k ) * f .<label>(5)</label></formula><p>In order to apply e tD to the posterior GP, we need only to be able to apply the operator to the posterior mean and covariance. This posterior mean and covariance in equation 1 are expressed in terms of k RBF = aN (x; x , 2 I) and the computation boils down to a convolution of two Gaussians:</p><formula xml:id="formula_10">e tD k RBF (x, x ) = N (x; −tβ, tΣ) * aN (x; x , 2 I) = aN (x; x − tβ, 2 I + tΣ) (6) e tD1 k RBF (x, x )e tD 2 = aN (x; x − t(β 1 − β 2 ), 2 I + tΣ 1 + tΣ 2 ) .<label>(7)</label></formula><p>The application of the channel mixing matrices W k and summation is also straightforward through matrix multiplication for the mean and covariance. To summarize, because of the closed form action on the RBF kernel, the layer can be implemented efficiently and exactly with no discretization or approximations.</p><p>We note that with the Green's function above, the action of A encompasses the ordinary convolution operator on the 2d lattice as a special case. Given drift β k ∈ {−1, 0, 1} ×2 , k = 1, . . . , 9 filling out the 9 elements of a 3 × 3 grid and as the diffusion Σ k → 0, the Green's function is a Dirac delta, so that:</p><formula xml:id="formula_11">Af (x) = k W k δ(x − β k ) * f (x) = i,j=−1,0,1 W ij f (x 1 − i, x 2 − j) = W * Z 2 f (x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GENERAL EQUIVARIANCE</head><p>The convolutional layers discussed so far are translation equivariant. We discuss how to extend the continuous linear operator layers to more general symmetries such as rotations. Feature fields in this more general case are described by tensor fields, where the symmetry group acts not only on the input space X but also on the vector space attached to each point x ∈ X . A linear layer A is equivariant if its action commutes with that of the symmetry. In appendix E we derive constraints for general linear operators and symmetries, which generalize those appearing in the steerable-CNN literature <ref type="bibr" target="#b50">(Weiler and Cesa, 2019;</ref><ref type="bibr" target="#b9">Cohen et al., 2019)</ref>. Then we show how equivariance under continuous roto-translations in 2d constrains the form of a convolutional layer by solving the equivariance constraint. Non-trivial solutions require that the operator D in the PDE of equation 3 has a non-trivial matrix structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">PROBABILISTIC NONLINEARITIES AND RECTIFIED GAUSSIAN PROCESSES</head><p>Gast and Roth (2018) derive the mean and variance for a univariate rectified Gaussian distribution for use in a neural network. We generalize these results to the full covariance function (and higher moments) of a rectified Gaussian process in appendix J and present the results here. For the input GP</p><formula xml:id="formula_12">A ( ) f ( ) (x) ∼ GP(µ(x), k(x, x )), we denote σ(x) = k(x, x), Σ the matrix with components Σ ij = k(x i , x j ) for i, j = 1, 2 and µ = [µ(x 1 ), µ(x 2 )].</formula><p>We use notation Φ(z) for the univariate standard normal CDF, and Φ(z; Σ) for (two dimensional) multivariate CDF of N (0, Σ) at z. Σ 1 and Σ 2 are the column vectors of Σ. The first and second moments of h = ReLU[Af ] are:</p><formula xml:id="formula_13">E[h(x)] = µ(x)Φ(µ(x)/σ(x)) + σ(x)Φ (µ(x)/σ(x)) ,<label>(8)</label></formula><formula xml:id="formula_14">E[h(x 1 )h(x 2 ))] = (k(x 1 , x 2 ) + µ(x 1 )µ(x 2 ))Φ(µ; Σ) (9) + (µ(x 1 )Σ 2 + µ(x 2 )Σ 1 )∇Φ(µ; Σ) + Σ 1 ∇∇ Φ(µ; Σ)Σ 2 .</formula><p>The first and higher order derivatives of the Normal CDF are just the PDF and products of the PDF with Hermite polynomials. Note that the mean and covariance interact through the nonlinearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">CHANNEL MIXING AND CENTRAL LIMIT THEOREM</head><p>After the non-linearity the process is no longer Gaussian. To overcome this issue we introduce a channel mixing matrix M ( ) ∈ R c +1 ×c and define the feature map in the following layer by</p><formula xml:id="formula_15">f ( +1) = M ( ) h ( ) , where h ( ) = ReLU[A ( ) f ( ) ]</formula><p>. So long as the channels of h ( ) are only weakly dependent, we can apply the central limit theorem (CLT) to each function f</p><formula xml:id="formula_16">( +1) α = c β=1 M ( ) α,β h ( ) β</formula><p>so that in the limit of large c , the statistics of the f ( +1) α 's converge to a GP with first and second moments given by:</p><formula xml:id="formula_17">E[f ( +1) (x)] = M E[h ( ) (x)], E[f ( +1) (x)f ( +1) (x ) ] = M E[h ( ) (x)h ( ) (x ) ]M . (10)</formula><p>We expand the argument in more detail in appendix I and we quantify the extent of convergence and normality in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">MEASUREMENT AND PROJECTION TO RBF GAUSSIAN PROCESS</head><p>As a last step we simplify the mean and covariance functions of the approximate GP f ( +1) . While we can readily compute the values of these functions, unlike in the RBF kernel case, we cannot apply the convolution operator e tD in closed form. In order to circumvent this challenge, we model the (approximately) Gaussian process f ( +1) with an RBF Gaussian process as follows: we evaluate the mean</p><formula xml:id="formula_18">y i = E[f ( +1) (x i )] and variance σ 2 i = Var[f ( +1) (x i )]</formula><p>of the approximate Gaussian process f ( +1) at a collection of points {x i } N i=1 using equations 8, 9 and 10. These values y i are treated as measurements of the underlying signal with a heteroscedastic noise σ 2 i that varies from point to point. We can then compute the RBF-based posterior GP of this signal</p><formula xml:id="formula_19">f ( +1) |{(x i , y i , σ i )} N i=1 ∼ GP(µ p , k p )</formula><p>with posterior mean and covariance given by equation 1 for the heteroschedastic noise model. The uncertainty in the input f ( ) is propagated through to the RBF posteriorf ( +1) |{(x i , y i , σ i )} N i=1 via the measurement noise σ i . Crucially, this Gaussian process mean and covariance functions are written in terms of the RBF kernel and we can therefore continue applying convolutions in closed form in future layers.</p><p>As we describe in the following section, the RBF kernel in each layer is trained to maximize the marginal likelihood of the data that it sees, and thereby minimize the discrepancy with the underlying generating distribution f ( +1) . While this measurement/projection approach is effective in many scenarios, in networks with many layers or a very large number of observations uncertainty information can get attenuated as it passes through the layers, a phenomenon which we investigate in appendix H. With a network that is trained on a version of MNIST that is randomly subsampled to 75 pixels, in figure 2 (left) we evaluate the mean and uncertainty of the internal feature maps as we vary the number the number of pixels of the inputs at test time. As expected, the mean functions for the feature maps slowly converge and the predicted uncertainties decrease in magnitude as the input resolution is increased. In figure 2 (middle) we show that in early layers the uncertainties decrease at a similar rate to the O(1/ √ N ) of discretization error that we would expect from a standard convolutional layer which is discretized to a square grid. <ref type="bibr">4</ref> Despite the fact that these resolutions differ substantially from those seen at training time and the fact that there are no explicit uncertainty targets for these internal layers, the predictions are reasonably well calibrated as demonstrated in figure 2 (right). While the prediction residuals have fatter tails than a standard Gaussian, the mean and standard deviation are close to the theoretically optimal 0 and 1 values across a range of resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">TRAINING PROCEDURE</head><p>Our neural network has two sets of parameters: the channel mixing and diffusion parame-</p><formula xml:id="formula_20">ters, {(M ( ) , W ( ) , β ( ) , Σ ( ) )} L =1 , as well as kernel hyperparameters of the Gaussian Processes {(l ( ) , a ( ) )} L =1 .</formula><p>We train all parameters jointly on the loss L task + λL GP , where L task is the cross entropy with logits given by the mean µ P of the pooled features P(f (L) ) ∼ N (µ P , Σ P ) and L GP is the marginal log likelihoods of the GP feature maps:</p><formula xml:id="formula_21">L GP (f ) = 1 2 L =1 c α=1 f T α [K XX + S α ] −1 f α + log det [K XX + S α ] + N log 2π ( ) ,<label>(11)</label></formula><p>where for each layer , f α = f α (x 1 ), ..., f α (x N ) ∈ R N are the observed values for channel α at locations X = [x 1 , . . . , x N ], K XX is the covariance of the RBF kernel and S α = diag(σ 2 α ) the measurement noise for each channel α and spatial location. Notably the GP marginal likelihood is independent of the class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>We evaluate the Probabilistic Numeric CNN on two different problems which have incomplete and irregular observations. Superpixel MNIST is an adaptation of the MNIST dataset where the 784 pixels of the original images are replaced by 75 salient superpixels that are non uniformly spread throughout the domain and are different for each image <ref type="bibr" target="#b31">(Monti et al., 2017)</ref>. Despite the simplicity of the underlying images, the lack of grid structure and high fraction of missing values make this a challenging task. Example inputs are visualized at the left of figure 1. We compare to Monet <ref type="bibr" target="#b31">(Monti et al., 2017)</ref>, SplineCNN <ref type="bibr" target="#b13">(Fey et al., 2018)</ref>, Graph Convolutional Gaussian Processes (GCGP) <ref type="bibr" target="#b48">(Walker and Glocker, 2019)</ref>, and Graph Attention Networks (GAT) <ref type="bibr" target="#b1">(Avelar et al., 2020)</ref>. As shown in table 5, the probabilistic numeric CNN greatly outperforms the competing methods reducing the classification error rate by more than 3× over the previous state of the art. 4 A 2D discrete convolution layer using N = m 2 points can be interpreted as a Riemann sum approximation of the continuous integral and will therefore have an error We conduct an ablation study where uncertainty propagation is removed: the probabilistic ReLU is replaced with the deterministic one applied to the mean, and the uncertainties in each layer are set to 0. This form of the network still makes use of the fact that the input function is continuous by use of the GP interpolation of the means, but crucially it does not integrate the uncertainty in the computation resulting from the missing data. While this variant (PNCNN w/o σ) with a 3.03% error rate outperforms existing methods from the literature, it is substantially worse than the PNCNN that integrates the uncertainty at 1.24% error. This validates both that the underlying architecture (using the continuous convolution operators) has good inductive biases and that reasoning about discretization errors probabilistically can improve performance directly.</p><p>In <ref type="figure">figure 3</ref> we evaluate the performance on zero shot generalization to a different test resolution for a variety of training resolutions. In order to compare to an ordinary CNN we sample MNIST on a regular square grid: PNCNN with uncertainty is the most robust to this train test sampling distribution shift, followed by PNCNN w/o uncertainty, and finally the ordinary CNN which is quite sensitive to these changes.</p><p>Irregularly Spaced Time Series For the second task, we evaluate our model on the irregularly spaced time series dataset PhysioNet2012 <ref type="bibr" target="#b41">(Silva et al., 2012)</ref> for predicting mortality from ICU vitals signs. This dataset is particularly challenging because different vital sign channels are observed at different times, even within a single patient record. This means that we cannot compute the GP inference formula of equation 1 efficiently for all channels simultaneously because the observation points {x i } and hence the matrices K in that formula differ between the channels, increasing computational complexity. To circumvent this difficulty, we employ a stochastic diagonal estimator to compute the variances as described in appendix G. We compare against IP-Nets (Shukla and Marlin, 2019), SEFT-ATTN <ref type="bibr" target="#b22">(Horn et al., 2019)</ref>, and GRU-D <ref type="bibr" target="#b4">(Che et al., 2018)</ref> as reported in <ref type="bibr" target="#b22">Horn et al. (2019)</ref>. PNCNN performs competitively, although not a breakout performance as in the image dataset which we attribute to the use of the stochastic variance estimates over an exact calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Based on the ideas of probabilistic numerics, we have introduced a new class of neural networks which model missing values and discretization errors probabilistically within the layers of a CNN. The layers of our network are a series of operators defined on continuous functions, removing dependence on shortcut features like the sampling locations and distribution. On irregularly sampled and incomplete spatial data we show improved generalization and robustness.</p><p>As a closing comment, we note that, owing to exchangeability of finite distributions of a stochastic process, our architecture is permutation equivariant. We therefore envision new applications of our framework to graph data in the future. Data on curved manifolds will be described by gauge equivariant GPs and PDEs, generalizing considerably the mathematical models of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A REVIEW OF GAUSSIAN PROCESSES</head><p>We briefly review here the main ideas of Gaussian Processes for machine learning, see Stein <ref type="formula" target="#formula_1">(2012)</ref>; <ref type="bibr" target="#b34">Rasmussen et al. (2006)</ref> for more details. We start to explain how to use stochastic processes for Bayesian inference. We see the stochastic process as prior over functions p(f ) and as we are given samples x = (x 1 , . . . , x N ), y = (y 1 , . . . , y N ), y i ≡ f (x i ), we update our beliefs about the function by constructing the posterior via Bayes rule: p(f |y, x) = p(y|f, x)p(f )/p(y|x). Here we need to specify the likelihood of the data with our model p(y|f, x) = N i=1 p(y i |f, x i ) and the denominator, called the evidence or marginal likelihood, follows: p(y|x) = E f ∼p(f ) <ref type="figure">[p(y|f, x)</ref>]. The power of this approach is that the value of the signal y at an unseen point x has an uncertainty which depends on our knowledge of its neighbourhood: p(y|x, y, x) = E f ∼p(f |y,x) [p(y|f, x)] and allows us to reason probabilistically about the underlying signal.</p><p>A particular convenient class of random function is Gaussian processes (GPs) for which inference can be done exactly. A stochastic process can be presented in terms of the finite distributions of the random variables {f (</p><formula xml:id="formula_22">x i )} M i=1 at points {x i } M i=1</formula><p>. For a GP these distributions are Gaussian and can be defined uniquely by specifying means and covariances, and so a GP is specified entirely by its mean function µ(x) and covariance kernel k(x, x ). We shall write f ∼ GP(µ, k). Let us assume a Gaussian likelihood model as well, i.e. p(y i |f, x i ) = N (f (x i ), σ 2 i ), where σ n represents aleatoric uncertainty on the measurement. (For simplicity we take here the function to be scalar valued but the reasoning can be easily generalized.) Then properties of the Gaussian distribution (see <ref type="bibr">(Rasmussen et al., 2006, Chap.</ref> 2) for a detailed derivation of the formulas) lead to the following posterior distributions after seeing data y, x: p(f |y,</p><formula xml:id="formula_23">x) = GP(µ p , k p ), with µ p (x) = k(x) T [K + S] −1 y , k p (x, x ) = k(x, x ) − k(x) T [K + S] −1 k(x ) .<label>(12)</label></formula><p>where</p><formula xml:id="formula_24">K ij = k(x i , x j ), k(x) i = k(x, x i ) and S = diag(σ 2 i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B FROM DISCRETE TO CONTINUOUS CONVOLUTIONAL LAYERS</head><p>We here show that the general formula</p><formula xml:id="formula_25">A = k W k e D k<label>(13)</label></formula><p>with D k a function of spatial derivatives, reduces in the case of discrete input space X to the usual convolution we encounter in deep learning.</p><p>For simplicity we shall assume a 1d grid as input space X = {1, . . . , N }. Let us start by recalling the form of the classical discrete convolution when C = C +1 = 1. We define a convolutional layer as a linear map that commutes with the translation operator. To make the symmetry exact, we need assume periodic boundaries. Then in the standard basis of R N , {e i } N i=1 of vectors localized at site i, the translation operator τ acts as τ e i = e i+1 mod N .</p><formula xml:id="formula_26">An N × N matrix B is translation invariant iff τ B = Bτ . Since τ is diagonal in Fourier space, the most general solution is B = F diag(b)F −1 , where F jk = e 2πi</formula><p>N jk is the discrete Fourier transform. Such matrices are called circulant and can be written alternatively as B =</p><formula xml:id="formula_27">N −1 i=0 b N −i τ i , b = Fb. Explicitly: B =        b 0 b 1 . . . b N −2 b N −1 b N −1 b 0 b 1 b N −2 . . . b N −1 b 0 . . . . . . b 2 . . . . . . b 1 b 1 b 2 . . . b N −1 b 0        .<label>(14)</label></formula><p>This shows that the most general convolutional layer is a circulant matrix. E.g. if b i = 0 unless i = 0, 1, N − 1, B coincides with the matrix representing a periodic convolution of filter size 3. The matrix B is invertible as long asb k = 0 for all k. In a convolutional network the parameters b i are random variables and the measure of the set where B is not invertible is zero. Thus the role of e D is replaced in the discrete case by the B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head><p>The discrete analog of A is then:</p><formula xml:id="formula_28">A = i W i ⊗ B i .<label>(15)</label></formula><p>Introducing the unit matrices E α,β which have 1 at the row α and column β and 0 otherwise, we can rewrite it as:</p><formula xml:id="formula_29">A = j,α,β E α,β ⊗ τ j W α,β j , W α,β j = i W α,β i b i,N −j .<label>(16)</label></formula><p>Since E α,β ⊗ τ j is a linear basis of the space of convolutional layers, we see that equation 13 indeed reduces to the usual one when discretizing the input domain and is a principled generalization to the continuous domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C GREENS FUNCTION</head><p>Given the operator D = β ∇ + 1 2 ∇ Σ∇ we can compute the action of e tD in terms of convolutions. Using the d dimensional Fourier transforms F[h](k) = (2π) − d 2 h(x)e −ik x dx and F −1 = F † , we can rewrite the derivative operator D in terms of elementwise multiplication in the Fourier domain, which diagonalizes D. Since ∇ = F −1 (ik)F,</p><formula xml:id="formula_30">D = F −1 (iβ k − 1 2 k Σk)F.<label>(17)</label></formula><p>Using the series definition e tD = ∞ n=0 (tD) n /n!, we have:</p><formula xml:id="formula_31">e tD = F −1 e t(iβ k− 1 2 k Σk) F.<label>(18)</label></formula><p>Applying this operator to a test function h(x) yields</p><formula xml:id="formula_32">e tD h = F −1 [e t(iβ k− 1 2 k Σk) F[h](k)] = F −1 [F[G t ] · F[h]] = G t * h,<label>(19)</label></formula><p>where the final step follows from the Fourier convolution theorem, and we define the function G t = F −1 [e t(iβ k− 1 2 k Σk) ]. Directly applying the Fourier integral yields a Gaussian integral</p><formula xml:id="formula_33">G t (x) = (2π) − d 2 e ik (x+tβ)− 1 2 k tΣk dk = e − 1 2 (x+tβ) (tΣ) −1 (x+tβ) det(2πtΣ) −1/2 .<label>(20)</label></formula><p>This function G t (x) = N (x; −tβ, tΣ) is nothing but a multivariate heat kernel, the Greens function (also known as the fundamental solution or time propagator) for the diffusion equation</p><formula xml:id="formula_34">∂ t G t (x − x ) = DG t (x − x ), and indeed lim t→0 G t (x − x ) = δ(x − x ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D INTEGRAL POOLING</head><p>The integral pooling operator P[f ] = R d f (x)dx can be applied to the Gaussian process just like any other linear operator. Given f (L) ∼ GP(µ, k), we have that</p><formula xml:id="formula_35">Pf (L) ∼ GP(Pµ, PkP ) = N (Pµ, PkP ).<label>(21)</label></formula><p>Again, computing the mean µ P = Pµ and covariance matrix Σ P = PkP we need just to be able to apply P to the RBF kernel.</p><formula xml:id="formula_36">Pk RBF (x ) = R d k RBF (x, x )dx = a<label>(22)</label></formula><formula xml:id="formula_37">Pk RBF P = R d ×R d k RBF (x, x )dxdx = ∞<label>(23)</label></formula><p>For many applications such as image classification using the mean logit value, we require only the predictive mean, so an unbounded covariance matrix Σ P is acceptable. We use this form for all of our experiments.</p><p>However for some applications an output uncertainty can be useful, so we also provide a variant that integrates over a finite region</p><formula xml:id="formula_38">[0, 1] d , Pf = [0,1] d f (x)dx. Pk RBF (x ) = [0,1] d k RBF (x, x )dx = a d i=1 Φ( x i ) − Φ( x i −1 ) (24) Pk RBF P = [0,1] d ×[0,1] d k RBF (x, x )dxdx = a 2 π (e −1/2 2 − 1) + 2Φ( 1 ) − 1 d (25)</formula><p>where Φ is again the univariate standard normal CDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E EQUIVARIANCE E.1 RELATED WORK</head><p>We note that there has been considerable research effort in the development of equivariant CNNs which we build on top of. The group equivariant CNN was introduced by Cohen and Welling (2016a) for discrete groups on lattices. This work has been extended for continuous groups <ref type="bibr" target="#b52">(Worrall et al., 2017;</ref><ref type="bibr" target="#b55">Zhou et al., 2017)</ref> and with steerable equivariance <ref type="bibr" target="#b8">(Cohen and Welling, 2016b;</ref><ref type="bibr" target="#b50">Weiler and Cesa, 2019)</ref> where other group representations are used. There have also been group equivariant networks designed for point clouds and other irregularly spaced data <ref type="bibr" target="#b45">(Thomas et al., 2018;</ref><ref type="bibr" target="#b14">Finzi et al., 2020;</ref><ref type="bibr" target="#b16">Fuchs et al., 2020;</ref><ref type="bibr" target="#b11">de Haan et al., 2020)</ref>. In <ref type="bibr" target="#b38">Shen et al. (2020)</ref>, layers using finite difference estimation of derivative operators are used for defining equivariant layers in an equivariant CNN, effectively a change of basis.</p><p>Most closely related to our PDE operator approach to equivariance is work by <ref type="bibr" target="#b42">Smets et al. (2020)</ref>. In this work, the authors define layers of their convolutional network through the time evolution of a PDE which is a nonlinear generalization of the diffusion equation, which includes pooling like behaviour. The PDEs explored <ref type="bibr" target="#b42">Smets et al. (2020)</ref> are equivariant by choice of the parameters in the PDE, and when incorporating multiple channels are very similar to enforcing equivariance on the operator A = k W k e D k which we investigate below and include it as a special case.</p><p>However, as we note, enforcing equivariances in this form of A outside of translation leads to only solutions without many degrees of freedom which is also mentioned in <ref type="bibr" target="#b42">Smets et al. (2020)</ref>. We prove that this approach to incorporating multiple channels and equivariance is very limited and leads only to trivial isotropic solutions (even with nontrivial feature representations) in Section E.6. We instead provide an alternate approach based on intrinsically coupled systems of PDEs over the channels A = e k W k D k which does not have this deficiency. We also derive the general conditions for equivariance of linear operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 TRANSLATION EQUIVARIANCE</head><p>A key factor in the generalization of convolutional neural networks is their translation equivariance. Patterns in different parts of an input signal can be seen in the same way because convolution is translation equivariant.  <ref type="figure">SO(d)</ref>, the operator must be an isotropic heat kernel: D k = c k ∇ ∇. The reason for this apparent restriction is a result of considering the different channels independently, as scalar fields.</p><p>The alternative is to use features fields which transform under more general representations of the symmetry group, introduced in steerable-CNNs <ref type="bibr" target="#b8">(Cohen and Welling, 2016b)</ref> and used in <ref type="bibr" target="#b52">(Worrall et al., 2017;</ref><ref type="bibr" target="#b45">Thomas et al., 2018;</ref><ref type="bibr" target="#b51">Weiler et al., 2018;</ref><ref type="bibr" target="#b50">Weiler and Cesa, 2019)</ref> and others. In this way, the symmetry transformation acts not only on the spatial domain X , but also transforms the channels. The way that the group acts on R c (i.e. the channels) is formalized by a representation matrix ρ(g) ∈ R c×c for each element g ∈ G in the transformation group that satisfies ∀g, h ∈ G : ρ(gh) = ρ(g)ρ(h). Choosing the type of each intermediate feature map is equivalent to choosing their representations, and we describe a simple way of doing this with tensor representations in the later section.</p><p>Operator Equivariance Constraint: Returning to linear operators, we derive the equivariance constraint and show how to use constructs from the previous sections to implement steerable rotation equivariance. Equivariance of a linear operator A : (R d → R cin ) → (R d → R cout ) requires that, for any input function, transforming the input function first (both argument and channels) and applying A is equivalent to first applying A and then transforming the output:</p><formula xml:id="formula_39">Aρ in (g)L g f = ρ out (g)L g Af where L g f (x) = f (g −1 x).</formula><p>Rearranging the terms, one sees that the equivariance constraint on the linear operator A is:</p><formula xml:id="formula_40">ρ out (g)L g AL g −1 ρ in (g −1 ) = A,<label>(26)</label></formula><p>where the operators L g and L −1 g are understood not to act on the representation matrices ρ (although implicitly a function of g). As shown in Appendix E.4, eq. 26 is a direct generalization of the equivariance constraint for convolutions ∀x : ρ out (g)K(g −1 x)ρ in (g −1 ) = K(x) described in the literature <ref type="bibr" target="#b50">(Weiler and Cesa, 2019;</ref><ref type="bibr" target="#b9">Cohen et al., 2019)</ref>.</p><p>As shown in Appendix E.6, the equivariance constraint for continuous rotations applied to the diffusion operators A = k W k e D k has only the trivial solutions of isotropic diffusion without any drift. For this reason we instead consider a more general form of diffusion operator where the PDE itself couples the different channels. For the coupled PDE:</p><formula xml:id="formula_41">∂f ∂t = k W k D k f<label>(27)</label></formula><p>the time evolution contains the matrices W k in the exponential A = e k W k D k . Like with the example of translation above, this operator is equivariant if and only if the infinitesmal generator k W k D k is equivariant. Because equation 26 applies generally to linear operators and not just convolutions, we can compute the equivariance constraint for these derivative operators. We can simplify the summation k W k D k = k W k (β T k ∇ + (1/2)∇ T Σ k ∇) by writing it in terms of the collection of matrices</p><formula xml:id="formula_42">B i = k W k β ki and S ij = (1/2) k W k Σ kij to express A deriv = i B i ∂ i + i,j S ij ∂ i ∂ j<label>where</label></formula><p>the indices i, j = 1, 2..., d enumerate the spatial dimensions of each vector β k and each matrix Σ k . As we derive in appendix E.5, the necessary and sufficient conditions for the equivariance of k W k D k and therefore A is that ∀g ∈ G : [ρ out ⊗ ρ * in ⊗ ρ (1,0) ](g)vec(B) = vec(B) and ∀g ∈ G : [ρ out ⊗ ρ * in ⊗ ρ (2,0) ](g)vec(S) = vec(S) where vec(·) denotes flattening the elements into a single vector and ρ (r,s) is the tensor representation with r covariant and s contravariant indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 GENERALIZATION OF EQUIVARIANCE CONSTRAINT FOR CONVOLUTIONS</head><p>This equivariance constraint is a direct generalization of the equivariance constraint for convolution kernels as described in <ref type="bibr" target="#b50">Weiler and Cesa (2019)</ref>; <ref type="bibr" target="#b9">Cohen et al. (2019)</ref>. In fact, when A is a convolution operator, Af = K * f , the action of L g by conjugation A is equivalent to transforming the argument of the kernel K:</p><formula xml:id="formula_43">L g (K * )L g −1 f (x) = K(g −1 x − x )f (gx )dµ(x ) = K(g −1 (x − x ))f (x )dµ(x ) = (L g [K]) * f.</formula><p>Letting both sides of eq 26 act on the product of a constant unit vector e i and a delta function, f = e i δ the expression ∀e i : ρ out (g)L g [K]ρ in (g −1 ) * e i δ = K * e i δ can be rewritten as ∀x : ρ out (g)K(g −1 x)ρ in (g −1 ) = K(x) which is precisely the constraint for steerable equivariance for convolution described in the literature. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 EQUIVARIANT DIFFUSIONS WITH MATRIX EXPONENTIAL</head><p>Below we solve for the necessary and sufficient conditions for the equivariance of the operator A deriv .</p><p>We will use tensor representations for their convenience, but the approach is general to allow other kinds of representations. A rank (p, q) tensor t is an element of the vector space T (p,q) := V ⊗p ⊗ (V * ) ⊗q where V is some underlying vector space, V * is its dual and (·) ⊗p is the tensor product iterated p times. In common language T (0,0) are scalars, T (1,0) are vectors, and T (1,1) are matrices. Given the action of a group G on the vector space V , the representation on T (p,q) is ρ (p,q) (g) = g ⊗p ⊗ (g − ) ⊗q where − is inverse transpose and ⊗ on the matrices is the tensor product (Kronecker product) of matrices. Composite representations can be formed by stacking different tensor ranks together, such as a representation of 50 scalars, 25 vectors, 10 matrices and 5 higher order tensors: T 50 (0,0) ⊕ T 25 (1,0) ⊕ T 10 (1,1) ⊕ T 5 (1,2) , where ⊕ in this context is the same as the Cartesian product. For a composite representation U = i T (pi,qi) the group representation is similarly ρ U (g) = i ρ (pi,qi) (g) where ⊕ concatenates matrices as blocks on the diagonal.</p><p>Noting that the operator L g that acts only on the argument and the matrix ρ in (g) acts only on the components, the two commute and we can rewrite the constraint for A deriv as</p><formula xml:id="formula_44">i ρ out (g)B i ρ in (g −1 )L g ∂ i L g −1 + ij ρ out (g)S ij ρ in (g −1 )L g ∂ i ∂ j L g −1 = A deriv<label>(28)</label></formula><p>We can simplify the expression L g ∂ i L g −1 by seeing how it acts on a function. For any differentiable</p><formula xml:id="formula_45">function ∂ i L g −1 f (x) = ∂ ∂xi [f (gx)] = j g ji [∂ j f ](gx) = L g −1 j g ji ∂ j f (x)</formula><p>where g ij are the components of the matrix g. Since this holds for any f , we find that L g ∇L g −1 = g T ∇ and therefore L g ∇∇ T L g −1 = L g ∇L g −1 L g ∇ T L g −1 = g T ∇∇ T g.</p><p>Since equation 28 holds as an operator equation, it must be true separately for each component ∂ i and ∂ i ∂ j . This means that the constraint separates into a constraint for B and a constraint for S:</p><formula xml:id="formula_46">1. ∀g, i : j g ij ρ out (g)B j ρ in (g −1 ) = B i 2. ∀g, i, j : kl g i g ik ρ out (g)S k ρ in (g −1 ) = S ij .</formula><p>These relationships can be expressed more succinctly by flattening the elements of B and S into vectors:</p><formula xml:id="formula_47">[ρ out (g) ⊗ ρ in (g −T ) ⊗ ρ (1,0) (g)]vec(B) = vec(B) and [ρ out (g) ⊗ ρ in (g −T ) ⊗ ρ (2,0) (g)]vec(S) = vec(S).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 ROTATION EQUIVARIANCE CONSTRAINT FOR SCALAR DIFFUSIONS HAS ONLY TRIVIAL SOLUTIONS</head><p>The diffusion operator A = k W k e D k leads to only trivial β k = 0 and Σ k ∝ I if it satisfies the continuous rotation equivariance constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>The application of e D k is just a convolution with the Greens function</p><formula xml:id="formula_48">k W k e D k f = k W k [e − 1 2 (x+β k ) Σ −1 k (x+β k ) det(2πΣ k ) −1/2 ] * f = k W k G k * f<label>(29)</label></formula><p>where the Greens function is the multivariate Gaussian density: G k (x) = N (x; −β k , Σ k ).</p><p>As shown in appendix E.4, for convolutions the operator constraint is equivalent to the kernel equivariance constraint ρ out (g)K(g −1 x)ρ in (g −1 ) = K(x) from <ref type="bibr" target="#b50">(Weiler and Cesa, 2019)</ref>. With K(x) = k W k G k (x) this reads:</p><formula xml:id="formula_49">∀x ∈ R d , g ∈ G : k ρ out (g)W k N (g −1 x; −β k , Σ k )ρ in (g −1 ) = k W k N (x; −β k , Σ k ),</formula><p>For rotations g ∈ SO(2) where we can parametrize g θ = e θJ in terms of the antisymmetric matrix J = [[0, 1], [−1, 0]] ∈ R 2×2 and the translation operator can be written L g = e −θx T J T ∇ , we can take derivatives with respect to θ to get (now with double sums implicit):</p><formula xml:id="formula_50">∀x ∈ R d : k dρ out W k N (x; −β k , Σ k )−W k N (x; −β k , Σ k )dρ in −W k (x T J T ∇)N (x; −β k , Σ k ) = 0.</formula><p>Here the Lie Algebra representation of J is dρ := ∂ ∂θ ρ(g θ )| θ=0 . Factoring out the normal density:</p><formula xml:id="formula_51">∀x ∈ R d : k dρ out W k − W k dρ in − W k (x T J T Σ −1 k (x + β k )) N (x; −β k , Σ k ) = 0.</formula><p>Without loss of generality we may assume that each of the Gaussians β k , Σ k pairs are distinct since if they were not then we could replace the collection with a single element. Since the (finite) sum of distinct Gaussian densities is never a Gaussian density, and monomials of order &gt; 0 multiplied by a Gaussian density cannot be formed with sums of Gaussian densities or sums multiplied by monomials of a different order and Gaussian densities are never 0, this constraint separates out into several independent constraints.</p><formula xml:id="formula_52">1. ∀i : dρ out W k = W k dρ in 2. ∀i, x : W k (x T J T Σ −1 k β k ) = 0 3. ∀i, x : W k (x T J T Σ −1 k x) = 0</formula><p>We may assume w.l.o.g. that W k is not 0 for all components of the matrix (otherwise we could have deleted this element of k and continue). Therefore there is some component which is nonzero, and the expressions in parentheses in equations 2 and 3 must be 0. Given that this holds for all x, eq 3 implies: J T Σ −1 k = 0 or equivalently Σ −1 k J = 0 because Σ k is symmetric, and since J = −J T this can be expressed concisely as [Σ −1 k , J] = 0 for which the only symmetric solution is proportional to the identity Σ k = c k I. Since both Σ k and J are invertible, equation 2 yields β k = 0. Therefore there are no nontrivial solutions for β, Σ in A = k W k e D k for continuous rotation equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F DATASET AND TRAINING DETAILS</head><p>In this section we elaborate on some of the details regarding hyperparameters, network architecture, and the datasets.</p><p>As described in the main text, the PNCNN is composed of a chain of convolutional blocks containing a convolution layer, a probabilistic ReLUs, and linear channel mixing layer (analogue of the colloquial 1 × 1 convolution). In each of these convolutional blocks, the input is a collection of points and feature mean value at those points along with the feature elementwise standard deviation at those points:</p><formula xml:id="formula_53">{(x i , µ(x i ), σ(x i )} N i=1</formula><p>. These observations seed the GP layer, and the block is evaluated at the same collection of points for the output (although it can be evaluated elsewhere since it is a continuous process, and we make use this fact to visualize the features in figures 1 and 2).</p><p>Hyperparameters: For the PNCNN on the Superpixel MNIST dataset, we use 4 PNCNN convolution blocks with c = 128 channels and with K = 9 basis elements for the different drift and diffusion parameters in K k=1 W k e D k . We train for 20 epochs using the Adam optimizer (Kingma and Ba, 2014) with lr = 310 −3 with batch size 50.</p><p>For the PNCNN on the PhysioNet2012 dataset, we use the variant of the PNCNN convolution layer that uses the stochastic diagonal estimator described in appendix G with P = 20 probes. In the convolution blocks we use c = 96 channels, K = 5 basis elements and we train for 10 epochs using the same optimizer settings above. For both datasets we tuned hyperparameters on a validation set of size 10% before folding the validation set back into the training set for the final runs. Both models take about 2 hours to train. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SuperPixel-MNIST</head><formula xml:id="formula_54">x i , f (x i ))} 75</formula><p>i=1 at the N = 75 super pixel centroids. PhysioNet2012 We follow the data preprocessing from <ref type="bibr" target="#b22">Horn et al. (2019)</ref> and the 10k-2k train test split. The individual data points consist of 42 irregularly spaced vital sign time series signals as well as 5 static variables: Gender, ICU Type, Age, Height, Weight. We use one hot embeddings for the first two categoric variables, and we treat each of these static signals as fully observed constant time series signals. As the binary classification task exhibits a strong label imbalance, 14% positive signals, we apply an inverse frequency weighting of 1/.14 to the binary cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G STOCHASTIC DIAGONAL ESTIMATION FOR PHYSIONET2012</head><p>In order to compute the mean and variance of the rectified Gaussian process, the activations of the probabilistic ReLU, we need compute the diagonal of Ak p A (x n , x n ) for the relevant points {x n } N n=1 . In the usual case where each of the channels α = 1, 2, ..., c are observed at the same locations this can be done efficiently. First one computes the application of e Di on the left and e D j on the right onto the posterior k p :</p><formula xml:id="formula_55">N ij = (e Di k p e D j )(x n , x n ) = (e Di ke D j )(x n , x n ) − (e Di k )(x n )[K + S] −1 (ke D j )(x n )</formula><p>where k is the RBF kernel and we have reused the notation from appendix A. Notably, this quantity is the same for each of the channels, and the elementwise variance is just:</p><formula xml:id="formula_56">v α (x n ) = (Ak p A ) αα (x n , x n ) = i,j,β W αβ i N ij W αβ j<label>(30)</label></formula><p>where the α, β index the channels of each of the matrices W i . Because N is the same for all channels, we can compute this quantity efficiently with a reasonable memory cost and compute.</p><p>For the PhysioNet2012 dataset where the observation points differ between the channels we must consider a different observation set {x β n } N n=1 for each channel β. This means that evaluated kernel depends on the channel and we have the objects: k β , K β and S β . As a result, we have an additional index for N β ij and the desired computation is</p><formula xml:id="formula_57">v α (x α n ) = (Ak p A ) αα (x α n , x α n ) = i,j,β W αβ i N β ij W αβ j .<label>(31)</label></formula><p>While each of the terms in the computation can be computed without much difficulty, performing the summation explicitly requires an unreasonably large memory cost and also compute.</p><p>However, by the same approach we can consider the full covariance matrix B (αn)(βm) = (Ak p A ) αβ (x α n , x β m ), and while it would not be feasible to compute this matrix directly we can define matrix vector multiplies onto vectors of size R cN implicitly using the sequence of operations that define it. Crucially, this sequence of operations has much more modest memory consumption (and compute cost) over the direct expression in equation 31. These implicit matrix vector multiplies can then be used to compute a stochastic diagonal estimator <ref type="bibr" target="#b2">(Bekas et al., 2007)</ref> given by:</p><formula xml:id="formula_58">v α (x α n ) = 1 P P p=1 z p Bz p<label>(32)</label></formula><p>with Gaussian probe vectors z p ∼ N (0, I), and where is elementwise multiplication (see <ref type="bibr" target="#b2">Bekas et al. (2007)</ref> for more details on this stochastic diagonal estimator). We use this estimator with P = 20 probes for computing the variances for PhysioNet. We note that with P = 20 the variance estimates are still quite noisy, however without the estimator cannot readily apply the PNCNN to PhysioNet. We leave a better approach for handling this kind of data to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H PATHOLOGIES IN PROJECTION TO RBF GAUSSIAN PROCESS</head><p>In section 4.7 describe an approach by which a Gaussian process with a complex mean and covariance function is projected down to the posterior of a (simpler) RBF kernel GP from a set of observations. We know given the representation capacity of the RBF kernel that with the right set of observations, a complex function can be well approximated in principle. However, the relationship for uncertainty is less straightforward.</p><p>The properties of the input Gaussian process must be conveyed to the output Gaussian process by only the (uncorrelated) noisy observations</p><formula xml:id="formula_59">{(x i , µ(x i ), σ(x i ))} N i=1 .</formula><p>As the uncertainty in original GP increases, so do the measurement uncertainties in the transmission, and therefore the output GP also has a higher uncertainty. However, the uncertainty in the input GP is in the form of a full covariance kernel k(x, x ) and it seems that individual observations will not easily be able to communicate the covariance of the values of the GP function at different spatial locations despite the heterogeneous noise model.</p><p>Fundamentally, the problem is that the observation values are treated as independent, an incorrect assumption which has other knock-on effects when the number of observations is large. With some fixed measurement error no matter how high but a large enough set of independent observations, the mean value can be pinned down precisely. If in contrast the observations are not independent, then there may be a situation where the mean value cannot be known more precisely than some limiting uncertainty. This effect leads the output GP to have less uncertainty and be more confident in the values that it should be given the input GP.</p><p>If the observations are sparse, then the effective sample size of the estimator for the mean of the GP at any given location is small, and then the amount by which uncertainty is underestimated is small. However, if there are many many observations then this kind of observation transmission of information with the independence assumption will attenuate the uncertainty. We would also expect that over the course of many layers, this attenuation can accumulate. We believe that this is what causes the poorer uncertainty calibration in layers 3 and 4 of the PNCNN shown in figure 2. We hope that this problem can be resolved perhaps by removing the independence assumption or providing an alternative projection method in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I CENTRAL LIMIT THEOREM FOR STOCHASTIC PROCESSES</head><p>We derive a variant a variant of the Lyapunov central limit theorem (CLT) holding for stochastic processes. The main ideas is that the result for processes follows from applying the multivariate Lyapunov CLT to the joint distribution of each finite collection of values as per the definition of a Gaussian process.</p><p>In more details the argument goes as follows. We are given C stochastic processes g c (x) and we assume that they are weakly dependent, i.e. E[g c (x)g c (x )] → E[g c (x)]E[g c (x )] as |c − c | 1, for any x, x . We would like to show thatḡ(x) ∼ GP(µ, k), where µ(x) = C c=1 g c (x) and k(x, x ) = C c,c =1 E[g c (x)g c (x )]. For these formulas to make sense, we need some bounds on the moments of g c . If the individual components g c scale as 1/ √ C, then the covariance if finite. Now choose any finite collection of indices x 1 , x 2 , . . . , x N . Then consider the random vector g(x i ) = C c=1 g c (x i ), i = 1, . . . , N . We can now apply the CLT to deduce that {ḡ(x i )} N i=1 is Gaussian distributed. Since a stochastic process is determined by its finite distributions, we can conclude that the random functionḡ(x) → GP(µ, k), as was to be shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J MOMENTS OF RECTIFIED GAUSSIAN RANDOM VARIABLES</head><p>Let f ∼ N (µ, Σ) be a d dimensional random Gaussian vector. We compute here E(ReLU(f 1 ) · · · ReLU(f d )) = 1</p><formula xml:id="formula_60">N Σ f &gt;0 d d f f 1 · · · f d exp − 1 2 (f − µ) T Σ −1 (f − µ)<label>(33)</label></formula><p>N Σ = (2π) d/2 det(Σ) 1/2 .</p><p>We use the generating function technique. Define</p><formula xml:id="formula_62">Z(b) = 1 N Σ f &gt;0 d d f exp[− 1 2 (f − µ) T Σ −1 (f − µ) + b T f ] (35) = 1 N Σ e b T µ f &gt;−µ d d f exp[− 1 2 f T Σ −1 f + b T f ] .<label>(36)</label></formula><p>Then E(ReLU(f 1 ) · · · ReLU(f d )) =</p><formula xml:id="formula_63">∂ ∂b 1 · · · ∂ ∂b d Z(b) b=0 .<label>(37)</label></formula><p>To compute Z(b) we proceed as in Gaussian case. We change variables to</p><formula xml:id="formula_64">f = Σb + g ,<label>(38)</label></formula><p>and define z = µ + Σb to get:</p><formula xml:id="formula_65">Z(b) = e b T µ+ 1 2 b T Σb 1 N Σ g&lt;+z d d g exp[− 1 2 g T Σ −1 g] (39) = e S(b) Φ (d) (z; 0, Σ) , S(b) = b T µ + 1 2 b T Σb .<label>(40)</label></formula><p>Φ being the multivariate standard Normal CDF:</p><formula xml:id="formula_66">Φ (d) (z; µ, Σ) = g&lt;+z d d gψ (d) (g; µ, Σ) ,<label>(41)</label></formula><formula xml:id="formula_67">ψ (d) (g; µ, Σ) = 1 N Σ exp[− 1 2 (g − µ) T Σ −1 (g − µ)] .<label>(42)</label></formula><p>Now we compute the first two derivatives. Note that in d = 1, denoting σ 2 = Σ:</p><p>∂ ∂z Φ (1) (z, 0, σ 2 ) = ψ (1) (z, 0, σ 2 ) .</p><p>In d = 2, we can use the conditional probability decomposition to get the required derivatives: ψ (2) (g; 0, Σ) = ψ (1) (g 1 ; α 1 g 2 , β 1 ) · ψ (1) (g 2 ; 0, Σ 22 ) (44) α 1 = Σ 12 Σ −1 22 , β 1 = Σ 11 − Σ 12 Σ −1 22 Σ 21 (45) ∂ z2 Φ (2) (z, 0, Σ) = ψ (1) (z 2 ; 0, Σ 22 ) z1 −∞ dg 1 ψ (1) (g 1 ; α 1 z 2 , β 1 ) ,</p><p>= ψ (1) (z 2 ; 0, Σ 22 )Φ (1) (z 1 , α 1 z 2 , β 1 )</p><p>∂ 2 z2 Φ (2) (z, 0, Σ) = − z 2 Σ 22 ψ (1) (z 2 ; 0, Σ 22 )Φ (1) (z 1 , α 1 z 2 , β 1 ) (48) + ψ (1) (z 2 ; 0, Σ 22 )∂ z2 Φ (1) (z 1 , α 1 z 2 , β 1 )</p><p>∂ z1 ∂ z2 Φ (2) (z, 0, Σ) = ψ (2) (z; 0, Σ) .</p><p>So denoting ∂ i = ∂ ∂bi , we get:</p><formula xml:id="formula_73">∂ i Z(b) = (µ i + j Σ ij b j )Z(b) + e S(b) ∂ z Φ (d) (z, 0, Σ)Σ ,i mi(b) (51) ∂ k ∂ i Z(b) = Σ ik Z(b) + (µ i + j Σ ij b j )∂ k Z(b)<label>(52)</label></formula><formula xml:id="formula_74">+ (µ k + j Σ kj b j )m i (b) + e S(b) ,q ∂ z ∂ zq Φ (d) (z, 0, Σ)Σ ,i Σ q,k .<label>(53)</label></formula><p>In particular, for the first moment d = 1 we have:</p><p>E(ReLU(f )) = µΦ (1) (µ; 0, σ 2 ) + ψ (1) (µ, 0, σ 2 )σ 2 ,</p><p>which coincides with equation 8. Note that ψ (1) (µ, 0, σ 2 ) = 1 σ ψ(µ/σ; 0, 1) because of the normalization factor. For the second moments d = 2 we have:</p><p>E(ReLU(f 1 )ReLU(f 2 )) = Σ 12 Φ (2) (µ; 0, Σ) + µ 1 µ 2 Φ (2) (µ; 0, Σ) + µ 1 m 2 (0) + µ 2 m 1 (0) (55) + ,q=1,2 Σ ,1 Σ q,2 ∂ z ∂ zq Φ (2) (z, 0, Σ)| b=0 .</p><p>(56) which can be rewritten in the form of equation 9.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Qualitative convergence of the mean and uncertainty of the first 3 channels of the feature maps is shown in RGB color as the input test resolution is increased. Middle: Median predicted uncertainties over spatial locations as a function of the test resolution. Right: Using the predictions of the highest resolution model as ground truth, the distribution of prediction residuals is shown in a Q-Q plot for each layer (shifted horizontally for clarity) with the black lines showing the theoretical relationship, and the overall distribution histogram is shown on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We source the SuperPixel MNIST dataset (Monti et al., 2017) from Fey and Lenssen (2019) consisting of 60k training examples and 10k test represented as collections of positions and grayscale values {(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Our learnable linear operators A are equivariant to continuous transformations. Two linear operators e C and e B commute [e C , e B ] = 0 if and only if their generators commute: [C, B] = 0. Since the generator of diffusions D i is a sum of derivative operators, and the generators of translations are just ∇ as mentioned in section 4.2, the two commute: [D k , ∇] = 0. Therefore [A, τ a ] = [ k W k e D k , τ a ] = k W k [e D k , e a ∇ ] = 0 and A is translation equivariant.</figDesc><table /><note>E.3 STEERABLE EQUIVARIANCE FOR LINEAR OPERATORS For some tasks like medical segmentation, aerial imaging, and chemical property prediction there are additional symmetries in the data it makes sense to exploit other than mere translation equivariance. Below we show how to enforce equivariance of the Linear operator A to other symmetry groups G such as the group of continuous rotations SO(d) in R d . Applying equivariance constraints separately on each of the components of e Di on top of translation equivariance yields very restricted set of operators. For example, enforcing equivariance to continuous rotations G =</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For convenience, we include the additional scale factor (2πl 2 ) d/2 relative to the usual definition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">More generally neural networks have affine layers including both convolutions and biases. An affine transformation Af + b of a Gaussian process is also a Gaussian process f ∼ GP(Aµp + b, AkpA ), and we include biases in our network but omit them from the derivations for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This assumes as is typically done that measure µ over which the convolution is performed is left invariant. For the more general case, see the discussion in Bekkers (2019).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><forename type="middle">R</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Da Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cláudio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05544</idno>
		<title level="m">Superpixel image classification with graph attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An estimator for the diagonal of a matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costas</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effrosyni</forename><surname>Kokiopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied numerical mathematics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1214" to="1229" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">B-spline cnns on lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12057</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Probabilistic numerical methods for partial differential equations and bayesian inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Cockayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07811</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic numerical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Cockayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">John</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="756" to="789" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Steerable cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A general theory of equivariant cnns on homogeneous spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9145" to="9156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Pim De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05425</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12880</idno>
		<title level="m">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gp-vae: Deep probabilistic time series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vincent Fortuin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1651" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Se (3)-transformers: 3d rototranslation equivariant attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to detect sepsis with a multitask gaussian process rnn classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Heller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04152</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3369" to="3378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07780</idno>
		<title level="m">Shortcut learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning with differential gaussian process flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pashupati</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Lähdesmäki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04066</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic interpretation of linear solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="234" to="260" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Set functions for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12064</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep probabilistic subsampling for task-adaptive compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Iris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huijben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruud Jg</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Sloun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Srijith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damianou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01655</idno>
		<title level="m">Deep gaussian processes with convolutional kernels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A scalable end-to-end gaussian process adapter for irregularly sampled time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin M Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning from irregularly-sampled time series: A missing data perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin M Marlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07599</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Kam Leonard</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01328</idno>
		<title level="m">Scalable gradients for stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deriving quadrature rules from gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Department</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning. Adaptive computation and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I T</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Proquest</forename></persName>
		</author>
		<ptr target="https://books.google.nl/books?id=Tr34DwAAQBAJ" />
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Latent odes for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03907</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A probabilistic model for the numerical solution of initial value problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="122" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pdo-econvs: Partial differential operator based equivariant convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingshen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10408</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-time attention networks for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Satya Narayan Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Interpolation-prediction networks for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satya Narayan Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin M Marlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07782</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting in-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikaro</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Cardiology</title>
		<imprint>
			<biblScope unit="page" from="245" to="248" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Smets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Portegies</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09046</idno>
		<title level="m">Erik Bekkers, and Remco Duits. Pde-based group equivariant convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Interpolation of spatial data: some theory for kriging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09883</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05739</idno>
		<title level="m">Graph convolutional gaussian processes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">General e (2)-equivariant steerable cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14334" to="14345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
