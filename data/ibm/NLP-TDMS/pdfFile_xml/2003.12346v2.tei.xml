<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Spiking Neural Networks for Spatio-Temporal Feature Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Samadzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Sadat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabatabaei</forename><surname>Far</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Javadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Nickabadi</surname></persName>
							<email>nickabadi@aut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><forename type="middle">Haghir</forename><surname>Chehreghani</surname></persName>
							<email>morteza.chehreghani@chalmers.se</email>
							<affiliation key="aff1">
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Spiking Neural Networks for Spatio-Temporal Feature Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spiking neural networks (SNNs) can be used in lowpower and embedded systems (such as emerging neuromorphic chips) due to their event-based nature. Also, they have the advantage of low computation cost in contrast to conventional artificial neural networks (ANNs), while preserving ANN's properties. However, temporal coding in layers of convolutional spiking neural networks and other types of SNNs has yet to be studied. In this paper, we provide insight into spatio-temporal feature extraction of convolutional SNNs in experiments designed to exploit this property. The shallow convolutional SNN outperforms state-ofthe-art spatio-temporal feature extractor methods such as C3D, ConvLstm, and similar networks. Furthermore, we present a new deep spiking architecture to tackle real-world problems (in particular classification tasks) which achieved superior performance compared to other SNN methods on NMNIST (99.6%), DVS-CIFAR10 (69.2%) and DVS-Gesture (96.7%) and ANN methods on UCF-101 (42.1%) and HMDB-51 (21.5%) datasets. It is also worth noting that the training process is implemented based on variation of spatio-temporal backpropagation explained in the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Traditional or Analog Neural Network (referred to ANNs in this paper) has helped AI field reach many paramount goals. ANNs have progressed a lot in many applications. In computer vision, Convolutional Neural Networks have been a significant breakthrough and helped to solve numerous complex tasks. Tasks like large-scale image classification (Imagenet image classification challenge) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50]</ref>, generating real-life images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref> and many other impressive tasks that were almost impossible to solve without CNNs. Despite their incredible po- tential, they have some shortcomings; the shortcomings include extreme computation power requirements and lack of memory. Many solutions have been presented to tackle these problems. To solve the problem of extreme computation power usage, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref> have been proposed. Also, to solve memory problems inside each neuron, ConvLSTMs has been the solution so far. However, the mentioned solutions are not perfect. For example, a practical convLSTM has in order of million parameters and will occupy in order of Gigabytes memory which makes it impossible to implement on an embedded vector processing unit. Another example is the MobileNet architecture which still struggles to cope with temporal dimension of data and tackle activity recognition tasks while optimizing accuracy and computational power altogether. There is a better solution called Spiking Neural Networks (SNNs) that solves both problems in a more straightforward manner. Spiking Neural Network encodes data in sequences of spike signals. It may execute more complex cognitive tasks in a way that becomes more similar to the brain cortex processing pattern <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">52]</ref>. When a neuron's membrane po-tential reaches a threshold, it is triggered and transmits a spiking signal. To be precise, spikes are binary codes which decay in time (like a electrical capacitor's charge). This binary nature of Spiking Neural Networks makes them efficient in terms of memory consumption and computation cost which leads to lower power consumption (as demonstrated in <ref type="bibr" target="#b45">[46]</ref>).</p><p>Despite SNNs' progress, training them is still challenging and an active area of research. There are some great attempts in the literature, namely Spiking Timing Dependent Plasticity (STDP), ANN-SNN conversion and backpropagation though time (BPTT) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>. STDP based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref> may have some advantages like the ability to train unsupervised but are limited to shallow networks and cannot train deeper ones required to achieve great performance in complex tasks. Conversely, ANN-SNN conversion methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> have achieved outstanding results with deep architectures (more than 18 layers). However, these methods as mentioned in <ref type="bibr" target="#b33">[34]</ref> assume that SNNs are sampling of ANNs in time. This assumption ignores the temporal properties of SNNs altogether. As some prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47]</ref> explained, there are some remarkable temporal properties to the SNNs which cannot be achieved with ANN-SNN conversion based training. BPTT based methods solve this problem by using training through time. These methods usually approximate the derivative of spike functions to be able to use backpropagation e.g. <ref type="bibr" target="#b47">[48]</ref>. This approximation will be the source of gradient vanishing as demonstrated in 2. These vanishing gradients will cause some issues in training deep architectures (more than 16 layers). Some solutions proposed by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">49]</ref> to overcome this problem, however they achieve lower performances than our method. To solve this problem, we propose a better solution by pre-initializing the weights with initial non-spiking training and using skip connections like the ResNet architecture (more explanation in section 4.2). Furthermore, as <ref type="bibr" target="#b3">[4]</ref> mentioned to demonstrate the spatiotemporal power and efficiency of SNNs, we design a series of test cases (synthetic dataset) to showcase the spatiotemporal property of SNNs compared to ANN ones and make the synthetic dataset a baseline for other recurrent methods to follow (more detail of this synthetic dataset in section 3.2). Then, we discuss the spatio-temporal properties of SNNs that differentiate them from conventional recurrent ANNs (see <ref type="figure" target="#fig_0">Figure 1</ref> for instance). Finally, we propose a deep architecture tested on NMNIST <ref type="bibr" target="#b26">[27]</ref>, IBM DVS-Gesture <ref type="bibr" target="#b1">[2]</ref>, DVS-CIFAR10 <ref type="bibr" target="#b21">[22]</ref>, UCF-101 <ref type="bibr" target="#b36">[37]</ref> and HMDB-51 <ref type="bibr" target="#b18">[19]</ref> datasets and it achieves state-of-the-art performance utilizing the proposed learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Since commercial release of event-cameras in 2008, applications for neural networks with low computation cost have emerged (Spiking Neural Networks to be exact). Interestingly, SNNs can be utilized in real-time applications and harsh environments (i.e. when we have noisy pixel values or against adverserial attacks as <ref type="bibr" target="#b34">[35]</ref> shows in detail). Real-time applications of SNNs consist of but not limited to visual simultaneous localization and mapping or visual odometry (also knows as VSLAM or VO) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>, pose tracking applications <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> and etc. Furthermore, they are useful in high-speed applications such as object recognition in self-driving cars <ref type="bibr" target="#b45">[46]</ref>. Moreover, due to very low power consumption and low latency and lighting condition robustness of event cameras, applications of SNNs can be extended to other vision domains if event-camera's price drops.</p><p>Training SNNs is not that easy, although numerous methods are proposed for training them in recent years. Most studied methods focus on converting weights of an ANN model to equivalent SNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref>. For example, <ref type="bibr" target="#b13">[14]</ref> and then <ref type="bibr" target="#b33">[34]</ref> converted deep residual ANN architecture to their SNN counterparts achieving near state-ofart performance. The ANN-SNN conversion methods may have a very good performance but they have two evident problems: first, they are less robust to noise and adversarial attacks (more details in <ref type="bibr" target="#b34">[35]</ref>) and second, they suppress temporal coding properties of SNNs. To mitigate this problem, the alternative method is to train SNNs directly (to be precise, by using backpropagation through time). As <ref type="bibr" target="#b25">[26]</ref> explained, the main problem of direct SNN training is non-differentiability of spiking function. <ref type="bibr" target="#b47">[48]</ref> overcame this problem by approximating the derivative of threshold function.</p><p>However, <ref type="bibr" target="#b47">[48]</ref> and existing methods similar to it have some challenges with going deeper in Spiking Neural Networks. <ref type="bibr" target="#b48">[49]</ref> tries to recreate batch normalization for SNNs to use its properties and build deeper networks. Nevertheless, the NeuNorm solution ( <ref type="bibr" target="#b48">[49]</ref>) does not have the same properties as batch-norm and is not much of a help in train-ing deeper networks (more than ten layers).</p><p>To summarize, we have tackled the problem of deeper SNNs via pre-initialization of weights BPTT proposed in <ref type="bibr" target="#b47">[48]</ref> (more details in section 4.1). We also adapted the method in <ref type="bibr" target="#b33">[34]</ref> to create deep residual SNNs and solve complex classification tasks (such as DVS-CIFAR10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatio-temporal property of SNNs</head><p>In this section, LIF model is explained in detail and is compared to recurrent ANN architectures. Moreover, a special synthetic dataset is designed to showcase the critical differences between SNNs and pier ANNs in terms of spatio-temporal feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Leaky integrate and fire (LIF)</head><p>One of the successful and excellent and simple modelings of SNNs is leaky integrate and fire (LIF) model. The model, from LIF neurons implementation perspective, is defined as follows:</p><formula xml:id="formula_0">U (t,n) = α(J − O (t−1,n) )U (t−1,n) + g(O (t,n−1) ) (1) O (t,n) = f (u (t,n) )<label>(2)</label></formula><p>In Eq ((1)), J is a matrix of ones and g(.) is the layerwise operation. For linear layers g(.) will be defined as:</p><formula xml:id="formula_1">g(O (t,n−1) ) = W n O T (t,n−1)<label>(3)</label></formula><p>The term (J − O (t−1,n) ) in the left side of (1) is for neuron rest and enforcing sparsity in the LIF neurons. f (.) is the activation function which can be interpreted as a threshold function. This function for each node is defined as:</p><formula xml:id="formula_2">f (U (t,n) i ) = 1 if U (t,n) i ≥ T 0 if U (t,n) i &lt; T<label>(4)</label></formula><p>In the equations above, n is the layer number, t is the time-stamp and α is decay factor in (1). Note that the decay factor needs careful tuning and all of the equations (except the activation function) are in the matrix form.</p><p>The difference between an ANN and SNN neuron is the left side of (1) and the activation function. The following operation needs to be performed on output of last layer to obtain output of the SNN (assuming rate encoding over an arbitrary time window): <ref type="figure" target="#fig_2">Figure 3</ref> summarizes the description of LIF model. The architectures presented in this paper employ the mentioned LIF neuron model. Furthermore, a synthetic dataset is proposed in this paper to experimentally prove and showcase SNNs' interesting spatio-temporal properties. In the next section, this synthetic dataset is explained in details. is the analog layer (e.g. convolutional layer), u (t,n) i is the membrane potential of each neuron i inside layer n in time instance of t, f (.) is the activation function which is the step function; therefore, O ( i t, n) is the output of the neuron i inside layer n in time instance of t, J is the matrix of ones which is used to identify if the neuron is fired to let it stay in rest for the time step and finally α is the decay factor which determines how much of the membrane potential (memory in general) should be passed to the next time instance</p><formula xml:id="formula_3">O (t,N ) = 1 T t k=t−T O (k,N )<label>(5)</label></formula><formula xml:id="formula_4">u i (t-1,n) u i (t,n) g(.) g(.) f(.) O i (t-1, n) X X J - Alpha O i (t, n-1) f(.) O i (t, n) O i (t-1, n-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Synthetic dataset</head><p>As earlier in the paper mentioned, a synthetic dataset is designed to demonstrate the spatio-temporal feature extraction property of Spiking Neural Networks (see <ref type="figure">Figure 4</ref>). This synthetic dataset is designed using MNIST numbers and consists of 5 cases, each challenging an aspect of spatiotemporal feature extraction.</p><p>In detail, sequence 1 is a 0 to 100 percent zoom-in and zoom-out act performed in a time window of 10, each one 10 percent larger or smaller (based on zoom-in or zoomout) than the previous one (see <ref type="figure">Figure 4</ref>, row 1 and 2, to get a better sense). In this sequence, the network should classify the number and the act which is a zoom-in or zoomout act; therefore, there are 20 classes to be specified (two zoom-in and zoom-out on ten characters of MNIST dataset). The challenge here is to remember the zoomed-out number (long term memory required).</p><p>Sequence 2 is a regular rotation from 0 to 360 degrees and reverse within the same time window of ten. Each frame is 36 degrees rotated version of its previous frame (clockwise or counterclockwise). In <ref type="figure">Figure 4</ref>, row 3 and 4, a 7 character from MNIST being rotated inside a time window of 10 is demonstrated. Sequence 3 is very similar to sequence 1 with the difference in zoom percentage. In this sequence, the zoomin (and out) is performed from 50 percent (to 50 percent); therefore, basically, the challenge of sequence 1 is less evident here, and the methods should only be able to recognize the difference of almost subtle zooming-in (or zooming-out) inside a time window.</p><p>Sequence 4 is designed to challenge the networks in terms of noise robustness and occlusion happened in many scenarios. In this sequence, the characters of MNIST are masked by a square with dimensions of 14x14 pixels (half the dimensions of the original character). This mask is moved randomly over the characters inside the time window of 10. Therefore, this sequence consists of ten classes of occluded MNIST videos. Using this masking dimension, it is almost impossible for the network to determine the class of characters and reach high accuracy without any memory (because for some characters, like 8 and 3, there is no clear frame that can specify the exact character inside the window). An example of character 4's frames inside the time window is available in <ref type="figure">Figure 4</ref> row 7.</p><p>Lastly, sequence 5 is the most challenging sequence in terms of the human eye (see <ref type="figure">Figure 4</ref> rows 9 and 10 to get a great sense of the challenge). In this sequence, each character is rotated clockwise or counterclockwise with a random degree between 0 and 360 degrees. This random selection is based on the uniform distribution (therefore, the mean rotation would be 180 degrees). The rotations are also performed in a time window of 10. Similar to sequence 2, this sequence also consists of 20 classes (counterclockwise or clockwise rotated frames of MNIST characters). This sequence challenges the methods that use regular rotation patterns in frames because there are no two equal rotation patterns in the train and test part of this sequence.</p><p>In summary, this synthetic dataset can verify most aspects of spatio-temporal feature extraction in a simple way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Traning Deep SNNs</head><p>This section provides a new method to train deep Spiking Neural Network architectures. Then, a new architecture is proposed to prove usablity of the training method that reaches the state of art performance. This architecture, shown in <ref type="figure" target="#fig_3">Figure 5</ref>, is inspired by Resnet architecture and the work of <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training method</head><p>As explained in Section 2, ANN-SNN conversion methods achieve very high accuracy over very complex datasets like Imagenet, although they do not showcase SNNs' temporal property and don't have such noise robustness as BPTT methods. The proposed training method is almost a hybrid of both and has a few differences from BPTT proposed in <ref type="bibr" target="#b47">[48]</ref>. To be specific, at first n epochs, the SNN <ref type="figure">Figure 4</ref>. Synthetic dataset designed to challenge spatio-temporal extraction properties. The base images are derived from MNIST dataset. The columns from left to right indicate time which is limited to a window with length of 10. The frames in top two rows belong to sequence1(zoom-in from 0% to 100% and zoom-out from 100% to 0%), frames in the 3rd and 4th rows belong to se-quence2(360 degree clock-wise and counter clock-wise rotations from 0 degree to 360 degree and vice-versa), frames in 5th and 6th rows belong to sequence3 (zoom-in from 50% to 100% and zoom-out from 100% to 50%), the frames in the 7th row belong to sequence4 (occlusion of the character) and the last two rows in the bottom belong to sequence5(random rotation clock-wise and counter clock-wise).</p><p>model is altered to be trained similar to an ANN without the problem of gradient vanishing. To do this step, the output activation function of all layers but the output layer is changed to shifted leaky-relu. Also, there is no resting mechanism embedded. So, basically the Eq. 1 and 4 change into the Eq. 6 and 7. In the shifted leaky relu function demonstrated in Eq. 7, the β hyper-parameter is a small constant near zero.</p><formula xml:id="formula_5">U (t,n) = αU (t−1,n) + g(O (t,n−1) ) (6) f (U (t,n) i ) = U (t,n) i if U (t,n) i ≥ T −βU (t,n) i if U (t,n) i &lt; T<label>(7)</label></formula><p>After n epochs, the activation function is switched into the step function and LIF neuron model is used. From this point forward, the outputs are binary as they should be in an SNN and the training procedure is similar to BPTT proposed in <ref type="bibr" target="#b47">[48]</ref> and <ref type="bibr" target="#b48">[49]</ref>. The proposed inital step will make the training much more robust (as depicted in <ref type="figure" target="#fig_5">Figure 7</ref>)  and will enable the deep Spiking Neural Networks to reach much higher accuracy; therefore, almost resolving the problem of gradient-vanishing as demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>. In the following section, a deep SNN architecture is proposed to showcase this training method's efficiency and state-ofart performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The STS-ResNet architecture</head><p>As mentioned in the previous sections, the principal difficulty of training deep SNNs is gradient-vanishing. Resnet architectures, solve the obstacle of gradient vanishing by utilizing the skip connections. The skip connections increase performance at a drastic rate as well. Inspired by <ref type="bibr" target="#b33">[34]</ref>, a Resnet18 like architecture is proposed (see <ref type="figure" target="#fig_3">Figure 5</ref>). The main difference of our proposed network to the one in <ref type="bibr" target="#b33">[34]</ref> is synapse places, dropouts and additional skip connections and the fact that it can be trained directly. Hence, this architecture is called spatio-temporal Spiking Resnet (STS-ResNet). Details of each block are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. There are two sub-blocks in each block similar to Resnet18 architecture. In order to force binary outputs after each layer, thresholding activation function namely synapse is applied to the output of each layer. Inside subblocks, there is a dropout to ensure generality and force sparsity. Dropouts are somehow playing the role of batch-normalization for generalization. Average pooling layers do not conflict with the nature of binary outputs in each layer. They are part of the next layer operation; therefore, for example the last average pooling layer is part of the FC layer defined as a new layer (a fact never mentioned before in previous works). Another major difference of this architecture to ResNet18 is the block skip connections. They are added from blocks 3 and 4 to the input of average pooling layer. In order to increase performance, instead of conventional summation operation, concatenation operation is used afterward. This concatenation does not happen in Resnet skip connections.</p><p>The STS-ResNet architecture consists of 18 layers, 16 layers in the blocks, 2 convolutional layers and a fully connected layer at the top and bottom of architecture respectively. There is a synapse at the input to allow every kind of inputs (binary, grayscale, color image). It is the first time an 18 layer SNN is trained in space and time domain to classify spatio-temporal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Spatio-temporal feature extraction experiments</head><p>In this section, the goal is to evaluate the performance of top-quality ANN spatio-temporal feature extractors (namely CNN, C3D, ConvLSTM, conv+LSTM) against SNNs. Convolutional Neural Networks are known for their usage in computer vision tasks <ref type="bibr" target="#b39">[40]</ref>. Moreover, CNN+LSTM <ref type="bibr" target="#b31">[32]</ref> and C3D network <ref type="bibr" target="#b42">[43]</ref> are appropriate for modeling spatio-temporal information. ConvLSTM proposed in <ref type="bibr" target="#b50">[51]</ref> is also suitable for spatio-temporal feature learning. As mentioned in <ref type="bibr" target="#b37">[38]</ref>, there are some spatiotemporal datasets such as moving MNIST and CIFAR10-DVS to evaluate these methods. Therefore, the synthetic dataset, as explained in Section 3.2, is designed to demonstrate the critical factors of these architectures. For this matter, a shallow network (&lt; 5 layers) of each architecture is used for comparison. Furthermore, to make the comparison more difficult for ourselves, ConvSNN has fewer neurons than it's competitors. Less parameters means less computational power need (as demostrated in <ref type="figure" target="#fig_4">Figure 6</ref>). In the following paragraph, we will explain how we feed the synthetic dataset into the Neural Networks (NNs).</p><p>The synthetic dataset is fed into CNN via frame concatenation; therefore, CNN gets a stack of images as input. This type of dataset feeding holds for the C3D too, as we concatenate the frames in 3rd dimension and give it to the C3D network.  and ConvSNN) and frames are fed into the networks in each time step and then backpropagation through time is performed at the end. The output for these architectures is determined by a voting mechanism which indicates that a neuron with the most amount of firing in the time window will win (also known as the winner-takes-all or fire-ratebased output). For example, considering time window with length of ten, each frame is given to the Convolutional SNN. At the end of each time window the neuron with most firing rate will be the winner. In the following paragraph the essence of memory for NNs is explained.</p><p>Due to the structure of synthetic dataset, this frame by frame feeding though time clearly requires the networks to have some sort of memory. To explain this memory need further, take the example of seq2. If the NN has no memory, feeding this sequence frame by frame will cause it to not recognize clockwise and counterclockwise movement. Since it has received the same frames with different orders at the end of the time window. Fire-rate-based output mechanism will not make an acceptable prediction for two mentioned frame sequences. In the following, we will discuss about training methods and evaluation criteria.</p><p>Training method used for the ConvSNN is the simple spatio-temporal backpropagation presented in <ref type="bibr" target="#b47">[48]</ref>. Training algorithm for other methods were conventional BP and BPTT. The training is performed for at least 100 epochs and the maximum performance on the test set is recorder for each architecture. For the purpose of evaluation, each NN is tested 10 times and the average top-1 accuracy is reported in <ref type="table">Table 1</ref>.</p><p>As discussed in Section 3.2, the general property of spatio-temporal feature extraction is examined with sequences 2 and 3. All architectures show promising results on those two sequences <ref type="table">(Table 1)</ref>. Then, long-term preservation of data in memory is investigated using seq1 as explained in Section 3.2. Typical LSTM layer (lacking cut connections) does not have this property and therefore the accuracy will drop as shown in <ref type="table">Table 1</ref>. Moreover, the robustness of architectures to noise and their ability to extract spatial features through time is examined via seq4. The results prove SNN superiority in this aspect over all other networks. The CNN+LSTM performance is considerably worse than the others. The output of CNN is given to the LSTM layer and it does not contain all properties of spatial features; therefore, making it strenuous for the LSTM layer to determine the occluded character. Finally, testing with seq5, the random spatio-temporal features generated in time challenges the non-repeatable temporal feature extraction qualities of the mentioned NNs. This sequence highlights SNNs great ability in classifying stochastic nonrepeating time patterns over the rivals. Results in <ref type="table">Table 1</ref> show that only CNN+LSTM can outperform SNN in this sequence with little margin, other NNs drastically fail to achieve comparable accuracy, specially the CNN architecture. Although, the conv+LSTM network has too many layers compared to SNN and <ref type="figure" target="#fig_4">Figure 6</ref> can show the difference.</p><p>To further evaluate the amazing achievement of Con-vSNN. The confusion matrices were illustrated for sequences 1 and 5 in <ref type="figure" target="#fig_6">Figure 8 and 9</ref>. <ref type="figure" target="#fig_6">Figure 8</ref> illustrates imperfection of convSNNs in extracting random temporal properties (the problem of distinguishing between clockwise and counterclockwise rotation in seq5). <ref type="figure" target="#fig_7">Figure 9</ref> shows long-term memory preservation issue in conventional LSTM layers. Moreover, feature maps of ConvSNN and Conv+LSTM is shown in <ref type="figure" target="#fig_0">Figure 1</ref> to show the vast difference of ConvSNNs and ConvANNs in a short example. As <ref type="figure" target="#fig_0">Figure 1</ref> depicts, SNN architecture tries to extract both spatial and temporal property inside the feature map whereas ANN only extracts the spatial property of MNIST charac- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ters.</head><p>The mentioned results prove claim of the paper (SNNs are suitable for spatio-temporal feature extraction specially when featuers aren't in a regular time or space pattern or they are noisy). In order to make the SNNs more suitable for complex conditions, we proposed STS-Resnet architecture as mentioned in Section 4.2. The results of this architecture is reported in <ref type="table">Table 1</ref> along other methods. The performance of this architecture over more challenging datasets is explained in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments with Deep residual SNN</head><p>In order to showcase the advantages of our learning method and designed architecture over the prior SNNs and ANNs, we chose NMIST and DVS-Gesture datasets to exhibit the performance of our learning method and architecture over non-complex neuromorphic image classification task. We have compared STS-ResNet with other state-ofthe-art (SOTA) SNN methods. <ref type="table">Table 2</ref> depicts the performance comparison. Furthermore, we chose CIFAR10-DVS dataset to depict performance of this architecture in complex neuromorphic task against the SOTA SNN methods. Finally, we have evaluated the ResNet18 (traditional ANN) and STS-ResNet architecture on complex activity recognition of UCF-101 and HMDB-51 datasets to truly challenge the STS-ResNet architecture against its ANN counterpart.</p><p>The output of the networks is calculated based on the fire-rate-based method and the inputs are given to networks in raw form (neuromorphic datasets are fed into NNs with event stacking through time). The evaluation of the networks is based on top-1 accuracy calculated over 10 runs of training for at least 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dataset Accuracy SPA <ref type="bibr" target="#b22">[23]</ref> NMIST 96.3% SpiNNaker <ref type="bibr" target="#b27">[28]</ref> NMIST 98.5% STBP tested in <ref type="bibr" target="#b27">[28]</ref> NMIST 97.92% Natural ConvSNN <ref type="bibr" target="#b3">[4]</ref> NMNIST 99.42% NeuNorm <ref type="bibr" target="#b48">[49]</ref> NMNIST 99.53%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS-ResNet</head><p>NMNIST 99.6% CNN <ref type="bibr" target="#b1">[2]</ref> DVS-Gesture 94.59% pointnet++ <ref type="bibr" target="#b44">[45]</ref> DVS-Gesture 95.32%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS-ResNet</head><p>DVS-Gesture 96.7% HAT <ref type="bibr" target="#b35">[36]</ref> DVS-CIFAR10 52.4% Natural ConvSNN <ref type="bibr" target="#b3">[4]</ref> DVS-CIFAR10 60.3% NeuNorm <ref type="bibr" target="#b48">[49]</ref> DVS-CIFAR10 60.5% SPA <ref type="bibr" target="#b22">[23]</ref> DVS-CIFAR10 67.8%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS-ResNet</head><p>DVS-CIFAR10 69.2% ResNet18 (ANN+scratch) <ref type="bibr">[</ref>  <ref type="table">Table 2</ref> elucidates the significant improvement of STS-ResNet over the previous outstanding methods. The only downside of STS-ResNet is being inferior to Resnet18 over the UCF-101 dataset. The reason for this mediocre performance is the extreme importance of spatial features in UCF-101. It is evident from this observation that STS-ResNet, despite being trained by our method, is still inferior to the ANN counterpart; therefore, the training technique is not perfect. On the other hand, the results on HMDB-51 is promising, showing great performance over this extremely challenging dataset. The HMDB-51 dataset has more temporal features embedded than UCF-101 dataset. The temporal features makes the dataset more suitable for SNNs, since they have neuron memories to accommodate for temporal feature extraction. It is worth noting that it is the first time that a SNN architecture is tested on complex datasets like UCF-10 and HMDB-51.</p><p>Furthermore, our architecture needs much less kernels than <ref type="bibr" target="#b48">[49]</ref> and is much more memory efficient. To be precise, our model concentrates on making the network deep, whereas <ref type="bibr" target="#b48">[49]</ref> tries to increase number of kernels per layer. In order to see the exact parameters, refer to implementation details in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation details</head><p>In this subsection, we provide the experiment details about networks architectures and parameters. Basic implementation details for the test cases are as follows: Frame window size for all networks was set to 10. Learning rate for all network architectures was set to 1e-3 (except ConvSNN which was set to 5e-4). All architectures were trained enough to reach maximum accuracy (more than 100 epochs). These experiments were performed at least 10 times and the mean value for them is reported in the tables. For training networks, Adam optimizer and least mean square error was used (except for ConvLSTM network where Binary cross-entropy was used). Batch sizes were 100 except for ConvSNN which was 20.</p><p>As for the ConvSNN specific parameters, threshold value was set to 0.5. This value is extremely important and slight change in it will result in better or worse results. α or decay factor is set to 0.5. Increasing this value will result in better preservation of memory and vulnerability to noise. Also, resting mechanism was disabled. Resting mechanism maintains more sparisity in spike patterns but results in accuracy drop. Derivative of Dirac function was aproximated with Gaussian function N (T hreshold, <ref type="bibr">1 6</ref> ) (Rect function is better approximation in terms of performance but it will learn harder and the mean accuracy for several runs will drop dramatically). The initial pre-training was performed for almost 50 epochs and then the BPTT presented in <ref type="bibr" target="#b47">[48]</ref> was used. The comparison of our learning method and previos BPTT based method is depicted in <ref type="figure" target="#fig_5">figure 7</ref> Parameters of the proposed STS-ResNet architecture to tackle CIFAR10-DVS are as follows: Frame window length and learning rate was set to 10 and 5e-4 respectively and training was perfomed for more than 50 epochs and more than 5 runs as before. The optimizer was SGD with momentum of 0.9. Chosen loss function was binary cross entropy. Batch size was 10 and 1000 events were concatanated per frame. Other SNN specific parameters(threshold, resting mechanism, deravative approximate function) were as before, except for decay factor which was set to 0.8 (in CIFAR10-DVS dataset memory is more important than noise robustness).</p><p>All of the experiments were performed on a system with Intel Core i5-6500, NVIDIA GTX 1080 and NVIDIA GTX 1080ti with 32 GB RAM and SSD storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper demonstrated the potentials of SNNs in terms of spatio-temporal feature extraction. Particularly, their capacity to extract randomly distributed features in the time and space domain. This claim was supported by experiments with a special type of synthetic dataset designed for the matter. Furthermore, a modified training method is proposed to enable training of deeper networks. Then, a new deep SNN architecture was proposed to showcase its performance gain over multiple known datasets. The introduced SNN architecture was tested on challenging datasets incuding CIFAR10-DVS, NMNIST and DVS-Gesture to depict it's gain over previous architectures.</p><p>Regarding the results, a shallow SNN outperformes shal-  The remaining problem to be solved is adaptation of batch normalization properties to SNNs. These properties are required to have very deep SNNs e.g. 101 layers. Also, there should be a better solution for BP other than approximating the derivative of activation function; the approximate functions are the primary cause of gradient-vanishing. Another step in the journey of analyzing these networks might be an analysis of other types of SNNs such as GANs.</p><p>To sum it all, this work renders the advantages of SNNs transparent and proposes a new deep architecture with some solutions to train other deeper SNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Feature map comparison of SNNs and ANNs. This feature map belong to training on synthetic dataset of rotated character of MNIST (more details about this synthetic dataset in Section 3.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Gradient vanishing phenomenon happened because of the step function and its approximate derivative in the proposed BPTT methods. Compared to great solution like leaky-relu function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>LIF neurons in SNNs, expanded in time and space. g(.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Proposed Deep SNN architecture (STS-ResNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Accuracy Vs complexity (MOps) of tested architectures (CNN, C3D, ConvLSTM, Conv+SLTM, ConvSNN) over the se-quence5 of sythetic dataset proposed in Section 3.2. It is evident that the ConvSNN complexity is far less than its competitors while preserving comparable performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Training accuracy per epoch plot. This plot shows the robustness of performance using our learning method over multiple tries on the DVS-CIFAR10. Three random tries attempted to train the STS-ResNet for each training method. As for our training method, first five epochs where pre-training and the rest is almost equal to STBP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Confusion matrix over seq5, comparing result of CNN+LSTM and ConvSNN. The left image shows performance of Conv+LSTM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Confusion matrix over seq1, comparing result of CNN+LSTM and ConvSNN. The left image shows performance of ConvSNN model. low ANNs over extreme conditions (synthetic dataset), and surpassed SNNs over the typical event-based dataset (CIFAR10-DVS). Moreover, SNNs have much lower memory consumption (with the assumption of binary connections) and computation cost which refers to less overall hardware power consumption. Also, in some situations, SNNs with few number of neurons can achieve what oversized ANNs can barely achieve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CNN) + 256(LSTM) 96.84% 67.74% 98.93% 98.96% 94.88% 91.2%</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell>Config</cell><cell>MNIST</cell><cell>Seq1</cell><cell>Seq2</cell><cell>Seq3</cell><cell>Seq4</cell><cell>Seq5</cell></row><row><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell>256-256</cell><cell>99%</cell><cell>98.89%</cell><cell>98.2%</cell><cell>98.8%</cell><cell>98.27% 20.1%</cell></row><row><cell></cell><cell cols="4">CNN+LSTM 256-256(ConvLSTM 256-256</cell><cell>99%</cell><cell>99.11%</cell><cell>98.9%</cell><cell>30.0%</cell><cell>97.43% 40.7%</cell></row><row><cell></cell><cell>C3D</cell><cell></cell><cell></cell><cell>256-256</cell><cell cols="2">99.03% 98.49%</cell><cell>98.32%</cell><cell>99.17% 97.73% 64.0%</cell></row><row><cell></cell><cell cols="2">ConvSNN</cell><cell></cell><cell>48-48</cell><cell>99.4%</cell><cell>98.6%</cell><cell>98.4%</cell><cell>99.36% 98.8%</cell><cell>89.5%</cell></row><row><cell></cell><cell cols="3">STS-ResNet</cell><cell>18 layers (see Sec 4.2)</cell><cell>99.7%</cell><cell>99.26%</cell><cell>99.2%</cell><cell>99.43% 99.1% 92.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Table 1. Classification accuracy over synthetic dataset explained in Section 3.2</cell></row><row><cell>Complexity (MOps) or</cell><cell>number of parameters</cell><cell>60</cell><cell>C3D</cell><cell>Accuracy Conv+LSTM ConvSNN 90</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time-domain is considered for feed-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ing data into other architectures (ConvLSTM, Conv+LSTM</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cognitive processing using spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacob N Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Abdel-Aty-Zohdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 2009 National Aerospace &amp; Electronics Conference (NAECON)</title>
		<meeting>the IEEE 2009 National Aerospace &amp; Electronics Conference (NAECON)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A low power, fully event-based gesture recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnon</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Melano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmelo</forename><forename type="middle">Di</forename><surname>Nolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapan</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spiking deep convolutional neural networks for energy-efficient object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="66" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the performance comparison between snns and anns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangshe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast-classifying, highaccuracy spiking deep networks through weight and threshold balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>ieee</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Backpropagation for energy-efficient neuromorphic computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rathinakumar</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmendra S</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1117" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Leaky integrate-and-fire spiking neuron with learnable membrane time parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05785</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Event-based, 6-dof camera tracking from photometric depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Ea</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2402" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopalakrishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13558" to="13567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A biologically plausible supervised learning method for spiking neural networks using the symmetric stdp rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhe</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02183</idno>
		<title level="m">Comparing snns and rnns on neuromorphic vision datasets: Similarities and differences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01352</idno>
		<title level="m">Yueming Wang, and Gang Pan. Spiking deep residual network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spiking neural network methodology for modelling, classification and understanding of eeg spatio-temporal data measuring cognitive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Capecci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="565" to="575" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stdp-based spiking deep convolutional neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time 3d reconstruction and 6-dof tracking with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanme</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="349" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-latency visual odometry using eventbased feature tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Kueng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training deep spiking convolutional neural networks with stdp-based unsupervised pretraining followed by supervised fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chankyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyadarshini</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopalakrishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">435</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cifar10-dvs: an event-stream dataset for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">309</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective aer object classification using segmented probability-maximization learning in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1308" to="1315" />
		</imprint>
	</monogr>
	<note>Huajin Tang, and Gang Pan</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised learning based on temporal coding in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Mostafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3227" to="3235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Event-based, 6-dof pose tracking for high-speed maneuvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2761" to="2768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Emre O Neftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="51" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Converting static image datasets to spiking neuromorphic datasets using saccades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>Jayawant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thakor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">437</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Event-driven implementation of deep spiking convolutional neural networks for supervised classification using the spinnaker neuromorphic platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Patino-Saucedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Rostro-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernabé</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="319" to="328" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evo: A geometric approach to eventbased 6-dof parallel tracking and mapping in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Horstschäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="593" to="600" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conversion of continuous-valued deep networks to efficient event-driven networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rueckauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia-Alexandra</forename><surname>Lungu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">682</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haşim</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper in spiking neural networks: Vgg and residual architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhronil</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comprehensive analysis on adversarial robustness of spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saima</forename><surname>Sharmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyadarshini</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chankyu</forename><surname>Syed Shakib Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wachirawit</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Ponghiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hats: Histograms of averaged time surfaces for robust event-based object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuele</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An eventdriven classifier for spiking neural networks fed with synthetic or dynamic vision sensor data. Frontiers in neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Stromatias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernabé</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">350</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Tavanaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bp-stdp: Approximating backpropagation using spike timing dependent plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Tavanaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8252" to="8262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Space-time event clouds for gesture recognition: from rgb cameras to event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1826" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Temporal pulses driven spiking neural network for fast object recognition in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09220</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A spiking neural network framework for robust sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><forename type="middle">Chen</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">836</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatio-temporal backpropagation for training highperformance spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">331</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Direct training for spiking neural networks: Faster, larger, better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spike-based indirect training of a spiking neural networkcontrolled virtual insect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Henriquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd IEEE Conference on Decision and Control</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6798" to="6805" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
