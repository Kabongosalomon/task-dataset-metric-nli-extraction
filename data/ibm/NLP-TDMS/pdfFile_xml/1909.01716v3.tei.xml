<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
							<email>michihiro.yasunaga@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
							<email>jkasai@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>r.zhang@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
							<email>alexander.fabbri@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
							<email>irene.li@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Friedman</surname></persName>
							<email>dan.friedman@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
							<email>dragomir.radev@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article's impacts on research community. This paper provides novel solutions to these two challenges. We 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors' original highlights (abstract) and the article's actual impacts on the community (citations), to create comprehensive, hybrid summaries. We conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. Our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research. 1 Abstract:</p><p>We present an approach to pronoun resolution based on syntactic paths. Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities. This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints. Highly coreferent paths also allow mining of precise probabilistic gender/number information. We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier. Significant gains in performance are observed on several datasets. (mostly about technique)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Citation Sentences:</head><p>Bergsma and Lin (2006) determine the like-lihood of coreference along the syntactic path connecting a pronoun to a possible antecedent, by looking at the distribution of the path in text. (about technique)</p><p>We use the approach of Bergsma and Lin <ref type="formula">(2006)</ref>, both because it achieves state-ofthe-art gender classification performance, and because a database of the obtained noun genders is available online. (about both technique and dataset)</p><p>For the gender task that we study in our experiments, we acquire class instances by filtering the dataset of nouns and their genders created by <ref type="bibr" target="#b1">Bergsma and Lin (2006)</ref>. (about dataset)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Fast-paced publications in scientific domains motivate us to develop automatic summarizers for scientific articles. Recent work in automatic summarization has achieved remarkable performance for news articles: Single-Document Summarization <ref type="bibr" target="#b38">(Parveen, Ramsl, and Strube 2015;</ref><ref type="bibr" target="#b5">Cheng and Lapata 2016;</ref><ref type="bibr" target="#b42">See, Liu, and Manning 2017;</ref><ref type="bibr" target="#b34">Narayan, Cohen, and Lapata 2018)</ref>, Multi-Document Summarization <ref type="bibr" target="#b18">(Hong and Nenkova 2014;</ref><ref type="bibr" target="#b2">Cao et al. 2015;</ref><ref type="bibr" target="#b3">Cao et al. 2017)</ref>. Scientific article summarization, on the other hand, is less explored, and differs from news article or other general summarization. For example, scientific papers are typically longer and contain more complex concepts and technical terms. Moreover, they are structured by section and contain citations.</p><p>To encourage research in scientific article summarization, several shared tasks have been organized recently: TAC 2014 (biomedical domain), CL-SciSumm <ref type="bibr" target="#b19">(Jaidka et al. 2016;</ref><ref type="bibr" target="#b20">Jaidka et al. 2017;</ref><ref type="bibr" target="#b21">Jaidka et al. 2018</ref>) (computational linguistics domain, consisting of ACL Anthogoly papers). While these shared tasks have established a foundation for scientific paper summarization, their datasets are small, with <ref type="figure">Figure 1</ref>: Abstract and citations of <ref type="bibr" target="#b1">(Bergsma and Lin 2006)</ref>. The abstract emphasizes their pronoun resolution techniques and improved performance; the citation sentences reveal that their noun gender dataset is also a major contribution to the research community, but it is not covered in the abstract. just 30-50 articles. As understanding and annotating a scientific paper require domain-specific expert knowledge, annotation does not scale to a large corpus as compared to news articles, preventing us from applying data-driven approaches such as neural networks shown powerful in news article summarization <ref type="bibr" target="#b5">(Cheng and Lapata 2016;</ref><ref type="bibr" target="#b42">See, Liu, and Manning 2017)</ref>. In news article summarization, on the other hand, prior work <ref type="bibr" target="#b46">(Woodsend and Lapata 2010;</ref><ref type="bibr" target="#b5">Cheng and Lapata 2016)</ref> has manually created gold summaries for 9,000 documents and extended them to 200K documents by heuristics. This type of annotation or crowdsourcing is not realistic for scientific papers due to their length and technical content.</p><p>Another characteristic of scientific papers is that they may have impacts that are not expected at the time of publication. For instance <ref type="figure">(Figure 1)</ref>, the abstract of Bergsma and Lin emphasizes their techniques and improved performance in pronoun resolution, but a citation analysis reveals that their contribution to subsequent work lies largely in the noun gender dataset they created. While the abstract of a paper provides a solid summary of the content from the authors' point of view, it may fail to convey the actual impact of the paper on the research community. Additionally, the significance of a paper may change over time due to the progress and evolution of research <ref type="bibr" target="#b31">(Mei and Zhai 2008)</ref>. In such situations our summary should ideally accommodate not only the major points highlighted by the authors (abstract) but also the views offered by the scientific community (citations).</p><p>This paper presents a novel dataset and summarization method to tackle the aforementioned problems in scientific paper summarization. Our corpus, which contains the citation network of ACL Anthology papers and human-written summaries for the 1,000 most cited papers, expands the existing CL-SciSumm project <ref type="bibr" target="#b19">(Jaidka et al. 2016</ref>) and provides the largest manually-annotated dataset for scientific paper summarization. For each of the 1,000 papers (we call reference papers, or RPs), experts in CL/NLP read its abstract and incoming citation sentences to create a gold summary. This way, annotators can grasp broad, major aspects of the RP without reading the whole text, enabling faster annotation. We also conduct studies to validate that summaries created in this method are actually as comprehensive as summaries created by reading the full papers. Our dataset (1,000 papers) is significantly larger than the prior CL-SciSumm corpus (30 papers) and serves as a useful resource for supervised scientific paper summarization.</p><p>Further, we propose two novel summarization models for scientific papers that capture both the papers' content highlighted by the authors and impact perceived by the research community (hybrid summarization). In both models, given a reference paper (RP) to summarize, we take its abstract as the authors' insight, and identify a set of text spans (cited text spans) in the RP that are referred to by incoming citation sentences (i.e., community's views). The first approach then summarizes the union of the abstract and cited text spans, to integrate both components. The second approach, motivated by the fact that we already have the abstract as a clean self-summary of the paper, augments the abstract by adding salient texts extracted from the cited text spans (i.e., the community's views not covered in the abstract). For both approaches we also exploit the citation counts of the RP and its citing papers as an additional feature, to better reflect the authority of each work in the research community. To experiment with these two methods, we implement two neural network-based summarization models, which are also motivated by the architecture of <ref type="bibr" target="#b47">Yasunaga et al. (2017)</ref>'s neural multi-document summarizer.</p><p>In evaluation, we use the CL-SciSumm shared task <ref type="bibr" target="#b19">(Jaidka et al. 2016)</ref>, an established benchmark for scientific paper summarization. This benchmark dataset contains gold summaries that are created by experts who read papers and their citation sentences. First, we find that our large training corpus enables neural summarizers to boost their performance and outperform all prior participants in the shared task. This confirms the usefulness of the proposed dataset. Second, we demonstrate that the proposed hybrid summarization methods can indeed incorporate both the authors' and research community's views, thereby producing more comprehensive summaries than abstracts. In summary, our contributions are as follows.</p><p>• A large manually-annotated corpus (1,000 examples) for scientific article summarization that facilitates research on supervised approaches. • Novel scientific paper summarization methods that integrate both the authors' and research community's insights (hybrid summarization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background &amp; Motivation Text Summarization</head><p>Many existing summarization systems employ extractive methods to produce a summary, typically by ranking the salience of each sentence in a given document and then selecting sentences to be included in the summary <ref type="bibr" target="#b16">(Erkan and Radev 2004;</ref><ref type="bibr" target="#b38">Parveen, Ramsl, and Strube 2015)</ref>. Recently, in news article summarization, neural network-based approaches have proven successful <ref type="bibr" target="#b2">(Cao et al. 2015;</ref><ref type="bibr" target="#b5">Cheng and Lapata 2016;</ref><ref type="bibr" target="#b33">Nallapati, Zhai, and Zhou 2017;</ref><ref type="bibr" target="#b42">See, Liu, and Manning 2017)</ref>. This work presents neural networkbased extractive models for scientific paper summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scientific Paper Summarization</head><p>Scientific paper summarization has been studied for decades <ref type="bibr" target="#b36">(Paice 1981;</ref><ref type="bibr" target="#b15">Elkiss et al. 2008;</ref><ref type="bibr" target="#b28">Lloret, Romá-Ferri, and Palomar 2013;</ref><ref type="bibr" target="#b19">Jaidka et al. 2016;</ref><ref type="bibr" target="#b37">Parveen, Mesgar, and Strube 2016)</ref>. While early work <ref type="bibr" target="#b29">(Luhn 1958;</ref><ref type="bibr" target="#b36">Paice 1981;</ref><ref type="bibr" target="#b35">Paice and Jones 1993)</ref> focused on producing content-based summaries of target papers, the use of citations was later proposed to summarize target papers' contributions and lasting influence on the research community.</p><p>Citation-based summarization. Early work in citationbased summarization <ref type="bibr" target="#b32">(Nakov, Schwartz, and Hearst 2004;</ref><ref type="bibr" target="#b15">Elkiss et al. 2008;</ref><ref type="bibr" target="#b41">Qazvinian and Radev 2008;</ref><ref type="bibr" target="#b0">Abu-Jbara and Radev 2011)</ref> aimed to summarize the contribution of a target paper (often called reference paper, or RP, in this context) by extracting a set of sentences from the citation sentences. We call a sentence that cites the RP a citation sentence (or citing sentence). A citation sentence can be viewed as a short summary of the RP written from the citing authors' perspective. Hence, a collection of citation sentences reflects the impact of the RP on the research community <ref type="bibr" target="#b15">(Elkiss et al. 2008)</ref>. While citation sentences provide the community's views of the RP, prior work <ref type="bibr" target="#b43">(Siddharthan and Teufel 2007;</ref><ref type="bibr" target="#b31">Mei and Zhai 2008)</ref> pointed out issues in using citation sentences directly for summarization. In citing sentences, the discussion of the RP is often mixed with the content of the citing paper or with the discussion of other papers cited jointly, containing much irrelevant information.</p><p>To address such issues, recent work <ref type="bibr" target="#b31">(Mei and Zhai 2008;</ref><ref type="bibr" target="#b6">Cohan and Goharian 2015;</ref><ref type="bibr" target="#b19">Jaidka et al. 2016;</ref><ref type="bibr" target="#b26">Li et al. 2017;</ref><ref type="bibr" target="#b8">Cohan and Goharian 2017a;</ref><ref type="bibr" target="#b9">Cohan and Goharian 2017b)</ref> considers cited text spans-based summarization, where they identify a set of text spans (cited text spans; often a set of sentences) in the RP that its citing sentences refer to, and perform summarization on the identified text spans. This way, while the summary consists of words in the RP, it reflects the research community's insights. Experimental results in Mei and Zhai show that their cited text span-based  model outperforms direct summarization of citing sentences. Cited text span-based summarization is also adopted as the default approach in two recent shared tasks on scientific paper summarization: TAC 2014 and CL-SciSumm <ref type="bibr" target="#b19">(Jaidka et al. 2016)</ref>. Their datasets provide each RP and its incoming citation sentences, cited text spans, and a gold summary written by experts; the participants are asked to produce summaries using the RP and its citation sentences.</p><p>Our hybrid models. While the aforementioned citationbased summarization techniques inform us of the impact of an RP, they may overlook the authors' original message. For example, citation sentences (and consequently, cited text spans) often focus on the conclusion of the RP and may not cover other important aspects such as the motivation of the work. Moreover, in our preliminary study conducted on the CL-SciSumm shared task, we found that the quality of cited text span-based summaries produced by participants, often falls short of the abstracts in ROUGE evaluation against gold summaries. Conroy and Davis (2017) also find that the terms from abstracts in scientific documents often cover a large portion of human summaries. Our motivation in this work is therefore to integrate both the authors' original highlights (abstract) and research community's views (citations), and ultimately to improve upon the abstract.</p><p>Datasets. Previous datasets for scientific document summarization are small <ref type="bibr" target="#b45">(Teufel and Moens;</ref><ref type="bibr">TAC 2014)</ref>, with only several dozen articles. Consequently, most of the existing summarizers for scientific papers are unsupervised or tuned on small data (Abu-Jbara and Radev 2011; <ref type="bibr" target="#b6">Cohan and Goharian 2015;</ref><ref type="bibr" target="#b25">Li et al. 2016)</ref>. In fact, in the previous CL-SciSumm shared task (30 data examples), no data-driven approaches like neural networks saw great success. The new dataset we introduce here (1,000 examples) is much larger than the prior CL-SciSumm corpus, enabling data-driven approaches to scientific paper summarization.</p><p>In our experiments, we show that our dataset indeed allows neural network-based summarization models to outperform all prior participants in the shared task.</p><p>Recent work by <ref type="bibr" target="#b12">Collins, Augenstein, and Riedel (2017)</ref> and <ref type="bibr" target="#b10">Cohan et al. (2018)</ref> is related to ours in that they also introduce large-scale datasets and neural summarization models for scientific papers. Yet, while they focus on contentbased summarization with automatically created gold summaries, our work constructs manually-annotated gold summaries as well as citation information to study the research community's view on each reference paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Construction</head><p>To overcome data scarcity in scientific paper summarization, we develop and release a manually-annotated, largescale corpus for research papers in computational linguistics (CL). Our corpus contains the 1,000 most cited papers in the ACL Anthology Network (AAN) <ref type="bibr" target="#b41">(Radev et al. 2013)</ref>, their citation information, and gold summaries annotated by experts in the field. We follow the format of two prior datasets of scientific paper summarization, CL-SciSumm <ref type="bibr" target="#b19">(Jaidka et al. 2016</ref>) and TAC 2014 (biomedical domain), so that systems trained / tested on our corpus can also be applied to or evaluated on those established datasets. <ref type="figure" target="#fig_0">Figure 2</ref> depicts our data construction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Processing</head><p>We extract the 1,000 most cited papers and their citation sentences from AAN. The 1,000 papers have 21 -928 citations in the anthology. For each of the RPs, we sample and clean 20 citation sentences, which are usually sufficient to study the research community's views of the RP <ref type="bibr" target="#b31">(Mei and Zhai 2008)</ref>. Specifically, following the prior datasets, we keep the oldest and latest citations and randomly sample the rest so that the 20 citations cover an extended period of time. We then remove inappropriate citation sentences (i.e., list citations, tables, those with bugs) and clean the rest, resulting in 15 citation sentences on average for each RP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation</head><p>We aim to develop gold summaries for the 1,000 papers in CL. In the prior datasets (CL-SciSumm and TAC 2014, containing ∼30 papers), gold summaries were prepared by humans with domain expertise, in the following manner (i.e., expert summaries): given a RP to summarize, the annotators read all the text of the RP and its citation sentences to grasp its content and impact, and wrote a comprehensive summary. Yet, due to the length and technical content of scientific papers, such annotation requires a significant amount of time as well as expertise, hindering the construction of a largescale corpus for scientific article summarization.</p><p>To scale our annotation to the 1,000 papers, we develop a faster annotation procedure in this work. Five PhD students in NLP or people with equivalent expertise divide the 1,000 RPs, and read each paper's abstract and incoming citation sentences. Then, he or she identifies a few salient citation sentences that convey the RP's specific contributions not covered in the abstract and write a gold summary based on the abstract and selected citation sentences. This way, the annotators can save the time of reading the whole text of the  RP, but can still grasp broad aspects of the paper to create a comprehensive summary. To take Bergsma and Lin ( <ref type="figure">Figure  1)</ref> as an example again, while its abstract elaborates on the pronoun resolution techniques, its citation sentences reveal other major aspects of the RP such as the contribution of a noun gender dataset, which is discussed in some part of the RP but is not highlighted in the abstract. An expert summary would include both of these aspects to describe the RP. By reading the citation sentences in addition to the abstract, we can comprehend much of the content and impact of the RP without reading all its text. Lastly, the citation sentence cleaning / selection process is double-checked to prevent mistakes. Validation of our annotation procedure. Prior to annotation, we conducted preliminary studies on 30 sample papers. For each, we had the annotators list out the summaryworthy points they found 1) by reading the abstract + citing sentences, and 2) by reading the full paper; we observed that on average the former (our annotation method) covered over 90% of the major points found by reading the full papers, while just requiring 30% annotation time. This study suggests that by reading the abstract + citing sentences, annotators can create summaries comparable in quality to the ground truth in an inexpensive way. Statistics. Our annotated summaries resulted in 151 words on average (similar to the gold summaries in the CL-SciSumm corpus, 150 words). To study the inter-annotator agreement on determining salient citations, we randomly picked 40 RPs and assigned another annotator; the Kappa coefficient <ref type="bibr" target="#b11">(Cohen 1960)</ref> for inter-annotator agreement was 0.75, substantial agreement, on Landis and Koch scale. The high inter-annotator agreement further supports the efficacy of using abstract + citing sentences to create summaries. Human evaluation. We also conducted human evaluation of our gold summaries against the gold summaries in the CL-SciSumm corpus, which were created by reading full papers. We studied the 15 papers in our corpus that already exist in CL-SciSumm. For each paper, we asked 5 computer science students who took an NLP course to evaluate which gold summary (ours or CL-SciSumm's) is more comprehensive, on an integer scale -2 to 2: 2 if the former (ours) is more comprehensive; -2 if the latter; 0 if they are similar; and 1, -1 are in between. The evaluated scores were 54% zero, 22% positive, and 20% negative, with average +0.02. This result indicates that our annotated summaries are comprehensive, and comparable to or slightly better than the summaries created by reading full papers.</p><p>The dataset construction took 600+ person-hours. This large corpus can be used to train scientific paper summarization models that utilize citations, facilitating research in supervised methods. In the next sections, we introduce datadriven hybrid summarization models and experiment on the proposed corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid Summarization Models</head><p>Given a reference paper (RP) and its incoming citation sentences, our hybrid summarization models aim to reflect both the authors' and research community's voices on the RP. Specifically, we regard the abstract of the RP as the authors' original perspective, and obtain the research community's insights by identifying cited text spans in the RP (i.e., sentences in the PR that are referred to by the citation sentences). In this work, we consider the following two versions of hybrid summarization. Hybrid 1: Summarizing the combination of the abstract and cited text spans Hybrid 2: Augmenting the abstract with salient texts extracted from cited text spans The motivation of Hybrid 2 is to build upon the clean selfsummary provided by the authors and to add the community's views not covered in it. In both models, we take the union of the abstract and cited text spans as input I for summarization. Note that the input sentences in I (in particular, cited text spans) are not necessarily contiguous in the RP. This situation is analogous to multi-document summarization (MDS), which aims to produce a summary for a set of separate documents. Motivated by graph-based MDS methods <ref type="bibr" target="#b16">(Erkan and Radev 2004;</ref><ref type="bibr" target="#b47">Yasunaga et al. 2017</ref>), we build a graph capturing the relations among the input sentences in I, and apply a Graph Convolutional Network (GCN) (Kipf and Welling 2017) on top to perform summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-processing</head><p>Given a reference paper (RP) and its incoming citation sentences, we first prepare the input sentences for summarization (i.e., abstract ∪ cited text spans), and build their sentence relation graph. Cited text spans. We extract cited text spans in the RP for each incoming citation sentence, and then compile them for all the given citations. To identify cited text spans for a given citation sentence, we choose top two sentences in the RP that are most similar in terms of the tf-idf cosine similarity measure (stop words excluded).</p><p>We repeat the extraction for all the given citation sentences, and take the union to construct the complete cited text spans of the RP. The union of the abstract and cited text spans of the RP will be the input I for summarization. In our experiments I contained about 40 sentences on average. Sentence relation graph. We build a graph that takes the input sentences as nodes and captures their relationships via edges. We adopt the widely-used cosine similarity graph <ref type="bibr" target="#b16">(Erkan and Radev 2004)</ref>, where every pair of sentences has an edge with a weight equal to their tf-idf cosine similarity. Authority feature. While cited text spans provide insights by the research community, they do not necessarily reflect the authority of each citation. <ref type="bibr" target="#b31">Mei and Zhai (2008)</ref> argue that a citation made by a highly authoritative paper should be weighted more than that made by a less authoritative paper. To better reflect the authority in the research community, we consider an extra feature (authority score) for each cited text span, which is the sum of its citing papers' citation counts. Sentences in the abstract are given the citation count of the RP. We obtain citation counts from the ACL Anthology Network <ref type="bibr" target="#b41">(Radev et al. 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Architecture</head><p>Given the input sentences and their relation graph, we apply a GCN <ref type="bibr" target="#b23">(Kipf and Welling 2017)</ref> to encode the whole input text together with the graph and to estimate the salience of each sentence in the global context. Based on the salience scores, Hybrid 1 and 2 employ two greedy heuristics to select sentences to be included in the summaries. Graph convolutional network (GCN). GCNs are neural networks that operate on graphs to induce node features based on graph structure. GCNs have been shown effective not only in node classification tasks <ref type="bibr" target="#b23">(Kipf and Welling 2017)</ref>, but also in NLP applications such as syntactic treebased sentence encoding <ref type="bibr" target="#b30">(Marcheggiani and Titov 2017)</ref>.</p><p>Given a graph G with N nodes, a GCN takes •Ã ∈ R N ×N , the adjacency matrix of graph G with added self-connections. • X ∈ R N ×D , the input node features (D is the dimension of the feature vector for each node). and outputs high-level node features, Z ∈ R N ×D , which encode the graph structure. The function takes a form of layer-wise propagation. Specifically, in an L-layer GCN, the propagation from the l-th layer to the (l+1)-th layer is:</p><formula xml:id="formula_0">H (l+1) = σ D − 1 2ÃD − 1 2 H (l) W (l)<label>(1)</label></formula><p>where H (l) ∈ R N ×D denotes the l-th hidden layer, with H (0) = X, H (L) = Z. The adjacency matrixÃ is normalized via the degree matrixD. σ is an activation function such as tanh. W (l) is the learnable parameter in the l-th layer. Sentence encoding. Given the input sentences {s 1 , s 2 , . . . , s N } in I and their relation graph G, we first encode each sentence s i by applying a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) on its word embeddings, and taking the final state of the LSTM as its initial sentence embedding, x i ∈ R D−1 . The authority score of sentence s i can be appended to x i as an additional feature. The sentence embeddings x i ∈ R D (i = 1, 2, . . . , N ) are then grouped as a node feature matrix X ∈ R N ×D , and fed into a GCN with the adjacency matrixÃ of the sentence relation graph G. Through multiple layers of propagation, the GCN encodes the whole input text and induces higher-level sentence embeddings based on the structure of G. The output of the GCN, Z ∈ R N ×D , gives updated sentence embeddings s i ∈ R D that incorporate the global context. Salience estimation. For each sentence s i in our input, we estimate its salience score as follows:</p><formula xml:id="formula_1">R(s i ) = exp(v T s i ) sj ∈I exp(v T s j ) (2)</formula><p>where s i is the updated embedding of sentence s i , and v is a learnable parameter for projecting embeddings to be scalar scores. Note that the salience scores are normalized via softmax to be a probability distribution over all the input sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>The model parameters include the weights in the LSTM and GCN, and v. The model is trained to minimize the crossentropy loss between the target salience scores (true labels) R and the estimated salience scoresR of the input sentences:</p><formula xml:id="formula_2">L = − si∈I R(s i ) log(R(s i ))<label>(3)</label></formula><p>To construct the target scores R, we first take the average of ROUGE-1 &amp; 2 scores for each sentence s i evaluated with the gold summary <ref type="bibr" target="#b2">(Cao et al. 2015)</ref>, and then rescale the scores as a probability distribution over all the input sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary Generation</head><p>Based on the salience scores estimated for the input sentences, Hybrid 1 and 2 employ two greedy heuristics to select sentences for the summaries. Hybrid 1 (extractive summarization of abstract ∪ cited text spans). First, we sort all sentences in I in descending order of the salience score. We dequeue one sentence from the list and append it to the current summary if the sentence is of a reasonable length (more than 8 words, as in <ref type="bibr" target="#b16">(Erkan and Radev 2004)</ref>) and is non-redundant. A sentence is redundant if it is similar to any sentence already in the summary, with tf-idf cosine similarity above 0.5 <ref type="bibr" target="#b18">(Hong and Nenkova 2014)</ref>. We keep adding sentences to the summary in this way until we reach the length limit. Finally, sentences in the summary are sorted in the original order in the RP. Hybrid 2 (augmentation of abstract with salient cited text spans). We take all the cited text spans from I and sort them in descending order of the salience scores. Starting from the full abstract as the initial summary, we deque one sentence from the list of cited text spans and add to the current summary if it is of a reasonable length and is nonredundant. We repeat until the length limit, and finally sort the summary sentences in the original order in the RP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We experiment the hybrid summarization models on our training corpus to study the efficacy of the proposed dataset and models. We aim to show that our large-scale corpus allows the data-driven neural models to outperform prior work. We also analyze the outputs of hybrid summarization and illustrate their advantage over abstracts and traditional citation-based summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets &amp; Evaluation</head><p>We train the GCN summarization models on our proposed corpus with 1,000 examples of RPs, citation sentences, and gold summaries. All models are validated and tested on established benchmarks, CL-SciSumm 2016 dev / test, where the gold summaries were created by experts reading full papers. In training, we exclude the few RPs in our corpus that also appear in the validation or test set.</p><p>We evaluate system summaries against the gold summaries by ROUGE <ref type="bibr" target="#b27">(Lin 2004)</ref>, which serves as a good metric for this work, as we aim to measure the comprehensiveness of summaries. To ensure comparability with the CL-SciSumm shared task, we measure ROUGE-2 Recall, F1 (2-R, 2-F) and -SU4 F1 (SU4-F), with the same configurations: -n 4 -2 -4 -u -m -s -f A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Design</head><p>We conduct the following two experiments to study the proposed 1) corpus and 2) hybrid methods.</p><p>Exp 1. First, we study the usefulness of our dataset for data-driven models, by comparing the model performance after training on our corpus and after training on the existing CL-SciSumm corpus. For data-driven systems, we experiment with our GCN model. As the participants in the CL-SciSumm shared task adopted cited text span-based summarization, to ensure a fair comparison, we also let these models just summarize cited text spans. Specifically, we just select cited text spans, given the predicted salience scores (we call this GCN Cited text spans). We follow the same protocol as the shared task (no authority feature; summary length 250 words).</p><p>Exp 2. Next, we study the efficacy of the hybrid summarization models. As our goal is to learn to produce the gold summaries (average length 150 words) and compare them with abstracts 2 or traditional citation-based summaries, we experiment with the GCN Hybrid models with summary length 150 words (with/without authority feature), and analyze the output hybrid summaries against those baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We use 100-dimensional word embeddings for the input to the LSTM sentence encoder. The word embeddings are initialized with GloVe <ref type="bibr" target="#b40">(Pennington, Socher, and Manning 2014)</ref>. We set the dimension of the LSTM / GCN hidden states to be 200, 201 (i.e., D = 201), and use two hidden layers for the GCN (i.e., L = 2). We apply dropout <ref type="bibr" target="#b44">(Srivastava et al. 2014)</ref> to the input word embeddings as well as the outputs of the LSTM and GCN, with dropout rate 0.5.</p><p>The model parameters and word embeddings are trained by the Adam optimizer <ref type="bibr" target="#b22">(Kingma and Ba 2015)</ref>, with batch size 5, learning rate 0.001, and a gradient clipping of 2.0 <ref type="bibr" target="#b39">(Pascanu, Mikolov, and Bengio 2012)</ref>. We employ early stopping <ref type="bibr" target="#b4">(Caruana, Lawrence, and Giles 2001)</ref> based on the validation loss to prevent overfitting.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results &amp; Discussion</head><p>Exp 1. <ref type="table" target="#tab_3">Table 1</ref> shows the result of Exp 1, along with the top two participants in the CL-SciSumm shared task <ref type="bibr">(Li et al.; Conroy and Davis)</ref>. The upper part shows the model performance after training on our proposed corpus (1000 examples), and the lower part the existing CL-SciSumm corpus (30 examples). We find that the neural model, GCN Cited text spans, performs on par with the participants when trained on CL-SciSumm, but when trained on our corpus, it gains significant boosts in all the ROUGE metrics (e.g., +5 in ROUGE-3-F) and greatly outperform all the models trained on CL-SciSumm. With orders of magnitude more training examples than prior datasets, our corpus actually enables the data-driven neural network-based models to perform well on scientific paper summarization. This result suggests both the usefulness of the proposed corpus for training, and the feasibility of neural models in summarization given sufficient data.</p><p>Exp 2. <ref type="table" target="#tab_4">Table 2</ref> shows the result of Exp 2, along with the baselines (Abstract and GCN Cited text spans). We observe that both of the hybrid models perform clearly better than pure cited text span summaries. Moreover, Hybrid 1 surpasses abstracts in Recall, and Hybrid 2 outperforms abstracts in all ROUGE metrics, including the F1 of R-2 and - </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cited Text Spans Only:</head><p>This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1. Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a supersense for the unknown nouns. These problems demonstrate the need for automatic or semi-automatic methods for the creation and maintenance of lexical-semantic resources. The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible. Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second <ref type="bibr">(Curran and Clark, 2003)</ref>, trained on the entire Penn Treebank <ref type="bibr">(Marcus et al., 1994)</ref>. Widdows <ref type="formula" target="#formula_2">(2003)</ref> uses a similar technique to insert words into the WORDNET hierarchy. Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.</p><p>(150 words limit) (105 words) (150 words limit, in decreasing order of salience)</p><p>Red is from cited text spans; providing the technical details that are most influential to the community. Green is the authors' original motivation.</p><p>Red and orange provide technical details influential to the community (red is more salient). <ref type="figure">Figure 4</ref>: Comparison of our hybrid summary with the abstract and pure cited text spans summary, for paper P05-1004 in the CL-SciSumm 2016 test set. Our hybrid summary covers both the authors' original motivations (green) and the technical details influential to the research community (red).</p><p>3, which have the highest correlation with human judgments <ref type="bibr" target="#b7">(Cohan and Goharian 2016)</ref>. Hybrid 2 performs better than Hybrid 1, most likely because Hybrid 2 builds on existing summaries (abstracts) and can ensure higher quality. In this experiment, Hybrid 2 added two sentences on average to the original abstract.</p><p>To qualitatively study the advantage of the hybrid summarization, we also compare and analyze the output summaries. As an example, <ref type="figure">Figure 4</ref> shows the output summary of Hybrid 2 together with the abstract and pure cited text span summary for paper P05-1004 in the CL-SciSumm 2016 test set. The hybrid summary, which augments the abstract by taking in the most salient cited text spans, includes the technical contributions that are most influential to the community but are not covered in the abstract (red). The cited text span-based summary, on the other hand, provides more technical details, but lacks some of the author's original messages such as the motivation and objective of their work (green). Thus, the hybrid summary is indeed more comprehensive than the abstract and cited text span summary because it incorporates both the authors' original insights and the community's views on the paper.</p><p>Human evaluation. The above evaluation and analysis show the advantage of the hybrid models over the baselines. Here we conduct human evaluation of the hybrid summaries against gold summaries to study their utility. We asked 5 computer science students who took an NLP course to evaluate the coverage and coherence of the output summaries by our hybrid model, in a scale 1-5 (5 is the level of gold summaries). The model achieved 4.5 and 4.2 on average for these two metrics. While there is room for improving coherence, these scores suggest that the model can generate comprehensive and readable summaries.</p><p>Finally, we observe in <ref type="table" target="#tab_4">Table 2</ref> that all our models obtain moderate improvements by introducing the authority feature to reflect the authority of each citation made by the research community, suggesting the usefulness of this feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a novel dataset and hybrid models for scientific paper summarization. Our corpus, which contains 1,000 examples of papers, citation information and human summaries, is orders of magnitude larger than prior datasets and facilitates future research in supervised scientific paper summarization. We also presented hybrid summarization methods that integrate both authors' and community's insights, to overcome the limitations of abstracts (may not convey actual impacts) and traditional citation-based summaries (may overlook authors' original messages). Our experiments demonstrated that 1) the proposed dataset is indeed effective in training data-driven neural models, and that 2) the hybrid models produce more comprehensive summaries than abstracts and traditional citation-based summaries. We hope that our large annotated corpus and hybrid methods would open up new avenues for scientific paper summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the dataset construction process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of our summarization models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>: higher than all models trained on the CL-SciSumm corpus.</figDesc><table><row><cell>Summarizer</cell><cell>2-R</cell><cell>2-F</cell><cell>3-F</cell><cell>SU4-F</cell></row><row><cell>Trained on Our Corpus (size: 1000)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN Hybrid 2 (Ours)</cell><cell cols="4">*41.69* *29.30* *24.65* *18.56*</cell></row><row><cell>GCN Hybrid 1 (Ours)</cell><cell cols="4">*36.47* *26.31* *21.33* *16.18*</cell></row><row><cell>GCN Cited text spans (Ours)</cell><cell cols="4">*33.03* *23.49* *17.86* *14.15*</cell></row><row><cell>Trained on CL-SciSumm (size: 30)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN Cited text spans (Ours)</cell><cell cols="4">24.93 18.46 12.77 12.21</cell></row><row><cell>Best participant 1</cell><cell cols="4">32.36 21.94 16.79 13.63</cell></row><row><cell>Best participant 2</cell><cell cols="4">26.67 18.85 12.83 12.45</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="5">: Results of Exp 1, showing ROUGE evaluations on</cell></row><row><cell cols="5">the CL-SciSumm Test benchmark. Models trained on our</cell></row><row><cell cols="5">corpus outperform all the models trained on the existing CL-</cell></row><row><cell>SciSumm Train set.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Summarizer</cell><cell>2-R</cell><cell>2-F</cell><cell>3-F</cell><cell>SU4-F</cell></row><row><cell>Abstract</cell><cell cols="4">29.52 29.40 23.16 23.34</cell></row><row><cell>GCN Hybrid 2 w/ auth</cell><cell cols="4">33.88 31.54 24.32 24.36</cell></row><row><cell>GCN Hybrid 2</cell><cell cols="4">32.44 30.08 23.43 23.77</cell></row><row><cell>GCN Hybrid 1 w/ auth</cell><cell cols="4">29.65 28.05 21.83 20.22</cell></row><row><cell>GCN Hybrid 1</cell><cell cols="4">29.64 27.96 21.81 19.41</cell></row><row><cell cols="5">GCN Cited text spans w/ auth 26.30 24.39 18.85 17.31</cell></row><row><cell>GCN Cited text spans</cell><cell cols="4">25.16 24.26 18.79 17.67</cell></row><row><cell>w/ auth: using authority feature.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of Exp 2, showing ROUGE evaluations on the CL-SciSumm Test benchmark. All models are trained on our corpus. The hybrid models outperform abstracts and pure citation summaries.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Supersense Tagging of Unknown Nouns using Semantic Similarity. The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger. We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.</figDesc><table><row><cell></cell><cell>Abstract:</cell></row><row><cell></cell><cell>Supersense Tagging of Unknown Nouns using</cell></row><row><cell></cell><cell>Semantic Similarity. The limited coverage of</cell></row><row><cell></cell><cell>lexical-semantic resources is a significant</cell></row><row><cell></cell><cell>problem for NLP systems which can be alleviated</cell></row><row><cell></cell><cell>by automatically classifying the unknown words.</cell></row><row><cell></cell><cell>Supersense tagging assigns unknown nouns one</cell></row><row><cell></cell><cell>of 26 broad semantic categories used by</cell></row><row><cell></cell><cell>lexicographers to organise their manual insertion</cell></row><row><cell></cell><cell>into WORDNET. Ciaramita and Johnson (2003)</cell></row><row><cell></cell><cell>present a tagger which uses synonym set glosses</cell></row><row><cell></cell><cell>as annotated training examples. We describe an</cell></row><row><cell></cell><cell>unsupervised approach, based on vector-space</cell></row><row><cell>This approach</cell><cell>similarity, which does not require annotated</cell></row><row><cell>significantly outperforms the multi-class perceptron on the same</cell><cell>examples but significantly outperforms their</cell></row><row><cell>dataset based on WORDNET 1.6 and 1.7.1. Our approach uses</cell><cell>tagger. We also demonstrate the use of an</cell></row><row><cell>voting across the known supersenses of automatically extracted</cell><cell>extremely large shallow-parsed corpus for</cell></row><row><cell>synonyms, to select a supersense for the unknown nouns.</cell><cell>calculating vector-space semantic similarity.</cell></row></table><note>Hybrid:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The average length of abstracts is 110 words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Kokil Jaidka, Muthu Kumar Chandrasekaran, Min-Yen Kan, Yavuz Nuzumlali, Arman Cohan, as well as all the anonymous reviewers for their helpful feedback. We also thank everyone who helped the evaluation in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coherent citationbased summarization of scientific papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abu-Jbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bootstrapping path-based pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ranking with recursive neural networks and its application to multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving multidocument summarization via text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scientific article summarization using citation-context and article&apos;s discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting summarization evaluation for scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextualizing citations for scientific summarization using word embeddings and domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Scientific document summarization via citation contextualization and scientific discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJDL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A supervised approach to extractive summarisation of scientific papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vector space and language models for scientific document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Section mixture models for scientific document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Blind men and elephants: What do citation summaries tell us about a research article</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elkiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>States</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JASIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the estimation of word importance for news multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the cl-scisumm 2016 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIRNDL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The cl-scisumm shared task 2017: Results and key insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIRNDL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cl-scisumm shared task 2018: Results and key insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIRNDL @SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cist system for cl-scisumm 2016 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIRNDL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Computational linguistics literature and citations oriented citation linkage, classification and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJDL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Compendium: A text summarization system for generating abstracts of research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Romá-Ferri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The automatic creation of literature abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of research and development</title>
		<imprint>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generating impact-based summaries for scientific literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<idno>ACL-08: HLT</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Citances: Citation sentences for semantic analysis of bioscience text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The identification of important concepts in highly structured technical papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Paice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The automatic generation of literature abstracts: An approach based on the identification of selfindicating phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Paice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating coherent summaries of scientific articles using coherence patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Topical coherence for graph-based extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Ramsl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<title level="m">On the difficulty of training recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scientific paper summarization using citation summary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R. ; D R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abu-Jbara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACL anthology network corpus. Language Resources and Evaluation</title>
		<editor>COLING. Radev,</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Whose idea was this, and why does it matter? attributing scientific work to citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Summarizing scientific articles: Experiments with relevance and rhetorical status</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="445" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic generation of story highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph-based neural multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
