<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Pretraining for Dense Video Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Huang</surname></persName>
							<email>gabriel.huang@umontreal.cabopang</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila &amp; University of Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Rivera</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
							<email>rsoricut@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Pretraining for Dense Video Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning specific hands-on skills such as cooking, car maintenance, and home repairs increasingly happens via instructional videos. The user experience with such videos is known to be improved by meta-information such as time-stamped annotations for the main steps involved. Generating such annotations automatically is challenging, and we describe here two relevant contributions. First, we construct and release a new dense video captioning dataset, Video Timeline Tags (ViTT), featuring a variety of instructional videos together with time-stamped annotations. Second, we explore several multimodal sequenceto-sequence pretraining strategies that leverage large unsupervised datasets of videos and caption-like texts. We pretrain and subsequently finetune dense video captioning models using both YouCook2 and ViTT. We show that such models generalize well and are robust over a wide variety of instructional videos.</p><p>2016. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>YouTube recently reported that a billion hours of videos were being watched on the platform every day <ref type="bibr" target="#b22">(YouTubeBlog, 2017)</ref>. In addition, the amount of time people spent watching online videos was estimated to grow at an average rate of 32% a year between 2013 and 2018, with an average person forecasted to watch 100 minutes of online videos per day in 2021 <ref type="bibr" target="#b23">(ZenithMedia, 2019)</ref>.</p><p>An important reason for this fast-growing video consumption is information-seeking. For instance, people turn to YouTube "hungry for how-to and learning content" <ref type="bibr" target="#b2">(O'Neil-Hart, 2018)</ref>. Indeed, compared to traditional content format such as text, video carries richer information to satisfy such * This work was done while Gabriel Huang was an intern at Google Research.  needs. But as a content media, videos are also inherently more difficult to skim through, making it harder to quickly target the relevant part(s) of a video. Recognizing this difficulty, search engines started showing links to "key moments" within videos in search results, based on timestamps and short descriptions provided by the content creators themselves. 1 This enables users to get a quick sense of what the video covers, and also to jump to a particular time in the video if so desired. This effort echoes prior work in the literature showing how users of instructional videos can benefit from human-curated meta-data, such as a timeline pointing to the successive steps of a tutorial <ref type="bibr">(Kim et al., 2014;</ref><ref type="bibr">Margulieux et al., 2012;</ref><ref type="bibr" target="#b20">Weir et al., 2015)</ref>. Producing such meta-data in an automatic way would greatly scale up the efforts of providing easier information access to videos. This task is closely related to the dense video captioning task considered in prior work <ref type="bibr">(Zhou et al., 2018a,c;</ref><ref type="bibr">Krishna et al., 2017)</ref>, where an instructional video is first segmented into its main steps, followed by segment-level caption generation.</p><p>To date, the YouCook2 data set <ref type="bibr" target="#b26">(Zhou et al., 2018a)</ref> is the largest annotated data set for dense video captioning. It contains annotations for 2,000 cooking videos covering 89 recipes, with per-recipe training / validation split. Restricting to a small number of recipes is helpful for early exploratory work, but such restrictions impose barriers to model generalization and adoption that are hard to overcome. We directly address this problem by constructing a larger and broader-coverage annotated dataset that covers a wide range of instructional topics <ref type="bibr">(cooking, repairs, maintenance, etc.)</ref> We make the results of our annotation efforts publicly available as Video Timeline Tags (ViTT) 2 , consisting of around 8,000 videos annotated with timelines (on average 7.1 segments per video, each segment with a short free-text description).</p><p>Using YouCook2 and the new ViTT dataset as benchmarks for testing model performance and generalization, we further focus on the subproblem of video-segment-level caption generation, assuming segment boundaries are given <ref type="bibr">(Hessel et al., 2019;</ref><ref type="bibr" target="#b13">Sun et al., 2019b;</ref><ref type="bibr">Luo et al., 2020)</ref>. Motivated by the high cost of collecting human annotations, we investigate pretraining a video segment captioning model using unsupervised signals -ASR (Automatic Speech Recognition) tokens and visual features from instructional videos, and unpaired instruction steps extracted from independent sources: Recipe1M <ref type="bibr">(Marin et al., 2019)</ref> and <ref type="bibr">Wik-iHow (Koupaee and Wang, 2018)</ref>. In contrast to prior work that focused on BERT-style pretraining of encoder networks <ref type="bibr">(Sun et al., 2019b,a)</ref>, our approach entails jointly pretraining both multimodal encoder and text-based decoder models via MASSstyle pretraining <ref type="bibr" target="#b10">(Song et al., 2019)</ref>. Our experiments show that pretraining with either text-only or multi-modal data provides significant gains over no pretraining, on both the established YouCook2 benchmark and the new ViTT benchmark. The results we obtain establish state-of-the-art performance on YouCook2, and present strong performance numbers on the ViTT benchmark. These findings help us conclude that the resulting models generalize well and are quite robust over a wide variety of instructional videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Text-only Pretraining. Language pretraining models based on the Transformer neural net-2 Available at https://github. com/google-research-datasets/ Video-Timeline-Tags-ViTT work architecture <ref type="bibr">(Vaswani et al., 2017a)</ref> such as <ref type="bibr">BERT (Devlin et al., 2018)</ref>, GPT <ref type="bibr" target="#b5">(Radford et al., 2018)</ref>, <ref type="bibr">RoBERTa (Liu et al., 2019)</ref>, MASS <ref type="bibr" target="#b10">(Song et al., 2019)</ref> and <ref type="bibr">ALBERT (Lan et al., 2020)</ref> have achieved state-of-the-art results on many NLP tasks. MASS <ref type="bibr" target="#b10">(Song et al., 2019)</ref> has been recently proposed as a joint encoder-decoder pretraining strategy. For sequence-to-sequence tasks, this strategy is shown to outperform approaches that separately pretrain the encoder (using a BERT-style objective) and the decoder (using a language modeling objective). UniLM <ref type="bibr">(Dong et al., 2019)</ref>, <ref type="bibr">BART (Lewis et al., 2019)</ref>, and T5 <ref type="bibr" target="#b6">(Raffel et al., 2019)</ref> propose unified pretraining approaches for both understanding and generation tasks.</p><p>Multimodal Pretraining. VideoBERT <ref type="bibr" target="#b13">(Sun et al., 2019b)</ref>, CBT <ref type="bibr" target="#b12">(Sun et al., 2019a)</ref> and ActBERT <ref type="bibr" target="#b30">(Zhu and Yang, 2020</ref>) use a BERT-style objective to train both video and ASR text encoders. <ref type="bibr">Alayrac et al. (2016)</ref> and <ref type="bibr" target="#b0">Miech et al. (2020)</ref> use margin-based loss functions to learn joint representations for video and ASR, and evaluate them on downstream tasks such as video captioning, action segmentation and anticipation, and action localization. An independent and concurrent work (UniViLM) by <ref type="bibr">Luo et al. (2020)</ref> is closely related to ours in that we share some similar pretraining objectives, some of the pretraining setup -HowTo100M <ref type="bibr">(Alayrac et al., 2016)</ref>, and the down-stream video captioning benchmark using YouCook2 <ref type="bibr" target="#b26">(Zhou et al., 2018a)</ref>. The main difference is that they use BERT-style pretraining for encoder and language-modeling style pretraining for decoder, whereas we use MASS-style pre-training to pretrain encoder and decoder jointly.</p><p>Other approaches such as <ref type="bibr">ViLBERT (Lu et al., 2019)</ref>, LXMERT <ref type="bibr">Bansal, 2019), Unicoder-VL (Li et al., 2019)</ref>, VL-BERT <ref type="bibr" target="#b11">(Su et al., 2019)</ref>, and UNITER (Chen et al., 2019) focus on pretraining joint representations for text and image, evaluating them on downstream tasks such as visual question answering, image-text retrieval and referring expressions.</p><p>Dense Video Captioning. In this paper, we focus on generating captions at the segment-level, which is a sub-task of the so-called dense video captioning task <ref type="bibr">(Krishna et al., 2017)</ref>, where finegrained captions are generated for video segments, conditioned on an input video with pre-defined event segments. This is different from the video captioning models that generate a single summary for the entire video <ref type="bibr" target="#b19">(Wang et al., 2019)</ref>. Hessel et al. (2019) make use of ASR and video for segment-level captioning on YouCook2 and show that most of the performance comes from ASR. <ref type="bibr" target="#b9">Shi et al. (2019);</ref><ref type="bibr">Luo et al. (2020)</ref> train their dense video captioning models on both video frames and ASR text and demonstrate the benefits of adding ASR as an input to the model. There are also a number of video captioning approaches that do not use ASR directly <ref type="bibr" target="#b29">(Zhou et al., 2018c;</ref><ref type="bibr" target="#b3">Pan et al., 2020;</ref><ref type="bibr" target="#b25">Zheng et al., 2020;</ref><ref type="bibr" target="#b24">Zhang et al., 2020;</ref><ref type="bibr">Lei et al., 2020)</ref>.</p><p>Instructional video captioning data sets. In addition to YouCook2 <ref type="bibr" target="#b26">(Zhou et al., 2018a)</ref>, there are two other smaller data sets in the instructional video captioning category. Epic <ref type="bibr">Kitchen (Damen et al., 2018)</ref> features 55 hours of video consisting of 11.5M frames, which were densely labeled for a total of 39.6K action segments and 454.3K object bounding boxes. How2 <ref type="bibr" target="#b7">(Sanabria et al., 2018)</ref> consists of instructional videos with video-level (as opposed to segment-level) descriptions, authored by the video creators themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>We present the datasets used for pretraining, finetuning, and evaluation in <ref type="table">Table 1</ref>. We also describe in detail the newly introduced dense video captioning dataset, Video Timeline Tags (ViTT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dense Video-Captioning Datasets</head><p>Our goal is to generate captions (CAP) for video segments. We consider two datasets with segment-level captions for fine-tuning and evaluating ASR+Video→CAP models.</p><p>YouCook2. Up to this point, YouCook2 <ref type="bibr" target="#b26">(Zhou et al., 2018a)</ref> has been the largest human-annotated dense-captioning dataset of instructional videos publicly available. It originally contained 2,000 cooking videos from YouTube. Starting from 110 recipe types (e.g., "shrimp tempura"), 25 unique videos per recipe type were collected; the recipe types that did not gather enough videos were dropped, resulting in a total of 89 recipe types in the final dataset. In addition, <ref type="bibr" target="#b27">Zhou et al. (2018b)</ref> "randomly split the videos belonging to each recipe into 67%:23%:10% as training, validation and test sets 3 ," which effectively guarantees that videos in the validation and test sets are never about unseen recipes. Annotators were then asked to construct recipe steps for each video -that is, identify the start and end times for each step, and provide a recipe-like description of each step. Overall, they reported an average of 7.7 segments per video, and 8.8 words per description. After removing videos that had been deleted by users, we obtained a total of 11,549 segments.</p><p>ViTT. One limitation of the YouCook2 dataset is the artificially imposed (almost) uniform distribution of videos over 89 recipes. While this may help making the task more tractable, it is difficult to judge whether performance on its validation / test sets can be generalized to unseen topics.</p><p>The design of our ViTT dataset annotation process is aimed at fixing some of these drawbacks. We started by collecting a large dataset of videos containing a broader variety of topics to better reflect topic distribution in the wild. Specifically, we randomly sampled instructional videos from the YouTube-8M dataset <ref type="bibr">(Abu-El-Haija et al., 2016)</ref>, a large-scale collection of YouTube videos that also contain topical labels. Since much of prior work in this area revolved around cooking videos, we aimed at sampling a significant proportion of our data from videos with cooking labels (specifically, "Cooking" and "Recipe"). Aside from the intentional bias regarding cooking videos, the rest of the videos were selected by randomly sampling non-cooking videos, including only those that were considered to be instructional videos by our human annotators.</p><p>Once candidate videos were identified, timeline annotations and descriptive tags were collected.</p><p>Our motivation was to enable downstream applications to allow navigating to specific content sections. Therefore, annotators were asked to identify the main steps in a video and mark their start time. They were also asked to produce a descriptive-yetconcise, free-text tag for each step (e.g., "shaping the cookies", "removing any leftover glass"). A subset of the videos has received more than one complete annotation (main steps plus tags).</p><p>The resulting ViTT dataset consists of a total of 8,169 videos, of which 3,381 are cooking-related. A total of 5,840 videos have received only one annotation, and have been designated as the training split. Videos with more than one annotation have been designated as validation / test data. Overall, there are 7.1 segments per video on average (max: 19). Given the dataset design, descriptions are much shorter in length compared to YouCook2: on average there are 2.97 words per tag (max: 16) -20% of the captions are single-word, 22% are two-words, and 25% are three words. Note that the average caption length is significantly shorter than for YouCook2, which is not surprising given our motivation of providing short and concise timeline tags for video navigation. We standardized the paraphrases among the top-20 most frequent captions. For instance, {"intro", "introduction"} → "intro". Otherwise, we have preserved the original tags asis, even though additional paraphrasing most definitely exists. Annotators were instructed to start and end the video with an opening and closing segment as possible. As a result, the most frequent tag (post-standardization) in the dataset is "intro", which accounts for roughly 11% of the 88,455 segments. More details on the data collection process and additional analysis can be found in the Supplementary Material (Section A.1).</p><p>Overall, this results in 56,027 unique tags, with a vocabulary size of 12,509 token types over 88,455 segments. In this paper, we consider two variants: the full dataset (ViTT-All), and the cooking subset (ViTT-Cooking).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pretraining Datasets: ASR+Video</head><p>We consider two large-scale unannotated video datasets for pretraining, as described below. Timestamped ASR tokens were obtained via YouTube Data API, 4 and split into ASR segments if the timestamps of two consecutive words are more than 2 seconds apart, or if a segment is longer than a pre-specified max length (in our case, 320 words). They were paired with concurrent video frames in the same segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YT8M-cook</head><p>We extract the cooking subset of YouTube-8M (Abu-El-Haija et al., 2016) by taking, from its training split, videos with "Cooking" or "Recipe" labels, and retain those with English ASR, subject to YouTube policies. After preprocessing, we obtain 186K ASR+video segments with an average length of 64 words (24 seconds) per segment.</p><p>HowTo100M. This is based on the 1.2M YouTube instructional videos released by <ref type="bibr" target="#b1">Miech et al. (2019)</ref>, covering a broad range of topics. After preprocessing, we obtain 7.99M ASR+video segments with an average of 78 words (28.7 seconds) per segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pretraining Datasets: CAP-style</head><p>We also consider two text-only datasets for pretraining, containing unpaired instruction steps similar in style to the target captions.</p><p>Recipe1M is a collection of 1M recipes scraped from a number of popular cooking websites <ref type="bibr">(Marin et al., 2019)</ref>. We use the sequence of instructions extracted for each recipe in this dataset, and treat each recipe step as a separate example during pretraining. This results in 10,767,594 CAP-style segments, with 12.8 words per segment.</p><p>WikiHow is a collection of 230,843 articles extracted from the WikiHow knowledge base (Koupaee and <ref type="bibr">Wang, 2018)</ref>. Each article comes with a title starting with "How to". Each associated step starts with a step summary (in bold) followed by a detailed explanation. We extract the all step summaries, resulting in 1,360,145 CAP-style segments, with 8.2 words per segment. Again, each instruction step is considered as a separate example during pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Differences between Pretraining and Finetuning Datasets</head><p>First, note that video segments are defined differently for pretraining and finetuning datasets, and may not match exactly. For ASR+Video pretraining datasets, which are unsupervised, the segments are divided following a simple heuristic (e.g., two consecutive words more than 2 seconds apart), whereas for finetuning ASR+Video→CAP datasets, which are supervised, the segments are defined by human annotators to correspond to instruction steps. Otherwise, the ASR data are relatively similar between pretraining and finetuning datasets, as both come from instructional videos and are in the style of spoken language. Second, compared to the target captions in finetuning datasets, the CAP-like pretraining datasets are similar in spirit -they all represent summaries of steps, but they may differ in length, style and granularity. In particular, the CAP-like pretraining datasets are closer in style to captions in YouCook2, where annotators were instructed to produce a recipe-like description for each step. This is reflected in their similar average length (YouCook2: 8.8 words, Recipe1M: 12.8 words, WikiHow: 8.2 words); whereas captions in ViTT are significantly shorter (2.97 words on average).</p><p>Despite these differences -some are inevitable due to the unsupervised nature of pretraining datasets -the pretraining data is very helpful for our task as shown in the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>To model segment-level caption generation, we adopt MASS-style pretraining <ref type="bibr" target="#b10">(Song et al., 2019)</ref> with Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017b)</ref> as the backbone architecture. For both pre-training and fine-tuning objectives, we have considered two variants: text-only and multi-modal. They are summarized in <ref type="table" target="#tab_4">Table 2</ref> and more details are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Separate-Modality Architecture</head><p>Both ASR tokens and video segment features are given as input in the multimodal variants. We consider an architecture with a separate transformer for each modality (text or video), see <ref type="figure" target="#fig_3">Figure 2</ref> for details. When available, the text and video encoders attend to each other at every layer using cross-modal attention, as in <ref type="bibr">ViLBERT (Lu et al., 2019)</ref>. The text decoder attends over the final-layer output of both encoders. We discuss in more detail the differences between using a separate-modality architecture vs. a vanilla-Transformer approach for all modalities in Appendix A.2.</p><p>The inputs to the text encoder is the sum of three components: text token embeddings, positional embeddings and the corresponding style embeddings, 5 depending on the style of the text (ASR or Caption-like). The inputs to the video encoder could be either precomputed frame-level 2D CNN features or 3D CNN features, pretrained on the Kinetics <ref type="bibr">(Carreira and Zisserman, 2017;</ref><ref type="bibr">Kay et al., 2017)</ref> data set. The visual features are projected with fully-connected layers to the same dimension as the text embeddings.</p><p>The main architecture we consider is a 2-layer encoder (E2), 6-layer decoder (D6) Transformer. We use E2D6 to refer to the text-only version, and E2vidD6 to refer to the multimodal version with an active video encoder. We also experiment with E2D2 and E2vidD2 (2-layer decoder). 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretraining with Text-only MASS</head><p>Text-only pretraining is essentially the unsupervised learning of the style transfer between ASRstyle and caption-style texts using unpaired data sources: ASR strings from video segments in YT8M-cook or HowTo100M; and CAP-style instruction steps found in Recipe1M or HowTo100M. Just like using MASS for unsupervised machine translation involves pretraining the model on unpaired monolingual datasets, we alternate between ASR→ASR and CAP→CAP MASS steps during our pretraining stage, which does not require the "source" (ASR) and "target" (CAP-style) data to be aligned.</p><p>In an ASR→ASR step, we mask a random subsequence of the ASR and feed the masked ASR to the text encoder. The text decoder must reconstruct the hidden subsequence while attending to the encoder output. A CAP→CAP step works similarly by trying to reconstruct a masked sequence of a CAP-style text. The encoder and decoder are trained jointly using teacher-forcing on the decoder. We denote this text-only strategy as MASS in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pretraining with Multimodal MASS</head><p>During multimodal pretraining, we alternate between text-only CAP→CAP MASS steps and multimodal MASS steps. During each multimodal MASS step ASR+video→ASR, we feed a masked ASR to the text-encoder and the co-occurring video features to the video-encoder. The text decoder must reconstruct the masked ASR subsequence. We denote this pretraining strategy as MASSvid in the experiments. This trains cross-modal attention between the text-encoder and video-encoder    at every layer, jointly with the text decoder that attends to the output layer of both the text and video encoders. 7 To force more cross-modal attention between encoder and decoder, we also investigate a strategy of hiding the text-encoder output from the decoder for some fraction of training examples. We refer to this strategy as MASSdrop in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Pretraining with Alignment and Ordering Tasks</head><p>We also explore encoder-only multimodal pretraining strategies. We take the last-layer representation for the CLS (beginning of sentence) token from the encoder, and add a multi-layer perceptron on top of it for binary predictions <ref type="figure" target="#fig_3">(Figure 2)</ref>. Given a pair of ASR and video segment, we train the encoder to predict the following objectives:</p><p>• Segment-Level Alignment. An (ASR, video) pair is aligned if they occur in the same pretraining segment; negative examples are constructed by sampling pairs from the same video but at least 2 segments away. <ref type="bibr">7</ref> In preliminary experiments, we had attempted to directly adapt the MASS objective <ref type="bibr" target="#b10">(Song et al., 2019)</ref> to video reconstruction -by masking a subsequence of the input video and making the video decoder reconstruct the input using the Noise Constrastive Estimator Loss <ref type="bibr" target="#b12">(Sun et al., 2019a</ref>). Due to limited success, we did not further pursue this approach.</p><p>• Segment-Level Ordering. We sample (ASR, video) pairs that are at least 2 segments away, and train the model to predict whether the ASR occurs before or after the video clip.</p><p>During this MASSalign pretraining stage, we alternate between two text-only MASS steps (CAP→CAP, ASR→ASR) and the two binary predictions (Alignment and Ordering) described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Finetuning on Video Captioning</head><p>For text-only finetuning, we feed ASR to the text encoder and the decoder has to predict the corresponding CAP (ASR→CAP). For multimodal finetuning, we also feed additional video representations to the video encoder (ASR+video→CAP). When finetuning a multimodal model from textonly pretraining, everything related to video (weights in the video encoder and any cross-modal attention modules) will be initialized randomly. In addition to these uni-directional (UniD) finetuning, we also experiment with several variants of bidirectional (BiD) finetuning <ref type="table" target="#tab_4">(Table 2)</ref>. For instance, adding CAP→ASR (predicting ASR from CAP) to text-only finetuning. In the experiments, we find some variants of bidirectional finetuning beneficial whether training from scratch or finetuning from a pretrained model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>We tokenize ASR and CAP inputs using byte-pairencoding subwords <ref type="bibr" target="#b8">(Sennrich et al., 2015)</ref>, and truncate them to 240 subwords. We truncate video sequences to 40 frames (40 seconds of video), compute the 128-dim features proposed by <ref type="bibr" target="#b18">Wang et al. (2014)</ref> (which we will refer to as Compact 2D features), and project them to the embedding space using a two-layer perceptron with layer normalization and GeLU activations. We instantiate the E2xDx models defined in Section 4.1 with 128-dimensional embeddings and 8 heads respectively for self-attention, encoderdecoder, and cross-modal attention modules. We define each epoch to be 3,125 iterations, where each iteration contains one repetition of each training step as represented in <ref type="table" target="#tab_4">Table 2</ref>. We pretrain for 200 epochs and finetune for 30 epochs.</p><p>For evaluation, we consider BLEU-4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE-L (Lin and Och, 2004) and CIDEr <ref type="bibr" target="#b17">(Vedantam et al., 2015)</ref> metrics.</p><p>Please refer to Appendix A.3 for full implementation details, hyperparameters and computation cost.</p><p>Notes on ViTT evaluation: With the exception of ROUGE-L, all other metrics are sensitive to short groundtruth. 67% of the groundtruth tags in ViTT have less than 4 words, where a perfect prediction will not yield a full score in, say, BLEU-4. Thus, we focus mainly on ROUGE-L, report BLEU-1 instead of BLEU-4 for ViTT, and provide the other two metrics only as reference points.</p><p>We had originally decided to use videos with multiple annotations as validation and test data, so that we could explore evaluation with multiple reference groundtruth captions. But as annotators do not always yield the same set of segment boundaries, this became tricky. Instead, we simply treat each segment as a separate instance with one single reference caption. Note that all segments annotated for the same video will be in either validation or test to ensure no content overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>We run several variants of our method on YouCook2, ViTT-All and ViTT-Cooking, using different architectures, modalities, pretraining datasets, pretraining and finetuning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing with other methods on YouCook2</head><p>For YouCook2, we report our method alongside several methods from the literature <ref type="bibr">(Hessel et al., 2019;</ref><ref type="bibr" target="#b13">Sun et al., 2019b;</ref><ref type="bibr" target="#b29">Zhou et al., 2018c;</ref><ref type="bibr">Lei et al., 2020)</ref>, as well as state-of-the-art concurrent work <ref type="bibr">(Luo et al., 2020)</ref>. The related work is provided for reference and to give a ballpark estimate of the relative performance of each method, but results are not always strictly and directly comparable. Beyond the usual sources of discrepancy in data processing, tokenization, or even different splits, an additional source of complication comes from the fact that videos are regularly deleted by content creators, causing video datasets to shrink over time. Additionally, when comparing to other work incorporating pretraining, we could differ in (videos available in) pretraining datasets, segmentation strategies, etc. To this end, we perform an extensive ablation study, which at least helps us to understand the effectiveness of different components in our approach.</p><p>Effect of pretraining The main experimental results for the three datasets we consider are summarized in  improvement also holds in ViTT-Cooking (+4.22 in ROUGE-L) and ViTT-All (+2.97 in ROUGE-L). We do not observe consistent and significant trends among the different multimodal pretraining strategies: MASS pretraining with video (MASSvid), with video and droptext (MASSdrop), or with alignment tasks (MASSalign). 8 Furthermore, we observe that most of the pretraining improvement is achievable via text-only MASS pretraining. Across all three datasets, while Multimodal Pretraining (E2vidD6-MASSvid-BiD) is consistently better than Text Pretraining (E2vidD6-MASS-BiD), the differences are quite small (under one ROUGE-L point).</p><p>It's worthy noting that for MASSalign, the best validation accuracies for the pretraining tasks are reasonably high: for YT8M-cook, we observed 90% accuracy for the alignment task, and 80% for the ordering task (for HowTo100M: 87% and 71.4%, respectively), where random guess would yield 50%. This suggests that our video features are reasonably strong, and the findings above are not due to weak visual representations. <ref type="bibr">8</ref> Limited improvement with MASSalign suggests that such alignment tasks are better suited for retrieval <ref type="bibr">(Luo et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of other modeling choices</head><p>We experiment with 2-layer decoder (D2) vs 6-layer decoder (D6), combined with either unidirectional finetuning (UniD) or bidirectional fine-tuning (BiD). <ref type="table" target="#tab_9">Table 5</ref> shows ablation results of the four possible combinations when finetuning a multimodal model using text-only pretraining on YouCook2 (a more complete list of results can be found in Appendix A.5, showing similar trends). The D6xBiD combination tends to yield the best performance, with the differences among the four configurations being relatively small (under one ROUGE-L point). For visual features, we also explored using 3D features <ref type="bibr" target="#b21">(Xie et al., 2018)</ref> instead of 2D features during finetuning (with no pretraining or text-only pretraining), and do not find much difference in model performance on YouCook2. As a result, we use the simpler 2D features in our multimodal pretraining. We leave more extensive experiments with visual features as future work.</p><p>Generalization implications An important motivation for constructing the ViTT dataset and evaluating our models on it has been related to generalization. Since the YouCook2 benchmark is restricted to a small number of cooking recipes, there is little to be understood about how well models Method Input</p><p>ViTT-All ViTT-Cooking   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Motivated to improve information-seeking capabilities for videos, we have collected and annotated a new dense video captioning dataset, ViTT, which is larger with higher diversity compared to YouCook2. We investigated several multimodal pretraining strategies for segment-level video captioning, and conducted extensive ablation studies. We concluded that MASS-style pretraining is the most decisive factor in improving the performance on all the benchmarks used. Even more to the point, our results indicate that most of the performance can be attributed to leveraging the ASR signal. We achieve new state-of-the-art results on the YouCook2 benchmark, and establish strong performance baselines for the new ViTT benchmark, which can be used as starting points for driving more progress in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Supplementary Material for "Multimodal Pretraining for Dense Video Captioning".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The ViTT dataset</head><p>Sampling video for annotation. The goal of the ViTT dataset design is to mirror topic distribution in the "wild". Therefore, instead of starting from specific how-to instructions and searching for corresponding videos, we sampled videos from the validation set of the YouTube-8M dataset <ref type="bibr">(Abu-El-Haija et al., 2016)</ref>, a large-scale collection of YouTube videos with topical labels, subject to YouTube policies. Exclusion criteria were lack of English ASR and the topic label "Game". The latter was motivated by the fact that in this type of videos, the visual information predominantly features video games, while the ViTT dataset was intended to contain only videos with real-world human actions. Cooking videos can be easily identified by sampling videos that came with "Cooking" or "Recipe" topic labels. Given the convenience and the fact that much of prior work in this area had focused on cooking videos, approximately half of the dataset was designed to include cooking videos only, while the remaining videos would be randomly sampled non-cooking videos, as long as they were verified as instructional by human annotators.</p><p>Annotation process Annotators were presented with a video alongside its timestamped, automatic transcription shown in sentence-length paragraphs. They were asked to watch the video and first judge whether the video was instructional. For the purpose of our dataset, we determine that a video is instructional if it focuses on real-world human actions that are accompanied by procedural language explaining what is happening on screen, in reasonable details. Also for our purposes, instructional videos need to be grounded in real life, with a real person in the video exemplifying the action being verbally described.</p><p>For videos judged to be instructional, annotators were then asked to:</p><p>• Delimit the main segments of the video.</p><p>• Determine their start time if different from the automatically suggested start time (explained below).</p><p>• Provide a label summarizing or explaining the segment.</p><p>Annotation guidelines Annotators were instructed to identify video segments with two potential purposes:</p><p>• Allow viewers to jump straight to the start of a segment for rewatch.</p><p>• Present viewers with an index to decide whether to watch the video in full or directly skip to the segment of interest.</p><p>Our guidelines suggested a range of five to ten segments as long as the the structure and content of the video permitted. For short videos, the direction was to prioritize quality over quantity and to only define those segments that formed the narrative structure of the video, even if the resulting number of segments was below 5.</p><p>To help annotators determine segment start times, transcriptions were shown in "sentences" -we expected that sentence start times might be good candidates for segment start times. We obtained sentence boundaries automatically as follows. Given the stream of timestamped ASR tokens for a video, we first separated them into blocks by breaking two consecutive tokens whenever they were more than 2 seconds apart. We then used a punctuation prediction model to identify sentence boundaries in each resulting block. Each sentence was shown with the timestamp corresponding to its first token. Annotators were advised that transcriptions had been automatically divided into paragraphs that may or may not correspond to a video segment -if they decided that a segment started from a particular sentence, they could choose to use the start time of the sentence as the start time for the segment, or, if needed, they could put in an adjusted start time instead.</p><p>Once the start time had been identified, annotators were asked to provide a free-text label to summarize each segment. We instructed the annotators to use nouns or present participles (-ing form of verbs) to write the labels for the video segments, whenever possible. Additionally, we asked that the labels be succinct while descriptive, using as few words as possible to convey as much information as possible.</p><p>Data statistics and post-processing The resulting dataset consists of 8,169 instructional videos that received segment-level annotations, of which 3,381 are cooking-related. Overall there are an average of 7.1 segments per video (max: 19). Given our instructions, the descriptions are much shorter in lengths compared to a typical captioning dataset: on average there are 2.97 words per description (max: 16); 20% of the captions are single-word, 22% are two-words, and 25% are three words. We refer to these descriptions as "tags" given how short they are.</p><p>When possible, annotators were also asked to start and end the video with an opening and closing segment. As a result, most annotations start with an introduction segment: this accounts for roughly 11% of the 88455 segments in the dataset ("intro": 8%, "introduction": 2.3%). Note that while "intro" and "introduction" are clearly paraphrases of each other, an automatic metric will penalize a model predicting "intro" when the groundtruth is "introduction". Similarly, the ending segment was described in several varieties: "outro": 3.4%, "closing": 1%, "closure", "conclusion", "ending", "'end of video": each under 1%. Penalizing paraphrases of the ground truth is an inherent weakness of automatic metrics. To mitigate this, we decided to reduce the chance of this happening for the most frequent tags in the dataset. That is, in our experiments, we identified three groups of tags among the top-20 most frequent tags, and standardized them as follows.</p><p>intro intro, introduction, opening outro outro, closing, closure, conclusion, ending, end of video, video closing result finished result, final result, results Note that this does not mean we can solve this problem as a classification task like in visual question answering (VQA): overall, there are 56,027 unique tags with a vocabulary size of 12,509 for the 88,455 segments; 51,474 tags appeared only once in the dataset, making it infeasible to reduce the segment-level captioning problem into a pure classification task. <ref type="table" target="#tab_12">Table 7</ref> shows the top 10 most frequent tags after standardization.</p><p>Estimate of human performance. A subset of the candidate videos were given to three annotators 9 , to help us understand variations in human annotations. 5,840 videos received dense captioning 9 A small set were unintentionally given to six annotators.  from exactly one annotator and were used as training data. Videos with more than one annotation were used as validation / test data. Note that not all the videos with multiple timeline annotations have exactly three sets of them -in fact, 1368 videos received 3-way segment-level annotations. This is because not all annotators agreed on whether a video was instructional. Computing annotator agreement for the annotated timelines is non-trivial.</p><p>Here we focus on an estimate of tagging agreement when a pair of annotators agreed over the segment start time. Specifically, we go through each video that received multiple segment-level annotations.</p><p>For each segment where two annotators chose the same ASR sentence as its starting point, we take the tags they produced for this segment and consider one of them as groundtruth, the other as prediction, and add that into our pool of (groundtruth, prediction) pairs. We can then compute standard automatic evaluations metrics over this pool. The results are as follows.</p><p>BLEU-1 METEOR ROUGE-L CIDEr 43.34 33.56 41.88 1.26  Note that METEOR, and CIDEr scores are both penalized by the lack of n-grams for higher n. That is, when both groundtruth and prediction are singleword, say, "intro", this pair will not receive a full score from any of these metrics. But the ROUGE-L score is in the same ballpark as estimate of human performance in prior work <ref type="bibr">(Hessel et al., 2019)</ref>. One might note that perhaps this pool of label pairs contains a higher share of "intro", since annotators might be more likely to agree over where an opening segment starts. Indeed, 20% of the time, one of the tags is "intro". Interestingly, in spite of standardization of top tags, 14% of the time one tag is "intro", the other tag is not "intro": they can be less frequent paraphrases (e.g., "welcoming", "greeting", "opening and welcoming") or something semantically different (e.g., "using dremel tool").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Separated vs. Concatenated-Modality Architecture</head><p>Prior work has explored both concatenating different modalities and feeding them into the same multimodal Transformer encoder <ref type="bibr" target="#b13">(Sun et al., 2019b;</ref><ref type="bibr">Hessel et al., 2019)</ref>, as well as separating them into unimodal transformers <ref type="bibr" target="#b12">(Sun et al., 2019a;</ref>. We opt for the separated architecture because it offers more flexibility. First, the concatenated architecture requires embedding the text and video features into the same space. When the video features are projected using a simple network, there is no guarantee that we can meaningfully project them into the text embedding space.</p><p>VideoBERT <ref type="bibr" target="#b13">(Sun et al., 2019b)</ref> gives more flexibility to the video embeddings by quantizing video features and learning an embedding for each codeword. However, the quantization step has subsequently been claimed to be detrimental <ref type="bibr" target="#b12">(Sun et al., 2019a)</ref>. Moreover, the concatenated architecture uses the same sets of forward and attention weights to process text and video, and performs layer normalization jointly between the two modalities, which is not necessarily meaningful. Finally, the separated architecture makes it easy to switch between variable length text-only, video-only, or text+video modalities, whereas concatenated architectures might rely on separating tokens, modalities embeddings, and using fixed sequence lengths <ref type="bibr">(Luo et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Implementation Details</head><p>We optimize all models on a nVidia v100 GPU using the Adam optimizer with inverse square root schedule, batch size 32, warm-up period of 4,000 iterations, and maximum learning rate of 0.0001, following MASS <ref type="bibr" target="#b10">(Song et al., 2019)</ref>. The positional embeddings are initialized randomly. We use dropout and attention dropout with probabilities 0.1. With E2vidD6, pretraining takes 3-6 days depending on the objective and bidirectional finetuning takes up to 1.5 days, however those times could be improved by optimizing the data pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Example Predictions</head><p>We show examples of good and bad predictions on YouCook2 ( <ref type="figure">Figure 5</ref> and ViTT-All <ref type="figure">(Figure 4 and 5)</ref>. The captions are generated by  and E2vidD6-MASS-BiD (text-only MASS pretraining).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Full result tables</head><p>We present here tables with all the ablation results that we run. There are two main takeaway messages from the results involving the pretraining approach: (a) the accuracy improvements, as measured across all the metrics we use, indicate the value of using a pretraining approach to this problem, specifically one that is capable of leveraging the ASR signals at both pretraining and finetuning stages, and (b) the training speedup achieved from pretraining is impressive, as a pretrained model converges much faster than training from scratch. This is especially visible on ViTT-All where finetuning after MASS pretraining reaches best ROUGE-L score at epoch 2, whereas it takes around 11 epochs to converge when training from scratch.  <ref type="figure">Figure 3</ref>: Example good and bad predictions on YouCook2. The pretrained model is generally but not always better. Note that there are no "intro" or "outro"-like labels on YouCook2 because the dataset was specifically curated to only contain actual recipe steps.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Groundtruth</head><label></label><figDesc>Varying stiching speeds Ø-Pretraining Showing other parts MASS-Pretraining Explaining how to do a stitch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Dense video captioning using ViTT-trained models. For the given video scene, we show the ViTT annotation (Groundtruth) and model outputs (no pretraining and MASS-based pretraining).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>A diagram for the separate-modality architecture. It consists of a two-stream (text and video inputs) encoder with cross-modal attention and a text-only decoder, jointly trained using the MASS objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>CLS output text output text output text output text output video output video output video output video output</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Pretraining Objectives</cell><cell></cell><cell cols="3">Fine-tuning Objectives</cell></row><row><cell>Segment Alignment 0/1</cell><cell>Segment Ordering 0/1</cell><cell>MASS for ASR "butter on"</cell><cell>MASS for Cap "spread butter"</cell><cell>Reverse Captioning (cap to asr) "after spread@@ ing the"</cell><cell cols="3">Captioning (asr to cap) "spread butter"</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Decoder (Text-only)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder-Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multimodal Attention</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Text Decoder Layer 2</cell></row><row><cell></cell><cell></cell><cell cols="2">Cross-Modal</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Text Decoder Layer 1</cell></row><row><cell></cell><cell>or</cell><cell cols="2">Encoder (Multimodal)</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>text</cell><cell>text</cell><cell>text</cell><cell>text</cell><cell>text</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLS</cell><cell>after</cell><cell>spread@</cell><cell>ing</cell><cell>the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Text Embedding Layer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[CLS]</cell><cell cols="3">after spread@@ ing</cell><cell>the</cell></row><row><cell></cell><cell>or</cell><cell>(Masked) Cap</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>"spread butter"</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Decoder Input (teacher forcing)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Pretraining and Fine-tuning objectives. For</cell></row><row><cell>each strategy, indicates whether the text (T) and</cell></row><row><cell>video (V) encoders are active, followed by a summary</cell></row><row><cell>of training objectives involved in one training step.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(YouCook2) and Table 4 (ViTT-All</cell></row><row><cell>and ViTT-Cooking). Across all three datasets, the</cell></row><row><cell>best performance is achieved by finetuning a mul-</cell></row><row><cell>timodal captioning model under the Multimodal</cell></row><row><cell>Pretraining condition. For instance, on YouCook2,</cell></row><row><cell>E2vidD6-MASSvid-BiD improves over the no-</cell></row><row><cell>pretraining model E2vidD6-BiD by 4.37 ROUGE-L,</cell></row><row><cell>a larger improvement than UniViLM with pretrain-</cell></row><row><cell>ing (#5) vs without (#2) (Luo et al., 2020). This</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Segment-level captioning results on YouCook2. We use YT8M-cook and Recipe1M for pretraining. The numbers for the related work (first group) are directly reported from the corresponding papers. The last line is an estimate of human performance as reported by Hessel et al. (2019), and can be taken as a rough upper bound of the best performance achievable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Segment-level captioning results on ViTT. For ViTT-All we pretrain on HowTo100M and WikiHow; for</cell></row><row><cell cols="4">ViTT-Cooking we pretrain on YT8M-cook and Recipe1M. We report baseline scores for predicting the most com-</cell></row><row><cell cols="4">mon caption "intro". We also estimate the human performance as a rough upper bound (details in Supplementary</cell></row><row><cell cols="2">Material A.1; Table 9).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">BLEU-4 METEOR ROUGE-L CIDEr</cell></row><row><cell>D2-UniD</cell><cell>10.84</cell><cell>17.39</cell><cell>38.24 1.16</cell></row><row><cell>D6-UniD</cell><cell>11.39</cell><cell>18.00</cell><cell>38.71 1.22</cell></row><row><cell>D2-BiD</cell><cell>11.38</cell><cell>18.04</cell><cell>38.67 1.19</cell></row><row><cell>D6-BiD</cell><cell>11.47</cell><cell>17.70</cell><cell>38.80 1.25</cell></row><row><cell>D6-BiDalt</cell><cell>11.07</cell><cell>17.68</cell><cell>38.43 1.22</cell></row><row><cell>D6-BiD (S3D)</cell><cell>11.64</cell><cell>18.04</cell><cell>38.75 1.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Ablation study on YouCook2. We finetune</cell></row><row><cell>a multimodal captioning model (E2vid) with either</cell></row><row><cell>2-layer decoder (D2) or 6-layer decoder (D6) using</cell></row><row><cell>YT8M-cook /Recipe1M for MASS pretraining, com-</cell></row><row><cell>bined with either unidirectional (UniD) or bidirectional</cell></row><row><cell>(BiD) finetuning. We find no significant difference be-</cell></row><row><cell>tween using 2D and 3D features (marked as S3D).</cell></row><row><cell>trained and evaluated on it generalize. In contrast,</cell></row><row><cell>the ViTT benchmark has a much wider coverage</cell></row><row><cell>(for both cooking-related videos and general in-</cell></row><row><cell>structional videos), and no imposed topic overlap</cell></row><row><cell>between train/dev/test. As such, there are two find-</cell></row><row><cell>ings here that are relevant with respect to general-</cell></row><row><cell>ization: (a) the absolute performance of the models</cell></row><row><cell>on the ViTT benchmark is quite high (ROUGE-L</cell></row><row><cell>scores above 0.30 are usually indicative of decent</cell></row><row><cell>performance), and (b) the performance on ViTT</cell></row><row><cell>vs. YouCook2 is clearly lower (31.5 ROUGE-L</cell></row><row><cell>vs. 39.0 ROUGE-L, reflecting the increased diffi-</cell></row><row><cell>culty of the new benchmark), but it is maximized</cell></row><row><cell>under similar pretraining and finetuning conditions,</cell></row><row><cell>which allows us to claim that the resulting models</cell></row><row><cell>generalize well and are quite robust over a wide</cell></row><row><cell>variety of instructional videos.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Standardization of top tags</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>10 most frequent tags after standardization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">: Estimate of human performance for the</cell></row><row><cell cols="4">segment-level captioning on ViTT-All (computed over</cell></row><row><cell>7528 pairs).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">BLEU-1 METEOR ROUGE-L CIDEr</cell></row><row><cell>41.61</cell><cell>32.50</cell><cell>41.59</cell><cell>1.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Estimate of human performance for the</cell></row><row><cell>segment-level captioning on ViTT-Cooking (computed</cell></row><row><cell>over 2511 pairs).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Example good predictions on ViTT-All (Part 1). The pretrained model is generally but not always better. Example ok and bad predictions on ViTT (Part 2). The pretrained model is generally but not always better.</figDesc><table><row><cell>Sample Frame</cell><cell>Ground Truth</cell><cell>Ø-Pretraining</cell><cell>MASS-Pretraining</cell><cell>Comments</cell></row><row><cell></cell><cell>tightening extra loop</cell><cell>tightening the loop</cell><cell>tightening the loop</cell><cell>both models perform</cell></row><row><cell></cell><cell></cell><cell>(good)</cell><cell>(good)</cell><cell>well</cell></row><row><cell cols="5">adding eyeshadow showcasing the finished look rolling and folding the clay highlighting brow bone covering the chicken and cooking connecting spray hose and sprayer implementing second layer making decorative trim checking bleach con-tainer demonstrating the flip tilting board Ground Truth securing the bar in place removing the cover blending eye shadow (good) showing finished look(good) rolling and blending (ok) applying eye shadow (ok) cooking the bread (bad) cooking the chicken applying eye shadow (good) showing finished look(good) rolling and folding the clay (good) brushing on the brows(good) (good) connecting the new cover (ok) connecting the valve (good) showing finished prod-uct (ok) showing second layer (good) cutting the edges (good) cutting the fabric (good) both models yield good both models perform well both models perform well MASS is a bit more spe-cific MASS is a bit more spe-cific only MASS got the right ingredient spray hose is more spe-cific than valve MASS is more specific predictions outro (bad) checking the container (good) MASS is a bit more spe-cific checking the battery (bad) flipping the board (good) Ø model got influenced by car mechanics tutori-als setting up the oven (bad) turning the board (good) Ø overfitted on cooking videos Ø-Pretraining MASS-Pretraining Comments (bad) checking for the other side (bad) predictions are not spe-cific enough starting with unlocking bars opening the box (bad) pulling the car on (bad) predictions are incorrect or not specific enough demonstrating tech-nique attaching paper (bad) stamping paper (good) the technique is about stamping the paper spritzing in additional water pouring water into the water (ok) adding water to water (ok) understandable but un-grammarly checking for leaks checking for the new new new new new new new new new new new new new new new (bad) checking the process (ok) Ø got into a loop, MASS not specific enough displaying materials needed intro (bad) removing paste (ok) prediction makes sense because narrator is dis-playing thermal paste re-mover sketching on the swirls drawing the lines (good) drawing on the eyes (bad) pretrained model overfit-ted on makeup tutorials crimping wire and com-pleting project attaching the screws (bad) attaching the wire to the wire (ok) both models have trou-ble with the concept of crimping a wire cutting with guide line cutting the top of the top of the top of the top of the top of the top (bad) explaining process (ok) Ø model got into a loop, MASS model is not spe-cific enough Input Pretraining BLEU-4 METEOR ROUGE-L CIDEr Constant Pred (Hessel et al., 2019) Figure 4: Sample Frame --2.70 10.30 21.70 0.15 MART (Lei et al., 2020) Video -8.00 15.90 -0.36 DPC (Shi et al., 2019) Video + ASR -2.76 18.08 --EMT (Zhou et al., 2018c) Video -4.38 11.55 27.44 0.38 CBT (Sun et al., 2019a) Video Kinetics + HowTo100M 5.12 12.97 30.44 0.64 AT (Hessel et al., 2019) ASR -8.55 16.93 35.54 1.06 AT+Video (Hessel et al., 2019) Video + ASR -9.01 17.77 36.65 1.12 UniViLM #1 (Luo et al., 2020) Video -6.06 12.47 31.48 0.64 UniViLM #2 (Luo et al., 2020) Video + ASR -8.67 15.38 35.02 1.00 UniViLM #5 (Luo et al., 2020) Video + ASR HowTo100M 10.42 16.93 38.02 1.20 Ø Pretraining E2D2-UniD ASR -7.42 15.15 33.26 0.85 E2D6-UniD ASR -7.88 15.29 34.10 0.87 E2D2-BiD ASR -6.85 15.64 34.26 0.91 E2D6-BiD ASR -7.90 15.70 34.86 0.93 E2vidD2-UniD Video + ASR -7.47 15.11 34.77 0.90 E2vidD6-UniD Video + ASR -7.61 15.57 34.28 0.89 E2vidD2-BiD Video + ASR -8.39 15.36 34.54 0.91 E2vidD6-BiD Video + ASR -8.01 16.19 34.66 0.91 E2vidD2-BiDalt Video + ASR -8.12 15.83 34.83 0.93 E2vid,D6-BiDalt Video + ASR -7.70 16.11 34.78 0.91 E2vidD2-BiD (S3D) Video + ASR -8.04 16.17 36.01 0.96 E2vidD6-BiD (S3D) Video + ASR -7.91 16.28 35.23 0.93 Text Pretraining E2D2-MASS-UniD ASR YT8M-cook + Recipe1M 10.52 17.14 37.39 1.14 E2D6-MASS-UniD ASR YT8M-cook + Recipe1M 10.72 17.74 37.85 1.17 E2D2-MASS-BiD ASR YT8M-cook + Recipe1M 10.84 17.44 37.20 1.13 E2D6-MASS-BiD ASR YT8M-cook + Recipe1M 10.60 17.42 38.08 1.20 E2vidD2-MASS-UniD Video + ASR YT8M-cook + Recipe1M 10.84 17.39 38.24 1.16 E2vidD6-MASS-UniD Video + ASR YT8M-cook + Recipe1M 11.39 18.00 38.71 1.22 E2vidD2-MASS-BiD Video + ASR YT8M-cook + Recipe1M 11.38 18.04 38.67 1.19 E2vidD6-MASS-BiD Video + ASR YT8M-cook + Recipe1M 11.47 17.70 38.80 1.25 E2vid,D2-MASS-BiDalt Video + ASR YT8M-cook + Recipe1M 11.49 17.85 38.60 1.18 E2vid,D6-MASS-BiDalt Video + ASR YT8M-cook + Recipe1M 11.07 17.68 38.43 1.22 E2vidD2-MASS-BiD (S3D) Video + ASR YT8M-cook + Recipe1M 11.13 17.71 38.57 1.12 E2vidD6-MASS-BiD (S3D) Video + ASR YT8M-cook + Recipe1M 11.64 18.04 38.75 1.24 Multimodal Pretraining E2vidD2-MASSalign-BiD Video + ASR YT8M-cook + Recipe1M 11.54 17.57 37.70 1.15 E2vidD6-MASSalign-BiD Video + ASR YT8M-cook + Recipe1M 11.53 17.62 39.03 1.22 E2vidD2-MASSvid-BiD Video + ASR YT8M-cook + Recipe1M 11.17 17.71 38.32 1.17 E2vidD6-MASSvid-BiD Video + ASR YT8M-cook + Recipe1M 12.04 18.32 39.03 1.23 E2vidD2-MASSdrop-BiD Video + ASR YT8M-cook + Recipe1M 11.21 17.99 38.72 1.23 E2vidD6-MASSdrop-BiD Video + ASR YT8M-cook + Recipe1M 10.45 17.74 38.82 1.22 Figure 5: Method Human (Hessel et al., 2019) Video + ASR -15.20 25.90 45.10 3.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Video Captioning Results on YouCook2. We use YT8M-cook/Recipe1M for pretraining. All video features are Compact 2D<ref type="bibr" target="#b18">(Wang et al., 2014)</ref> except when marked as S3D<ref type="bibr" target="#b21">(Xie et al., 2018)</ref>.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>Pretraining</cell><cell cols="4">BLEU-1 METEOR ROUGE-L CIDEr</cell></row><row><cell>Constant baseline ("intro")</cell><cell>-</cell><cell>-</cell><cell>1.42</cell><cell>3.32</cell><cell>11.15</cell><cell>0.28</cell></row><row><cell>Ø Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E2D2-UniD</cell><cell>ASR</cell><cell>-</cell><cell>17.94</cell><cell>8.55</cell><cell>27.06</cell><cell>0.64</cell></row><row><cell>E2D6-UniD</cell><cell>ASR</cell><cell>-</cell><cell>18.91</cell><cell>8.96</cell><cell>27.80</cell><cell>0.67</cell></row><row><cell>E2D2-BiD</cell><cell>ASR</cell><cell>-</cell><cell>18.81</cell><cell>8.82</cell><cell>27.63</cell><cell>0.65</cell></row><row><cell>E2D6-BiD</cell><cell>ASR</cell><cell>-</cell><cell>19.60</cell><cell>9.12</cell><cell>27.88</cell><cell>0.68</cell></row><row><cell>E2vidD2-UniD</cell><cell>Video + ASR</cell><cell>-</cell><cell>18.94</cell><cell>8.99</cell><cell>28.05</cell><cell>0.67</cell></row><row><cell>E2vidD6-UniD</cell><cell>Video + ASR</cell><cell>-</cell><cell>19.29</cell><cell>9.15</cell><cell>27.97</cell><cell>0.69</cell></row><row><cell>E2vidD2-BiD</cell><cell>Video + ASR</cell><cell>-</cell><cell>19.37</cell><cell>9.21</cell><cell>28.56</cell><cell>0.69</cell></row><row><cell>E2vidD6-BiD</cell><cell>Video + ASR</cell><cell>-</cell><cell>19.49</cell><cell>9.23</cell><cell>28.53</cell><cell>0.69</cell></row><row><cell>Text Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E2D2-MASS-UniD</cell><cell>ASR</cell><cell>HowTo100M + WikiHow</cell><cell>21.53</cell><cell>10.24</cell><cell>29.95</cell><cell>0.77</cell></row><row><cell>E2D6-MASS-UniD</cell><cell>ASR</cell><cell>HowTo100M + WikiHow</cell><cell>22.09</cell><cell>10.58</cell><cell>30.67</cell><cell>0.79</cell></row><row><cell>E2D2-MASS-BiD</cell><cell>ASR</cell><cell>HowTo100M + WikiHow</cell><cell>20.73</cell><cell>10.20</cell><cell>30.15</cell><cell>0.76</cell></row><row><cell>E2D6-MASS-BiD</cell><cell>ASR</cell><cell>HowTo100M + WikiHow</cell><cell>21.93</cell><cell>10.60</cell><cell>30.45</cell><cell>0.79</cell></row><row><cell>E2vidD2-MASS-UniD</cell><cell cols="2">Video + ASR HowTo100M + WikiHow</cell><cell>21.46</cell><cell>10.45</cell><cell>30.56</cell><cell>0.78</cell></row><row><cell>E2vidD6-UniD</cell><cell cols="2">Video + ASR HowTo100M + WikiHow</cell><cell>22.21</cell><cell>10.75</cell><cell>30.86</cell><cell>0.81</cell></row><row><cell>E2vidD2-MASS-BiD</cell><cell cols="2">Video + ASR HowTo100M + WikiHow</cell><cell>21.78</cell><cell>10.64</cell><cell>30.72</cell><cell>0.79</cell></row><row><cell>E2vidD6-MASS-BiD</cell><cell cols="2">Video + ASR HowTo100M + WikiHow</cell><cell>22.44</cell><cell>10.83</cell><cell>31.27</cell><cell>0.81</cell></row><row><cell>Multimodal Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">E2vidD2-MASSalign-BiD Video + ASR HowTo100M + WikiHow</cell><cell>22.07</cell><cell>10.33</cell><cell>30.60</cell><cell>0.77</cell></row><row><cell cols="3">E2vidD6-MASSalign-BiD Video + ASR HowTo100M + WikiHow</cell><cell>22.31</cell><cell>10.66</cell><cell>31.13</cell><cell>0.79</cell></row><row><cell>E2vidD2-MASSvid-BiD</cell><cell cols="2">Video + ASR HowTo100M + WikiHow</cell><cell>22.15</cell><cell>10.75</cell><cell>31.06</cell><cell>0.80</cell></row><row><cell>E2vidD6-MASSvid-BiD</cell><cell cols="2">Video + ASR HowTo100M + WikiHow</cell><cell>22.45</cell><cell>10.76</cell><cell>31.49</cell><cell>0.80</cell></row><row><cell cols="3">E2vidD2-MASSdrop-BiD Video + ASR HowTo100M + WikiHow</cell><cell>21.84</cell><cell>10.55</cell><cell>31.10</cell><cell>0.79</cell></row><row><cell cols="3">E2vidD6-MASSdrop-BiD Video + ASR HowTo100M + WikiHow</cell><cell>22.37</cell><cell>11.00</cell><cell>31.40</cell><cell>0.82</cell></row><row><cell>Human estimate</cell><cell>Video + ASR</cell><cell>-</cell><cell>43.34</cell><cell>33.56</cell><cell>41.88</cell><cell>1.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Video captioning results on ViTT-All. We use HowTo100M/WikiHow for pretraining. We also estimate human performance (details in Appendix A.1;Table 9).</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>Pretraining</cell><cell cols="4">BLEU-1 METEOR ROUGE-L CIDEr</cell></row><row><cell>Constant baseline ("intro")</cell><cell>-</cell><cell>-</cell><cell>1.16</cell><cell>2.93</cell><cell>10.21</cell><cell>0.25</cell></row><row><cell>Ø Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E2D2-UniD</cell><cell>ASR</cell><cell>-</cell><cell>19.73</cell><cell>9.43</cell><cell>27.95</cell><cell>0.69</cell></row><row><cell>E2D6-UniD</cell><cell>ASR</cell><cell>-</cell><cell>20.24</cell><cell>9.93</cell><cell>28.59</cell><cell>0.71</cell></row><row><cell>E2D2-BiD</cell><cell>ASR</cell><cell>-</cell><cell>19.73</cell><cell>9.72</cell><cell>27.92</cell><cell>0.68</cell></row><row><cell>E2D6-BiD</cell><cell>ASR</cell><cell>-</cell><cell>20.77</cell><cell>10.08</cell><cell>28.63</cell><cell>0.72</cell></row><row><cell>E2vidD2-UniD</cell><cell>Video + ASR</cell><cell>-</cell><cell>19.97</cell><cell>9.75</cell><cell>28.30</cell><cell>0.69</cell></row><row><cell>E2vidD6-UniD</cell><cell>Video + ASR</cell><cell>-</cell><cell>20.46</cell><cell>9.93</cell><cell>28.62</cell><cell>0.69</cell></row><row><cell>E2vidD2-BiD</cell><cell>Video + ASR</cell><cell>-</cell><cell>20.60</cell><cell>10.08</cell><cell>29.45</cell><cell>0.71</cell></row><row><cell>E2vidD6-BiD</cell><cell>Video + ASR</cell><cell>-</cell><cell>20.45</cell><cell>9.88</cell><cell>28.88</cell><cell>0.69</cell></row><row><cell>Text Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E2D2-MASS-UniD</cell><cell>ASR</cell><cell>YT8M-cook + Recipe1M</cell><cell>22.89</cell><cell>11.53</cell><cell>31.62</cell><cell>0.84</cell></row><row><cell>E2D6-MASS-UniD</cell><cell>ASR</cell><cell>YT8M-cook + Recipe1M</cell><cell>24.47</cell><cell>12.22</cell><cell>32.51</cell><cell>0.90</cell></row><row><cell>E2D2-MASS-BiD</cell><cell>ASR</cell><cell>YT8M-cook + Recipe1M</cell><cell>22.75</cell><cell>11.63</cell><cell>31.54</cell><cell>0.84</cell></row><row><cell>E2D6-MASS-BiD</cell><cell>ASR</cell><cell>YT8M-cook + Recipe1M</cell><cell>24.79</cell><cell>12.25</cell><cell>32.40</cell><cell>0.88</cell></row><row><cell>E2vidD2-MASS-UniD</cell><cell cols="2">Video + ASR YT8M-cook + Recipe1M</cell><cell>23.86</cell><cell>11.85</cell><cell>32.32</cell><cell>0.86</cell></row><row><cell>E2vidD6-MASS-UniD</cell><cell cols="2">Video + ASR YT8M-cook + Recipe1M</cell><cell>24.32</cell><cell>12.32</cell><cell>32.90</cell><cell>0.90</cell></row><row><cell>E2vidD2-MASS-BiD</cell><cell cols="2">Video + ASR YT8M-cook + Recipe1M</cell><cell>22.93</cell><cell>11.68</cell><cell>32.15</cell><cell>0.87</cell></row><row><cell>E2vidD6-MASS-BiD</cell><cell cols="2">Video + ASR YT8M-cook + Recipe1M</cell><cell>24.22</cell><cell>12.22</cell><cell>32.60</cell><cell>0.89</cell></row><row><cell>Multimodal Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">E2vidD2-MASSalign-BiD Video + ASR YT8M-cook + Recipe1M</cell><cell>24.02</cell><cell>11.91</cell><cell>32.73</cell><cell>0.86</cell></row><row><cell cols="3">E2vidD6-MASSalign-BiD Video + ASR YT8M-cook + Recipe1M</cell><cell>24.92</cell><cell>12.25</cell><cell>33.09</cell><cell>0.90</cell></row><row><cell>E2vidD2-MASSvid-BiD</cell><cell cols="2">Video + ASR YT8M-cook + Recipe1M</cell><cell>24.15</cell><cell>12.10</cell><cell>32.96</cell><cell>0.88</cell></row><row><cell>E2vidD6-MASSvid-BiD</cell><cell cols="2">Video + ASR YT8M-cook + Recipe1M</cell><cell>24.87</cell><cell>12.43</cell><cell>32.97</cell><cell>0.90</cell></row><row><cell cols="3">E2vidD2-MASSdrop-BiD Video + ASR YT8M-cook + Recipe1M</cell><cell>23.70</cell><cell>12.01</cell><cell>32.71</cell><cell>0.88</cell></row><row><cell cols="3">E2vidD6-MASSdrop-BiD Video + ASR YT8M-cook + Recipe1M</cell><cell>24.48</cell><cell>12.22</cell><cell>33.10</cell><cell>0.89</cell></row><row><cell>Human estimate</cell><cell>Video + ASR</cell><cell>-</cell><cell>41.61</cell><cell>32.50</cell><cell>41.59</cell><cell>1.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Video captioning results on ViTT-Cooking. We use YT8M-cook and Recipe1M for optional pretraining.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.blog.google/products/ search/key-moments-video-search/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that no annotations are provided for the test split; we conducted our own training/dev/test split over available videos.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://developers.google.com/ youtube/v3/docs/captions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This is similar to the way language-ID embeddings are used in machine translation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We found in a preliminary study that using 6-layer encoders did not improve performance for our application.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We send warm thanks to Ashish Thapliyal for helping the first author debug his code and navigate the computing infrastructure, and to Sebastian Goodman for his technical help (and lightning fast responses!). We also thank the anonymous reviewers for their comments and suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Zisserman Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Why you should lean into how-to content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;neil-Hart</forename><surname>Celie</surname></persName>
		</author>
		<ptr target="www.thinkwithgoogle.com/advertising-channels/video/self-directed-learning-youtube/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph for video captioning with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Boxiao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1910.10683</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Palaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00347</idno>
		<title level="m">How2: A large-scale dataset for multimodal language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense procedure captioning in narrated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6382" to="6391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pretraining of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017a. Attention is all you need. ArXiv, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yuan Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4580" to="4590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learnersourcing subgoal labels for how-to videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSCW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">You know what&apos;s cool? a billion hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtubeblog</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Online video view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zenithmedia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object relational graph with teacher-recommended learning for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaya</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Syntax-aware action targeting for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<ptr target="http://youcook2.eecs.umich.edu/static/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">YouCookII dataset</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Youcookii/Youcookii_Readme</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
