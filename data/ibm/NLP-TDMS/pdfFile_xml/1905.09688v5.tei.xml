<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THE CONVOLUTIONAL TSETLIN MACHINEÅ PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-30">December 30, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole-Christoffer</forename><surname>Granmo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Goodwin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geir</forename><surname>Thore</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berge</forename><surname>Sshf</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cair</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAIR</orgName>
								<orgName type="institution" key="instit2">University of Agder Sondre Glimsdal CAIR</orgName>
								<orgName type="institution" key="instit3">University of Agder</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAIR</orgName>
								<orgName type="institution" key="instit2">University of Agder</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CAIR</orgName>
								<orgName type="institution" key="instit2">University of Agder</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">CAIR</orgName>
								<orgName type="institution" key="instit2">University of Agder</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">THE CONVOLUTIONAL TSETLIN MACHINEÅ PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-30">December 30, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) have obtained astounding successes for important pattern recognition tasks, but they suffer from high computational complexity and the lack of interpretability. The recent Tsetlin Machine (TM) attempts to address this lack by using easy-to-interpret conjunctive clauses in propositional logic to solve complex pattern recognition problems. The TM provides competitive accuracy in several benchmarks, while keeping the important property of interpretability. It further facilitates hardware-near implementation since inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on straightforward bit manipulation. In this paper, we exploit the TM paradigm by introducing the Convolutional Tsetlin Machine (CTM), as an interpretable alternative to CNNs. Whereas the TM categorizes an image by employing each clause once to the whole image, the CTM uses each clause as a convolution filter. That is, a clause is evaluated multiple times, once per image patch taking part in the convolution. To make the clauses location-aware, each patch is further augmented with its coordinates within the image. The output of a convolution clause is obtained simply by ORing the outcome of evaluating the clause on each patch. In the learning phase of the TM, clauses that evaluate to 1 are contrasted against the input. For the CTM, we instead contrast against one of the patches, randomly selected among the patches that made the clause evaluate to 1. Accordingly, the standard Type I and Type II feedback of the classic TM can be employed directly, without further modification. The CTM obtains a peak test accuracy of 99.4% on MNIST, 96.31% on Kuzushiji-MNIST, 91.5% on Fashion-MNIST, and 100.0% on the 2D Noisy XOR Problem, which is competitive with results reported for simple 4-layer CNNs, BinaryConnect, Logistic Circuits, and a recent FPGA-accelerated Binary CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Tsetlin Machine (TM) <ref type="bibr" target="#b11">[12]</ref> is a novel machine learning paradigm introduced in 2018. It is based on the Tsetlin Automaton (TA) <ref type="bibr" target="#b35">[36]</ref>, one of the pioneering solutions to the well-known multi-armed bandit problem <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b10">11]</ref> and the first Finite State Learning Automaton (FSLA) <ref type="bibr" target="#b24">[25]</ref>. The TM has the following main properties that make it attractive as a building block for machine learning <ref type="bibr" target="#b11">[12]</ref>: (a) it solves complex pattern recognition problems with interpretable propositional formulae. This is crucial for high stakes decisions <ref type="bibr" target="#b33">[34]</ref>. (b) It learns on-line, persistently increasing both training-and test accuracy, before converging to a Nash equilibrium. The Nash equilibrium balances false positive against false negative classifications, while combating overfitting with frequent pattern mining principles. (c) Resource allocation dynamics are leveraged to optimize usage of limited pattern representation resources. By allocating resources uniformly across sub-patterns, local optima are avoided. (d) Inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on straightforward bit manipulation. This facilitates low-energy consuming hardware design <ref type="bibr" target="#b39">[40]</ref>. (e) Finally, the TM has provided competitive accuracy in comparison with classical and neural network based techniques, while keeping the important property of interpretability <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Source code and demos for this paper can be found at https://github.com/cair/pyTsetlinMachineParallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1905.09688v5 [cs.LG] 27 Dec 2019</head><p>The TM currently is state-of-the-art in FSLA-based pattern recognition. However, when compared with CNNs, it struggles with attaining competitive accuracy, providing e.g. 98.5% mean accuracy on MNIST (without augmenting the data) <ref type="bibr" target="#b11">[12]</ref>. To address this deficiency, we here introduce the Convolutional Tsetlin Machine (CTM), a new kind of TM designed for image classification.</p><p>FSLA. The simple Tsetlin Automaton approach has formed the core of more advanced FSLA designs that solve a wide range of problems. This includes resource allocation <ref type="bibr" target="#b14">[15]</ref>, decentralized control <ref type="bibr" target="#b36">[37]</ref>, knapsack problems <ref type="bibr" target="#b15">[16]</ref>, searching on the line <ref type="bibr" target="#b26">[27]</ref>, meta-learning <ref type="bibr" target="#b27">[28]</ref>, the satisfiability problem <ref type="bibr" target="#b12">[13]</ref>, graph colouring <ref type="bibr" target="#b3">[4]</ref>, preference learning <ref type="bibr" target="#b41">[42]</ref>, frequent itemset mining <ref type="bibr" target="#b16">[17]</ref>, adaptive sampling <ref type="bibr" target="#b13">[14]</ref>, spatio-temporal event detection <ref type="bibr" target="#b42">[43]</ref>, equi-partitioning <ref type="bibr" target="#b28">[29]</ref>, streaming sampling for social activity networks <ref type="bibr" target="#b9">[10]</ref>, routing bandwidth-guaranteed paths <ref type="bibr" target="#b25">[26]</ref>, faulty dichotomous search <ref type="bibr" target="#b43">[44]</ref>, and learning in deceptive environments <ref type="bibr" target="#b45">[46]</ref>, to list a few examples. The unique strength of all of these FSLA designs is that they provide state-of-the-art performance when problem properties are unknown and stochastic, and the problem must be solved as quickly as possible through trial and error.</p><p>Rule-based Machine Learning. While the present paper focuses on extending the field of FSLA, we acknowledge the extensive work on rule-based interpretable pattern recognition from other fields of machine learning. Learning propositional formulae to represent patterns in data has a long history. One prominent example is frequent itemset mining for association rule learning <ref type="bibr" target="#b1">[2]</ref>, for instance applied to predicting sequential events <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b23">24]</ref>. Other examples include the work of Feldman who investigated the hardness of learning formulae in disjunctive normal form (DNF) <ref type="bibr" target="#b8">[9]</ref>. Furthermore, Probably Approximately Correct (PAC) learning has provided fundamental insight into machine learning, as well as a framework for learning formulae in DNF <ref type="bibr" target="#b37">[38]</ref>. Approximate Bayesian approaches have recently been introduced to provide more robust learning of rules <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref>. However, in general, rule-based machine learning scales poorly and is prone to noise. Indeed, for data-rich problems, in particular those involving natural language and sensory inputs, state-of-the-art rule-based machine learning is inferior to deep learning. The recent hybrid Logistic Circuits <ref type="bibr" target="#b21">[22]</ref>, however, have had success in image classification. This approach uses local search to build Bayesian models that capture logical expressions, and learns to classify by employing stochastic gradient descent. The CTM, on the other hand, attempts to bridge the gap between the interpretability of pure rule-based machine learning and the accuracy of deep learning, by allowing the TM to more effectively deal with images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs.</head><p>A myriad of image recognition techniques have been reported. However, after AlexNet won the ImageNet recognition challenge by a significant margin in 2012, the entire field of computer vision has been dominated by CNNs. The AlexNet architecture was built upon earlier work by LeCun et al. <ref type="bibr" target="#b20">[21]</ref> who introduced CNNs in 1998. Countless CNN architectures, all following the same basic principles, have since been published, including the now state-of-the-art Squeeze-and-Excitation networks <ref type="bibr" target="#b18">[19]</ref>. In this paper, we introduce convolution to the TM paradigm of machine learning. By doing so, we simultaneously propose a new kind of convolution filter: an interpretable filter expressed as a propositional formula. Our intent is to address a well-known disadvantage of CNNs, namely, that the CNN models in general are complex and non-transparent, making them hard to interpret. Consequently, the knowledge on why CNNs perform so well and what steps are need to improve the models is limited <ref type="bibr" target="#b44">[45]</ref>.</p><p>Binary CNNs. Relying on a large number of multiply-accumulate operations, training CNNs is computationally intensive. To mitigate this, there is an increasing interest in binarizing the CNNs. With only two possible values for the synapse weights, e.g.,´1 and 1, many of the multiplication operations can be replaced with simple accumulations. This could potentially open up for more specialized hardware and more compressed and efficient models. One recent approach is BinaryConnect <ref type="bibr" target="#b5">[6]</ref>, which reached a near state-of-the-art accuracy of 98.99% on MNIST, and 99.13% in combination with an SVM. Binary CNNs have been further improved with the introduction of XNOR-Nets, which replace the standard CNN filters with binary equivalents <ref type="bibr" target="#b31">[32]</ref>. BNN+ <ref type="bibr" target="#b6">[7]</ref> is the most recent binary CNN, extending the XNOR-Nets with two additional regularization functions and an adaptive scaling factor. The CTM can be seen as an extreme Binary CNN in the sense that it is entirely based on fast summation and logical operators. Additionally, learning in the CTM is bandit based (online learning from reinforcement), while Binary CNNs are based on backpropagation.</p><p>Contributions and Paper Outline. Our contributions can be summarized as follows. First, in Sect. 2, we provide a brief introduction to the TM and a succinct definition of both recognition and learning. Then, in Sect. 3, we introduce the concept of convolution to TMs. Whereas the classic TM categorizes an image by employing each clause once on the whole image, the CTM uses each clause as a convolution filter. In all brevity, we propose a recognition and learning approach for clause-based convolution that produces interpretable filters. In Sect. 4, we evaluate the CTM on MNIST, Kuzushiji-MNIST, Fashion-MNIST, and the 2D Noisy XOR Problem, and discuss the merits of the new scheme. Finally, we conclude and provide pointers to further work in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Include</head><p>Exclude </p><formula xml:id="formula_0">x 2 x 1 ¬x 2 ¬x 1 ⌵ ⌵ ⌵ x 2 x 1 ¬x 2 ¬x 1 ⌵ ⌵ ⌵ Clause + Clause - X = (x 1 , x 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Majority voting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Tsetlin Machine</head><p>The TM solves complex pattern recognition problems using conjunctive clauses in propositional logic, composed by a collective of TAs. Its roots in game theory, the bandit problem, resource allocation, and frequent pattern mining are explored in depth in the original paper on the TM, which also includes pseudo code <ref type="bibr" target="#b11">[12]</ref>. We here provide a more succinct overview, together with a running example to clarify core concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structure</head><p>The structure of the TM is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As seen, the TM takes a vector X of o propositional variables as input: X " px k q P t0, 1u o , with o being the size of the vector and t0, 1u o its domain. The input vector in the figure, for instance, is of size 2 and consists of the variables x 1 and x 2 with domain tp0, 0q, p0, 1q, p1, 0q, p1, 1qu.</p><p>The TM formulates patterns using conjunctive clauses. A conjunctive clause is built from literals, that is, the input variables and their negations. With o input variables, we have 2o literals, L " pl k q P t0, 1u 2o :</p><formula xml:id="formula_1">l k " " x k if 1 ď k ď o, x k´o otherwise pi.e., o`1 ď k ď 2oq.<label>(1)</label></formula><p>Above, the first o literals are the unnegated variables, while the o following are the negated ones. Thus, in our example, there are four literals: x 1 , x 2 , x 1 , and x 2 .</p><p>The number of clauses is a user set parameter that decides the expression power of the TM. Assume we have m clauses in our TM structure. Each clause, denoted by C j and indexed by j P t1, . . . , mu, is simply a conjunction of a subset I j Ď t1, . . . , 2ou of the literals:</p><formula xml:id="formula_2">C j pXq " ľ kPIj l k<label>(2)</label></formula><p>or equivalently:</p><formula xml:id="formula_3">C j pXq " ź kPIj l k .<label>(3)</label></formula><p>The subset I j contains the indexes of the literals that have been Included in the clause j. For the special case of I j " H, i.e., an empty clause, we have:</p><formula xml:id="formula_4">C j pXq " " 1 during learning 0 otherwise.<label>(4)</label></formula><p>That is, during learning, empty clauses output 1 and during classification they output 0. Accordingly, overall, the output c j " C j pXq P t0, 1u of a clause is fully specified by the input X and the selection of literals I j . Continuing our example, in the figure we have two clauses (m " 2). The first consists of the literals with indexes I 1 " t1, 4u:</p><formula xml:id="formula_5">C 1 pXq " x 1^ x 2 .</formula><p>The second consists of the literals with indexes I 2 " t1, 2u:</p><formula xml:id="formula_6">C 2 pXq " x 1^x2 .</formula><p>Each clause is further assigned a fixed polarity p P t´1,`1u, which decides whether the clause output is negative or positive. In the figure, the first clause has positive polarity, whereas the second has negative. Positive clauses are used to recognize class y " 1, while negative clauses are used to recognize class y " 0. By default, half of the clauses are positive and half are negative.</p><p>In the last step, the outputs of the clauses decide the classŷ P t0, 1u assigned to the input X. That is, a summation operator aggregates the output of the clauses to form a majority vote.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recognition</head><p>We now look at recognition in more detail. First of all, the output c p j of each clause is organized as vectors C`" pcj q P t0, 1u m 2 and C´" pcj q P t0, 1u m 2 , grouped by polarity. Given an input X, these outputs can then be calculated as follows (equivalent to Eqn. 2):</p><formula xml:id="formula_7">c p j " " 1 if I p j Ď I 1 L , 0 otherwise.<label>(5)</label></formula><p>Here, the subset I 1 L Ď t1, . . . , 2ou contains the indexes of the literals that take the value 1 for the current input X:</p><formula xml:id="formula_8">I 1 L " tk|l k " 1, 1 ď k ď 2ou.</formula><p>To express this, the lower index L stands for the literals and the upper index 1 constrains these to those of value 1. Thus, Eqn. 5 compactly states that a clause outputs 1 if and only if all of its literals are of value 1. In <ref type="figure" target="#fig_0">Figure 1</ref>, for instance, the input px 1 , x 2 q " p1, 0q produces the literal values l 1 " 1, l 2 " 0, l 3 " 0, and l 4 " 1. Since the first clause only consists of l 1 and l 4 , it outputs 1. The second clause, on the other hand, consists of l 1 and l 2 , hence it outputs 0.</p><p>The final classification decision is made as follows. A majority vote is first organized by summing the output C`of the positive clauses, and subtracting the output C´of the negative clauses: v " ř j cj´ř j cj . This majority vote decides the prediction of the TM:ŷ " p0 ď v), with a tie resolved in favor of y " 1. 2 So, in our example, the input px 1 , x 2 q " p1, 0q gathers a single positive vote, and no negative votes. Consequently, the classification becomesŷ " 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tsetlin Automata Collective</head><p>We now introduce the collective of TAs, which decides the composition of each clause. There are 2o TAs per clause j, one for each literal l k P tx 1 , . . . , x o , x 1 , . . . , x o u. Each individual TA has a state aj ,k , aj ,k P t1, . . . , 2N u, and the states of all of the TAs are organized in two m 2ˆo matrices. The first matrix, A`" paj ,k q P t1, . . . , 2N u m 2ˆ2 o , contains the states of the TAs belonging to the positive polarity clauses, whereas the matrix A´" paj ,k q P t1, . . . , 2N u m 2ˆ2 o covers the negative polarity clauses. In <ref type="figure" target="#fig_0">Figure 1</ref>, there are eight TAs, one per literal for clause 1 (positive polarity) and one per literal for clause 2 (negative polarity). Each of these TAs has 6 states: a p j,k P t1, . . . , 6u. The states decide the actions taken by the TAs. A TA decides to exclude its literal if in states a p j,k ď N , otherwise, it includes the literal. That is, for each clause j, some literals are Included:</p><formula xml:id="formula_9">I p j " ! k|a p j,k ą N, 1 ď k ď 2o )</formula><p>, while the remaining are Excluded: I p j c " t1, . . . , 2ouzI p j (the complement of the included ones). So, in the figure, the TA associated with literal x 1 in the first clause is in state 4 and thus x 1 is included in that clause. The second TA, on the other hand, is in state 3 and, accordingly, literal x 2 is excluded. By checking the state of each TA in this manner, the composition of all of the clauses is decided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning</head><p>Learning in the TM is based on coordinating the collective of TAs using a novel FSLA game. The game leverages resource-allocation <ref type="bibr" target="#b15">[16]</ref> and frequent pattern mining <ref type="bibr" target="#b16">[17]</ref> principles, indicated by the feedback loop in <ref type="figure" target="#fig_0">Figure 1</ref>. The feedback is handed out based on training examples pX, yq, consisting of an input X and an output y.</p><p>As explored below, the TM employs two kinds of feedback: Type I and Type II. Type I feedback jointly combats false negatives and overfitting by stimulating recognition of frequent patterns. Type II feedback, on the other hand, suppresses false positives by increasing the discrimination power of the patterns learnt.</p><p>Feedback is further regulated by the sum of clause outputs v (the majority vote) and a target value T P Z ą set for v by the user. A larger T (with a corresponding increase in the number of clauses) makes the learning more robust. This is because more clauses are involved in learning each specific pattern, introducing an ensemble effect. However, note that the resulting gain in accuracy comes at the cost of increased computational cost (cf. <ref type="table" target="#tab_2">Table 3</ref>). In the following, we first define the Type I and Type II feedback operations for clauses with positive polarity. As opposed to the feedback tables in <ref type="bibr" target="#b11">[12]</ref>, the representation here is in a more compact matrix form.</p><p>Type I: For training output y " 1, TAs belonging to positive clauses are given Type I feedback to make v approach T .</p><p>A vector R1 " prj ,1 q P t0, 1u m 2 picks out the positive clauses selected for feedback (the lower index of R1 refers to the output y " 1):</p><formula xml:id="formula_10">rj ,1 " # 1 with probability T´clamppv,´T,T q 2T , 0 otherwise.<label>(6)</label></formula><p>Here, the clamp operation restricts v to lie between´T and T . Further, the lower index of rj ,1 refers to the clause j and the output y " 1. As illustrated by plot y " 1 in <ref type="figure" target="#fig_1">Figure 2</ref>, clauses are randomly selected for feedback, with a larger chance of being selected for lower values of v. In effect, clauses are updated more aggressively the farther away the voting sum is from T , up to´T . We now divide the Type I feedback into two parts, Type Ia and Type Ib. Type Ia reinforces Include actions to make the patterns finer. Only TAs taking part in clauses that output 1 and whose associated literal takes the value 1 are select for Type Ia feedback. We collect the indexes of these TAs in the set I Ia</p><formula xml:id="formula_11">C`"</formula><p>pj, kq|l k " 1^cj " 1^rj ,1 " 1 ( . Type Ib feedback is designed to reinforce Exclude actions to combat over-fitting. Type Ib feedback is handed out to the TAs stochastically, using a user set parameter s ě 1.0 (a larger s provides finer patterns). The stochastic part of the calculation is organized in a matrix Q`" pqj ,k q P t0, 1u m 2ˆ2 o with entries:</p><formula xml:id="formula_12">qj ,k " "</formula><p>1 with probability 1 s , 0 otherwise.</p><p>Here, the lower index of qj ,k refers to the clause j and the TA k, considering the positive clauses (upper index). The indexes of the TAs selected for Type Ib feedback are collected in the set I Ib</p><formula xml:id="formula_14">C`" ! pj, kq|pl k " 0 _ cj " 0q^rj ,1 " 1^qj ,k " 1 )</formula><p>. That is, TAs taking part in clauses that output 0, or whose associated literal takes the value 0, are selected for Type Ib feedback, however, only if stochastically pinpointed by rj ,1 " 1 and qj ,k " 1.</p><p>After the TAs has been selected for feedback, their states are updated by two state update operators ' and a: A`Ð`A`' I Ia C`˘a I Ib C`T hese operators add/subtract 1 from the states of the singled out TAs, however, not beyond the given state space. In our example, if the TA associated with literal x 1 in the first clause is singled out for Type Ia feedback, it would move from state 4 to state 5, reinforcing the Include action. If it instead receives Type Ib feedback, it would move to state 3, reinforcing Exclude.</p><p>Type II: For training output y " 0, TAs belonging to positive clauses are given Type II feedback to suppress clause output 1. This, together with the negative clauses, makes v approach´T . A matrix R0 " prj ,0 q P t0, 1u m 2 picks out the positive clauses selected for feedback:</p><formula xml:id="formula_15">rj ,0 " # 1 with probability T`clamppv,´T,T q 2T , 0 otherwise.<label>(8)</label></formula><p>The lower index of rj ,0 refers to the clause j and the output y " 0, respectively. Here too, clauses are randomly selected for feedback, but now with a larger chance of being selected for higher v (cf. plot y " 0 in <ref type="figure" target="#fig_1">Figure  2</ref>). Next, the TAs selected are the ones that will turn clauses that output 1 into clauses that output 0, that is, those that have excluded literals that take the value 0: I II C`" pj, kq|l k " 0^cj " 1^rj ,0 " 1 ( . Again, the states of the selected TAs are updated using the dedicated operators: A`Ð A`' I II C`. Thus, if the TA associated with literal x 2 in the first clause of our example is selected for Type II feedback, it would change state from 3 to 4, reinforcing the Include action.</p><p>All of the above operations are for positive clauses. For negative clauses, Type I feedback is simply replaced with Type II feedback and vice versa! </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Integer Clause Weighting</head><p>It turns out that introducing clause weighting improves TM performance, both accuracy-and computation-wise <ref type="bibr" target="#b30">[31]</ref>. By weighting the output of the clauses, one clause can replace multiple and the impact of a clause can be fine-tuned. In all brevity, classification is then performed as a weighted majority vote:</p><formula xml:id="formula_16">s 1 pxq " m{2 ÿ j"1 wj Cj pxq´m {2 ÿ j"1 wj Cj pxq.<label>(9)</label></formula><p>Whereas real-valued weighting was introduced in [31], we here address learning of integer weights, based on Stochastic Searching on the Line <ref type="bibr" target="#b26">[27]</ref>:</p><p>Initialization. First of all, the weights are initialized to 1:</p><formula xml:id="formula_17">wj Ð 1,<label>(10)</label></formula><p>wj Ð 1.</p><p>In other words, the behaviour of the clauses are identical to those of a standard TM at the start of learning.</p><p>Weight Updating. The TA state updating is unmodified with weights. The weights, in turn, are updated based on Type I and Type II feedback.</p><p>Type I: For Type I feedback, a clause weight is increased by 1, however, only if the clause outputted 1:</p><formula xml:id="formula_19">wj Ð wj`1, if Cj pxq " 1,<label>(12)</label></formula><p>wj Ð wj`1, if Cj pxq " 1.</p><p>Type II: For Type II feedback, we instead update the weights by subtracting 1, with 1 being the minimum weight value:</p><formula xml:id="formula_21">wj Ð wj´1, if Cj pxq " 1^wj ą 1,<label>(14)</label></formula><p>wj Ð wj´1, if Cj pxq " 1^wj ą 1.</p><p>When a clause evaluates to 0, its weight remains unchanged:</p><formula xml:id="formula_23">wj Ð wj , if Cj pxq " 0,<label>(16)</label></formula><p>wj Ð wj , if Cj pxq " 0.</p><p>As seen, for Type I feedback, the weights are increased to strengthen the impact of the associated clauses, thus reinforcing true positive frequent patterns. For Type II feedback, on the other hand, the weights are decreased instead. This is to diminish the impact of the associated clauses, thus combating false positives, increasing discrimination power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Convolutional Tsetlin Machine</head><p>Consider a set of images X " tX e |1 ď e ď Eu, where e is the index of the images. Each image is of size XˆY , and consists of Z binary layers (which together encode the pixel colors using thresholding <ref type="bibr" target="#b7">[8]</ref>, one-hot encoding, or any other appropriate binary encoding). A classic TM models such an image with an input vector X " px k q P t0, 1u XˆYˆZ that contains XˆYˆZ propositional variables. Further, each clause is composed from XˆYˆZˆ2 literals. Inspired by the impact convolution has had on deep neural networks, we here introduce the Convolutional Tsetlin Machine (CTM).</p><p>Interpretable Rule-based Filters. The CTM uses filters with spatial dimensions WˆW , again with Z binary layers. Further, the clauses of the CTM take the role of filters. Each clause is accordingly composed from WˆWˆZˆ2 literals. Additionally, to make the clauses location-aware <ref type="bibr" target="#b22">[23]</ref>, we augment each clause with binary encoded coordinates. Location awareness may prove useful in applications where both patterns and their location are distinguishing features, e.g. recognition of facial features such as eyes, eyebrows, nose, mouth, etc. in facial expression recognition. In all brevity, when applying a filter of size WˆW on an image px k q P t0, 1u XˆYˆZ , the filter will be evaluated on</p><formula xml:id="formula_25">B " B XˆBY image patches. Here, B X " P X´W d T`1 and B Y " P Y´W d T`1</formula><p>, with d being the step size of the convolution. Each image patch thus has a certain location within the image, and we augment the input vector with the coordinates of this location. We denote the resulting augmented input vector X b : X b " px k q P t0, 1u WˆWˆZ`B X`BY . As seen, the input vector is extended with one propositional variable per position along each dimension, with the position being encoded using thresholding <ref type="bibr" target="#b7">[8]</ref> or one-hot encoding. <ref type="figure">Figure 4b</ref> illustrates an example of the image, patches, and a filter for X " Y " 6, W " 3, and d " 2. In this example, the 3ˆ3 filter moves from left to right, from top to bottom, 2 pixels per step.</p><p>Recognition. The CTM uses the classic TM procedure for recognition (see Sect. 2). However, for the CTM each clause outputs B values per image (one value per patch), as opposed to a single output for the TM (Eq. 5). We denote the output of a positive clause j on patch b by c b,j . To turn the multiple outputs c 1,j , . . . , c B,j of clause j into a single output denoted by cj , we simply OR the individual outputs:</p><formula xml:id="formula_26">cj " B ł b"1 c b,j .<label>(18)</label></formula><p>Learning. Learning in the CTM leverages the TM learning procedure. As seen in Sect. 2, Type Ia, Type Ib, and Type II feedback are influencing each clause j based on the literals of the input vector X. For the CTM, the input vector is an image patch, and there are B patches in an image. There is thus B literal inputs L b , 1 ď b ď B, per clause. Therefore, to decide which patch to use when updating a clause, the CTM randomly selects a single patch among the patches that made the clause evaluate to 1. The clause is then updated according to this patch. That is, the input X b is drawn from the set: tX b |c b,j " 1, 1 ď b ď Bu. Observe that if the set is empty, only Type Ib feedback is applicable, and then the input vector is not needed. For non-empty sets, the TAs to be updated are finally singled out using the randomly selected patch:</p><formula xml:id="formula_27">I Ia C`" pj, kq|l b k " 1^cj " 1^rj ,1 " 1 ( ,<label>(19)</label></formula><formula xml:id="formula_28">I Ib C`" ! pj, kq|pl b k " 0 _ cj " 0q^rj ,1 " 1^qj ,k " 1 ) ,<label>(20)</label></formula><formula xml:id="formula_29">I II C`" pj, kq|l b k " 0^cj " 1^rj ,0 " 1 ( .<label>(21)</label></formula><p>The reason for randomly selecting a patch is to have each clause extract a certain sub-pattern, and the randomness of the uniform distribution statistically spreads the clauses for different sub-patterns in the target image. Finally, observe that the computational complexity of the CTM grows linearly with the number of clauses m, and with the number of patches B. However, the computations can be easily parallelized due to their decentralized nature.</p><p>Step-by-step Walk-through of Inference on Noisy 2D XOR. Rather than providing hand-crafted features which can be used for image classification, the CTM learns feature detectors. We will explain the workings of the CTM by an illustrative example of noisy 2D XOR recognition and learning (see <ref type="figure">Figure 4</ref> and Sect. 4). Consider the CTM depicted in <ref type="figure">Figure 3a</ref>. It consists of four positive clauses which represent XOR patterns that must be present in a positive example image (positive features) and four negative clauses which represent patterns that will not trigger a positive image classification (negative features). The number of positive and negative clauses is a user-defined parameter. The bit patterns inside each clause are represented by the output of eight TAs, two for each bit in a 2ˆ2 filter.</p><p>Consider the 3ˆ3 image shown in <ref type="figure">Figure 3b</ref>. The filter represented by the second positive clause matches the patch in the top-right corner of the image and it is the only clause with output 1; similarly, none of the negative clauses respond since their patterns do not match the pattern found in the current patch <ref type="figure">(Figure 3b</ref>). Thus, the TM's combined output is v " 1. Learning of feature detectors proceeds as follows: with the CTM's voting target set to T " 2, the probability of feedback is T´v 2T " 1 4 , and thus learning takes place, which pushes the CTM's output v towards T " 2. Note that Type I feedback reinforces true positive output and reduces false negative output whereas Type II feedback reduces false positive output.</p><p>A subsequent state of the CTM is shown in <ref type="figure">Figure 4a</ref>. There are now two positive clauses which detect their pattern in the top-right corner patch. The combined output of all clauses is 2; thus, no further learning is necessary for the detection of the XOR pattern in this patch. Also, the location of the occurrence of each pattern is included. The location information uses a bit representation as follows: Suppose an XOR pattern occurs at the three X-coordinates 1, 4, and 6.</p><p>For the corresponding binary location representation, these coordinates are considered as thresholds: If a coordinate is greater than a threshold, then the corresponding bit in the binary representation will be 0; otherwise, it is set to 1. Thus, the representation of the X-coordinates 1, 4, and 6 will be "111", "011" and "001", respectively. These representations of the location of 2ˆ2 patterns are also learned by TAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Range 2D Noisy XOR MNIST K-MNIST Fashion-MNIST #Class Clauses</head><p>1´8 <ref type="formula">000</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Results</head><p>In this section, we evaluate the CTM on four different datasets.</p><p>2D Noisy XOR. The 2D Noisy XOR dataset contains 4ˆ4 binary images, 2500 training examples and 10 000 test examples. The image bits have been set randomly, except for the 2ˆ2 patch in the upper right corner, which reveals the class of the image. A diagonal line is associated with class 1, while a horizontal or vertical line is associated with class 0. Thus the dataset models a 2D version of the XOR-relation. Furthermore, the dataset contains a large number of random non-informative features to measure susceptibility towards the curse of dimensionality. To examine robustness towards noise we have further randomly inverted 40% of the outputs in the training data.</p><p>MNIST. The MNIST dataset has been used extensively to benchmark machine learning algorithms, consisting of 28ˆ28 grey scale images of handwritten digits <ref type="bibr" target="#b20">[21]</ref>.</p><p>Kuzushiji-MNIST. This dataset contains 28ˆ28 grayscale images of Kuzushiji characters, cursive Japanese. Kuzushiji-MNIST is more challenging than MNIST because there are multiple distinct ways to write some of the characters <ref type="bibr" target="#b4">[5]</ref>.</p><p>Fashion-MNIST. This dataset contains 28ˆ28 grayscale images of articles from the Zalando catalogue, such as t-shirts, sandals, and pullovers <ref type="bibr" target="#b40">[41]</ref>. This dataset is quite challenging, with a human accuracy of 83.50%.</p><p>The latter three datasets contain 60 000 training examples and 10 000 test examples. We binarize these datasets using an adaptive Gaussian thresholding procedure with window size 11 and threshold value 2. Accordingly, the CTM operates on images with merely 1 bit per pixel. <ref type="table" target="#tab_4">Table 4</ref> reports test accuracy for the CTM, while <ref type="table">Table 1</ref> contains the corresponding configurations. The results are based on single-run averages, obtained from the last 100 epochs of 250.</p><p>All of the experiments were run on a NVIDIA DGX-2 server. Note that these datasets are rather simple to solve for deep CNNs, and have been selected to facilitate our study of interpretability.</p><p>The hyperparameters were set using a light manual binary search. One particularly critical hyperparameter is the number of clauses employed per class. <ref type="table" target="#tab_2">Table 3</ref> reports execution time per epoch as well as mean test accuracy for an increasing number of class clauses. As seen, test accuracy increases steadily with the number of clauses employed, while execution time increases sub-linearly due to parallelization. 0: U L 0: U R 9: U L 9: U R 4: L L 00*000*000 *000*0*000 **00*00000 00000000** 00*000**** 000000***1 000**0*111 0*00**111* *****111*0 00**11**00 0 ă X ď 6 5 ă Y ď 8 0********* 0******0** ********00 *111111**0 *111*11**0 *1*1*11*** 111**11**0 1****111** 11***111** *****111** 8 ă X ď 17 0 ă Y ď 6 0*0*****1* 00000**1** 000******* 00***1**00 000**1**00 00**1*00*0 00****00** 0*******11 0********* 000******* 2 ă X ď 7 5 ă Y ď 10 0**0*****0 *00*0***** **0******* *******0** *11****0*0 ****0**000 ****1**00* ****1***0* ****11**00 ********0* 12 ă X ď 18 2 ă Y ď 8 ******1*** *11*1*1*** ***111**** *****11*** *000**1*** 000**1**** 0000*1*0** 0*00*1**0* 0**0***0** 0000****0* 7 ă X ď 11 12 ă Y ď 17 0: L L 0: L R 9: L L 9: L R 3: U R *000**1*** ********** **0******0 *00****110 *********0 *0*0***1*0 ******11** ***0****** **000**1*1 0*0****11* 0 ă X ď 5 10 ă Y ď 16 *******1** ******1*** ******1*** *****1**1* ****1*1*** ****11**** *1*11*1*** 1**11***** *11*11**** *1*1****** 4 ă X ď 18 13ă Y ď 18 0000000*** 00000001** 0000000110 0000000*1* 000000*0*1 00000000** 0000000000 0000*00000 0000000000 0000000000 0 ă X ď 3 8 ă Y ď 14 ******00** *****000*0 **11*0000* **11*00000 1*11000*00 11**0000*0 111****00* 1**0000000 1**00*00** 1*0**00*00 12ă X ď 16 9 ă Y ď 15 **0000*000 00000*0000 ***0000000 *11**00000 *1*1**0000 0**1*00*00 0*1**0***0 **1*0000*0 *1***0*000 ***0000000 12 ă X ď 18 0ă Y ď 3 <ref type="table">Table 2</ref>: Example 10ˆ10 bit patterns produced by CTM for MNIST, including valid convolution positions. Here "0: UL" means "upper left" of the image for digit "0". We can see clearly that "0: UL", "0: UR", "0: LL" and "0: LR" jointly construct the shape "0". The clauses for the other digits behave similarly, and we thus just illustrate selected patch positions for each digit.  The CTM performs rather robustly, providing tight 95% confidence intervals for the mean performance. As seen in <ref type="table" target="#tab_4">Table 4</ref>, we have also included results for selected popular algorithms, as points of reference. Note that the CTM is an interpretable approach that does not employ multiple layers at this stage.  Model MNIST F-MNIST Logistic Circuit (binary) <ref type="bibr" target="#b21">[22]</ref> 1 072 kB 2 456 kB Logistic Circuit (real-valued) <ref type="bibr" target="#b21">[22]</ref> 728 kB 1 768 kB CNN w/3 conv. layers <ref type="bibr" target="#b21">[22]</ref> 8 784 kB 8 784 kB ResNet <ref type="bibr" target="#b21">[22]</ref> 19 352 kB 19 352 kB TM w/4 000 class clauses 7 657 kB 7 657 kB CTM w/250 class clauses 127 kB 127 kB CTM w/8 000 class clauses 2 109 kB 2 109 kB In <ref type="table" target="#tab_5">Table 5</ref>, the size of the trained models are listed for MNIST and F-MNIST. Size is measured in kilobytes of memory used. The memory usage of CTM is similar to Logistic Circuits, which both use significantly less memory than the neural network architectures. A potential advantage of CTM is the ability to trade off accuracy against computation time and memory usage, by reducing the number of clauses employed. E.g., a CTM with 250 clauses per class reduces memory usage by more than an order of magnitude at a relatively small loss in test accuracy (cf. <ref type="table" target="#tab_2">Table 3</ref>). <ref type="table">Table 2</ref> contains example patterns produced by CTM for MNIST (with one bit per pixel using 8000 clauses). The "*" symbol can either take the value "0" or "1". The remaining bit values require strict matching. The examples have been selected to illustrate how several patterns together form complete numbers. As seen, the patterns are relatively easy to interpret for humans compared to, e.g., a neural network. They are also efficient to evaluate for computers, involving only logical operators, followed by summation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Further Work</head><p>This paper introduced the Convolutional Tsetlin Machine (CTM), leveraging the learning mechanism of the Tsetlin Machine (TM). Whereas the TM categorizes images by employing each clause once per XˆY input, the CTM uses each clause as a WˆW convolution filter. The filters learned by the CTM are interpretable, being formulated using propositional formulae. To make the clauses location-aware, each patch is further enhanced with its coordinates within the image. Location awareness may prove useful in applications where both patterns and their location are distinguishing features. By randomly selecting which patch to learn from, the standard Type I and Type II feedback of the classic TM can be employed directly. In this manner, the CTM obtains results on MNIST, Kuzushiji-MNIST, Fashion-MNIST, and the 2D Noisy XOR Problem that compare favorably with simple 4-layer CNNs, Logistic Circuits, as well as two binary neural network architectures. In our further work, we intend to investigate more advanced binary encoding schemes, to go beyond grey-scale images (e.g., addressing CIFAR-10 and ImageNet). We further intend to develop schemes for deeper CTMs, with the first step being a two-layer CTM, to introduce more compact and expressive patterns with nested propositional formulae.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FeedbackFigure 1 :</head><label>1</label><figDesc>The basic TM structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Clause feedback probability for T " 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Example of inference (a) and learning (b) for the Noisy 2D XOR Problem. (a) Goal state for the Noisy 2D XOR Problem. (b) Illustration of image, filter and patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Single-run test accuracy per epoch for CTM on MNIST, K-MNIST, and F-MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>CTM mean test accuracy and execution time per epoch for an increasing number of clauses.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Therefore, we believe a fair comparison should target traditional/simple CNNs. Results listed in italic are reported in the corresponding papers. Results for BinaryConnect and FPGA-accelerated BNNs on K-MNIST and Fashion-MNIST were not available, so are not reported. Notice that the CTM outperforms the binary CNNs for all the datasets, as well several simple baseline configurations, including 4-layer CNNs, Random Forests, Gradient Boosting, SVMs, and a 4-nearest neighbour classifier. Further observe that both the TM and the CTM obtain higher accuracy than Logistic Circuits<ref type="bibr" target="#b21">[22]</ref> with binary data. Thus, in our experiments, the TM learning paradigm outperforms the probabilistic circuits paradigm<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> at extracting information from binary representations. However, only the CTM performs competitively with Logistic Circuits when the Logistic Circuits are given the advantage of real-valued inputs. Note that the CTM supports using multiple bits to encode the color of each pixel, however, in the present paper, we only investigate single bit representations. Finally, the CTM is outperformed by the more advanced deep learning network architectures PreActResNet-18 and ResNET18+VGG.</figDesc><table><row><cell>Model</cell><cell>2D N-XOR</cell><cell>MNIST</cell><cell>K-MNIST</cell><cell>F-MNIST</cell></row><row><cell>4-Nearest Neighbour [5, 41]</cell><cell>61.62</cell><cell>97 .14</cell><cell>91 .56</cell><cell>85.40</cell></row><row><cell>SVM [5]</cell><cell>94.63</cell><cell>98 .57</cell><cell>92 .82</cell><cell>89 .7</cell></row><row><cell>Random Forest [22]</cell><cell>70.73</cell><cell>97 .3</cell><cell>-</cell><cell>81 .6</cell></row><row><cell>Gradient Boosting Classifier [41]</cell><cell>87.15</cell><cell>96 .9</cell><cell>-</cell><cell>88 .0</cell></row><row><cell>Simple CNN [5, 22]</cell><cell>91.05</cell><cell>99 .06</cell><cell>95 .12</cell><cell>90 .7</cell></row><row><cell>BinaryConnect [6]</cell><cell>-</cell><cell>98 .99</cell><cell>-</cell><cell>-</cell></row><row><cell>FPGA-accelerated BNN [20]</cell><cell>-</cell><cell>98 .70</cell><cell>-</cell><cell>-</cell></row><row><cell>Logistic Circuit (binary) [22]</cell><cell>-</cell><cell>97 .4</cell><cell>-</cell><cell>87 .6</cell></row><row><cell>Logistic Circuit (real-valued) [22]</cell><cell>-</cell><cell>99 .4</cell><cell>-</cell><cell>91 .3</cell></row><row><cell>PreActResNet-18 [5]</cell><cell>-</cell><cell>99 .56</cell><cell>97 .82</cell><cell>92 .00</cell></row><row><cell>ResNet18 + VGG Ensemble [5]</cell><cell>-</cell><cell>99 .60</cell><cell>98 .90</cell><cell>-</cell></row><row><cell>TM</cell><cell>99.12</cell><cell>98.57</cell><cell>92.03</cell><cell>90.09</cell></row><row><cell>CTM (Mean)</cell><cell cols="4">99.99˘0.0 99.33˘0.0 96.08˘0.01 91.18˘0.01</cell></row><row><cell>CTM (95 %ile)</cell><cell>100.0</cell><cell>99.38</cell><cell>96.25</cell><cell>91.39</cell></row><row><cell>CTM (Peak)</cell><cell>100.0</cell><cell>99.40</cell><cell>96.31</cell><cell>91.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MNIST test accuracy</cell></row></table><note>Figure 5 depicts test accuracy for the CTM on MNIST, K-MNIST, and F-MNIST, epoch-by-epoch. As seen, test accuracy climbs quickly in the first epochs, e.g., passing 99% already in epoch 8 for MNIST.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Empirical results -test accuracy in percent.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Model size in kilobytes (kB), assuming 32-bit real-valued weights. peaks at 99.4% after 234 epochs, while K-MNIST peaks at 96.31% after 169 epochs. For F-MNIST, test accuracy surpasses 90.0% in epoch 9 and peaks at 91.5% in epoch 170.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Multi-class pattern recognition problems are modelled by employing multiple instances of this structure, replacing the threshold operator with an argmax operator<ref type="bibr" target="#b11">[12]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Regression Tsetlin Machine -A Novel Approach to Interpretable Non-Linear Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Abeyrathna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imieliński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using the Tsetlin Machine to Learn Human-Interpretable Rules for High-Accuracy Text Categorization with Medical Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Berge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Tveit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Matheussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="115134" to="115146" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic Learning for SAT-Encoded Graph Coloring Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouhmala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Metaheuristic Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01718</idno>
		<title level="m">Deep Learning for Classical Japanese Literature</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belbahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Partovi Nia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11800</idno>
		<title level="m">BNN+: Improved Binary Network Training</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Scheme for Continuous Input to the Tsetlin Machine with Applications to Forecasting Disease Outbreaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darshana Abeyrathna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goodwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04199</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hardness of Approximate Two-Level Logic Minimization and PAC Learning with Membership Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="26" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A streaming sampling algorithm for social activity networks using fixed structure learning automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Meybodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bandit processes and dynamic allocation indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gittins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="177" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Tsetlin Machine -A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01508</idno>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Solving the Satisfiability Problem Using Finite Learning Automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouhmala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal sampling for estimation with constrained resources using a learning automaton-based solution for the nonlinear fractional knapsack problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving Stochastic Nonlinear Resource Allocation Problems Using a Hierarchy of Twofold Resource Allocation Automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="560" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Automata-based Solutions to the Nonlinear Fractional Knapsack Problem with Applications to Optimal Resource Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Myrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="175" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A two-armed bandit collective for hierarchical examplar based mining of frequent itemsets with applications to intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haugland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kjølleberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computational Collective Intelligence XIV</title>
		<imprint>
			<biblScope unit="volume">8615</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Befurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dzyabura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lammie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Rahimi</forename><surname>Azghadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06105</idno>
		<title level="m">Accelerating Deterministic and Stochastic Binarized Neural Networks on FPGAs Using OpenCL</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning logistic circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Broeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 33rd Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9628" to="9639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Hierarchical Model for Association Rule Mining of Sequential Events: An Approach to Automated Medical Symptom Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning Automata: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A L</forename><surname>Thathachar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Routing bandwidth-guaranteed paths in MPLS traffic engineering: A multiple race track learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic Searching on the Line and its Applications to Parameter Learning in Nonlinear Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="733" to="739" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A solution to the stochastic point location problem in metalevel nonstationary environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="466" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deterministic Learning Automata Solutions to The Equipartitioning Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Probabilistic deep learning using random sum-product networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stelzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Weighted Tsetlin Machine: Compressed Representations with Clause Weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phoulady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gorji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Phoulady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Statistical Relational AI</title>
		<meeting>the Ninth International Workshop on Statistical Relational AI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Some aspects of the sequential design of experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning theory analysis for association rules and sequential event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3441" to="3492" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On behaviour of finite automata in random medium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Tsetlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avtomat. i Telemekh</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1354" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using Finite State Automata to Produce Self-Optimization and Self-Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kleinrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="47" to="61" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Theory of the Learnable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Bayesian Framework for Learning Rule Sets for Interpretable Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macneille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2357" to="2393" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tsetlin Machine: A New Paradigm for Pervasive AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wheeldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yakovlev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Haddadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SCONA Workshop at Design, Automation and Test in Europe (DATE)</title>
		<meeting>the SCONA Workshop at Design, Automation and Test in Europe (DATE)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Service selection in stochastic environments: A learning-automaton based solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="617" to="637" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning-Automaton-Based Online Discovery and Tracking of Spatiotemporal Event Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1118" to="1130" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the analysis of a random walk-jump chain with tree-based transitions and its applications to faulty dichotomous search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B. John</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sequential Analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Symmetrical Hierarchical Stochastic Searching on the Line in Informative and Deceptive Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="635" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
