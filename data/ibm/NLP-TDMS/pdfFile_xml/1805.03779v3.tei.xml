<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">k-Space Deep Learning for Accelerated MRI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoseob</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Sunwoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
						</author>
						<title level="a" type="main">k-Space Deep Learning for Accelerated MRI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Compressed sensing MRI</term>
					<term>Deep Learning</term>
					<term>Han- kel structured low-rank completion</term>
					<term>Convolution framelets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The annihilating filter-based low-rank Hankel matrix approach (ALOHA) is one of the state-of-the-art compressed sensing approaches that directly interpolates the missing k-space data using low-rank Hankel matrix completion. The success of ALOHA is due to the concise signal representation in the k-space domain thanks to the duality between structured low-rankness in the k-space domain and the image domain sparsity. Inspired by the recent mathematical discovery that links convolutional neural networks to Hankel matrix decomposition using datadriven framelet basis, here we propose a fully data-driven deep learning algorithm for k-space interpolation. Our network can be also easily applied to non-Cartesian k-space trajectories by simply adding an additional regridding layer. Extensive numerical experiments show that the proposed deep learning method consistently outperforms the existing image-domain deep learning approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECENTLY, inspired by the tremendous success of deep learning <ref type="bibr">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, many researchers have investigated deep learning approaches for MR reconstruction problems and successfully demonstrated significant performance gain <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b9">[10]</ref>.</p><p>In particular, Wang et al <ref type="bibr" target="#b3">[4]</ref> used the deep learning reconstruction either as an initialization or a regularization term. Deep network architecture using unfolded iterative compressed sensing (CS) algorithm was also proposed to learn a set of regularizers and associated filters <ref type="bibr" target="#b4">[5]</ref>. These works were followed by novel extension using deep residual learning <ref type="bibr" target="#b5">[6]</ref>, domain adaptation <ref type="bibr" target="#b6">[7]</ref>, data consistency layers <ref type="bibr" target="#b8">[9]</ref>, etc. An extreme form of the neural network called Automated Transform by Manifold Approximation (AUTOMAP) <ref type="bibr" target="#b9">[10]</ref> even attempts to estimate the Fourier transform itself using fully connected layers. All these pioneering works have consistently demonstrated superior reconstruction performances over the compressed sensing approaches <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref> at significantly lower run-time computational complexity.</p><p>Although the end-to-end recovery approach like AU-TOMAP <ref type="bibr" target="#b9">[10]</ref> may directly recover the image without ever interpolating the missing k-space samples (see <ref type="figure" target="#fig_0">Fig. 1(c)</ref>), it works only for the sufficiently small size images due to its huge memory requirement for fully connected layers. Accordingly, most of the popular deep learning MR reconstruction  <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, (b) cascaded network <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, (c) AUTOMAP <ref type="bibr" target="#b9">[10]</ref>, and (d) the proposed k-space learning. IFT: Inverse Fourier transform.</p><p>algorithms are either in the form of image domain postprocessing as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>  <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, or iterative updates between the k-space and the image domain using a cascaded network as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>  <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>One of the main purposes of this paper is to reveal that the aforementioned approaches are not all the available options for MR deep learning, but there exists another effective deep learning approach. In fact, as illustrated in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>, the proposed deep learning approach directly interpolates the missing k-space data so that accurate reconstruction can be obtained by simply taking the Fourier transform of the interpolated k-space data. In contrast to AUTOMAP <ref type="bibr" target="#b9">[10]</ref>, our network is implemented in the form of convolutional neural network (CNN) without requiring fully connected layer, so the GPU memory requirement for the proposed k-space deep learning is minimal.</p><p>In fact, our k-space deep learning scheme is inspired by the success of structured low-rank Hankel matrix approaches <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, exploiting concise signal representation in the k-space domain thanks to the duality between the structured low-rankness in the k-space and the image domain sparsity. In addition, the recent theory of deep convolutional framelets <ref type="bibr">[1]</ref> showed that an encoder-decoder network can be regarded as a signal representation that emerges from Hankel matrix decomposition. Therefore, by synergistically combining these findings, we propose a novel k-space deep learning algorithms that perform better and generalize well. We further show that our deep learning approach for k-space interpolation can handle general k-space sampling patterns beyond the Cartesian trajectory, such as radial, spiral, etc. Moreover, our theory and empirical results also shows that multi-channel calibration-free k-space interpolation can be easily realized using the proposed framework by simply stacking multi-coil k-space data along the channel direction as an input to feed in the neural network.</p><p>We are aware of recent k-space neural network approaches called the scan-specific robust artificial neural networks for kspace interpolation (RAKI) <ref type="bibr" target="#b19">[20]</ref>. Unlike the proposed method, RAKI considers scan-specific learning without training data, so the neural network weights needs to be recalculated for each k-space input data. On the other hand, the proposed method exploits the generalization capability of the neural network. More specifically, even with the same trained weights, the trained neural network can generate diverse signal representation depending on the input thanks to the combinatorial nature of the ReLU nonlinearity. This makes the neural network expressive and generalized well to the unseen test data. The theoretical origin of the expressiveness will be also discussed in this paper.</p><p>After the original version of this work was available on Arxiv, there appear several deep learning algorithms exploiting k-space learning <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. These works are based on hybrid formulation (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) and utilize the deep neural network as a regularization term for k-space denoising. On the other hand, the our method exploits the nature of deep neural network as a generalizable and expressive k-space representation to directly interpolate the missing k-space data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATHEMATICAL PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations</head><p>In this paper, matrices are denoted by bold upper case letters, i.e. A, B, whereas the vectors are represented by bold lower cases letters, i.e. x, y. In addition, [A] ij refers to the (i, j)-th element of the matrix A, and x[i] denotes the i-th element of the vector x. The notation v ∈ R d for a vector v ∈ R d denotes its flipped version, i.e. the indices of v are time-reversed such that v[n] = v[−n]. The N × N identity matrix is denoted as I N , while 1 N is an N -dimensional vector with 1's. The superscript T and for a matrix or vector denote the transpose and Hermitian transpose, respectively. R and C denote the real and imaginary fields, respectively. R + refers to the nonnegative real numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Forward Model for Accelerated MRI</head><p>The spatial Fourier transform of an arbitrary smooth function x : R 2 → R is defined bŷ</p><formula xml:id="formula_0">x(k) = F[x](k) := R d e −ιk·r x(r)dr,</formula><p>with spatial frequency k ∈ R 2 and ι = √ −1. Let {k n } N n=1 , for some integer N ∈ N, be a collection of finite number of sampling points of the k-space confirming to the Nyquist sampling rate. Accordingly, the discretized k-space data x ∈ C N is introduced by</p><formula xml:id="formula_1">x = x[0] · · ·x[N − 1] , wherex[i] = x(k i ). (1)</formula><p>For a given under-sampling pattern Λ for accelerated MR acquisition, let the downsampling operator P Λ : C N → C N be defined as</p><formula xml:id="formula_2">[P Λ [x]] i = x[i], i ∈ Λ 0, otherwise .<label>(2)</label></formula><p>Then, the under-sampled k-space data is given bŷ</p><formula xml:id="formula_3">y := P Λ [x]<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Low-Rank Hankel Matrix Approaches</head><p>From the undersampled data in <ref type="formula" target="#formula_3">(3)</ref>, CS-MRI <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> attempts to find the feasible solution that has minimum nonzero support in some sparsifying transform domain. This can be achieved by finding a function z :</p><formula xml:id="formula_4">R 2 → R such that min z T z 1 subject to P Λ [x] = P Λ [ẑ]<label>(4)</label></formula><p>where T denotes the image domain sparsifyting transform and</p><formula xml:id="formula_5">z = z(k 0 ) · · · z(k N −1 ) T .<label>(5)</label></formula><p>This optimization problem usually requires iterative update between the k-space and the image domain after the discretization of z(r) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>On the other hand, in recent structured low-rank matrix completion algorithms <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the compressed sensing problems was solved either by imposing the low-rank structured matrix penalty <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref> or by converting it a direct k-space interpolation problem using low-rank structure matrix completion <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. More specifically, let H d ( x) denote a Hankel matrix constructed from the k-space measurement x in (1), where d denotes the matrix pencil size (for more details on the construction of Hankel matrices and their relation to the convolution, see Section I in Supplementary Material). Then, if the underlying signal x(r) in the image domain is sparse and described as the signal with the finite rate of innovations (FRI) with rate s <ref type="bibr" target="#b22">[23]</ref>, the associated Hankel matrix H d (x) with d &gt; s is low-ranked <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Therefore, if some of k-space data are missing, we can construct an appropriate weighted Hankel matrix with missing elements such that the missing elements are recovered using low-rank Hankel matrix completion approaches <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_6">(P ) min z∈C N RANK H d ( z)<label>(6)</label></formula><formula xml:id="formula_7">subject to P Λ [ x] = P Λ [ z] .</formula><p>While the low-rank Hankel matrix completion problem (P ) can be solved in various ways, one of the main technical huddles is its relatively large computational complexity for matrix factorization and large memory requirement for storing Hankel matrix. Although several new approaches have been proposed to solve these problems <ref type="bibr" target="#b24">[25]</ref>, the following section shows that a deep learning approach is a novel and efficient way to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MAIN CONTRIBUTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ALOHA as a signal representation</head><p>Consider the following image regression problem under the low-rank Hankel matrix constraint:</p><formula xml:id="formula_8">min z∈C N x − F −1 [ z] 2<label>(7)</label></formula><p>subject to <ref type="bibr">RANK</ref> </p><formula xml:id="formula_9">H d ( z) = s, P Λ [ x] = P Λ [ z] ,<label>(8)</label></formula><p>where s denotes the estimated rank. In the above formulation, the cost in <ref type="formula" target="#formula_8">(7)</ref> is defined in the image domain to minimize the errors in the image domain, whereas the low-rank Hankel matrix constraint in <ref type="bibr" target="#b7">(8)</ref> is imposed in the k-space after the k-space weighting. Now, we convert the complex-valued constraint in (8) to a real-valued constraint. The procedure is as follows. First, the operator R :</p><formula xml:id="formula_10">C N → R N ×2 is defined as R[ z] := Re(ẑ) Im(ẑ) , ∀ z ∈ C N<label>(9)</label></formula><p>where Re(·) and Im(·) denote the real and imaginary part of the argument. Similarly, we define its inverse operator R −1 :</p><formula xml:id="formula_11">R N ×2 → C N as R −1 [ Z] :=ẑ 1 + ιẑ 2 , ∀ Z := [z 1 z 2 ] ∈ R N ×2<label>(10)</label></formula><p>Then, as shown in Section II in Supplementary Material, we can approximately convert <ref type="bibr" target="#b7">(8)</ref> to an optimization problem with real-valued constraint:</p><formula xml:id="formula_12">(P A ) min z∈C N x − F −1 [ z] 2 (11) subject to RANKH d|2 (R[ z]) = Q ≤ 2s, P Λ [ x] = P Λ [ z] .</formula><p>In the recent theory of deep convolutional framelets <ref type="bibr">[1]</ref>, this low-rank constraint optimization problem was addressed using learning-based signal representation. More specifically, for any z ∈ C N , let the Hankel structured matrix</p><formula xml:id="formula_13">H d|2 (R[ z]) have the singular value decomposition UΣV , where U = [u 1 · · · u Q ] ∈ R N ×Q and V = [v 1 · · · v Q ] ∈ R 2d×Q</formula><p>denote the left and the right singular vector bases matrices, respectively; Σ = (σ ij ) ∈ R Q×Q is the diagonal matrix with singular values. Now, consider matrix pair Ψ,Ψ ∈ R 2d×Q</p><formula xml:id="formula_14">Ψ := ψ 1 1 · · · ψ 1 Q ψ 2 1 · · · ψ 2 Q and Ψ := ψ1 1 · · ·ψ 1 Q ψ 2 1 · · ·ψ 2 Q<label>(12)</label></formula><p>that satisfy the low-rank projection constraint:</p><formula xml:id="formula_15">ΨΨ = P R(V) ,<label>(13)</label></formula><p>where P R(V) denotes the projection matrix to the range space of V. We further introduce the generalized pooling and unpooling matrices Φ, Φ ∈ R N ×M [1] that satisfies the condition</p><formula xml:id="formula_16">ΦΦ = I N ,<label>(14)</label></formula><p>Using Eqs. <ref type="bibr" target="#b12">(13)</ref> and <ref type="formula" target="#formula_4">(14)</ref>, we can obtain the following matrix equality:</p><formula xml:id="formula_17">H d|2 (R[ z]) = ΦΦ H d|2 (R[ z]) ΨΨ = ΦCΨ ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_18">C := Φ H d|2 (R[ z]) Ψ ∈ R N ×Q<label>(16)</label></formula><p>By taking the generalized inverse of Hankel matrix, <ref type="bibr" target="#b14">(15)</ref> can be converted to the framelet basis representation <ref type="bibr">[1]</ref>. Moreover, one of the most important observations in <ref type="bibr">[1]</ref> is that the resulting framelet basis representation can be equivalently represented by single layer encoder-decoder convolution architecture:</p><formula xml:id="formula_19">R[ z] = ΦC g(Ψ), where C = Φ (R[ z] h(Ψ))<label>(17)</label></formula><p>and denotes the multi-channel input multi-channel output convolution. The second and the first part of (17) correspond to the encoder and decoder layers with the corresponding convolution filters h(Ψ) ∈ R 2d×Q and g Ψ () ∈ R dQ×2 :</p><formula xml:id="formula_20">h(Ψ) := ψ 1 1 · · · ψ 1 Q ψ 2 1 · · · ψ 2 Q , g Ψ :=    ψ 1 1 ψ 2 1 . . . . . . ψ 1 Q ψ 2 Q    ,</formula><p>which are obtained by reordering the matrices Ψ and Ψ in <ref type="bibr" target="#b11">(12)</ref>. Specifically, ψ</p><formula xml:id="formula_21">1 i ∈ R d (resp. ψ 2 i ∈ R d )</formula><p>denotes the d-tap encoder convolutional filter applied to the real (resp. imaginary) component of the k-space data to generate the ith channel output. In addition, g(Ψ) is a reordered version ofΨ so thatψ 1 i ∈ R d (resp.ψ 2 i ∈ R d ) corresponds to the d-tap decoder convolutional filter to generate the real (resp. imaginary) component of the k-space data by convolving with the i-th channel input. We can further use recursive application of encoder-decoder representation for the resulting framelet coefficients C in <ref type="bibr" target="#b16">(17)</ref>. In Corollary 4 of our companion paper <ref type="bibr">[2]</ref>, we showed that the recursive application of the encoderdecoder operations across the layers increases the net length of the convolutional filters.</p><p>Since <ref type="formula" target="#formula_8">(17)</ref> is a general form of the signals that are associated with a Hankel structured matrix, we are interested in using it to estimate bases for k-space interpolation. Specifically, we consider a complex-valued signal space H determined by the filters Ψ andΨ:</p><formula xml:id="formula_22">H(Ψ,Ψ) = z ∈ C N R[z] = Φ C g(Ψ) , C = (ΦR[z]) h(Ψ) .<label>(18)</label></formula><p>Then, the ALOHA formulation P A can be equivalently represented by</p><formula xml:id="formula_23">(P A ) min z∈H(Ψ,Ψ) min Ψ,Ψ x − F −1 [ z] 2 subject to P Λ [ x] = P Λ [ z] ,</formula><p>In other words, ALOHA is to find the optimal filter Ψ,Ψ and the associated k-space signal z ∈ H(Ψ,Ψ) that satisfies the data consistency conditions. In contrast to the standard CS approaches in which signal presentation in the image domain is separately applied from the data-consistency constraint in the k-space, the success of ALOHA over CS can be contributed to more efficient signal representation in the k-space domain that simultaneously take care of the data consistency in the k-space domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization and Depth</head><p>To allow training for neural networks, the problem formulation in (P A ) should be decoupled into two steps: the learning phase to estimate Ψ,Ψ from the training data, and the inference phase to estimate the interpolate signal z for the given filter set Ψ,Ψ. Although we have revealed the relations between ALOHA and encoder-decoder architecture, the derivation is for specific input signal and it is not clear how the relations would translate when training is performed over multiple training data set, and the trained network can be generalized to the unseen test data. Given that the sparsity prior in dictionary learning enables the selection of appropriate basis functions from the dictionary for each given input, one may conjecture that there should be similar mechanisms in deep neural networks that enable adaptation to the specific input signals. In Section IV of Supplementary Material, we show that the ReLU nonlinearities indeed plays a critical role in the adaptation and generalization. In fact, in our companion paper <ref type="bibr">[2]</ref>, we have shown that ReLU offers combinatorial convolution frame basis selection depending on each input image. More specifically, thanks to ReLU, a trained filter set produce large number of partitions in the input space as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, in which each region shares the same linear signal representation. Therefore, depending on each k-space input data, a particular region and its associated linear representation are selected. Moreover, we show that the number of input space partition and the associated linear representation increases exponentially with the depth, channel and the skipped connection. By synergistically exploiting the efficient signal representation in the k-space domain, this enormous expressivity from the same filter sets can make the k-space deep neural network more powerful than the conventional image domain learning.</p><p>For the more details on the theoretical aspect of deep neural networks, see Section IV of Supplementary Material or our companion paper <ref type="bibr">[2]</ref>. For the parallel imaging, the input and output are multi-coil k-space data, after stacking them along the channel direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extension to parallel imaging</head><p>In <ref type="bibr" target="#b14">[15]</ref>, we formally showed that when {x i } P i=1 denote the k-space measurements from P receiver coils, the following extended Hankel structured matrix is low-ranked:</p><formula xml:id="formula_24">H d|P ( X) = H d ( x 1 ) · · · H d ( x P )<label>(19)</label></formula><p>where</p><formula xml:id="formula_25">X = x 1 · · · x P ∈ C N ×P .</formula><p>Thus, similar to the single channel cases, the date-driven decomposition of the extended Hankel matrix in <ref type="bibr" target="#b18">(19)</ref> can be represented by stacking the each k-space data along the channel direction and applies the deep neural network for the given multi-channel data. Therefore, except the number of input and output channels, the network structure for parallel imaging data is identical to the single channel k-space learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sparsification</head><p>To further improve the performance of the structured matrix completion approach, in <ref type="bibr" target="#b17">[18]</ref>, we showed that even if the image x(r) may not be sparse, it can be often converted to a sparse signal.</p><p>For example, the outmost skipped connection for the residual learning is another way to make the signal sparse. Note that fully sampled k-space datax can be represented by</p><formula xml:id="formula_26">x = y + ∆ x,</formula><p>where y is the undersampled k-space measurement in (3), and ∆ x is the residual part of k-space data that should be estimated. In practice, some of the low-frequency part of k-space data including the DC component are acquired in the undersampled measurement so that the image component from the residual k-space data ∆ x are mostly high frequency signals, which are sparse. Therefore, ∆ x has low-rank Hankel matrix structure, which can be effectively processed using the deep neural network. This can be easily implemented using a skipped connection before the deep neural network as shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. However, the skipped connection also works beyond the sparsification. In our companion paper <ref type="bibr">[2]</ref> (which is also repeated in Section IV of Supplementary Material), we showed that the skipped connection at the inner layers makes the frame basis more expressive. Therefore, we conjecture that the skipped connections play dual roles in our k-space learning.</p><p>Second, we can convert a signal to an innovation signal using a shift-invariant transform represented by the whitening filter h such that the resulting innovation signal z = h * x becomes an FRI signal <ref type="bibr" target="#b22">[23]</ref>. For example, many MR images can be sparsified using finite difference or wavelet transform <ref type="bibr" target="#b14">[15]</ref>. This implies that the Hankel matrix from the weighted k-space data,ẑ(k) =ĥ(k)x(k) are low-ranked, where the weightĥ(k) is determined from the finite difference or Haar wavelet transform <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Thus, the deep neural network is applied to the weighted k-space data to estimate the missing spectral dataĥ(x)x(k), after which the original k-space data is obtained by dividing with the same weight, i.e.x(k) = z(k)/ĥ(k). This can be easily implemented using a weighting and unweighting layer as shown in <ref type="figure" target="#fig_2">Fig. 3</ref></p><formula xml:id="formula_27">(b).</formula><p>In this paper, we consider these two strategies to investigate which strategy is better for different sampling trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extension to General Sampling Patterns</head><p>Since the Hankel matrix formulation in ALOHA is based on the Cartesian coordinate, we add extra regridding layers to handle the non-Cartesian sampling trajectories. Specifically, for radial and spiral trajectories, the non-uniform fast Fourier transform (NUFFT) was used to perform the regridding to the Cartesian coordinate. For Cartesian sampling trajectory, the regridding layer using NUFFT is not necessary, and we instead perform a zero-filling in the unacquired k-space regions as an initialization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Backbone</head><p>The network backbone follows the U-Net <ref type="bibr" target="#b2">[3]</ref> which consists of convolution, batch normalization, rectified linear unit (ReLU), and contracting path connection with concatenation as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Here, the input and output are the complex-valued k-space data, while R[·] and R −1 [·] denote the operators in <ref type="formula" target="#formula_10">(9)</ref> and <ref type="formula" target="#formula_11">(10)</ref>, respectively, that convert complex valued input to two-channel real value signals and vice versa. For parallel imaging, multi-coil k-space data are given as input and output after stacking them along channel direction. Specially, in our parallel imaging experiments, we use eight coils k-space data.</p><p>The yellow arrow is the basic operator that consists of 3 × 3 convolutions followed by a rectified linear unit (ReLU) and batch normalization. The same operation exists between the separate blocks at every stage, but the yellow arrows are omitted for visibility. A red and blue arrows are 2 × 2 average pooling and average unpooling operators, respectively, located between the stages. A violet arrow is the skip and concatenation operator. A green arrow is the simple 1 × 1 convolution operator generating interpolated k-space data from multichannel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Training</head><p>We use the l 2 loss in the image domain for training. For this, the Fourier transform operator is placed as the last layer to convert the interpolated k-space data to the complexvalued image domain so that the loss values are calculated for the reconstructed image. Stochastic gradient descent (SGD) optimizer was used to train the network. For the IFT layer, the adjoint operation from SGD is also Fourier transform. The size of mini batch is 4, and the number of epochs in single and multi coil networks is 1000 and 500, respectively. The initial learning rate is 10 −5 , which gradually dropped to 10 −6 until 300-th epochs. The regularization parameter was λ = 10 −4 .</p><p>The labels for the network were the images generated from direct Fourier inversion from fully sampled k-space data. The input data for the network was the regridded down-sampled kspace data from Cartesian, radial, and spiral trajectories. The details of the downsampling procedure will be discussed later. For each trajectory, we train the network separately.</p><p>The proposed network was implemented using MatConvNet toolbox in MATLAB R2015a environment <ref type="bibr" target="#b26">[27]</ref>. Processing units used in this research are Intel Core i7-7700 central processing unit and GTX 1080-Ti graphics processing unit. Training time lasted about 5 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MATERIAL AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Acquisition</head><p>The evaluations were performed on single coil and multi coils k-space data for various k-space trajectories such as Cartesian, radial, and spiral cases.</p><p>For the Cartesian trajectory, knee k-space dataset (http://mridata.org/) were used. The raw data were acquired from 3D fast-spin-echo (FSE) sequence with proton density weighting included fat saturation comparison by a 3.0T whole body MR system (Discovery MR 750, DV22.0, GE Healthcare, Milwaukee, WI, USA). The repetition time (TR) and echo time (TE) were 1550 ms and 25 ms, respectively. There were 320 slices in total, and the thickness of each slice was 0.6 mm. The field of view (FOV) defined 160 × 128 mm 2 and the size of acquisition matrix is 320 × 256. The voxel size was 0.5 mm. The number of coils is 8. Eight coils k-space data was used for multi-coil k-space deep learning. In addition, to evaluate the performance of the algorithm for the single coil experiment, coil compression (http://mrsrl.stanford.edu/tao/software.html) was applied to obtain a single coil k-space data. For the Cartesian trajectory as shown in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>, the input k-space was downsampled to a Gaussian pattern using x4 acceleration factor in addition to the 10% auto-calibration signal (ACS) line. Therefore, the net acceleration factor is about 3 (R = 3). Among the 20 cases of knee data, 18 cases were used for training, 1 case for validation, and the other for test.</p><p>For radial and spiral sampling patterns, a synthesized kspace data from Human Connectome Project (HCP) MR dataset (https://db.humanconnectome.org) were used. Specifically, the multi-coil radial and spiral k-space data are generated using MRI simulator (http://bigwww.epfl.ch/algorithms/mrireconstruction/). The T2 weighted brain images contained within the HCP were acquired Siemens 3T MR system using a 3D spin-echo sequence. The TR and TE were 3200 ms and 565 ms, respectively. The number of coils was 8, but the final reconstruction was obtained as the absolute of the sum. The FOV was 224 × 224 mm 2 , and the size of acquisition matrix was 320 × 320. The voxel size was 0.7 mm. The total of 199 subject datasets was used in this paper. Among the 199 subject, 180 were used for network training, 10 subject for validation, and the other subject for test. <ref type="figure" target="#fig_4">Fig. 5(b)</ref> shows the downsampled k-space radial sampling patterns. The downsampled radial k-space consists of only 83 spokes, which corresponds to R = 6 acceleration factor compared to the 503 spokes for the fully sampled data that were used as the ground-truth. On the other hand, <ref type="figure" target="#fig_4">Fig. 5(c)</ref> shows the down-sampled spiral sampling pattern, composed of 4 interleaves that corresponds to R = 4 acceleration compared to the he full spiral trajectory with 16 interleaves. The spiral k-space trajectory was obtained with a variable density factor (VDF) of 2.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation</head><p>For quantitative evaluation, the normalized mean square error (NMSE) value was used, which is defined as</p><formula xml:id="formula_28">N M SE = M i=1 N j=1 [x * (i, j) − x(i, j)] 2 M i=1 N j=1 [x * (i, j)] 2 ,<label>(20)</label></formula><p>where x and x * denote the reconstructed images and ground truth, respectively. M and N are the number of pixel for row and column. We also use the peak signal to noise ratio (PSNR), which is defined by</p><formula xml:id="formula_29">P SN R = 20 · log 10 N M x * ∞ x − x * 2 .<label>(21)</label></formula><p>We also use the structural similarity (SSIM) index <ref type="bibr" target="#b27">[28]</ref>, defined as</p><formula xml:id="formula_30">SSIM = (2µ x µ x * + c 1 )(2σ xx * + c 2 ) (µ 2 x + µ 2 x * + c 1 )(σ 2 x + σ 2 x * + c 2 ) ,<label>(22)</label></formula><p>where µ x is a average of x, σ 2 x is a variance of x and σ xx * is a covariance of x and x * . There are two variables to stabilize the division such as c 1 = (k 1 L) 2 and c 2 = (k 2 L) 2 . L is a dynamic range of the pixel intensities. k 1 and k 2 are constants by default k 1 = 0.01 and k 2 = 0.03.</p><p>For extensive comparative study, we also compared with the following algorithms: total variation (TV) penalized CS, ALOHA <ref type="bibr" target="#b14">[15]</ref>, and four types of CNN models including the variational model <ref type="bibr" target="#b4">[5]</ref>, a cascade model <ref type="bibr" target="#b8">[9]</ref>, the cross-domain model called KIKI network <ref type="bibr" target="#b28">[29]</ref>, and an image-domain model <ref type="bibr" target="#b6">[7]</ref>. In particular, <ref type="bibr" target="#b6">[7]</ref> is a representative example of <ref type="figure" target="#fig_0">Fig.  1(a)</ref>. Specifically, the image domain residual learning using the standard U-Net backbone in <ref type="figure" target="#fig_3">Fig. 4</ref> was used. Unlike the proposed network, the input and output are an artifact corrupted image and artifact-only image, respectively <ref type="bibr" target="#b7">[8]</ref>. In addition, the variational model <ref type="bibr" target="#b4">[5]</ref> and the cascade model <ref type="bibr" target="#b8">[9]</ref> represent <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. The cross-domain model is formed by linking the k-space model in <ref type="figure" target="#fig_0">Fig. 1(d)</ref> and the image-domain model in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Unfortunately, <ref type="figure" target="#fig_0">Fig. 1(c)</ref> does not scale well due to the enormous memory requirement, so cannot be used in the comparative study. For fair comparison, the cascade <ref type="bibr" target="#b8">[9]</ref> and cross-domain <ref type="bibr" target="#b28">[29]</ref> networks were modified for parallel imaging. All the neural networks were trained using exactly the same data set. For ALOHA <ref type="bibr" target="#b14">[15]</ref> and the proposed method in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>, the total variation based k-space weighting was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>To evaluate the performance of sparsifications in the single coil, the proposed method was trained using the sparsifications as shown in as Figs. 3(a)(b). <ref type="figure" target="#fig_5">Fig. 6</ref> shows the objective functions along the trajectories such as (a) Cartesian, (b) radial, and (c) spiral trajectory. In the Cartesian trajectory, the proposed network in <ref type="figure" target="#fig_2">Fig. 3(b)</ref> produces the lowest curve in the validation phase (see red curves in <ref type="figure" target="#fig_5">Fig. 6(a)</ref>). The proposed network in <ref type="figure" target="#fig_2">Fig 3(a)</ref> shows the best convergence during the training, but the generalization at the test phase was not good (see green curves in <ref type="figure" target="#fig_5">Fig. 6(a)</ref>). In the non-Cartesian trajectories, the best convergence appears using the proposed   network with only skipped connection in <ref type="figure" target="#fig_2">Fig. 3</ref>(a) (see green curves in <ref type="figure" target="#fig_5">Fig. 6(b)(c)</ref>). Based on these convergence behaviour and generalization, the proposed network was trained with different sparsification schemes. The network in <ref type="figure" target="#fig_2">Fig. 3(b)</ref> was trained for the Cartesian trajectory and the network in <ref type="figure" target="#fig_2">Fig.  3(a)</ref> was used for the non-Cartesian trajectories such as radial and spiral trajectories. <ref type="figure" target="#fig_6">Fig. 7</ref> shows the results of single coil reconstruction from Cartesian trajectory using the architecture with skipped connection and weighting layer as shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. While all the algorithms provide good reconstruction results, the pro- The results from first to last rows indicate ground-truth, downsampled, total variation, image-domain learning and the proposed method. Yellow and red boxes illustrate the enlarged and difference views, respectively. The difference images were amplified five times. The number written to the images is the NMSE value.</p><p>posed method most accurately recovered high frequency edges and textures as shown in the enlarged images and difference images of <ref type="figure" target="#fig_6">Fig. 7. Fig. 8</ref> shows the reformed images along the (i) coronal and (ii) sagittal directions. Again, the reformatted coronal and sagittal images by the proposed method preserved the most detailed structures of underlying images without any artifact along the slice direction. The quantitative comparison in <ref type="table" target="#tab_1">Table I</ref> in terms of average PSNR, NMSE, and SSIM values also confirmed that the proposed k-space interpolation method produced the best quantitative values in all area. The computation time of the proposed method is slightly slower than the image-domain learning because of the weighting and Fourier transform operations, but it is still about 3.5 times faster than the total variation penalized compressed sensing (CS) approach. <ref type="figure" target="#fig_8">Fig. 9</ref> shows the parallel imaging results from eight coil   measurement. Because the ALOHA <ref type="bibr" target="#b14">[15]</ref> and the proposed method directly interpolate missing k-space, these methods clearly preserve textures detail structures as shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. However, ALOHA <ref type="bibr" target="#b14">[15]</ref> is more than 100 times slower than the k-space deep learning as shown in <ref type="table" target="#tab_1">Table II</ref>. All CNN methods except the imaging-domain model outperform than CS methods. Although the cascade model <ref type="bibr" target="#b8">[9]</ref> was performed with data consistency step, the method did not completely overcome the limitations of image-domain learning. In the cross-domain learning <ref type="bibr" target="#b28">[29]</ref>, they proposed to train k-space and image-domain sequentially. The cross-domain network consists of deeper layers (100 layers; 25 layers × 4 individual models) than the proposed method, but the performance was worse than our method. As shown <ref type="table" target="#tab_1">Table II</ref>, the proposed method shows best performance in terms of average PSNR, NMSE, and SSIM values. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the reconstruction images from x6 accelerated radial sampling patterns using the architecture in <ref type="figure" target="#fig_2">Fig. 3(a)</ref> for 8 coils parallel imaging. The results for single coil are shown in <ref type="figure" target="#fig_0">Fig. S1</ref> in Supplementary Material. The proposed k-space deep learning provided realistic image quality and preserves the detailed structures as well as the textures, but the image domain network failed to remove the noise signals and the total variation method did not preserve the realistic textures and sophisticated structures. Our method also provides much smaller NMSE values, as shown at the bottom of each <ref type="figure" target="#fig_0">Fig.  10</ref> and <ref type="figure" target="#fig_0">Fig. S1</ref> in Supplementary Material. Average PSNR, NMSE and SSIM values are shown in <ref type="table" target="#tab_1">Table III and Table  S1</ref> in Supplementary Material for multi coils and single coil cases, respectively. The average values were calculated across </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Input Total variation Image-domain Ours ( 8 coils, ×6 ) learning ( <ref type="figure" target="#fig_2">Fig. 3(a)</ref>   all slices and 9 subjects. The proposed k-space deep learning provided the best quantitative values. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the reconstruction images from x4 accelerated spiral trajectory for 8 coils parallel imaging and <ref type="figure" target="#fig_1">Fig. S2</ref> in Supplementary Material illustrate the single coil results, respectively. Similar to the radial sampling patterns, the proposed method provides significantly improved image reconstruction results, and the average PSNR, NMSE and SSIM values in <ref type="table" target="#tab_1">Table IV and Table S2</ref> in Supplementary Material also confirm that the proposed method consistently outperform other method for all patients.</p><p>To evaluate the improvements of the proposed method, a   radiologist (with 9 years of experience) thoroughly reviewed the reconstructed images. For radial trajectory images, the pyramidal tract (arrows in <ref type="figure" target="#fig_2">Fig. S3</ref>(i) of the <ref type="figure">Supplementary  Material)</ref>, a bundle of nerve fibers conducting motor signal from the motor cortex to the brainstem or to the spinal cord, was evaluated. It was noted that high signal intensity of the pyramidal tract was exaggerated on the TV method, which could be misdiagnosed as an abnormal finding. In addition, the ability to discriminate a pair of internal cerebral veins were evaluated. While the TV and image-domain learning methods can not differentiate the two veins, the proposed method is able to show the two internal cerebral veins separately (arrows on <ref type="figure" target="#fig_2">Fig. S3</ref>(ii) in Supplementary Material). With regard to spiral trajectory images <ref type="figure" target="#fig_3">(Fig. S4 in the Supplementary Material)</ref>, the TV method shows bright dot-like artifacts along several slices ( <ref type="figure" target="#fig_3">Fig. S4(i)</ref>). When the small T2 hyperintensity lesions in the left frontal lobe were evaluated, the TV method fails to demonstrate the lesions. The image domain learning preserves the lesions, but the margin of the lesions is blurry in the noisy background. The small lesions are clearly depicted on the proposed method (arrows in <ref type="figure" target="#fig_3">Fig. S4(ii)</ref>). Overall, the quality of image reconstruction of the proposed method was superior to that of other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>In order to improve the performance of ALOHA, the matrix pencil size should be significantly large, which is not possible in standard ALOHA formulation due to the large memory requirement and extensive computational burden. We believe that one of the important reasons that the proposed k-space deep learning provides better performance than ALOHA is that the cascaded convolution results in much longer filter length. Nevertheless, with the same set of trained filters, our network can be adapted to different input images due to the combinatorial nature of ReLU nonlinearity. We believe that this contributes to the benefits of k-space learning over ALOHA. Moreover, efficient signal representation in k-space domain, which is the key idea of ALOHA, can synergistically work with the expressivity of the neural network to enable better performance than image domain learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Inspired by a link between the ALOHA and deep learning, this paper showed that fully data-driven k-space interpolation is feasible by using k-space deep learning and the image domain loss function. The proposed k-space interpolation network outperformed the existing image domain deep learning for various sampling trajectory. As the proposed k-space interpolation framework is quite effective and also supported by novel theory, so we believe that this opens a new area of researches for many Fourier imaging problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for "k-Space Deep Learning for Accelerated MRI"</head><p>Yoseob Han 1 , Leonard Sunwoo 2 , and Jong Chul Ye 1, * , Senior Member, IEEE Abstract-In this supplement, we provide a detailed discussion on the theoretical aspect of k-space deep learning. Additional experimental results are also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. HANKEL MATRIX CONSTRUCTION</head><p>For simplicity, here we consider 1-D signals, but its extension to 2-D is straightforward <ref type="bibr">[1]</ref>. In addition, to avoid separate treatment of boundary conditions, we assume the periodic boundary condition. Let f = f [0] · · · f [N − 1] ∈ C N be the signal vector. Then, a single-input single-output (SISO) convolution of the input f can be represented in a matrix form:</p><formula xml:id="formula_31">y = f h = H d (f )h ,<label>(23)</label></formula><p>where h denotes the time-reversal vector of h under periodic boundary condition, and H d (f ) is a wrap-around Hankel matrix defined by</p><formula xml:id="formula_32">H d (f ) =      f [0] f [1] · · · f [d − 1] f [1] f [2] · · · f [d] . . . . . . . . . . . . f [N − 1] f [N ] · · · f [d − 2]     <label>(24)</label></formula><p>where d denotes the matrix pencil parameter. On the other hand, multi-input multi-output (MIMO) convolution for the Pchannel input Z = [z 1 , · · · , z P ] to generate Q-channel output Y = [y 1 , · · · , y Q ] can be represented by</p><formula xml:id="formula_33">y i = P j=1 z j ψ j i , i = 1, · · · , Q<label>(25)</label></formula><p>where ψ j i ∈ R d denotes the length dfilter that convolves the j-th channel input to compute its contribution to the ith output channel. By defining the MIMO filter kernel Ψ as follows: the corresponding matrix representation of the MIMO convolution is then given by</p><formula xml:id="formula_34">Ψ =    Ψ 1 . . . Ψ P    where Ψ j = ψ j 1 · · · ψ j Q ∈ R d×Q<label>(</label></formula><formula xml:id="formula_35">Y = Z Ψ = P j=1 H d (z j )Ψ j = H d|P (Z) Ψ (27)</formula><p>where Ψ is a block structured matrix: <ref type="bibr" target="#b27">(28)</ref> and H d|P (Z) is an extended Hankel matrix by stacking P Hankel matrices side by side:</p><formula xml:id="formula_36">Ψ =    Ψ 1 . . . Ψ P    where Ψ j = ψ j 1 · · · ψ j Q ∈ R d×Q</formula><formula xml:id="formula_37">H d|P (Z) := H d (z 1 ) H d (z 2 ) · · · H d (z P ) .<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REAL AND IMAGINARY CHANNEL SPLITTING</head><p>For a given f ∈ C N , let</p><formula xml:id="formula_38">B := Re [H d (f )] Im [H d (f )] −Im [H d (f )] Re [H d (f )]<label>(30)</label></formula><p>and</p><formula xml:id="formula_39">T = 1 √ 2 I N I N ιI N −ιI N .</formula><p>Then, we can easily see that T is an orthonormal matrix and</p><formula xml:id="formula_40">T BT = H d (f ) 0 0 H * d (f )</formula><p>, which leads to</p><formula xml:id="formula_41">RANK Re [H d (f )] Im [H d (f )] −Im [H d (f )] Re [H d (f )] = RANKH d (f ) + RANKH * d (f ) = 2RANKH d (f ). Therefore, RANK Re [H d (f )] Im [H d (f )] ≤ 2RANKH d (f ),<label>(31)</label></formula><p>One could use (30) directly for Hankel matrix decomposition, but this introduce additional complication in the implementation due to the interaction between real and imaginary channels. Thus, for simplicity, we use (31) as a surrogate for the original rank constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ADDITIONAL EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single coil results for radial and spiral trajectories</head><p>The reconstruction results from radial trajectory at R = 6 in single coil imaging are shown in <ref type="figure" target="#fig_0">Fig. S1</ref> and their quantitative comparison are given in <ref type="table" target="#tab_9">Table S1</ref>. The proposed method outperformed the image domain approach in terms of image quality and quantitative metric except the SSIM. In the radial sampling pattern, the SSIM value is slightly lower than the image-domain network. However, when calculating the SSIM value within brain region, the image-domain network is 0.9741, whereas the proposed method is 0.9809, which is superior than the image-domain.</p><p>Reconstruction results from spiral trajectory at R = 4 in single coil imaging are shown in <ref type="figure" target="#fig_1">Fig. S2</ref> and their quantitative comparison are given in <ref type="table" target="#tab_10">Table S2</ref>. Similar to the radial case, the proposed method outperformed the image domain approach in terms of image quality and quantitative metric except the SSIM. However, the SSIM value within the brain region was 0.9872 by the proposed method which outperforms the image-domain SSIM value of 0.9747 in the brain region. These results suggest that the proposed method accurately recover the brain regions. <ref type="figure" target="#fig_0">Fig. S1</ref>: Reconstruction results from radial trajectory at R = 6 in single coil imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Input Image-domain Ours ( 1 coil, ×6 ) learning ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Radiological evaluation</head><p>The non-Cartesian parallel images were evaluated by an experienced radiologist, who again confirmed the superiority of k-space deep learning compared to the image domain approach. Specifically, for radial trajectory results in <ref type="figure" target="#fig_2">Fig. S3</ref>, the TV method shows the watercolor-like patterns, and the imagedomain learning dose not remove the streak-artifacts clearly. However, the proposed method preserved the sophisticated structure and detailed texture. In particular, yellow arrows in <ref type="figure" target="#fig_2">Fig. S3(i)</ref> indicate the pyramidal tract that is a bundle of nerve fibers to control the motor functions of the body, by conducting impulses from the motor cortex of the brain to the brainstem or spinal cord. Although it normally shows mild hyperintensity on T2-weighted images, the brighter T2 signal intensity of the pyramidal tract on TV method compared to the groundtruth poses a risk to be mistaken for abnormal finding such as motor neuron disease, acute ischemia, or hypoglycemia. In addition, yellow arrows in <ref type="figure" target="#fig_2">Fig. S3</ref>(ii) shows internal cerebral veins. The proposed method clearly displays the two separate internal cerebral veins, whereas the comparative studies such as TV and image-domain learning can not differentiate two veins. <ref type="figure" target="#fig_3">Fig. S4</ref> shows the reconstruction results from spiral trajectory. In the TV method in <ref type="figure" target="#fig_3">Fig. S4(i)</ref>, there are remains of the bright dot-like artifacts along several slices. Yellow arrows in <ref type="figure" target="#fig_3">Fig. S4</ref>(ii) indicate small T2 hyperintensity lesions in the left frontal lobe. The proposed method clearly depicts the small sized lesions, but the TV method fails to show the lesions. The image-domain learning also preserves the lesions, but the margin of the lesion is blurry in the noisy background.   learning method outperforms the image-domain learning for the noise scales between 0% and 3%. In the noise scale beyond the 3%, the proposed k-space learning method shows more errors in PSNR and NMSE than the image-domain method, but the structural similarity (SSIM) still outperforms. However, these results were generated from the neural network trained with only noiseless data. If the noisy data are included in the training phase, the proposed method can directly learn the relationship between noisy to noiseless k-space mapping, which may improve the robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effects of weighting</head><p>Fig. S6 compares the reconstruction results by the nonweighted network and weighted network. The non-weighted network does not preserve the detail structures and has the blurring artifacts. However, weighted networks not only provide sharper images than weightless networks, but also provide the better quantitative performance as shown in <ref type="table" target="#tab_12">Table S3</ref>. <ref type="figure" target="#fig_4">Fig. S5</ref>: Quantitative comparison with respect to noise. Gaussian noise data has been added to the input from 0% to 5% of themaximum intensity of ground-truth. 0% indicates the noise-free case.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness to adversarial conditions</head><p>In <ref type="figure" target="#fig_6">Fig. S7</ref>, additional experiments are performed to address the adversarial attacks or robustness of DL-based MR reconstruction. Here, the network was trained with R = 3 acceleration factors in <ref type="figure" target="#fig_7">Figure S8</ref>, which is used to reconstruction images from other acceleration factors in <ref type="figure" target="#fig_7">Figure S8</ref>. The data was from cartesian trajectory with 8 coils. As shown in <ref type="figure" target="#fig_6">Figure S7</ref>, image-domain methods such as Variational Net, Cascade Net, and image-domain learning showed performance degradation in R = 2, but the performance was improved in Fourier-domain methods such as KIKI Net and Ours.</p><p>Overall, the Fourier-domain methods were robust than the image-domain methods with respect to various acceleration and sampling patterns that were not used during training. This again shows the advantages of the k-space learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. UNDERSTANDING GEOMETRY OF ENCODER-DECODER CNNS</head><p>Although we have revealed the relations between ALOHA and deep learning in a single image setting, it is not clear how these relations would translate when training is performed over multiple images. Specifically, when multiple images are considered, one may suspect that large dimensional subspaces would be required to approximate all of them. Since the subspaces are related to sparsity of each image, it is important to understand how would the learning be generalizable to images whose sparsity patterns may be very different. In addition, there are multiple skipped connection in U-Nets, which should be understood in the context of approximating all of the training data.</p><p>One may expect that the above learning from multiple images seem to have some relation to dictionary learning. For example, the sparsity prior in dictionary learning enables the selection of appropriate basis functions from the dictionary for each specific image, enabling adaptation/generalization to the specific image.</p><p>In fact, inspired by recent theoretical understanding of neural networks, in our companion paper <ref type="bibr">[2]</ref>, we provide a unified theoretical framework that leads to a better understanding of geometry of encoder-decoder CNNs. Our unified mathematical framework shows that encoder-decoder CNN architecture is closely related to nonlinear basis representation using combinatorial convolution frames, whose expressibility increases exponentially with the network depth. We also demonstrate the importance of skipped connection in terms of expressibility, and optimization landscape.</p><p>While the technical details can be found in our companion paper <ref type="bibr">[2]</ref>, here we summarize our findings to make the paper self-contained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Expressivity</head><p>For simplicity, consider encoder-decoder networks in <ref type="figure" target="#fig_8">Fig. S9</ref> which have symmetric configuration similar to U-Net. Specifically, the encoder network maps a given input signal x ∈ X ⊂ R d0 to a feature space z ∈ Z ⊂ R dκ , whereas the decoder takes this feature map as an input, process it and produce an output y ∈ Y ⊂ R d L . In this paper, symmetric configuration is considered so that both encoder and decoder have the same number of layers, say κ; the input and output dimensions for the encoder layer E l and the decoder layer D l are symmetric:</p><formula xml:id="formula_42">E l : R d l−1 → R d l , D l : R d l → R d l−1</formula><p>where l ∈ [κ] with [n] denoting the set {1, · · · , n}; and both input and output dimension is d 0 .</p><p>In fact, one of the important roles of using ReLU is that it allows combinatorial basis selection such that exponentially large number of basis expansion is feasible once the network is trained. This is in contrast with the standard framelet basis estimation. For example, for a given target data Y = y (1) · · · y (M ) and the input data X = x (1) · · · x (M ) , the estimation problem of the frame basis and its dual without nonlinearity is optimal for the given training data, but the network is not expressive and does not generalize well when the different type of input data is given. Thus, one of the important requirements is to allow large number of expressions that are adaptive to the different inputs.</p><p>Indeed, ReLU nonlinearity serves to makes the network more expressive. For example, consider a trained two layer encoder-decoder CNN:</p><formula xml:id="formula_43">y =BΛ(x)B x<label>(32)</label></formula><p>whereB ∈ R d0×d1 and B ∈ R d0×d1 and Λ(x) is a diagonal matrix with 0, 1 elements that are determined by the ReLU output. Now, the matrix can be equivalently represented bỹ</p><formula xml:id="formula_44">BΛ(x)B = d1 i=1 σ i (x)b i b i<label>(33)</label></formula><p>where σ i (x) refers to the (i, i)-th diagonal element of Λ(x). Therefore, depending on the input data x ∈ R d0 , σ i (x) is either 0 or 1 so that a maximum 2 d1 distinct configurations of the matrix can be represented using (33), which is significantly more expressive than using the single representation with the frame and its dual. This observation can be generalized as shown in the following theorem which can be found in <ref type="bibr">[2]</ref>. where E l and D l denote the matrix representation of the l-th layer convolution layer for encoder and decoder, respectively; Λ l (x) andΛ l (x) refer to the diagonal matrices from ReLU at the l-th layer encoder and decoder, respectively, which have 1 or 0 values; Λ l S (x) refers to a similarly defined diagonal matrices from ReLU at the l-th skipped branch of encoder. Then, the following statements are true. 1) Under ReLUs, an encoder-decoder CNN without skipped connection can be represented by</p><formula xml:id="formula_45">Theorem 4.1. [2] Let Υ l =Υ l (x) :=Υ l−1Λl (x)D l , (34) Υ l = Υ l (x) := Υ l−1 E l Λ l (x),<label>(35)</label></formula><formula xml:id="formula_46">y =B(x)B (x)x = i x, b i (x) b i (x)<label>(38)</label></formula><p>where</p><formula xml:id="formula_47">B(x) = Υ κ (x) ,B(x) =Υ κ (x)<label>(39)</label></formula><p>Furthermore, the maximum number of available linear representation is given by</p><formula xml:id="formula_48">N rep = 2 κ i=1 di−dκ ,<label>(40)</label></formula><p>2) An encoder-decoder CNN with skipped connection under ReLUs is given by <ref type="figure" target="#fig_8">Fig. S9</ref>: An architecture of κ-layer symmetric encoder-decoder CNN with skipped connections. Here, q l denotes the number of channels at the l-th layer, whereas m l refers to each channel dimension, and d l represents the total dimension of the feature at the l-th layer.</p><formula xml:id="formula_49">y =B skp (x)B skp (x)x = i x, b skp i (x) b skp i (x) (41)</formula><p>where</p><formula xml:id="formula_50">B skp (x) := Υ κ Υ κ−1 M κ Υ κ−2 M κ−1 · · · M 1 (42) B skp (x) := Υ κΥκ−1MκΥκ−2Mκ−1 · · ·M 1<label>(43)</label></formula><p>Furthermore, the maximum number of available linear representation is given by</p><formula xml:id="formula_51">N rep = 2 κ i=1 di−dκ × 2 κ i=1 s k<label>(44)</label></formula><p>This implies that the number of representation increase exponentially with the network depth, which again confirm the expressive power of the neural network. Moreover, the skipped connection also significantly increases the expressive power of the encoder-decoder CNN. Another important consequence of Theorem 4.1 is that the input space X is partitioned into the maximum N rep non-overlapping regions so that inputs for each region shares the same linear representation.</p><p>Due to the ReLU, one may wonder whether the cascaded convolutional interpretation of the convolutional framelet still holds. A close look of the proof in <ref type="bibr">[2]</ref> reveals that this is still the case. Specifically, ReLUs provides spatially varying mask to the convolution filter so that the net effect is a convolution with the the spatially varying filters originated from masked version of convolution filters <ref type="bibr">[2]</ref>. This results in a spatially variant cascaded convolution, and only change in the interpretation is that the basis and its dual are composed of spatial variant cascaded convolution filters. Furthermore, the ReLU works to diversify the convolution filters by masking out the various filter coefficients. It is believed that this is another source of expressiveness from the same set of convolutional filters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Deep learning frameworks for accelerated MRI: (a) image domain learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>An example of R 2 input space partitioning for the case of two-channel three-layer ReLU neural network. Depending on input k-space data, a partition and its associated linear representation are selected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Overall reconstruction flows of the proposed method with (a) skipped connection, and (b) skipped connection and weighting layer. IFT denotes the inverse Fourier transform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>A network backbone of the proposed method. The input and output are complex-valued.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Various under-sampling patterns: (a) Cartesian undersampling at R = 3, (b) radial undersampling at R = 6, and (c) spiral undersampling at R = 4. Magnified views are provided for radial and spiral trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Objective functions of a single coil for (a) Cartesian, (b) radial, and (c) spiral trajectories. Dashed and solid lines indicate an objective function of the train and validation phase, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Reconstruction results from Cartesian trajectory at R = 3 in single coil. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>(i) Coronal and (ii) sagittal reformated reconstruction results from Cartesian trajectory at R = 3 in single coil.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Reconstruction results from Cartesian trajectory at R = 3 in multi coils: (a) axial, (b-i) coronal and (b-ii) sagittal reconstruction results. Yellow and red boxes illustrate the enlarged and difference views, respectively. The difference images were amplified five times. The number written to the images is the NMSE value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Reconstruction results from radial trajectory at R = 6 in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Reconstruction results from spiral trajectory at R = 4 in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>26) * Y. Han and J.C. Ye with the Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, Republic of Korea (e-mail: {hanyoseob,jong.ye}@kaist.ac.kr). L. Sunwoo is with the Department of Radiology, Seoul National University College of Medicine, Seoul National University Bundang Hospital, Seongnam, Republic of Korea. J.C. Ye is also with the Department of Mathematical Sciences, KAIST. † This work is supported by Korea Science and Engineering Foundation, Grant number NRF2016R1A2B3008104.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. S3 :</head><label>S3</label><figDesc>Reconstruction results from radial trajectory at R = 6 in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.C. Sensitivity with respect to noisy data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig</head><label></label><figDesc>. S5 shows the comparison results using noisy data for the case of single coil cartesian trajectory. The noise was generated as 0% to 5% of the maximum intensity of groundtruth, and added to the k-space domain. The proposed k-space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. S4 :</head><label>S4</label><figDesc>Reconstruction results from spiral trajectory at R = 4 in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. S6 :</head><label>S6</label><figDesc>Reconstruction results from Cartesian trajectory at R = 3 from single coil data. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference images, respectively. The number written in the images is the NMSE value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. S7 :</head><label>S7</label><figDesc>Quantitative comparison with respect to various acceleration factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. S8 :</head><label>S8</label><figDesc>Various sampling patterns. Except for a sampling pattern of R = 3 marked with a yellow box, no other patterns were used in the training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>withΥ 0</head><label>0</label><figDesc>(x) = I d0 and Υ 0 (x) = I d0 , andM l = M l (x) := S l Λ l S (x) (36) M l =M l (x) :=Λ l (x)S l(37)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>Quantitative comparison from Cartesian trajectory at R = 3 in single coil.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table /><note>Quantitative comparison from Cartesian trajectory at R = 3 in 8 coils parallel imaging.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Quantitative comparison from radial undersampling at R = 6 in 8 coils parallel imaging.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative comparison from spiral undersampling at R = 4 in 8 coils parallel imaging.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE S1 :</head><label>S1</label><figDesc>Quantitative comparison from radial undersampling at R = 6 in single coil.Fig. S2: Reconstruction results from spiral trajectory at R = 4 in single coil imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</figDesc><table><row><cell>Metric</cell><cell>Input ( 1 coil, ×4 )</cell><cell>Image-domain learning</cell><cell>Ours ( Fig. 2(a) )</cell></row><row><cell>PSNR [dB]</cell><cell>30.3733</cell><cell>42.0571</cell><cell>45.1545</cell></row><row><cell>NMSE (×10 −2 )</cell><cell>4.0373</cell><cell>0.2727</cell><cell>0.1391</cell></row><row><cell>SSIM</cell><cell>0.6507</cell><cell>0.9677</cell><cell>0.9480</cell></row><row><cell>Times (sec/slice)</cell><cell>-</cell><cell>0.0313</cell><cell>0.0393</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE S2 :</head><label>S2</label><figDesc>Quantitative comparison from spiral undersampling at R = 4 in single coil.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE S3 :</head><label>S3</label><figDesc>Quantitative comparison from cartesian undersampling at R = 3 in single coil.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating magnetic resonance imaging via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="514" to="517" />
		</imprint>
	</monogr>
	<note>Biomedical Imaging (ISBI)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a variational network for reconstruction of accelerated MRI data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klatzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Sodickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3055" to="3071" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for accelerated MRI using magnitude and phase networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning with domain adaptation for accelerated projection reconstruction MR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1002/mrm.27106</idno>
		<ptr target="https://doi.org/10.1002/mrm.27106" />
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep cascade of convolutional neural networks for dynamic mr image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="491" to="503" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image reconstruction by domain-transform manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="issue">7697</biblScope>
			<biblScope unit="page">487</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse MRI: The application of compressed sensing for rapid mr imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">k-t FOCUSS: A general compressed sensing framework for high resolution dynamic MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="116" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Calibrationless parallel imaging reconstruction based on structured low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ohliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Vigneron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="959" to="970" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-rank modeling of local k-space neighborhoods (loraks) for constrained mri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Haldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="668" to="681" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A general framework for compressed sensing and parallel MRI using annihilating filter based low-rank Hankel matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="480" to="495" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Off-the-grid recovery of piecewise constant images from few Fourier samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1004" to="1041" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MoDL: Model-based deep learning architecture for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="405" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compressive sampling using annihilating filter-based low-rank interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="777" to="801" />
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional framelets: A general deep learning framework for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="991" to="1048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scan-specific robust artificial-neural-networks for k-space interpolation (RAKI) reconstruction: Database-free deep learning for fast imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akçakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weingärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ugurbil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="439" to="453" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Off-the-grid model based deep learning (O-MODL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pramanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10747</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-shot sensitivityencoded diffusion MRI using model-based deep learning (MODL-MUSSELS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08115</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sampling signals with finite rate of innovation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1417" to="1428" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast algorithm for convolutional structured low-rank matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="535" to="550" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding geometry of encoderdecoder CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07647</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on International Conference on Machine Learning (ICML). also available as arXiv preprint</title>
		<meeting>the 2019 International Conference on International Conference on Machine Learning (ICML). also available as arXiv preprint</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for Matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kiki-net: crossdomain convolutional neural networks for reconstructing undersampled magnetic resonance images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep convolutional framelets: A general deep learning framework for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="991" to="1048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding geometry of encoder-decoder CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07647</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on International Conference on Machine Learning (ICML). also available as arXiv preprint</title>
		<meeting>the 2019 International Conference on International Conference on Machine Learning (ICML). also available as arXiv preprint</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
