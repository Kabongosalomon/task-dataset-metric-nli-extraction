<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification and understanding of cloud structures via satellite images with EfficientUNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Bangladesh</roleName><forename type="first">Tashin</forename><forename type="middle">Ahmed</forename><surname>Dhaka</surname></persName>
							<email>tashinahmed@aol.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noor</forename><forename type="middle">Hossain</forename><surname>Nuri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Bangladesh</roleName><forename type="first">Sabab</forename><surname>Dhaka</surname></persName>
						</author>
						<title level="a" type="main">Classification and understanding of cloud structures via satellite images with EfficientUNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-EfficientNet</term>
					<term>UNet</term>
					<term>clouds</term>
					<term>Dice coefficient</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Climate change has been a common interest and the forefront of crucial political discussion and decision-making for many years. Shallow clouds play a significant role in understanding the Earth's climate, but they are challenging to interpret and represent in a climate model. By classifying these cloud structures, there is a better possibility of understanding the physical structures of the clouds, which would improve the climate model generation, resulting in a better prediction of climate change or forecasting weather update. Clouds organise in many forms, which makes it challenging to build traditional rule-based algorithms to separate cloud features. In this paper, classification of cloud organization patterns was performed using a new scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet as the encoder and UNet as decoder where they worked as feature extractor and reconstructor of fine grained feature map and was used as a classifier, which will help experts to understand how clouds will shape the future climate. By using a segmentation model in a classification task, it was shown that with a good encoder alongside UNet, it is possible to obtain good performance from this dataset. Dice coefficient has been used for the final evaluation metric, which gave the score of 66.26% and 66.02% for public and private leaderboard on Kaggle competition respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Clouds play an important role in climate by controlling the amount of solar energy that reaches the surface of the Earth and the amount of the Earth's energy that is radiated back into space. The more energy that is trapped inside the planet, the warmer the atmosphere becomes, giving rise to sea level via meltdown of polar ice caps and contributing to global warming. The less energy that is trapped, the colder the temperature becomes. Understanding the structure of the clouds gives a better insight into the planet's weather. Hence it is crucial to climatologists <ref type="bibr" target="#b0">[1]</ref>. Albedo is a measure of how much energy is reflected without being absorbed <ref type="bibr" target="#b1">[2]</ref>. White surfaces reflect the most energy; hence it has a high albedo, while dark surfaces absorb most energy, indicating a low albedo. Earth's albedo is 0.3, which indicate the warming of the climate <ref type="bibr" target="#b2">[3]</ref>. Interpreting cloud structures provide useful insight into the abuse of Earth's climate and risks associated with it. Satellite images of clouds give a broader picture of the atmosphere, and interpreting the images provide information on the current situation of the planet.</p><p>It is assumed that as the overall temperature of Earth increases, it will evaporate more water from the oceans, resulting in more clouds with different structures and variations <ref type="bibr" target="#b3">[4]</ref>. The general effect of clouds on climate change depends on which cloud types change, and whether they become more or less abundant, thicker or thinner, and higher or lower in altitude. There are several types of cloud structures. Cirrus clouds are the most abundant of all top-level clouds. Cirrus means a "curl of hair" <ref type="bibr" target="#b4">[5]</ref>. These feathery clouds are composed of ice and consist of long, thin streamers that are also known as mare's tails. A few scattered cirrus clouds is a good sign of good weather. However, a gradually increasing cover of web-like cirrus clouds is a sign of more humid air mass and storm is approaching. Cirrostratus clouds look like thin scattered lines that spread themselves across the sky. When these icy fragments cover the sky, they give the air a subtle, white appearance. These clouds can indicate the approach of rain. They are translucent so that the sun and moon can be readily seen through them. Cirrostratus clouds are usually visible 12 to 24 hours before a period of rain or snow <ref type="bibr" target="#b5">[6]</ref>. Cirrocumulus is another type of high cloud, which tend to be broad groupings of white streaks that are sometimes seemingly neatly aligned. During the summertime in the tropics, they could indicate an incoming of a hurricane <ref type="bibr" target="#b6">[7]</ref>. There are many more forms of clouds which determine the weather and provides an indication of any natural disaster, which can be handled in a low-risk manner if correctly detected beforehand.</p><p>Formation of clouds and detecting their structures and patterns beforehand allows a lot of high-risk activities to be avoided previously. Aircraft flights are a high-risk activity that carries thousands of passengers at a given interval of time, and flying through clouds is similar to driving a car through a thick fog -it is difficult to see what is ahead, making it a challenging maneuver to accomplish, as a conequence learning about cloud formations and their potential dangers when flying is a vital part of pilot training in some countries <ref type="bibr" target="#b7">[8]</ref>. Cloud structures like cumulonimbus are a direct threat to aircraft <ref type="bibr" target="#b8">[9]</ref>. Cloud-borne updrafts and downdrafts result in rapid and unpredictable changes to the lift force on aircraft wings. These changes cause the plane to lurch and jump about during flight, known as turbulence, which sometimes makes less experienced pilots lose control of the craft, and the result is often fatal, with high casualties. Cargo ships, on the other hand, rely a lot on the weather of the sea, which is often unpredictable and ever-changing. Cargoes usually have a tight schedule and delay causes a loss of hundreds of thousands of dollars in fuel consumption. Storms also delay shipping which contributes to the loss of millions of dollars. An early prediction of storms or change in weather saves many lives and millions of dollars.</p><p>Interpretation of satellite images of cloud structures requires the expertise of a well-trained meteorologist, although it is not feasible and not always readily available. An intelligent automated system to interpret the satellite images, therefore, becomes a promising alternative with ease of access and hence becomes quite desirable for the understanding of cloud structures. In this paper, a model has been developed to classify the clouds into four categories using satellite images, with classification architecture like EfficientNet and segmentation architecture UNet [10] <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATASET</head><p>The dataset consists of satellite images, courtesy of NASA Worldview, gathered from Kaggle competition "Understanding Clouds from satellite images". The images contain clouds of four classes namely: Fish, Flower, Sugar and Gravel. The images were taken from three regions, spanning 21 • longitude and 14 • latitude. The true-color images were taken from two polar-orbiting satellites, Terra and Aqua. An image might be attached from two orbits, due to the small footprint of Moderate Resolution Imaging Spectroradiometer (MODIS) onboard these satellites. The remaining area, which has not been covered by two succeeding orbits, is marked black, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The dataset was split into train and validation of 80:20. The training sample was perfectly balanced with 22184 images, which consists of 5546 × 4 images, where each class has 5546 images. All images are of the same size, which is 1400 × 2100 pixels. 68 scientists labelled images in the train set, and 3 individual ones labelled each image. Several augmentation techniques were applied on the images, which is a process used to artificially expand the size of a training dataset by creating modified versions of images in the dataset, which improves the performance and ability of the model to generalise. Albumentations library has been used for augmentation <ref type="bibr" target="#b11">[12]</ref>. This library efficiently implements an abundant variety of image transform operations that are optimized for performance. The images were augmented into four types: horizontal flip, vertical flip, random rotation 20 • and grid distortion. Also some adjustment have been done in image size to feed into efficentUNet architecture mentioned in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MATERIALS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Public and Private Leaderboard Score</head><p>Since the dataset was gathered from a Kaggle competition, there were two sets of scores that competed among others. Public LB 1 scores are those which are shown while the competition is ongoing. It shows the outcome from a subset of the test dataset. Private LB scores get generated after the competition is over, that provides scores on the remaining test dataset. As a result, public LB scores are usually better than the private. For this particular competition, private score was calculated on 75% of the test data.</p><p>Pixel encoding technique was followed to participate in the submission of the competition since the image sizes were too large for Kaggle system <ref type="bibr" target="#b12">[13]</ref>. As a result, instead of submitting an exhaustive list of indices for segmentation, pairs of values were submitted, which contained the start position and the run length of the image pixels. For example, a pair value of (1, 3) indicates that the pixel starts at 1 and run 3 pixels. The competition also required a space-delimited list of pairs. The predicted encodings were scaled by 0.25 per side, which scaled down the images of size 1400 × 2100 pixels in both train and test set to 350×525 pixels, hence allowing the scope to achieve reasonable submission evaluation times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dice coefficient (DSC)</head><p>In this paper, the evaluation metric used is the Dice coefficient to measure the quality of the model, as instructed by Kaggle competition. It was used to compare the pixel-wise agreement between a predicted segmentation and corresponding ground truth, using the following equation:</p><formula xml:id="formula_0">DSC = 2 * |X ∩ Y | |X|+|Y |<label>(1)</label></formula><p>Here X is the predicted set of pixels, and Y is the ground truth. The dice coefficient is defined to be 1 when both X and Y are empty. The LB score is the mean of the Dice coefficients for each (Image, Label) pair in the test set.</p><p>Dice coefficients are slightly different from the more popular evaluation metric: accuracy of a model. They are used to quantify the performance of image segmentation methods. Some ground truth regions are annotated in the images, and then an automated algorithm is allowed to do it. The algorithm is validated by calculating the dice score, which is a measure of how similar the objects are and is calculated by the overlap of the two segmentations divided by the total size of the two objects <ref type="bibr" target="#b13">[14]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimizer</head><p>Rectified Adam (RAdam) was used instead of Adam as an optimizer for high accuracy and fewer epochs <ref type="bibr" target="#b14">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>Categorical Cross-entropy (CCE) has been applied as a loss function as it is a multi-label classification task. It is designed to quantify the difference between two probability distributions.</p><formula xml:id="formula_1">Loss = − output size i=1 y i · logŷ i</formula><p>Hereŷ i is the i-th scalar value in the model output, y i denotes the corresponding target value, and output size gives the number of scalar values in the model output. This function is important to measure and distinguish two discrete probability distribution. y i specifies probability that event i occurred and sum of all y i is 1, indicating that precisely one event occurred. The negative sign ensures that the loss gets smaller when the distributions get closer to each other.</p><p>Softmax activation function is recommended with categorical crossentropy, which rescales the model output to ensure it has the right properties, as positive outputs are desirable so that logarithm of every output valueŷ i exists. The main appeal of this loss function is to compare two probability distributions. For the classification part loss score was calculated from the CCE and for the segmentation part it was calculated as CCE × 0.7 + DICE × 0.3. DSC is a measure of overlap between corresponding pixel values of prediction and ground truth respectively. Range of DSC is between 0 and 1 as it is understandable from Subsection III-B and the larger the better. So, Dice Loss (DICE) try to maximize the overlap between the above mentioned two sets (predcitions and ground truth pixel values) <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DESCRIPTION OF APPLIED ARCHITECTURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EfficientNet</head><p>EfficientNet presented by Google AI research is considered as a group of CNN models, but with subtle improvements, it works better than its predecessors <ref type="bibr" target="#b9">[10]</ref>. It consists of 8 models from B0 to B7, where each subsequent model number refers to variants with more parameters and higher accuracy. EfficientNet works in three ways:</p><p>• Depthwise + Pointwise Convolution: Depthwise convolution performs independently over each channel of input. This is a spatial convolution. Pointwise convolution projects the channel's output by the depthwise convolution onto a new channel space. This is a 1×1 convolution. • Inverse Res: ResNet blocks consist of a layer that squeezes the channels a layer that extends the channels. In this way, it links skip connections to rich channel layers <ref type="bibr" target="#b16">[17]</ref>.</p><p>• Linear Bottlneck: In each block, it uses linear activation in the last layer to prevent loss of information from ReLU <ref type="bibr" target="#b17">[18]</ref>.</p><p>Among the 8 models of EfficientNet, 6 models, namely from B0 to B5, were explored in this paper. Due to the rise in complexity, the remaining models were ignored as they produced underappreciated results with poor performance while absorbing precious runtime.</p><p>As mentioned earlier, EfficientNet has 8 models, B0 -B7, among which, first 6 models have been explored in this paper. The layers in each of the models (B0 -B7) can be created by using 5 standard modules shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>• Module 1 acts as the starting block for the sub-blocks. • Module 2 acts as the initializing point for the first subblock of all the 7 main blocks except the 1st block. • Module 3 is used as a skip connection block for all the sub-blocks. • Module 4 combines the skip connections that occurred in the first sub-blocks. • Module 5 combines each sub-block that is connected to its previous sub-block in a skip connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depthwise Conv2D</head><p>Batch  The individual modules are further used in various order to create sub-blocks, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Its easy to observe the difference among the models, with a gradual increase in the number of sub-blocks. The main building block for EfficientNet is MBConv layer which is an inverted residual block originally applied in MobileNetV2 <ref type="bibr" target="#b18">[19]</ref>. Basic building block of EfficientNet-B0 with respect to MBConv layers have been shown in <ref type="figure">Figure 3</ref>. The 8 models of EfficientNet (B0 -B7) share the common blocks with subtle complexities in their architectures.</p><p>EfficientNet is a scaled-up neural network architecture, where the models scale all dimensions with a compound coefficient, which is a newly proposed method known as compound scaling <ref type="bibr" target="#b19">[20]</ref>. Here scaling up is defined as a systematic, principled scaling of three factors, which are depth, width and resolution. Every architecture has similarity with its earlier versions. The only difference is the different feature maps that increase the number of parameters. All the models have the same architecture as its previous one, except for the multiplied block (x2) that expands and covers more blocks. This gives a lot of parameters to be used in a calculation, making it a very robust model. Its not difficult to observe the changes among all the models, and they gradually increased the number of sub-blocks <ref type="bibr" target="#b9">[10]</ref>. Starting from EfficientNet-B0, compound scaling method was used to scale up with two steps:</p><p>• Step 1 coefficient was fixed to 1 assuming twice more resources to be available, and it enforces a small grid search for the networks depth, width and resolution constants.</p><formula xml:id="formula_2">• Step 2</formula><p>The constants then get fixed and scaled up the baseline network with different coefficient to obtain the successive versions from B1 to B7. EfficientNet was prioritized in this paper due to limitations of Kaggle notebook as well. It was also the core reason why the remaining EfficientNet models were avoided, since they calculate a substantial amount of parameters, which takes a lot of processing power and time, producing a disappointing outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. U-Net</head><p>U-Net is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to produce more precise segmentation. The idea here is to enhance a contracting layer by successive layers, where pooling operations are replaced by upsampling operators and these layers increase the resolution of the output.</p><p>A successive convolutional layer then learn to assemble a precise output based on this information <ref type="bibr" target="#b10">[11]</ref>.</p><p>U-Net have a large number of feature channels in the upsampling part and it allows the network to propagate context information to higher resolution layers. As a result, the expansive path is more or less symmetric to the contracting part, and produces a u-shaped architecture <ref type="bibr" target="#b20">[21]</ref>.</p><p>The network consists of a contracting path and an expansive path, which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of upconvolutions and concatenations with high-resolution features from the contracting path <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EFFICIENTUNET</head><p>In conventional UNet, expansion path is nearly symmetric to the contracting path. In this work, EfficientNet was used as an encoder in contracting path instead of conventional set of convolution layers. The decoder module is similar to the original UNet. Details of the proposed architecture are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The original input image size is 1400×2100 then resized the images to 1312 × 2080 for further processing. The number of levels, resolution and number of channels of each feature map is also shown in the <ref type="figure" target="#fig_1">Figure 2</ref>. First, the feature map of last logit of the encoder was upsampled bi-linearly by a factor of two and then concatenated with the feature map from encoder having same spatial resolution. It was followed by 33 convolution layers before again upsampling by the factor of two. The process was repeated till the segmentation map of size equal to input image was reconstructed. The proposed architecture is asymmetric unlike the original UNet. Here, the contracting path is deeper than the expansion path. Inclusion of powerful CNN like EfficientNet as encoder improved overall performance of the algorithm <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. METHODS, RESULTS AND DISCUSSION</head><p>Experimentation has been undertaken by applying six different versions of EfficentNet architectures mentioned previously. Mean Validation Accuracy (mVA) has been measured for all EfficentNet architectures for both baseline and fine-tuned versions. The outcome of this part of the inspection has been presented in <ref type="table">Table I</ref> The mean validation accuracy (mVA) score was best for the B0 model of EfficientNet architecture, as shown in <ref type="table">Table I</ref>, although the other versions (B1-B5) came close on both private and public leaderboard scores, as shown in <ref type="table">Table II</ref>. The cross validation scores weren't stable since the image segmentation wasn't reliable, with fluctuations in the outcome, having only 63.89% in B0 while the other models had a higher value, as a result it was important to use an efficient segmentation approach to reach a stable and accurate result, as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper implements classification of satellite images of cloud structures into four different classes: Sugar, Gravel, Fish and Flower. 6 versions of EfficientNet from B0 to B5 were used as encoder and UNet as decoder has been applied. Dice coefficient was used as the evaluation metric. The scores used were compared with both public and private LB scores of Kaggle competition. By using a segmentation model like UNet in a classification problem, it was proven that with a good encoder it is possible to achieve good performance from the dataset. Although EfficientNet was used in this paper, it could be replaced with a different model as well but was not tested in this research. Also, a good segmentation of the images boost the output of the classification drastically, which was also proven in this paper. In future, estimation of the distribution of classes and adjustment of the validation set could be implemented accordingly, although it would only be ideal for the competition. As the complex architectures of EfficientNet gave less appreciating results because of default coefficients, altering these hyperparameters according to the dataset will hopefully improve the outcome. Exploring gradient weighted class activation mapping to generate a baseline, which is a class explainability technique, could be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PR Curves</head><p>Precision-Recall(PR) curve demonstrates the relationship between precision (positive predictive value) and recall (sensitivity) of a machine learning model. Precision gives the percentage of the relevant outcome, while recall indicates the total consistent result. PR curve is a vital evaluation metric as it provides a more informative picture of an algorithm's performance. X-axis shows recall while the Y-axis shows precision.</p><p>The precision-recall curve shows the trade-off between precision and recall for different threshold. A high area under the curve shows both high recall and high precision, where high precision relates to a low false-positive rate, and high recall refers to a flat false-negative rate. Since cloud structures are complex, it was essential to understand whether the implemented model was correctly detecting the shapes and evaluating true positive, true negative scores appropriately, hence PR curve was a crucial evaluation metric to understand the learning rate of the model.</p><p>Precision-Recall curves for all four classes are shown below. Highest precision (0.74) corresponding to recall threshold was obtained for class: Sugar and lowest for class: Fish which was 0.55, while the maximum and minimum recall for class: Flower was 0.49 and class: Fish was 0.22. <ref type="figure">Fig. 6</ref>. PR-Curves for All Four Classes PR-AUC; EfficientNet-B0 It takes nearly 25 epochs to train the model to reach the best result while mean PR AUC still increases even after 30 epochs for both train and validation set, showed in <ref type="figure" target="#fig_4">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Graph; EfficientNet-B0</head><p>Train and validation loss graph of EfficentNet-B0 which clearly shows that EfficientNet architecture is not enough to train this data as the loss is not decreasing. Loss Graph; EfficientUNet-B0 Applying EfficientNet as encoder gave a huge success than only EfficientNet as segmentation model, since UNet was able to do the segmentation way better than a regular classification model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Masked Images of four different class samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of EfficientUNet with EfficientNet-B0 framework for semantic segmentation. Blocks of EfficientNet-B0 as encoder has been shown inFigure 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Common modules used to implement layers of all 8 models of EfficientNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•Fig. 5 .</head><label>5</label><figDesc>Width scale adds more feature maps in each layer. • Depth scale adds more layers to the network. • Resoultion scale increase resolution of input images. Sub-blocks using individual modules presented inFigure 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Training and Validation PR-AUC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Training and Validation Loss (CCE) for EfficentNet-B0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Training and Validation Loss (DICE) for EfficentUNet-B0Segmentation Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>656 X 1040 X 32 656 X 1040 X 16 328 X 520 X 24 164 X 260 X 40 164 X 260 X 80 82 X 130 X 112 41 X 65 X 192 41 X 65 X 320 82 X 130 X 256 164 X 260 X 128 328 X 520 X 64 656 X 1040 X 32 1312 X 2080 X 16 1312 X 2080 X 4</head><label></label><figDesc></figDesc><table><row><cell>1312 X 2080 X 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Image</cell></row><row><cell>Conv2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv2D</cell></row><row><cell>Block 1</cell><cell>Block 2</cell><cell>Block 3</cell><cell>Block 4</cell><cell>Concat</cell><cell>Up-Conv2D</cell><cell>Concat</cell><cell>Up-Conv2D</cell><cell>Concat</cell><cell>Up-Conv2D</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block 5</cell><cell>Up-Conv2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block 6</cell><cell>Concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block 7</cell><cell>Up-Conv2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1 LB: Leaderboard</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. DSC) has been presented inTable II.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Cross validation result along with the public and private LB</cell></row><row><cell></cell><cell></cell><cell>score (EfficientNet</cell><cell>Cross</cell><cell>LB score</cell></row><row><cell></cell><cell></cell><cell>Version</cell><cell>Validation</cell><cell>Private</cell><cell>Public</cell></row><row><cell></cell><cell></cell><cell>B0</cell><cell>0.6389</cell><cell>0.64595</cell><cell>0.65936</cell></row><row><cell></cell><cell></cell><cell>B1</cell><cell>0.6423</cell><cell>0.64640</cell><cell>0.65849</cell></row><row><cell></cell><cell></cell><cell>B2</cell><cell>0.6351</cell><cell>0.64551</cell><cell>0.65801</cell></row><row><cell></cell><cell></cell><cell>B3</cell><cell>0.6311</cell><cell>0.64585</cell><cell>0.65820</cell></row><row><cell></cell><cell></cell><cell>B4</cell><cell>0.6294</cell><cell>0.63911</cell><cell>0.65563</cell></row><row><cell></cell><cell></cell><cell>B5</cell><cell>0.6255</cell><cell>0.64059</cell><cell>0.65325</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell></row><row><cell></cell><cell></cell><cell cols="3">CROSS VALIDATION, PRIVATE AND PUBLIC DSC LB SCORES OF</cell></row><row><cell></cell><cell></cell><cell cols="3">EFFICIENTNET ARCHITECTURE FOR CLASSIFICATION</cell></row><row><cell>EfficientNet</cell><cell cols="2">mean validation accuracy</cell><cell></cell></row><row><cell>Version</cell><cell cols="2">Baseline Fine Tuned</cell><cell></cell></row><row><cell>B0</cell><cell>0.670</cell><cell>0.836</cell><cell></cell></row><row><cell>B1</cell><cell>0.656</cell><cell>0.829</cell><cell></cell></row><row><cell>B2</cell><cell>0.657</cell><cell>0.827</cell><cell></cell></row><row><cell>B3</cell><cell>0.659</cell><cell>0.825</cell><cell></cell></row><row><cell>B4</cell><cell>0.640</cell><cell>0.798</cell><cell></cell></row><row><cell>B5</cell><cell>0.601</cell><cell>0.732</cell><cell></cell></row><row><cell></cell><cell>TABLE I</cell><cell></cell><cell></cell></row><row><cell cols="3">MEAN VALIDATION ACCURACY SCORES</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III .</head><label>III</label><figDesc>It became visible that an improved segmentation of images stabilized the scores, with a gradual trend in the scores and model B0 showing the best response in both cross validation and LB values, with 66.54% in cross validation, 66.02% in private and 66.26% in public leaderboard. The other models werent far behind either, and the scores decreased in a descending order with B1 being the second-best model with 66.01% on cross validation, 65.53% on private and 65.98% on public leaderboard.</figDesc><table><row><cell>EfficientNet</cell><cell>Cross</cell><cell>LB score</cell><cell></cell></row><row><cell>Version</cell><cell>Validation</cell><cell cols="2">Private Public</cell></row><row><cell>B0</cell><cell>0.6654</cell><cell>0.6602</cell><cell>0.6626</cell></row><row><cell>B1</cell><cell>0.6601</cell><cell>0.6553</cell><cell>0.6598</cell></row><row><cell>B2</cell><cell>0.6589</cell><cell>0.6570</cell><cell>0.6578</cell></row><row><cell>B3</cell><cell>0.6588</cell><cell>0.6500</cell><cell>0.6582</cell></row><row><cell>B4</cell><cell>0.6425</cell><cell>0.6417</cell><cell>0.6421</cell></row><row><cell>B5</cell><cell>0.6322</cell><cell>0.6319</cell><cell>0.6333</cell></row><row><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="4">CROSS VALIDATION, PRIVATE AND PUBLIC DSC LB SCORES OF</cell></row><row><cell cols="4">EFFICIENTNETUNET FOR CLASSIFICATION</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank Kaggle for hosting such a great contest and for their kernel. We also thank Max Planck Institute for Meteorology for providing the dataset. We also admire Pavel Yakubovskiy for sharing his contribution in GitHub repository from where we managed to access the pretrained imagenet weights.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Thin liquid water clouds: Their importance and our challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vogelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cady-Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khaiyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liljegren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pollution and the planetary albedo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Twomey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmos. Environ</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1251" to="1256" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Changes in earth&apos;s reflectance over the past two decades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pallé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montanes-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koonin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5675</biblScope>
			<biblScope unit="page" from="1299" to="1301" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Increased insolation threshold for runaway greenhouse processes on earth-like planets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leconte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Forget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Charnay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wordsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pottier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">504</biblScope>
			<biblScope unit="issue">7479</biblScope>
			<biblScope unit="page" from="268" to="271" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A summary of the physical properties of cirrus clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="970" to="978" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noctilucent clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schröder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Noctilucent Clouds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cloud distributions in the vicinity of jet streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Mclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="579" to="583" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Impact of aerosol particles on cloud formation: Aircraft measurements in china</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmospheric Environment</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="672" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The ice particle threat to engines in flight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Strapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th AIAA Aerospace Sciences Meeting and Exhibit</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">206</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Method of encoding image pixel values for storage as compressed digital data and method of decoding the compressed digital data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">812</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations,&quot; in Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eff-unet: A novel architecture for semantic segmentation in unstructured environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Innani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gajre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Talbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="358" to="359" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
