<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linguistic Input Features Improve Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-27">27 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
							<email>rico.sennrich@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
							<email>bhaddow@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linguistic Input Features Improve Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-27">27 Jun 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder-decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-ofspeech tags, and syntactic dependency labels as input features to English↔German and English→Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An opensource implementation of our neural MT system is available 1 , as are sample files and configurations 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation has recently achieved impressive results <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b6">Jean et al., 2015)</ref>, while learning from raw, sentence-aligned parallel text and using little in the way of external linguistic information. <ref type="bibr">3</ref> However, we hypothesize that various levels of linguistic annotation can be valuable for neural machine translation. Lemmatisation can reduce data sparseness, and allow inflectional variants of the same word to explicitly share a representation in the model. Other types of annotation, such as parts-of-speech (POS) or syntactic dependency labels, can help in disambiguation. In this paper we investigate whether linguistic information is beneficial to neural translation models, or whether their strong learning capability makes explicit linguistic features redundant.</p><p>Let us motivate the use of linguistic features using examples of actual translation errors by neural MT systems. In translation out of English, one problem is that the same surface word form may be shared between several word types, due to homonymy or word formation processes such as conversion. For instance, close can be a verb, adjective, or noun, and these different meanings often have distinct translations into other languages. Consider the following English→German example:</p><p>1. We thought a win like this might be close.</p><p>2. Wir dachten, dass ein solcher Sieg nah sein könnte.</p><p>3. *Wir dachten, ein Sieg wie dieser könnte schließen.</p><p>For the English source sentence in Example 1 (our translation in Example 2), a neural MT system (our baseline system from Section 4) mistranslates close as a verb, and produces the German verb schließen (Example 3), even though close is an adjective in this sentence, which has the German translation nah. Intuitively, partof-speech annotation of the English input could disambiguate between verb, noun, and adjective meanings of close.</p><p>As a second example, consider the following German→English example: 4. Gefährlich ist die Route aber dennoch .</p><p>dangerous is the route but still .</p><p>5. However the route is dangerous .</p><p>6. *Dangerous is the route , however .</p><p>German main clauses have a verb-second (V2) word order, whereas English word order is generally SVO. The German sentence (Example 4; English reference in Example 5) topicalizes the predicate gefährlich 'dangerous', putting the subject die Route 'the route' after the verb. Our baseline system (Example 6) retains the original word order, which is highly unusual in English, especially for prose in the news domain. A syntactic annotation of the source sentence could support the attentional encoder-decoder in learning which words in the German source to attend (and translate) first.</p><p>We will investigate the usefulness of linguistic features for the language pair German↔English, considering the following linguistic features:</p><p>• lemmas</p><p>• subword tags (see Section 3.2)</p><p>• morphological features</p><formula xml:id="formula_0">• POS tags • dependency labels</formula><p>The inclusion of lemmas is motivated by the hope for a better generalization over inflectional variants of the same word form. The other linguistic features are motivated by disambiguation, as discussed in our introductory examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>We follow the neural machine translation architecture by <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>, which we will briefly summarize here.</p><p>The neural machine translation system is implemented as an attentional encoder-decoder network with recurrent neural networks.</p><p>The encoder is a bidirectional neural network with gated recurrent units <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> that reads an input sequence x = (x 1 , ..., x m ) and calculates a forward sequence of hidden states ( − → h 1 , ..., − → h m ), and a backward sequence</p><formula xml:id="formula_1">( ← − h 1 , ..., ← − h m ).</formula><p>The hidden states − → h j and ← − h j are concatenated to obtain the annotation vector h j .</p><p>The decoder is a recurrent neural network that predicts a target sequence y = (y 1 , ..., y n ). Each word y i is predicted based on a recurrent hidden state s i , the previously predicted word y i−1 , and a context vector c i . c i is computed as a weighted sum of the annotations h j . The weight of each annotation h j is computed through an alignment model α ij , which models the probability that y i is aligned to x j . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation.</p><p>A detailed description can be found in <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, although our implementation is based on a slightly modified form of this architecture, released for the dl4mt tutorial 4 . Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adding Input Features</head><p>Our main innovation over the standard encoderdecoder architecture is that we represent the encoder input as a combination of features (Alexandrescu and <ref type="bibr" target="#b0">Kirchhoff, 2006)</ref>.</p><p>We here show the equation for the forward states of the encoder (for the simple RNN case; consider <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref> for GRU):</p><formula xml:id="formula_2">− → h j = tanh( − → W Ex j + − → U − → h j−1 )<label>(1)</label></formula><p>where E ∈ R m×Kx is a word embedding matrix, − → W ∈ R n×m , − → U ∈ R n×n are weight matrices, with m and n being the word embedding size and number of hidden units, respectively, and K x being the vocabulary size of the source language.</p><p>We generalize this to an arbitrary number of features |F |:</p><formula xml:id="formula_3">− → h j = tanh( − → W ( |F | k=1 E k x jk ) + − → U − → h j−1 ) (2)</formula><p>where is the vector concatenation, E k ∈ R m k ×K k are the feature embedding matrices, with |F | k=1 m k = m, and K k is the vocabulary size of the kth feature. In other words, we look up separate embedding vectors for each feature, which are then concatenated. The length of the concatenated vector matches the total embedding size, and all other parts of the model remain unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linguistic Input Features</head><p>Our generalized model of the previous section supports an arbitrary number of input features.</p><p>In this paper, we will focus on a number of well-known linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoderdecoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP <ref type="bibr" target="#b19">(Toutanova et al., 2003;</ref><ref type="bibr" target="#b9">Minnen et al., 2001;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014)</ref> to annotate the English input for English→German, and ParZu <ref type="bibr" target="#b15">(Sennrich et al., 2013)</ref> to annotate the German input for German→English. We here discuss the individual features in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lemma</head><p>Using lemmas as input features guarantees sharing of information between word forms that share the same base form. In principle, neural models can learn that inflectional variants are semantically related, and represent them as similar points in the continuous vector space . However, while this has been demonstrated for high-frequency words, we expect that a lemmatized representation increases data efficiency; lowfrequency variants may even be unknown to wordlevel models. With character-or subword-level models, it is unclear to what extent they can learn the similarity between low-frequency word forms that share a lemma, especially if the word forms are superficially dissimilar. Consider the following two German word forms, which share the lemma liegen 'lie':</p><formula xml:id="formula_4">• liegt 'lies' (3.p.sg. present) • läge 'lay' (3.p.sg. subjunctive II)</formula><p>The lemmatisers we use are based on finite-state methods, which ensures a large coverage, even for infrequent word forms. We use the Zmorge analyzer for German <ref type="bibr" target="#b13">(Schmid et al., 2004;</ref><ref type="bibr" target="#b14">Sennrich and Kunz, 2014)</ref>, and the lemmatiser in the Stanford CoreNLP toolkit for English <ref type="bibr" target="#b9">(Minnen et al., 2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Subword Tags</head><p>In our experiments, we operate on the level of subwords to achieve open-vocabulary translation with a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE) <ref type="bibr" target="#b17">(Sennrich et al., 2016c)</ref>. We note that in BPE segmentation, some symbols are potentially ambiguous, and can either be a separate word, or a subword segment of a larger word. Also, text is represented as a sequence of subword units with no explicit word boundaries, but word boundaries are potentially helpful to learn which symbols to attend to, and when to forget information in the recurrent layers. We propose an annotation of subword structure similar to popular IOB format for chunking and named entity recognition, marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word. A separate tag (O) is used if a symbol corresponds to the full word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Morphological Features</head><p>For German→English, the parser annotates the German input with morphological features. Different word types have different sets of featuresfor instance, nouns have case, number and gender, while verbs have person, number, tense and aspect -and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">POS Tags and Dependency Labels</head><p>In our introductory examples, we motivated POS tags and dependency labels as possible disambiguators. Each word is associated with one POS tag, and one dependency label. The latter is the label of the edge connecting a word to its syntactic head, or 'ROOT' if the word has no syntactic head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">On Using Word-level Features in a Subword Model</head><p>We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units, and need no further modification. All other features are originally word-level features. To annotate the segmented source text with features, we copy the word's feature value to all its subword units. An example is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate our systems on the WMT16 shared translation task English↔German.  <ref type="bibr" target="#b17">(Sennrich et al., 2016c)</ref>, learning 89 500 merge operations on the concatenation of the source and target side of the parallel training data. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 <ref type="bibr" target="#b10">(Pascanu et al., 2013)</ref>. We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via BLEU and perplexity on a validation set (newstest2013).</p><p>For neural MT, perplexity is a useful measure of how well the model can predict a reference translation given the source sentence. Perplexity is thus a good indicator of whether input features provide any benefit to the models, and we report the best validation set perplexity of each experiment. To evaluate whether the features also increase translation performance, we report casesensitive BLEU scores with mteval-13b.perl on two test sets, newstest2015 and newstest2016. We also report CHRF3 <ref type="bibr" target="#b11">(Popović, 2015)</ref>, a character ngram F 3 score which was found to correlate well with human judgments, especially for translations out of English <ref type="bibr" target="#b18">(Stanojević et al., 2015)</ref>. <ref type="bibr">6</ref> The two metrics may occasionally disagree, partly because they are highly sensitive to the length of the output. BLEU is precision-based, whereas CHRF3 considers both precision and recall, with a bias for recall. For BLEU, we also report whether differences between systems are statistically significant according to a bootstrap resampling significance test <ref type="bibr" target="#b12">(Riezler and Maxwell, 2005)</ref>.</p><p>We train models for about a week, and report  results for an ensemble of the 4 last saved models (with models saved every 12 hours). The ensemble serves to smooth the variance between single models.</p><p>Decoding is performed with beam search with a beam size of 12.</p><p>To ensure that performance improvements are not simply due to an increase in the number of model parameters, we keep the total size of the embedding layer fixed to 500. <ref type="table" target="#tab_2">Table 1</ref> lists the embedding size we use for linguistic featuresthe embedding layer size of the word-level feature varies, and is set to bring the total embedding layer size to 500. If we include the lemma feature, we roughly split the embedding vector one-to-two between the lemma feature and the word feature. The table also shows the network vocabulary size; for all features except lemmas, we can represent all feature values in the network vocabulary -in the case of words, this is due to BPE segmentation. For lemmas, we choose the same vocabulary size as for words, replacing rare lemmas with a special UNK symbol. <ref type="bibr" target="#b16">Sennrich et al. (2016b)</ref> report large gains from using monolingual in-domain training data, auto-matically back-translated into the source language to produce a synthetic parallel training corpus. We use the synthetic corpora produced in these experiments 7 (3.6-4.2 million sentence pairs), and we trained systems which include this data to compare against the state of the art. We note that our experiments with this data entail a syntactic annotation of automatically translated data, which may be a source of noise. For the systems with synthetic data, we double the training time to two weeks.</p><p>We also evaluate linguistic features for the lower-resourced translation direction English→Romanian, with 0.6 million sentence pairs of parallel training data, and 2.2 million sentence pairs of synthetic parallel data.</p><p>We use the same linguistic features as for English→German.</p><p>We follow <ref type="bibr" target="#b15">Sennrich et al. (2016a)</ref> in the configuration, and use dropout for the English→Romanian systems. We drop out full words (both on the source and target side) with a probability of 0.1. For all other layers, the dropout probability is set to 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Table 2 shows our main results for German→English, and English→German. The baseline system is a neural MT system with only one input feature, the (sub)words themselves. For both translation directions, linguistic features improve the best perplexity on the development data (47.3 → 46.2, and 54.9 → 52.9, respectively). For German→English, the linguistic features lead to an increase of 1.5 BLEU (31.4→32.9) and 0.5 CHRF3 (58.0 → 58.5), on the newstest2016 test set. For English→German, we observe improvements of 0.6 BLEU (27.8 → 28.4) and 1.2 CHRF3 (56.0 → 57.2).</p><p>To evaluate the effectiveness of different linguistic features in isolation, we performed contrastive experiments in which only a single feature was added to the baseline. Results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Unsurprisingly, the combination of all features <ref type="table" target="#tab_4">(Table 2)</ref> gives the highest improvement, averaged over metrics and test sets, but most features are beneficial on their own. Subword tags give small improvements for English→German, but not for German→English. All other features outperform the baseline in terms of perplexity, and yield significant improvements in BLEU on at least 7 The corpora are available at http://statmt.org/rsennrich/wmt16_backtranslations/ one test set. The gain from different features is not fully cumulative; we note that the information encoded in different features overlaps. For instance, both the dependency labels and the morphological features encode the distinction between German subjects and accusative objects, the former through different labels <ref type="figure">(subj and obja)</ref>, the latter through grammatical case (nominative and accusative).</p><p>We also evaluated adding linguistic features to a stronger baseline, which includes synthetic parallel training data. In addition, we compare our neural systems against phrase-based (PB-SMT) and syntax-based (SBSMT) systems by <ref type="bibr" target="#b21">(Williams et al., 2016)</ref>, all of which make use of linguistic annotation on the source and/or target side.</p><p>Results are shown in <ref type="table" target="#tab_6">Table 4</ref>. For German→English, we observe similar improvements in the best development perplexity (45.2 → 44.1), test set BLEU (37.5→38.5) and <ref type="bibr">CHRF3 (62.2 → 62.8)</ref>. Our test set BLEU is on par to the best submitted system to this year's WMT 16 shared translation task, which is similar to our baseline MT system, but which also uses a right-to-left decoder for reranking <ref type="bibr" target="#b15">(Sennrich et al., 2016a)</ref>. We expect that linguistic input features and bidirectional decoding are orthogonal, and that we could obtain further improvements by combining the two.</p><p>For English→German, improvements in development set perplexity carry over (49.7 → 48.4), but we see only small, non-significant differences in BLEU and CHRF3. While we cannot clearly account for the discrepancy between perplexity and translation metrics, factors that potentially lower the usefulness of linguistic features in this setting are the stronger baseline, trained on more data, and the low robustness of linguistic tools in the annotation of the noisy, synthetic data sets. Both our baseline neural MT systems and the systems with linguistic features substantially outperform phrase-based and syntax-based systems for both translation directions.</p><p>In the previous tables, we have reported the best perplexity. To address the question about the randomness in perplexity, and whether the best perplexity just happened to be lower for the systems with linguistic features, we show perplexity on our development set as a function of training time for different systems <ref type="figure">(Figure 2)</ref>. We can see that perplexity is consistently lower for the systems     trained with linguistic features. <ref type="table" target="#tab_7">Table 5</ref> shows results for a lower-resourced language pair, English→Romanian. With linguistic features, we observe improvements of 1.0 BLEU over the baseline, both for the systems trained on parallel data only (23.8→24.8), and the systems which use synthetic training data (28.2→29.2). According to BLEU, the best submission to WMT16 was a system combination by <ref type="bibr" target="#b10">Peter et al. (2016)</ref>. Our best system is competitive with this submission. <ref type="table" target="#tab_9">Table 6</ref> shows translation examples of our baseline, and the system augmented with linguistic features. We see that the augmented neural MT systems, in contrast to the respective baselines, successfully resolve the reordering for the German→English example, and the disambiguation of close for the English→German example.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Linguistic features have been used in neural language modelling (Alexandrescu and <ref type="bibr" target="#b0">Kirchhoff, 2006)</ref>, and are also used in other tasks for which neural models have recently been employed, such as syntactic parsing <ref type="bibr" target="#b2">(Chen and Manning, 2014)</ref>. This paper addresses the question whether linguistic features on the source side are beneficial for neural machine translation. On the target side, linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time, and this is possible future work.</p><p>Among others, our model incorporates information from a dependency annotation, but is still a sequence-to-sequence model. <ref type="bibr">Eriguchi et al. (2016)</ref> propose a tree-to-sequence model whose encoder computes vector representations for each phrase in the source tree. Their focus is on exploiting the (unlabelled) structure of a syntactic annotation, whereas we are focused on the disambiguation power of the functional dependency labels.</p><p>Factored translation models are often used in phrase-based SMT <ref type="bibr" target="#b7">(Koehn and Hoang, 2007)</ref> as a means to incorporate extra linguistic information. However, neural MT can provide a much more flexible mechanism for adding such information. Because phrase-based models cannot easily generalize to new feature combinations, the individual models either treat each feature combination as an atomic unit, resulting in data sparsity, or assume independence between features, for instance by having separate language models for words and POS tags. In contrast, we exploit the strong generalization ability of neural networks, and expect that even new feature combinations, e.g. a word that appears in a novel syntactic function, are handled gracefully.</p><p>One could consider the lemmatized representation of the input as a second source text, and perform multi-source translation <ref type="bibr">(Zoph and Knight, 2016)</ref>. The main technical difference is that in our approach, the encoder and attention layers are shared between features, which we deem appropriate for the types of features that we tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we investigate whether linguistic input features are beneficial to neural machine translation, and our empirical evidence suggests that this is the case.</p><p>We describe a generalization of the encoder in the popular attentional encoder-decoder architecture for neural machine translation that allows for the inclusion of an arbitrary number of input features. We empirically test the inclusion of various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels, and morphological features, into English↔German, and English→Romanian neural MT systems. Our experiments show that the linguistic features yield improvements over our baseline, resulting in improvements on new-stest2016 of 1.5 BLEU for German→English, 0.6 BLEU for English→German, and 1.0 BLEU for English→Romanian.</p><p>In the future, we expect several developments that will shed more light on the usefulness of linguistic (or other) input features, and whether they will establish themselves as a core component of neural machine translation. On the one hand, the machine learning capability of neural architectures is likely to increase, decreasing the benefit provided by the features we tested. On the other hand, there is potential to explore the inclusion of novel features for neural MT, which might prove to be even more helpful than the ones we investigated, and the features we investigated may prove especially helpful for some translation settings, such as very low-resourced settings and/or translation settings with a highly inflected source language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Original dependency tree for sentence Leonidas begged in the arena ., and our feature representation after BPE segmentation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>root</cell><cell></cell><cell></cell><cell></cell><cell>root root</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>nsubj</cell><cell>prep</cell><cell></cell><cell>det pobj</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Leonidas</cell><cell>begged</cell><cell>in</cell><cell>the</cell><cell>arena</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NNP</cell><cell>VBD</cell><cell>IN</cell><cell>DT</cell><cell>NN</cell><cell>.</cell></row><row><cell></cell><cell>words</cell><cell></cell><cell>Le:</cell><cell>oni:</cell><cell>das</cell><cell></cell><cell>beg:</cell><cell>ged</cell><cell>in</cell><cell>the arena</cell><cell>.</cell></row><row><cell></cell><cell>lemmas</cell><cell></cell><cell cols="4">Leonidas Leonidas Leonidas</cell><cell>beg</cell><cell>beg</cell><cell>in</cell><cell>the arena</cell><cell>.</cell></row><row><cell></cell><cell cols="2">subword tags</cell><cell>B</cell><cell>I</cell><cell>E</cell><cell></cell><cell>B</cell><cell>E</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O</cell></row><row><cell></cell><cell>POS</cell><cell></cell><cell>NNP</cell><cell>NNP</cell><cell>NNP</cell><cell></cell><cell cols="2">VBD VBD</cell><cell>IN</cell><cell>DT</cell><cell>NN</cell><cell>.</cell></row><row><cell></cell><cell>dep</cell><cell></cell><cell>nsubj</cell><cell>nsubj</cell><cell>nsubj</cell><cell></cell><cell>root</cell><cell>root</cell><cell cols="2">prep det</cell><cell>pobj</cell><cell>root</cell></row><row><cell>Figure 1: To</cell><cell>enable</cell><cell cols="3">open-vocabulary</cell><cell>transla-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">tion, we encode words via joint BPE 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The parallel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">training data consists of about 4.2 million sentence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">pairs.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Vocabulary size, and size of embedding layer of linguistic features, in system that includes all features, and contrastive experiments that add a single feature over the baseline. The embedding layer size of the word feature is set to bring the total size to 500.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="11">: German↔English translation results: best perplexity on dev (newstest2013), and BLEU and</cell></row><row><cell cols="11">CHRF3 on test15 (newstest2015) and test16 (newstest2016). BLEU scores that are significantly different</cell></row><row><cell cols="6">(p &lt; 0.05) from respective baseline are marked with (*).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">German→English</cell><cell></cell><cell></cell><cell cols="3">English→German</cell><cell></cell></row><row><cell>system</cell><cell>ppl ↓</cell><cell cols="2">BLEU ↑</cell><cell cols="2">CHRF3 ↑</cell><cell>ppl ↓</cell><cell cols="2">BLEU ↑</cell><cell cols="2">CHRF3 ↑</cell></row><row><cell></cell><cell>dev</cell><cell cols="4">test15 test16 test15 test16</cell><cell>dev</cell><cell cols="4">test15 test16 test15 test16</cell></row><row><cell>baseline</cell><cell>47.3</cell><cell>27.9</cell><cell>31.4</cell><cell>54.0</cell><cell>58.0</cell><cell>54.9</cell><cell>23.0</cell><cell>27.8</cell><cell>52.6</cell><cell>56.0</cell></row><row><cell>lemmas</cell><cell>47.1</cell><cell>28.4</cell><cell>32.3*</cell><cell>54.6</cell><cell>58.7</cell><cell>53.4</cell><cell cols="2">23.8* 28.5*</cell><cell>53.7</cell><cell>56.7</cell></row><row><cell>subword tags</cell><cell>47.3</cell><cell>27.7</cell><cell>31.5</cell><cell>54.0</cell><cell>58.1</cell><cell>54.7</cell><cell cols="2">23.6* 28.1</cell><cell>53.2</cell><cell>56.4</cell></row><row><cell>morph. features</cell><cell>47.1</cell><cell>28.2</cell><cell>32.4*</cell><cell>54.3</cell><cell>58.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>POS tags</cell><cell>46.9</cell><cell>28.1</cell><cell>32.4*</cell><cell>54.1</cell><cell>57.8</cell><cell>53.2</cell><cell cols="2">24.0* 28.9*</cell><cell>53.3</cell><cell>56.8</cell></row><row><cell>dependency labels</cell><cell>46.9</cell><cell>28.1</cell><cell>31.8*</cell><cell>54.2</cell><cell>58.3</cell><cell>54.0</cell><cell cols="2">23.4* 28.0</cell><cell>53.1</cell><cell>56.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="11">: Contrastive experiments with individual linguistic features: best perplexity on dev (new-</cell></row><row><cell cols="11">stest2013), and BLEU and CHRF3 on test15 (newstest2015) and test16 (newstest2016). BLEU scores</cell></row><row><cell cols="9">that are significantly different (p &lt; 0.05) from respective baseline are marked with (*).</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">German→English</cell><cell></cell><cell></cell><cell cols="3">English→German</cell><cell></cell></row><row><cell>system</cell><cell>ppl ↓</cell><cell cols="2">BLEU ↑</cell><cell cols="2">CHRF3 ↑</cell><cell>ppl ↓</cell><cell cols="2">BLEU ↑</cell><cell cols="2">CHRF3 ↑</cell></row><row><cell></cell><cell>dev</cell><cell cols="4">test15 test16 test15 test16</cell><cell>dev</cell><cell cols="4">test15 test16 test15 test16</cell></row><row><cell>PBSMT (Williams et al., 2016)</cell><cell>-</cell><cell>29.9</cell><cell>35.1</cell><cell>56.2</cell><cell>60.9</cell><cell>-</cell><cell>23.7</cell><cell>28.4</cell><cell>52.6</cell><cell>56.6</cell></row><row><cell>SBSMT (Williams et al., 2016)</cell><cell>-</cell><cell>29.5</cell><cell>34.4</cell><cell>56.0</cell><cell>61.0</cell><cell>-</cell><cell>24.5</cell><cell>30.6</cell><cell>55.3</cell><cell>59.9</cell></row><row><cell>baseline</cell><cell>45.2</cell><cell>31.5</cell><cell>37.5</cell><cell>57.0</cell><cell>62.2</cell><cell>49.7</cell><cell>27.5</cell><cell>33.1</cell><cell>56.3</cell><cell>60.5</cell></row><row><cell>all features</cell><cell>44.1</cell><cell cols="2">32.1* 38.5*</cell><cell>57.5</cell><cell>62.8</cell><cell>48.4</cell><cell>27.1</cell><cell>33.2</cell><cell>56.5</cell><cell>60.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: German↔English translation results with additional, synthetic training data: best perplexity on</cell></row><row><cell>dev (newstest2013), and BLEU and CHRF3 on test15 (newstest2015) and test16 (newstest2016). BLEU</cell></row><row><cell>scores that are significantly different (p &lt; 0.05) from respective baseline are marked with (*).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: English→Romanian translation results:</cell></row><row><cell>best perplexity on newsdev2016, and BLEU and</cell></row><row><cell>CHRF3 on newstest2016. BLEU scores that are</cell></row><row><cell>significantly different (p &lt; 0.05) from respective</cell></row><row><cell>baseline are marked with (*).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Route aber dennoch. reference However the route is dangerous. baseline Dangerous is the route, however. all features However, the route is dangerous. source [We thought] a win like this might be close. reference [...] dass ein solcher Gewinn nah sein könnte. baseline [...] ein Sieg wie dieser könnte schließen. all features [...] ein Sieg wie dieser könnte nah sein.</figDesc><table><row><cell>system</cell><cell>sentence</cell></row><row><cell>source</cell><cell>Gefährlich ist die</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Translation examples illustrating the effect of adding linguistic input features.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/rsennrich/nematus 2 https://github.com/rsennrich/wmt16-scripts 3 Linguistic tools are most commonly used in preprocessing, e.g. for Turkish segmentation<ref type="bibr" target="#b5">(Gülçehre et al., 2015)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/nyu-dl/dl4mt-tutorial</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), and 644402 (HimL).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Factored Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirchhoff2006] Andrei Alexandrescu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Manning2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics. Eriguchi et al.2016</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Sequence Attentional Neural Machine Translation. ArXiv e-prints</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gülçehre</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<title level="m">Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On Using Monolingual Corpora in Neural Machine Translation. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Montreal Neural Machine Translation Systems for WMT&apos;15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factored Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="868" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>HLT-NAACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applied morphological processing of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Minnen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, USA; Hermann Ney, Matthias Huck, Fabienne Braune, Alexander Fraser, Aleš Tamchyna, Ondřej Bojar, Barry Haddow, Rico Sennrich, Frédéric Blain, Lucia Specia; Alex Waibel, Alexandre Allauzen, Lauriane Aufrant, Franck Burlot, Elena Knyazeva, Thomas Lavergne, François Yvon, and Marcis Pinnis; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-01" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
	<note>Proceedings of the First Conference on Machine Translation (WMT16)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">chrF: character ngram F-score for automatic MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Some Pitfalls in Automatic Evaluation and Significance Testing for MT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
	<note>Riezler and Maxwell2005. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A German Computational Morphology Covering Derivation, Composition, and Inflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IVth International Conference on Language Resources and Evaluation (LREC 2004)</title>
		<meeting>the IVth International Conference on Language Resources and Evaluation (LREC 2004)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1263" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zmorge: A German Morphological Lexicon Extracted from Wiktionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Kunz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014)</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation (LREC 2014)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Sennrich and Kunz2014</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting Synergies Between Open Resources for German Dependency Parsing, POS-tagging, and Morphological Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Volk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerold</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<editor>Rico Sennrich, Barry Haddow, and Alexandra Birch</editor>
		<meeting>the International Conference Recent Advances in Natural Language Processing<address><addrLine>Hissar, Bulgaria; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="601" to="609" />
		</imprint>
	</monogr>
	<note>Proceedings of the First Conference on Machine Translation (WMT16)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sennrich et al.2016b</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sennrich et al.2016c</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Results of the WMT15 Metrics Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Stanojević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="256" to="273" />
		</imprint>
	</monogr>
	<note>Miloš Stanojević, Amir Kamran, Philipp Koehn, and Ondřej Bojar</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Human Language Technology Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s Statistical Machine Translation Systems for WMT16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation (WMT16)</title>
		<meeting>the First Conference on Machine Translation (WMT16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-Source Neural Translation</title>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Zoph and Knight2016</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
