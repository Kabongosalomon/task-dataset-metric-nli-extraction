<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite great progress in supervised semantic segmentation, a large performance drop is usually observed when deploying the model in the wild. Domain adaptation methods tackle the issue by aligning the source domain and the target domain. However, most existing methods attempt to perform the alignment from a holistic view, ignoring the underlying class-level data structure in the target domain. To fully exploit the supervision in the source domain, we propose a fine-grained adversarial learning strategy for class-level feature alignment while preserving the internal structure of semantics across domains. We adopt a fine-grained domain discriminator that not only plays as a domain distinguisher, but also differentiates domains at class level. The traditional binary domain labels are also generalized to domain encodings as the supervision signal to guide the fine-grained feature alignment. An analysis with Class Center Distance (CCD) validates that our fine-grained adversarial strategy achieves better class-level alignment compared to other state-of-the-art methods. Our method is easy to implement and its effectiveness is evaluated on three classical domain adaptation tasks, i.e., GTA5→Cityscapes, SYNTHIA→Cityscapes and Cityscapes→Cross-City. Large performance gains show that our method outperforms other global feature alignment based and class-wise alignment based counterparts. The code is publicly available at https://github.com/JDAI-CV/FADA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of semantic segmentation <ref type="bibr" target="#b25">[26]</ref> in recent years is mostly driven by a large amount of accessible labeled data. However, collecting massive densely annotated data for training is usually a labor-intensive task <ref type="bibr" target="#b8">[9]</ref>. Recent advances in computer graphics provide an alternative for replacing expensive human labor. Through physically based rendering, we can obtain photo-realistic images with the pixel-level ground-truth readily available in an effortless way <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. However, performance drop is observed when the model trained with synthetic data (a source domain) is applied in realistic scenarios (a target domain), because the data from different domains usually share different distributions. This phenomenon is known as domain shift problem <ref type="bibr" target="#b26">[27]</ref>, which poses a challenge to cross-domain tasks <ref type="bibr" target="#b15">[16]</ref>. Domain adaptation aims to alleviate the domain shift problem by aligning the feature distributions of the source and the target domain. A group of works focus on adopting an adversarial framework, where a domain discriminator is trained to distinguish the target samples from the source ones, while the feature network tries to fool the discriminator by generating domain-invariant features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Although impressive progress has been achieved in domain adaptive semantic segmentation, most of prior works strive to align global feature distributions without paying much attention to the underlying structures among classes. However, as discussed in recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, matching the marginal feature distributions does not guarantee small expected error on the target domain. The class conditional distributions should also be aligned, meaning that class-level alignment also plays an important role. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the upper part shows the result of global feature alignment where the two domains are wellaligned but some samples are falsely mixed up. This motivates us to incorporate class information into the adversarial framework to enable fine-grained feature alignment. As illustrated in the bottom of <ref type="figure" target="#fig_0">Figure 1</ref>, features are expected to be aligned according to specific classes.</p><p>There have been some pioneering works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref> trying to address this problem. Chen et al. <ref type="bibr" target="#b6">[7]</ref> propose to use several independent discriminators to perform class-wise alignment, but independent discriminators might fail to capture the relationships between classes. Luo et al. <ref type="bibr" target="#b19">[20]</ref> introduce an self-adaptive adversarial loss to apply different weights to each region. However, in fact, they do not explicitly incorporate class information in their methods, which might fail to promote class-level alignment.</p><p>Our motivation is to directly incorporate class information into the discriminator and encourage it to align features at a fine-grained level. Traditional adversarial training has been proven effective for aligning features by using a binary domain discriminator to model distribution P (d|f ) (d refers to domain and f is the feature extracted from input data). By confusing such a discriminator, expecting P (d = 0|f ) ≈ P (d = 1|f ) where 0 stands for the source domain and 1 for the target domain, the features become domain invariant and well aligned. To further take classes into account, we split the output into multiple channels according to P (d|f ) = K c=1 P (d, c|f ) (where c refers to classes {1, . . . , K}). We directly model the discriminator as P (d, c|f ) to formulate a fine-grained domain alignment task. Although in the setting of domain adaptation the category-level labels for target domain are inaccessible, we find that the model predictions on target domain also contain class information and prove that it is possible to supervise the discriminator with the predictions on both domains. In the adversarial learning process, class information is incorporated and the features are expected to be aligned according to specific classes.</p><p>In this paper, we propose such a fine-grained adversarial learning framework for domain adaptive semantic segmentation (FADA). As illustrated in <ref type="figure" target="#fig_0">Figure  1</ref>, we represent the supervision of traditional discriminator at a fine-grained semantic level, which enables our fine-grained discriminator to capture rich classlevel information. The adversarial learning process is performed at fine-grained level, so the features are expected to be adaptively aligned according to their corresponding semantic categories. The class mismatch problem, which broadly exists in the global feature alignment, is expected to be further suppressed. Correspondingly, by incorporating class information, the binary domain labels are also generalized to a more complex form, called "domain encodings" to serve as the new supervision signal. Domain encodings could be extracted from the network's predictions on both domains. Different strategies of constructing domain encodings will be discussed. We conduct an analysis with Class Center Distance to demonstrate the effectiveness of our method regarding class-level alignment. Our method is also evaluated on three popular cross-domain benchmarks and presents new state-of-the-art results.</p><p>The main contributions of this paper are summarized below.</p><p>-We propose a fine-grained adversarial learning framework for cross-domain semantic segmentation that explicitly incorporates class-level information. -The fine-grained learning framework enables class-level feature alignment, which is further verified by analysis using Class Center Distance. -We evaluate our methods with comprehensive experiments. Significant improvements compared to other state-of-the-art methods are achieved on popular domain adaptive segmentation tasks including GTA5 → Cityscapes, SYNTHIA → Cityscapes and Cityscapes → Cross-City.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>Semantic segmentation is a task of predicting unique semantic label for each pixel of the input image. With the advent of deep convolutional neural networks, the academia of computer vision witnesses a huge progress in this field. FCN <ref type="bibr" target="#b25">[26]</ref> triggered the interests in introducing deep learning for this task. Many followup methods are proposed to enlarge the receptive fields to cover more context information <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b35">36]</ref>. Among all these works, the family of Deeplab <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> attracts a lot of attention and has been widely applied in many works for their simplicity and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain Adaptation</head><p>Domain adaptation strives to address the performance drop caused by the different distributions of training data and testing data. In the recent years, several works are proposed to approach this problem in image classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>. Inspired by the theoretical upper bound of risk in target domain <ref type="bibr" target="#b1">[2]</ref>, some pioneering works suggest to optimize some distance measurements between the two domains to align the features <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>. Recently, motivated by GAN <ref type="bibr" target="#b12">[13]</ref>, adversarial training becomes popular for its power to align features globally <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Adaptive Semantic Segmentation</head><p>Unlike domain adaptation for image classification task, domain adaptive semantic segmentation receives less attention for its difficulty even though it supports many important applications including autonomous driving in the wild <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. Based on the theoretical insight <ref type="bibr" target="#b1">[2]</ref> on domain adaptive classification, most works follow the path of shortening the domain discrepancy between the two domains. Large progress is achieved through optimization by adversarial training or explicit domain discrepancy measures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref>. In the context of domain adaptive semantic segmentation task, AdaptSegnet <ref type="bibr" target="#b29">[30]</ref> attempts to align the distribution in the output space. Inspired by CycleGAN <ref type="bibr" target="#b36">[37]</ref>, CyCADA <ref type="bibr" target="#b14">[15]</ref> suggests to adapt the representation in pixel-level and feature-level. There are also many works focusing on aligning different properties between two domains such as entropy <ref type="bibr" target="#b31">[32]</ref> and information <ref type="bibr" target="#b18">[19]</ref>.</p><p>Although huge progress has been made in this field, most of existing methods share a common limitation: Enforcing global feature alignment would inevitably mix samples with different semantic labels together when drawing two domains closer, which usually results in a mismatch of classes from different domains. CLAN <ref type="bibr" target="#b19">[20]</ref> is a pioneer work to address category-level alignment. It suggests applying different adversarial weight to different regions, but it does not directly and explicitly incorporate the classes into the model. Overview of the proposed fine-grained adversarial framework. Images from the source domain and target domain are randomly picked and fed to the feature extractor and the classifier. A segmentation loss is computed with the source predictions and the source annotations to help the segmentation network to generate discriminative features and learn task specific knowledge. The semantic features from both domains are fed to the convolutional fine-grained domain discriminator. The discriminator strives to distinguish the feature's domain information at a fine-grained class level using the domain encodings processed from the sample predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisit Traditional Feature Alignment</head><p>Semantic segmentation aims to predict per-pixel unique label for the input image <ref type="bibr" target="#b25">[26]</ref>. In an unsupervised domain adaptation setting for semantic segmentation, we have access to a collection of labeled data</p><formula xml:id="formula_0">X S = {(x (s) i , y (s) i )} ns i=1 in a source domain S, and unlabeled data X T = {x (t) j } nt</formula><p>j=1 in a target domain T where n s and n t are the numbers of samples from different domains. Domain S and domain T share the same K semantic class labels {1, . . . , K}. The goal is to learn a segmentation model G which could achieve a low expected risk on the target domain. Generally, segmentation network G could be divided into a feature extractor F and a multi-class classifier C, where G = C • F .</p><p>Traditional feature-level adversarial training relies on a binary domain discriminator D to align the features extracted by F on both domains. Domain adaptation is tackled by alternatively optimizing G and D with two steps:</p><p>(1) D is trained to distinguish features from different domains. This process is usually achieved by fixing F and C and solving:</p><formula xml:id="formula_1">min D L D = − ns i=1 (1 − d) log P (d = 0|f i ) − nt j=1 d log P (d = 1|f j )<label>(1)</label></formula><p>where f i and f j are the features extracted by F on source sample x (s) i and target sample x (t) j ; d refers to the domain variable where 0 refers to the source domain and 1 refers to the target domain. P (d|f ) is the probability output from the discriminator.</p><p>(2) G is trained with the task loss L seg on the source domain and the adversarial loss L adv on the target domain. This process requires fixing D and updating F and C:</p><formula xml:id="formula_2">min F,C L seg + λ adv L adv<label>(2)</label></formula><p>The cross-entropy loss L seg on source domain minimizes the difference between the prediction and the ground truth, which helps G to learn the task specific knowledge.</p><formula xml:id="formula_3">L seg = − ns i=1 K k=1 y (s) ik log p (s) ik ,<label>(3)</label></formula><p>where p ik is the entry for the one-hot label. The adversarial loss L adv is used to confuse the discriminator to encourage F to generate domain invariant features.</p><formula xml:id="formula_4">L adv = − nt j=1 log P (d = 0|f j )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-grained Adversarial Learning</head><p>To incorporate the class information into the adversarial learning framework, we propose a novel discriminator and enable a fine-grained adversarial learning process. The whole pipeline is illustrated in <ref type="figure">Figure 2</ref>.</p><p>The traditional adversarial training strives to align the marginal distribution by confusing a binary discriminator. To make the discriminator not merely focus on distinguishing domains, we split each of the two output channels of the binary discriminator into K channels and encourage a fine-grained level adversarial learning. With this design, the predicted confidence for domains is represented as a confidence distribution over different classes, which enables the new finegrained discriminator to model more complex underlying structures between classes, thus encouraging class-level alignment.</p><p>Correspondingly, the binary domain labels are also converted to a general form, namely domain encodings, to incorporate class information. Traditionally, the domain labels used for training the binary discriminator are <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref>  vector; 0 is an all-zero K-dimensional vector. The choices of how to generate domain knowledge a will be discussed in Section 3.3.</p><p>During the training process, the discriminator not only tries to distinguish domains, but also learns to model class structures. The L D in Equation 1 becomes:</p><formula xml:id="formula_5">L D = − ns i=1 K k=1 a (s) ik log P (d = 0, c = k|f i ) − nt j=1 K k=1 a (t) jk log P (d = 1, c = k|f j )<label>(5)</label></formula><p>where a </p><formula xml:id="formula_6">L adv = − nt j=1 K k=1 a (t) jk log P (d = 0, c = k|f j ),<label>(6)</label></formula><p>L adv is designed to maximize the probability of features from target domain being considered as the source features without hurting the relationship between features and classes.</p><p>The overall network in <ref type="figure">Figure 2</ref> is used in the training stage. During inference, the domain adaptation component is removed and one only needs to use the original segmentation network with the adapted weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extracting class knowledge for domain encodings</head><p>Now that we have a fine-grained domain discriminator, which could adaptively align features according to the class-level information contained in domain encodings, another challenge raises: how to get the class knowledge a in Equations 5 and 6 to construct domain encoding for each sample? Considering that in the unsupervised domain adaptive semantic segmentation task none of annotations in target domain is accessible, it seems contradictory to use the class knowledge on the target domain for guiding class-level alignment. However, during training, with ground-truth annotations from the source domain, the classifier C learns to map features into the semantic classes. Considering that the source domain and the target domain share the same semantic classes, it would be a natural choice to use the predictions of C as knowledge to supervise the discriminator.</p><p>As illustrated in equations 5 and 6, the class knowledge for optimizing the fine-grained discriminator works as the supervision signal. The choices of a jk are open to many possibilities. For specific tasks, people could design different forms to produce class knowledge with prior knowledge. Here we discuss two general solutions to extract class knowledge from network predictions for constructing domain encodings. Because the class-level knowledge for different domains could be extracted in the same way, in the following discussion we would use a k to represent kth entry for a single sample without differentiating the domain.</p><p>The one-hot hard labels could be a straightforward solution for generating knowledge, which could be denoted as:</p><formula xml:id="formula_7">a k = 1 if k = arg max k p k 0 otherwise<label>(7)</label></formula><p>where p k is the softmax probability output of C for class k. In this way, only the most confident class is selected. In practice, in order to remove the impact of noisy samples, we can select samples whose confidence is higher than a certain threshold and ignore those with low confidence. Another alternative is multi-channel soft labels, which has the following definition:</p><formula xml:id="formula_8">a k = exp ( z k T ) K j=1 exp ( zj T )<label>(8)</label></formula><p>where z k is kth entry of logits and T is a temperature to encourage soft probability distribution over classes. Note that during training, an additional regularization could also be applied. For example, we practically find that clipping the values of the soft labels by a given threshold achieves more stable performance because it prevents from overfitting to certain classes. An illustrative comparison of these two strategies with the traditional binary domain labels is presented in <ref type="figure">Figure 3</ref>. We also conduct experiments in section 4.6 to demonstrate the performance of different strategies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We present a comprehensive evaluation of our proposed method on three popular unsupervised domain adaptive semantic segmentation benchmarks, e.g., Cityscapes → Cross-City, SYNTHIA → Cityscapes, and GTA5 → Cityscapes. Cityscapes Cityscapes <ref type="bibr" target="#b8">[9]</ref> is a real-world urban scene dataset consisting of a training set with 2,975 images, a validation set with 500 images and a testing set with 1,525 images. Following the standard protocols <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref>, we use the 2,975 images from Cityscapes training set as the unlabeled target domain training set and evaluate our adapted model on the 500 images from the validation set. Cross-City Cross-City <ref type="bibr" target="#b6">[7]</ref> is an urban scene dataset collected with Google Street View. It contains 3,200 unlabeled images and 100 annotated images of four different cities respectively. The annotations of Cross-City share 13 classes with Cityscapes. SYNTHIA SYNTHIA <ref type="bibr" target="#b23">[24]</ref> is a synthetic urban scene dataset. We pick its subset SYNTHIA-RAND-CITYSCAPES, which shares 16 semantic classes with Cityscapes, as the source domain. In total, 9,400 images from SYNTHIA dataset are used as source domain training data for the task. GTA5 GTA5 dataset <ref type="bibr" target="#b22">[23]</ref> is another synthetic dataset sharing 19 semantic classes with Cityscapes. 24,966 urban scene images are collected from a physically-based rendered video game Grand Theft Auto V (GTAV) and are used as source training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>The metrics for evaluating our algorithm is consistent with the common semantic segmentation task. Specifically, we compute PSACAL VOC intersection-overunion (IoU) <ref type="bibr" target="#b10">[11]</ref> of our prediction and the ground truth label. We have IoU = TP TP+FP+FN , where TP, FP and FN are the numbers of true positive, false positive and false negative pixels respectively. In addition to the IoU for each class, a mIoU is also reported as the mean of IoUs over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Our pipeline is implemented by PyTorch <ref type="bibr" target="#b21">[22]</ref>. For fair comparison, we employ DeeplabV2 <ref type="bibr" target="#b3">[4]</ref> with VGG-16 <ref type="bibr" target="#b27">[28]</ref> and ResNet-101 <ref type="bibr" target="#b13">[14]</ref> as the segmentation base networks. All models are pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>. For the fine-grained discriminator, we adopt a simple structure consisting of 3 convolution layers with channel numbers {256, 128, 2K}, 3 × 3 kernels, and stride of 1. Each convolution layer is followed by a Leaky-ReLU <ref type="bibr" target="#b20">[21]</ref> parameterized by 0.2 except for the last layer.</p><p>To train the segmentation network, we use the Stochastic Gradient Descent (SGD) optimizer where the momentum is 0.9 and the weight decay is 10 −4 . The learning rate is initially set to 2.5 × 10 −4 and is decreased following a 'poly' learning rate policy with power of 0.9. For training the discriminator, we adopt the Adam optimizer with β 1 = 0.9, β 2 = 0.99 and the initial learning rate as 10 −4 . The same 'poly' learning rate policy is used. λ adv is constantly set to 0.001. Temperature T is set as 1.8 for all experiments. Regarding the training procedure, the network is first trained on source data for 20k iterations and then fine-tuned using our framework for 40k iterations. The batch size is eight. Four are source images and the other four are target images. Some data augmentations are used including random flip and color jittering to prevent overfitting. Although our model is already able to achieve new state-of-the-art results, we further boost the performance by using self distillation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref> and multi-scale testing. A detailed ablation study is conducted in Section 4.5 to reveal the effect of each component, which, we hope, could provide more insights into the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-art Methods</head><p>Small shift: Cross city adaptation. Adaptation between real images from different cities is a scenario with great potential for practical applications. <ref type="table" target="#tab_1">Table  1</ref> shows the results of domain adaptation on Cityscapes → Cross-City dataset. Our method has different performance gains for the four cities. On average over four cities, our FADA achieves 8.5% improvement compared with the source-only baselines, and 2.25% gain compared with the previous best method. Large shift: Synthetic to real adaptation. . Our FADA shows a better aligned structure in class-level compared with other state-of-the-art methods.</p><p>Cityscapes. FADA also obtains 15.5% and 12.4% improvement on different baelines for GTA5 → Cityscapes task. Besides, compared to the state-of-the-art feature-level methods, a general improvement of over 4% is witnessed. Note that as mentioned in <ref type="bibr" target="#b33">[34]</ref>, the "train" images in Cityscapes are more visually similar to the "bus" in GTA5 instead of the "train" in GTA5, which is also a challenge to other methods. Qualitative results for GTA5 → Cityscapes task are presented at <ref type="figure" target="#fig_6">Figure 5</ref>, reflecting that FADA also brings a significant visual improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Feature distribution</head><p>To verify whether our fine-grained adversarial framework aligns features on a class-level, we design an experiment to investigate to what degree the class-level features are aligned. Considering different networks map features to different feature spaces, it's necessarily to find a stable metric. CLAN <ref type="bibr" target="#b19">[20]</ref> suggests to use a Cluster Center Distance, which is defined as the ratio of intra-class distance between the trained model and the initial model, to measure class-level alignment degree. To better evaluate the effectiveness of class-level feature alignment on the same scale, we propose to modify the Cluster Center Distance to the Class Center Distance (CCD) by taking inter-class distance into account. The CCD for class i is defined as follows:</p><formula xml:id="formula_9">CCD(i) = 1 K − 1 K j=1,j =i 1 |Si| x∈Si x − µ i 2 µ i − µ j 2<label>(9)</label></formula><p>where µ i is the class center for class i, S i is the set of all features belonging to class i. With CCD, we could measure the ratio of intra-class compactness over inter-class distance. A low CCD suggests the features of same class are clustered densely while the distance between different classes is relatively large. We randomly pick 2,000 source samples and 2,000 target samples respectively, and compare the CCD values with other state-of-the-art methods: AdaptSegNet for global alignment and CLAN for class-wise alignment without explicitly modeling the class relationship. As shown in the <ref type="figure">Figure 4</ref>, FADA achieves a much lower CCD on most classes and get the lowest mean CCD value 1.1 compared to other algorithms. With FADA, we can achieve better class-level alignment and preserve consistent class structures between domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation studies</head><p>Analysis of different components. <ref type="table" target="#tab_5">Table 4</ref> presents the impact of each component on DeeplabV2 with ResNet-101 on GTA5 → Cityscapes task. The fine-grained adversarial training brings an improvement of 10.1%, which already makes it the new state of the art. To further explore the potential of the model, the self distillation strategy leads to an improvement of 2.3% and multi-scale testing further boosts the performance by 0.7%. Hard labels vs. Soft labels. As discussed in Section 3.3, the knowledge extracted from the classifier C could be produced from hard labels or soft labels.</p><p>Here we compare these two forms of label on GTA5 → Cityscapes and SYN-THIA → Cityscapes tasks with DeeplabV2 ResNet-101. For soft labels, we use "confidence clipping" with threhold 0.9 as regularization. For hard labels, we only keep high-confidence samples, while ignoring the samples with confidence lower than 0.9. The results are reported in <ref type="table" target="#tab_6">Table 5</ref>. Both choices give great boost to the baseline global feature alignment model. We observe that soft label is a more flexible choice and present more superior performance. Impact of Confidence Clipping. In our experiments, we use "confidence clipping" as a regularizer to prevent overfitting on noisy soft labels. The values of the confidence are truncated by a given threshold, therefore the values are not encouraged to heavily fit to a certain class. We test several thresholds and the results are shown in <ref type="table" target="#tab_7">Table 6</ref>. Note that when the threshold is 1.0, it means no regularization is used. We observe constant performance gain using the confidence clipping. The best result is found when the threshold is 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we address the problem of domain adaptive semantic segmentation by proposing a fine-grained adversarial training framework. A novel fine-  grained discriminator is designed to not only distinguish domains, but also capture category-level information to guide a fine-grained feature alignment. The binary domain labels used to supervise the discriminator are generalized to domain encodings correspondingly to incorporate class information. Comprehensive experiments and analysis validate the effectiveness of our method. Our method achieves new state-of-the-art results on three popular tasks, outperforming other methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Before adaptation After adaptation Ground-truth Acknowledgement: This work was partially supported by Beijing Academy of Artificial Intelligence (BAAI).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of traditional and our fine-grained adversarial learning. Traditional adversarial learning pursues the marginal distribution alignment while ignoring the semantic structure inconsistency between domains. We propose to use a fine-grained discriminator to enable class-level alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2: Overview of the proposed fine-grained adversarial framework. Images from the source domain and target domain are randomly picked and fed to the feature extractor and the classifier. A segmentation loss is computed with the source predictions and the source annotations to help the segmentation network to generate discriminative features and learn task specific knowledge. The semantic features from both domains are fed to the convolutional fine-grained domain discriminator. The discriminator strives to distinguish the feature's domain information at a fine-grained class level using the domain encodings processed from the sample predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(s)ik is the probability confidence of source sample x (s) i belonging to semantic class k predicted by C, y (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>jk are the kth entries of the class knowledge for the source sample i and target sample j. The adversarial loss L adv used to confuse the discriminator and guide the generation of domain-invariant features in Equation 4 becomes:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative segmentation results for GTA5 → Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and [0, 1] for the source and target domains respectively. The domain encodings are represented as a vector [a; 0] and [0; a] for the two domains respectively, where a is the knowledge extracted from the classifier C represented by a K-dimensional</figDesc><table><row><cell>Segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>Target</cell><cell>Binary</cell><cell cols="2">Domain encoding</cell><cell>Domain encoding</cell></row><row><cell>Domain</cell><cell>Domain</cell><cell>domain labels</cell><cell></cell><cell>hard labels</cell><cell>soft labels</cell></row><row><cell></cell><cell>Source flow</cell><cell cols="2">Target flow</cell><cell cols="2">Domain boundary</cell></row></table><note>Fig. 3: Illustration of different strategies to generate domain encodings. Here we compare three different strategies to extract knowledge from segmentation network for constructing domain encodings: binary domain labels, one-hot hard labels and multi channel soft labels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results for Cityscapes → Cross-City.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cityscapes → Cross-City</cell></row><row><cell cols="2">City Method</cell><cell cols="3">road sidewalk building light sign veg sky person rider car bus mbike bike mIoU</cell></row><row><cell></cell><cell cols="2">Source Dilation-Frontend 77.7 21.9</cell><cell>83.5</cell><cell>0.1 10.7 78.9 88.1 21.6 10.0 67.2 30.4 6.1 0.6 38.2</cell></row><row><cell></cell><cell>Cross-City [7]</cell><cell>79.5 29.3</cell><cell>84.5</cell><cell>0.0 22.2 80.6 82.8 29.5 13.0 71.7 37.5 25.9 1.0 42.9</cell></row><row><cell>Rome</cell><cell>Source DeepLab-v2</cell><cell>83.9 34.3</cell><cell cols="2">87.7 13.0 41.9 84.6 92.5 37.7 22.4 80.8 38.1 39.1 5.3 50.9</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell>83.9 34.2</cell><cell cols="2">88.3 18.8 40.2 86.2 93.1 47.8 21.7 80.9 47.8 48.3 8.6 53.8</cell></row><row><cell></cell><cell>FADA</cell><cell>84.9 35.8</cell><cell cols="2">88.3 20.5 40.1 85.9 92.8 56.2 23.2 83.6 31.8 53.2 14.6 54.7</cell></row><row><cell></cell><cell cols="2">Source Dilation-Frontend 69.0 31.8</cell><cell>77.0</cell><cell>4.7 3.7 71.8 80.8 38.2 8.0 61.2 38.9 11.5 3.4 38.5</cell></row><row><cell></cell><cell>Cross-City [7]</cell><cell>74.2 43.9</cell><cell>79.0</cell><cell>2.4 7.5 77.8 69.5 39.3 10.3 67.9 41.2 27.9 10.9 42.5</cell></row><row><cell>Rio</cell><cell>Source DeepLab-v2</cell><cell>76.6 47.3</cell><cell cols="2">82.5 12.6 22.5 77.9 86.5 43.0 19.8 74.5 36.8 29.4 16.7 48.2</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell>76.2 44.7</cell><cell cols="2">84.6 9.3 25.5 81.8 87.3 55.3 32.7 74.3 28.9 43.0 27.6 51.6</cell></row><row><cell></cell><cell>FADA</cell><cell>80.6 53.4</cell><cell>84.2</cell><cell>5.8 23.0 78.4 87.7 60.2 26.4 77.1 37.6 53.7 42.3 54.7</cell></row><row><cell></cell><cell cols="2">Source Dilation-Frontend 81.2 26.7</cell><cell>71.7</cell><cell>8.7 5.6 73.2 75.7 39.3 14.9 57.6 19.0 1.6 33.8 39.2</cell></row><row><cell></cell><cell>Cross-City [7]</cell><cell>83.4 35.4</cell><cell cols="2">72.8 12.3 12.7 77.4 64.3 42.7 21.5 64.1 20.8 8.9 40.3 42.8</cell></row><row><cell>Tokyo</cell><cell>Source DeepLab-v2</cell><cell>83.4 35.4</cell><cell cols="2">72.8 12.3 12.7 77.4 64.3 42.7 21.5 64.1 20.8 8.9 40.3 42.8</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell>81.5 26.0</cell><cell cols="2">77.8 17.8 26.8 82.7 90.9 55.8 38.0 72.1 4.2 24.5 50.8 49.9</cell></row><row><cell></cell><cell>FADA</cell><cell>85.8 39.5</cell><cell cols="2">76.0 14.7 24.9 84.6 91.7 62.2 27.7 71.4 3.0 29.3 56.3 51.3</cell></row><row><cell></cell><cell cols="2">Source Dilation-Frontend 77.2 20.9</cell><cell>76.0</cell><cell>5.9 4.3 60.3 81.4 10.9 11.0 54.9 32.6 15.3 5.2 35.1</cell></row><row><cell></cell><cell>Cross-City [7]</cell><cell>78.6 28.6</cell><cell cols="2">80.0 13.1 7.6 68.2 82.1 16.8 9.4 60.4 34.0 26.5 9.9 39.6</cell></row><row><cell>Taipei</cell><cell>Source DeepLab-V2</cell><cell>78.6 28.6</cell><cell cols="2">80.0 13.1 7.6 68.2 82.1 16.8 9.4 60.4 34.0 26.5 9.9 39.6</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell>81.7 29.5</cell><cell cols="2">85.2 26.4 15.6 76.7 91.7 31.0 12.5 71.5 41.1 47.3 27.7 49.1</cell></row><row><cell></cell><cell>FADA</cell><cell>86.0 42.3</cell><cell cols="2">86.1 6.2 20.5 78.3 92.7 47.2 17.7 72.2 37.2 54.3 44.0 52.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results for SYNTHIA → Cityscapes.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">SYNTHIA → Cityscapes</cell><cell></cell><cell></cell></row><row><cell cols="2">Backbone Method</cell><cell cols="6">Road SW Build Wall Fence Pole TL TS Veg. Sky PR Rider Car Bus Motor Bike mIoU mIoU*</cell></row><row><cell></cell><cell>FCNs in the wild [16]</cell><cell cols="6">11.5 19.6 30.8 4.4 0.0 20.3 0.1 11.7 42.3 68.7 51.2 3.8 54.0 3.2 0.2 0.6 20.2 22.9</cell></row><row><cell></cell><cell>CDA [34]</cell><cell cols="6">65.2 26.1 74.9 0.1 0.5 10.7 3.5 3.0 76.1 70.6 47.1 8.2 43.2 20.7 0.7 13.1 29.0 34.8</cell></row><row><cell></cell><cell>ST [38]</cell><cell cols="6">0.2 14.5 53.8 1.6 0.0 18.9 0.9 7.8 72.2 80.3 48.1 6.3 67.7 4.7 0.2 4.5 23.9 27.8</cell></row><row><cell></cell><cell>CBST [38]</cell><cell cols="6">69.6 28.7 69.5 12.1 0.1 25.4 11.9 13.6 82.0 81.9 49.1 14.5 66.0 6.6 3.7 32.4 35.4 36.1</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell>78.9 29.2 75.5 -</cell><cell>-</cell><cell>-</cell><cell>0.1 4.8 72.6 76.7 43.4 8.8 71.1 16.0 3.6 8.4</cell><cell>-</cell><cell>37.6</cell></row><row><cell>VGG-16</cell><cell>SIBAN [19] CLAN [20]</cell><cell>70.1 25.7 80.9 -80.4 30.7 74.7 -</cell><cell>--</cell><cell>--</cell><cell>3.8 7.2 72.3 80.5 43.3 5.0 73.3 16.0 1.7 3.6 1.4 8.0 77.1 79.0 46.5 8.9 73.8 18.2 2.2 9.9</cell><cell>--</cell><cell>37.2 39.3</cell></row><row><cell></cell><cell>AdaptPatch [31]</cell><cell cols="6">72.6 29.5 77.2 3.5 0.4 21.0 1.4 7.9 73.3 79.0 45.7 14.5 69.4 19.6 7.4 16.5 33.7 39.6</cell></row><row><cell></cell><cell>ADVENT [32]</cell><cell cols="6">67.9 29.4 71.9 6.3 0.3 19.9 0.6 2.6 74.9 74.9 35.4 9.6 67.8 21.4 4.1 15.5 31.4 36.6</cell></row><row><cell></cell><cell>Source only</cell><cell cols="6">10.0 14.7 52.4 4.2 0.1 20.9 3.5 6.5 74.3 77.5 44.9 4.9 64.0 21.6 4.2 6.4 25.6 29.6</cell></row><row><cell></cell><cell cols="7">Baseline (feat. only) [30] 63.6 26.8 67.3 3.8 0.3 21.5 1.0 7.4 76.1 76.5 40.5 11.2 62.1 19.4 5.3 13.2 31.0 36.2</cell></row><row><cell></cell><cell>FADA</cell><cell cols="6">80.4 35.9 80.9 2.5 0.3 30.4 7.9 22.3 81.8 83.6 48.9 16.8 77.7 31.1 13.5 17.9 39.5 46.0</cell></row><row><cell></cell><cell>SIBAN [19]</cell><cell>82.5 24.0 79.4 -</cell><cell>-</cell><cell cols="2">-16.5 12.7 79.2 82.8 58.3 18.0 79.3 25.3 17.6 25.9</cell><cell>-</cell><cell>46.3</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell>84.3 42.7 77.5 -</cell><cell>-</cell><cell>-</cell><cell>4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3</cell><cell>-</cell><cell>46.7</cell></row><row><cell></cell><cell>CLAN [20]</cell><cell>81.3 37.0 80.1 -</cell><cell>-</cell><cell cols="2">-16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7</cell><cell>-</cell><cell>47.8</cell></row><row><cell>ResNet-101</cell><cell>AdaptPatch [31] ADVENT [32]</cell><cell cols="6">82.4 38.0 78.6 8.7 0.6 26.0 3.9 11.1 75.5 84.6 53.5 21.6 71.4 32.6 19.3 31.7 40.0 46.5 85.6 42.2 79.7 8.7 0.4 25.9 5.4 8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2 48.0</cell></row><row><cell></cell><cell>Source only</cell><cell cols="6">55.6 23.8 74.6 9.2 0.2 24.4 6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 13.7 25.0 33.5 38.6</cell></row><row><cell></cell><cell cols="7">Baseline (feat. only) [30] 62.4 21.9 76.3 11.5 0.1 24.9 11.7 11.4 75.3 80.9 53.7 18.5 59.7 13.7 20.6 24.0 35.4 40.8</cell></row><row><cell></cell><cell>FADA</cell><cell cols="6">84.5 40.1 83.1 4.8 0.0 34.3 20.1 27.2 84.8 84.0 53.5 22.6 85.4 43.7 26.8 27.8 45.2 52.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results for GTA5 → Cityscapes.</figDesc><table><row><cell></cell><cell></cell><cell>GTA5 → Cityscapes</cell></row><row><cell cols="2">Backbone Method</cell><cell cols="2">Road SW Build Wall Fence Pole TL TS Veg. Terrain Sky PR Rider Car Truck Bus Train Motor Bike mIoU</cell></row><row><cell></cell><cell>FCNs in the wild [16]</cell><cell>70.4 32.4 62.1 14.9 5.4 10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8.0 7.3 0.0</cell><cell>3.5 0.0 27.1</cell></row><row><cell></cell><cell>CDA [34]</cell><cell cols="2">74.9 22.0 71.7 6.0 11.9 8.4 16.3 11.1 75.7 13.3 66.5 38.0 9.3 55.2 18.8 18.9 0.0 16.8 14.6 28.9</cell></row><row><cell></cell><cell>ST [38]</cell><cell cols="2">83.8 17.4 72.1 14.6 2.9 16.5 16.0 6.8 81.4 24.2 47.2 40.7 7.6 71.7 10.2 7.6 0.5 11.1 0.9 28.1</cell></row><row><cell></cell><cell>CBST [38]</cell><cell cols="2">90.4 50.8 72.0 18.3 9.5 27.2 28.6 14.1 82.4 25.1 70.8 42.6 14.5 76.9 5.9 12.5 1.2 14.0 28.6 36.1</cell></row><row><cell></cell><cell>CyCADA [15]</cell><cell>85.2 37.2 76.5 21.8 15.0 23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5</cell><cell>9.8 0.0 35.4</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell cols="2">87.3 29.8 78.6 21.1 18.2 22.5 21.5 11.0 79.7 29.6 71.3 46.8 6.5 80.1 23.0 26.9 0.0 10.6 0.3 35.0</cell></row><row><cell>VGG-16</cell><cell>SIBAN [19]</cell><cell cols="2">83.4 13.0 77.8 20.4 17.5 24.6 22.8 9.6 81.3 29.6 77.3 42.7 10.9 76.0 22.8 17.9 5.7 14.2 2.0 34.2</cell></row><row><cell></cell><cell>CLAN [20]</cell><cell cols="2">88.0 30.6 79.2 23.4 20.5 26.1 23.0 14.8 81.6 34.5 72.0 45.8 7.9 80.5 26.6 29.9 0.0 10.7 0.0 36.6</cell></row><row><cell></cell><cell>AdaptPatch [31]</cell><cell cols="2">87.3 35.7 79.5 32.0 14.5 21.5 24.8 13.7 80.4 32.0 70.5 50.5 16.9 81.0 20.8 28.1 4.1 15.5 4.1 37.5</cell></row><row><cell></cell><cell>ADVENT [32]</cell><cell cols="2">86.9 28.7 78.7 28.5 25.2 17.1 20.3 10.9 80.0 26.4 70.2 47.1 8.4 81.5 26.0 17.2 18.9 11.7 1.6 36.1</cell></row><row><cell></cell><cell>Source only</cell><cell cols="2">35.4 13.2 72.1 16.7 11.6 20.7 22.5 13.1 76.0 7.6 66.1 41.1 19.0 69.8 15.2 16.3 0.0 16.2 4.7 28.3</cell></row><row><cell></cell><cell cols="3">Baseline (feat. only) [30] 85.7 22.8 77.6 24.8 10.6 22.2 19.7 10.8 79.7 27.8 64.8 41.5 18.4 79.7 19.9 21.8 0.5 16.2 4.2 34.1</cell></row><row><cell></cell><cell>FADA</cell><cell cols="2">92.3 51.1 83.7 33.1 29.1 28.5 28.0 21.0 82.6 32.6 85.3 55.2 28.8 83.5 24.4 37.4 0.0 21.1 15.2 43.8</cell></row><row><cell></cell><cell>AdaptSegNet [30]</cell><cell cols="2">86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4</cell></row><row><cell></cell><cell>SIBAN [19]</cell><cell cols="2">88.5 35.4 79.5 26.3 24.3 28.5 32.5 18.3 81.2 40.0 76.5 58.1 25.8 82.6 30.3 34.4 3.4 21.6 21.5 42.6</cell></row><row><cell></cell><cell>CLAN [20]</cell><cell cols="2">87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2</cell></row><row><cell></cell><cell>AdaptPatch [31]</cell><cell cols="2">92.3 51.9 82.1 29.2 25.1 24.5 33.8 33.0 82.4 32.8 82.2 58.6 27.2 84.3 33.4 46.3 2.2 29.5 32.3 46.5</cell></row><row><cell>ResNet-101</cell><cell>ADVENT [32]</cell><cell cols="2">89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5</cell></row><row><cell></cell><cell>Source only</cell><cell cols="2">65.0 16.1 68.7 18.6 16.8 21.3 31.4 11.2 83.0 22.0 78.0 54.4 33.8 73.9 12.7 30.7 13.7 28.1 19.7 36.8</cell></row><row><cell></cell><cell cols="3">Baseline (feat. only) [30] 83.7 27.6 75.5 20.3 19.9 27.4 28.3 27.4 79.0 28.4 70.1 55.1 20.2 72.9 22.5 35.7 8.3 20.6 23.0 39.3</cell></row><row><cell></cell><cell>FADA</cell><cell cols="2">92.5 47.5 85.1 37.6 32.8 33.4 33.8 18.4 85.3 37.7 83.5 63.2 39.7 87.5 32.9 47.8 1.6 34.9 39.5 49.2</cell></row><row><cell></cell><cell>FADA-MST</cell><cell cols="2">91.0 50.6 86.0 43.4 29.8 36.8 43.4 25.0 86.8 38.3 87.4 64.0 38.0 85.2 31.6 46.1 6.5 25.4 37.1 50.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>and 3 demonstrate the semantic segmentation performance on SYNTHIA → Cityscapes and GTA5 → Cityscapes tasks in comparison with existing state-of-the-art domain adaptation methods. We could observe that our FADA outperforms the existing methods by a large margin and obtain new state-of-the-art performance in terms of mIoU. Compared to the source model without any adaptation, a gain of 16.4% and 13.9% are achieved for VGG16 and ResNet101 respectively on SYNTHIA → Quantitative analysis of the feature joint distributions. For each class, we show the Class Center Distance as defined in Equation 9</figDesc><table><row><cell></cell><cell>2.0</cell><cell>AdaptSegNet CLAN Ours</cell><cell>1.9</cell></row><row><cell>Class Center Distance</cell><cell>1.0 1.5</cell><cell></cell><cell>1.3 1.1</cell></row><row><cell></cell><cell>0.5</cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>Road SW Build Wall Fence Pole TL</cell><cell>TS Veg. Terrain Sky PR Rider Car Truck Bus Train Motor Bike Mean</cell></row><row><cell cols="2">Fig. 4:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies of each component. F-Adv refers to fine-grained adversarial training; SD refers to self distillation; MST refers to multi-scale testing.</figDesc><table><row><cell>F-Adv SD MST mIoU</cell></row><row><cell>36.8</cell></row><row><cell>46.9</cell></row><row><cell>49.2</cell></row><row><cell>50.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different strategies for extracting class-level knowledge on GTA5 → Cityscapes and SYNTHIA → Cityscapes tasks.</figDesc><table><row><cell cols="2">GTA5 SYNTHIA</cell></row><row><cell>baseline [30] 39.4</cell><cell>35.4</cell></row><row><cell>hard labels 45.7</cell><cell>40.8</cell></row><row><cell>soft labels 46.9</cell><cell>41.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Influence of threshold for confidence clipping.</figDesc><table><row><cell>GTA5 → Cityscapes</cell></row><row><cell>threshold 0.7 0.8 0.9 1.0</cell></row><row><cell>mIoU 46.2 46.3 46.9 45.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Label refinery: Improving imagenet classification through label progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1805.02641</idno>
		<ptr target="http://arxiv.org/abs/1805.02641" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-009-5152-4</idno>
		<ptr target="https://doi.org/10.1007/s10994-009-5152-4" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2699184</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2699184" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.7062" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptive faster rcnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="page" from="1602" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2672-2680. NIPS&apos;14</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=2969033.2969125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cycada: Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jun-Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P I</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-regularized alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8146-co-regularized-alignment-for-unsupervised-domain-adaptation.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9345" to="9356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045118.3045130" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
	<note>ICML&apos;15, JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., Alché-Buc, F., Fox, E., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2572683</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2572683" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<editor>Hua, G., Jégou, H.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.754</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.754" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="7130" to="7138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno>abs/1804.08286</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
