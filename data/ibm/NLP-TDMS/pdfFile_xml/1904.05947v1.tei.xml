<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Absolute Human Pose Estimation with Depth Prediction Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Márton</forename><surname>Véges</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Technology and Methodology</orgName>
								<orgName type="institution">Eötvös Loránd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Lőrincz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Technology and Methodology</orgName>
								<orgName type="institution">Eötvös Loránd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Absolute Human Pose Estimation with Depth Prediction Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-depth prediction</term>
					<term>human pose estimation</term>
					<term>global coordinates</term>
					<term>absolute pose estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The common approach to 3D human pose estimation is predicting the body joint coordinates relative to the hip. This works well for a single person but is insufficient in the case of multiple interacting people. Methods predicting absolute coordinates first estimate a root-relative pose then calculate the translation via a secondary optimization task. We propose a neural network that predicts joints in a camera centered coordinate system instead of a root-relative one. Unlike previous methods, our network works in a single step without any postprocessing. Our network beats previous methods on the MuPoTS-3D dataset and achieves state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human pose estimation has received a lot of attention recently due to its various potential applications, for example in augmented reality, sports analytics or rehabilitation. While 2D pose estimators have reached good results <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, 3D pose prediction still has areas to improve.</p><p>One difficulty of the problem comes from the fact that fully annotated 3D databases are hard to create. To create accurate measurements, special equipment with multiple cameras, depth sensors and adequate synchronization are needed. There are only a few in-the-wild datasets, most databases were created in a studio. Also, monocular 3D pose estimation is inherently ambiguous. Most methods relax the problem and only predict the coordinates of the body skeleton relative to a root joint, typically the hip <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. In other words, the translation of the skeleton does not have to be calculated, only the limb lengths and orientations.</p><p>This may be sufficient if the image contains a single person only, as is the case with the popular human pose datasets like HumanEva <ref type="bibr" target="#b6">[7]</ref> or Human3.6m <ref type="bibr" target="#b7">[8]</ref>. However, in videos containing interactions, the distance between actors and the environment can be important too. For example, detecting hand-shakes, object manipulation and passing all require more information than the root-relative pose. To our knowledge, the only solution for absolute pose estimation is finding an optimal translation vector that minimizes the reprojection error <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The search for the optimal translation is performed as a This work was completed in the ELTE Institutional Excellence Program (1783-3/2018/FEKUTSRAT) supported by the Hungarian Ministry of Human Capacities. A.L. was supported by part through grant EFOP-3.6.3-VEKOP- <ref type="bibr">16-2017-00002.</ref> post-processing step after the root-relative 3D coordinates of the body joints have been determined.</p><p>This approach has several drawbacks: the relative 3D pose estimator is trained without the knowledge of the postprocessing step. Thus, it misses information during backpropagation such as the size and distance of the person. Also, if the 3D pose estimator returns incorrect predictions, the translation vector that minimizes the reprojection error may diverge without limit (see <ref type="figure">Fig. 4</ref>). To overcome these issues, we propose a network that predicts absolute 3D coordinates instead of relative ones, circumventing the need for a translation optimization step. In our approach the origin of the coordinate system is the center of the camera.</p><p>Since absolute pose estimation is important in multi-people scenes and because studio videos have a very limited variance in depth, we use the MuPoTS-3D dataset <ref type="bibr" target="#b10">[11]</ref> that has multiple actors performing different activities in both outdoor and indoor settings. This dataset contains only evaluation data and no training data. Following <ref type="bibr" target="#b10">[11]</ref>, we have the MuCo-3DHP dataset as our training set. MuCo-3DHP was introduced in the same paper <ref type="bibr" target="#b10">[11]</ref>.</p><p>However, MuCo-3DHP consists of synthesized studio scenes. To overcome the lack of variety in the training data, our network has a multi-stage architecture. The network first predicts 2D poses from the image and then predicts 3D coordinates using only the 2D output of the previous stage. Since large 2D annotated databases exist, the first step can be performed with high accuracy. This approach was successfully employed in a number of algorithms <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, reaching state-of-the-art results.</p><p>Since we would like to estimate absolute coordinates, not only root-relative ones, image details such as the relative position of people, the location of furniture, etc. might hold important information. To exploit those features without the need of a very large training set, we employ a depth estimation network.</p><p>Depth predictor networks try to represent the scene geometry by predicting the depth for each pixel, essentially producing a 3D point-cloud from an input image <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Although the predicted depth might not be accurate, for pose estimation predicting good ordinal ordering of joints (whether point A is closer or further from the camera than point B) already yields large improvements <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. <ref type="bibr">Birmingham et al.</ref> found that the results of MonoDepth <ref type="bibr" target="#b13">[14]</ref> correlate with human predictions of depth ranking <ref type="bibr" target="#b18">[19]</ref>. This motivates our choice to include a depth prediction network into our pipeline. The depth predictor network can be used as a separate component, extracting features from the image that the 2Dto-3D network uses. This can be improved by training the network together with the 2D-to-3D network end-to-end.</p><p>To summarize, our contributions are as follows: 1) we introduce an architecture that predicts absolute 3D coordinates in one step, 2) we show that the addition of depth features provide significant performance increases and the depth network can be trained end-to-end with the pose estimating network, 3) we beat the previous state-of-the-art method on the MuPoTS-3D dataset. We make our code publicly available <ref type="bibr" target="#b0">1</ref> .</p><p>In what follows, we treat related works (Sec. II) followed by the overview of our methods (Sect. III), the section about the experiments (Sect. IV) and our results (Sect. V). We conclude in the last section (Sec. VI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relative pose estimation</head><p>Most 3D pose predictors estimate the body joint coordinates relative to a root joint, usually the hip. In <ref type="bibr" target="#b19">[20]</ref> the authors propose a multi-stage architecture using a probabilistic model to predict depth coordinates. Pavlakos et al. <ref type="bibr" target="#b5">[6]</ref> predicts a 3D heatmap instead of single coordinates. Another set of methods use the soft-argmax function to go from a predicted 2D heatmap to 3D coordinates <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>One problem with 3D prediction is the lack of varied inthe-wild datasets. Annotating an image with 3D information requires special equipment with multiple cameras. In contrast, 2D pose estimation datasets are easy to create so it seems beneficial to use them. One approach is to split the 3D estimation into two steps: first predict the 2D coordinates from the image and then predict 3D coordinates from the 2D coordinates only <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>. While it seems too limiting using just 2D coordinates and no other image features, these methods achieved state-of-the-art results nonetheless.</p><p>Another approach is based on the idea that humans are good at telling which of two points of an image is closer to the camera. Whereas it is nearly impossible for a person to guess distances in a photograph with high accuracy, annotating ordinal ranking of joints requires less than a minute <ref type="bibr" target="#b16">[17]</ref>. 2D datasets supplied with ordinal rankings can provide weak supervisory information <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Even without annotating additional data, just predicting joint ranking information as an auxiliary task leads to improvements <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Absolute pose estimation</head><p>While methods to estimate root-relative coordinates are numerous, only a handful predicts coordinates in a global system. Mehta et al. <ref type="bibr" target="#b8">[9]</ref> first predicts a 3D pose from an image and then finds an optimal translation minimizing the squared reprojection error. The least squares problem has an exact solution assuming weak projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">https://github.com/vegesm/depthpose</head><p>In <ref type="bibr" target="#b9">[10]</ref>, the authors predict a full body mesh using the skinned multi-person linear (SMPL) model <ref type="bibr" target="#b24">[25]</ref>. First they predict an initial pose with the DMHS detector <ref type="bibr" target="#b25">[26]</ref> and refine the prediction using multiple constraints, including reprojection error, a semantic loss involving body part segmentation and matching to ground plane. Similarly to <ref type="bibr" target="#b8">[9]</ref>, the positioning of the person in the global scene happens as a separate step. Our work is different to previous approaches in that the global pose estimation is performed directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth estimation</head><p>We shortly review here recent approaches to depth estimation. Current methods all use some form of fully convolutional networks. Laina et al. <ref type="bibr" target="#b15">[16]</ref> uses a slightly modified ResNet-50 with faster up-convolution blocks. In <ref type="bibr" target="#b26">[27]</ref>, the authors use an HourGlass <ref type="bibr" target="#b1">[2]</ref> like architecture, where convolutional filters were replaced with Inception-style modules <ref type="bibr" target="#b27">[28]</ref>. Finally, MegaDepth <ref type="bibr" target="#b14">[15]</ref> introduces a training set synthesized from images from the Internet, making learned models more robust.</p><p>Another branch of research uses consistency between different views to learn depth in an unsupervised manner. MonoDepth <ref type="bibr" target="#b13">[14]</ref> takes as input a pair of images from stereo cameras and learns to reconstruct one view from the other via estimating depth. The method is very strong, even beating supervised algorithms. In contrast to MonoDepth, DF-Net <ref type="bibr" target="#b28">[29]</ref> does not need stereo images but consecutive frames from a monocular video. It predicts optical flow and depth jointly, seeking consistency between successive frames. Note that during inference, both methods need only a single picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section we briefly describe the baseline method introduced in <ref type="bibr" target="#b8">[9]</ref> and also detail our network's architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline</head><p>The problem of finding global coordinates for a skeleton having the root-relative coordinates can be formulated as follows:t = argmin</p><formula xml:id="formula_0">t∈R 3 P 2D − Π P 3D + t 2 2 ,</formula><p>where P 2D and P 3D are the body joint coordinates in the image and 3D space, and Π is the projection from 3D space to camera frame. The optimal translationt can be added to the root-relative pose P 3D to get the final coordinates in a global coordinate system.</p><p>The minimization problem can be solved exactly, assuming a weak perspective projection:</p><formula xml:id="formula_1">t = α P 2D f − P 3D 0 , α = i P 3D i −P 3D 2 2 i P 2D i −P 2D , P 3D i −P 3D ,</formula><p>where P 3D is the x and y coordinates from P 3D ,P 2D and P 3D are the means of P 2D and P 3D . The derivation can be found in <ref type="bibr" target="#b8">[9]</ref>. Note that in their paper the authors added </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. One residual module in 3D</head><p>PoseNet. It has two fully connected layers each followed by a Batch Normalization layer, a ReLU activation and a Dropout. The 3D PoseNet has two of these modules. The figure is taken from <ref type="bibr" target="#b22">[23]</ref> . an approximation at the last step which we do not include. Without the approximation we get better results.</p><p>In the full pipeline, the relative body pose is predicted using the same 2D pose estimator and 3D PoseNet that our method uses. To keep the results comparable, all details, including the network architecture, normalization and loss are the same in the baseline and our method. These are described in the next two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our model</head><p>The architecture of our network is sketched in <ref type="figure">Fig. 1</ref>. Based on the success of numerous earlier work ( <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>), we separate the 3D detection task into two steps: first we detect the 2D coordinates then regress the 3D coordinates from the 2D coordinates only. The 2D pose detector is the state-of-theart multi-person pose detector OpenPose <ref type="bibr" target="#b0">[1]</ref>. In our network, the 2D-to-3D component is called 3D PoseNet.</p><p>To be robust against the change of cameras between the training and test set we normalize the 2D pose by multiplying with the inverse of the intrinsic camera matrix K. The normalized 2D pose is further processed by splitting the representation into two parts: the hip-relative coordinates of all the joints (except the hip as it is zero) and the original coordinates of the hip. This way the root-relative coordinates remain translation invariant. The disadvantage of this approach is that if the hip was not found by the 2D pose estimator then the entire person must be reported as undetected. However, we have found that an invisible hip implies a mostly invisible body in nearly all the cases. The number of poses thrown away because of this is just 3% of all the frames.</p><p>To make the 3D PoseNet able to use image features in addition to the 2D coordinates, we employ a depth estimator network. We could use an off-the-shelf depth estimator as a separate component, reading out the results at joint coordinates predicted by the 2D pose estimator. The produced depth values act as additional features for the 3D PoseNet. This already leads to improvements as shown in Section V.</p><p>However, a direct readout have problems: the 2D pose estimator can predict an incorrect location that falls on the background, the joint can be occluded by another person or the depth prediction misses a limb. To overcome these issues, we train the depth estimator and the 3D PoseNet together, starting from a pretrained depth estimator. We chose the MegaDepth algorithm <ref type="bibr" target="#b14">[15]</ref> as it was robust against the different indoor and outdoor settings in the MuPoTS-3D test set. In preliminary studies, other depth detectors trained either on an indoor or outdoor database performed worse.</p><p>Since MegaDepth outputs the logarithm of the depth, we use log-depth as the input of 3D PoseNet. Also, to avoid the network having to learn an exponential function, the output hip depth is also given in a logarithmic scale. The predicted depth is not logarithmic for the other joints to keep translation invariance.</p><p>The depth prediction and normalized coordinates are fed to the 3D PoseNet that generates the absolute 3D coordinates <ref type="figure">(Fig. 1)</ref>. Similarly to the 2D poses, the output 3D pose is split into hip-relative and absolute part, helping the generalization ability of the network.</p><p>The architecture of the 3D PoseNet was inspired by <ref type="bibr" target="#b3">[4]</ref> and is illustrated on <ref type="figure">Fig. 2</ref>. It consists of two blocks of residual modules, each having a dense layer followed by BatchNorm <ref type="bibr" target="#b29">[30]</ref> and Dropout <ref type="bibr" target="#b30">[31]</ref> layers. The activation function was ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training loss</head><p>The loss function is calculated on the predicted coordinates for each person in the picture. We use the L1 loss to be robust against outliers. This is in line with the findings of previous work <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The final loss is thus:</p><formula xml:id="formula_2">L = 1 N P N P i=1 P 3D i − P 3D i ,</formula><p>where N P is the number of detected poses in a batch. Note that N P changes across batches, as the number of people on an image varies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We used the recently released MuPoTS-3D dataset <ref type="bibr" target="#b10">[11]</ref> for evaluation. Unlike other popular datasets, videos in MuPoTS-3D were not shot in a studio and have multiple people interacting. The database has 5 indoor and 15 outdoor scenes with buildings, trees and other objects. In total it has 8300 frames and 20k poses from 8 actors.</p><p>Since MuPoTS-3D does not have a training set, following <ref type="bibr" target="#b10">[11]</ref> we use the MuCo-3DHP dataset <ref type="bibr" target="#b10">[11]</ref> for training. The dataset contains synthetic images generated from the poses in the MPI-INF-3DHP database <ref type="bibr" target="#b8">[9]</ref>. Each picture in MuCo-3DHP is a composition of 4 frames from MPI-INF-3DHP from the same camera. The authors only provide the generating scripts for the dataset, not the images themselves. We created 150k training images with 4 people on each image. The script has an option for background augmentation, though it simply places an image behind the actors. This would interfere with the depth estimation and we chose not to use it.</p><p>Note that the training and test sets contain quite different pictures: the camera characteristics (resolution, focal length, position), scene backgrounds and the actors are all different. This ensures that the measured results are robust and not a product of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>We employ a variation of the standard mean per joint position error (MPJPE) metric to evaluate our models <ref type="bibr" target="#b7">[8]</ref>. The MPJPE metric is the root-relative Euclidean error averaged over all joints and poses. In a root-relative pose, the hip (the root joint) is set at the origin. Since we are interested in coordinates in a global space, we do not move the hip to the origin. We call the latter metric Absolute MPJPE or A-MPJPE for short. The original MPJPE is called Relative or R-MPJPE to avoid confusion.</p><p>In absolute pose estimation there could be two sources of errors: the (root-relative) pose is incorrectly estimated, or the absolute location of the pose is incorrect. The scale of the second type of error can be much larger then the first type. We report both metrics to avoid that the absolute error hides an inaccurate pose prediction.</p><p>To summarize, the definition of the metrics:</p><p>• A-MPJPE or Absolute MPJPE. The average Euclidean distance between the ground truth and predicted joints in millimeters. • R-MPJPE or Relative MPJPE. The average Euclidean distance between the ground truth and predicted hip-relative joint coordinates in millimeters. Previous work calls this the MPJPE metric. Thus, the A-MPJPE metric is a natural extension of the common MPJPE metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>For the 2D Pose estimator we used the OpenPose <ref type="bibr" target="#b0">[1]</ref> multiperson pose estimator. It predicts 25 joints with a confidence score. We only use 14 joints as the rest is noisy and leads to degraded performance. The selected joints were: nose, neck, pelvis and left/right hip, knee, ankle, shoulder, elbow and wrist. Also note that annotations in the test set contains none of the 11 excluded joints. If OpenPose was unable to detect the hip the pose was discarded as undetected. We have found that the poses excluded this way were heavily occluded and hard to detect.</p><p>In the baseline algorithm, we used OpenPose with the 3D PoseNet together (using the same normalization techniques as described above). The 3D PoseNet had two blocks of residual modules depicted in <ref type="figure">Fig. 2</ref>, both module had two fully connected layers of 1024 neurons. The dropout rate was set to 0.5 as in <ref type="bibr" target="#b3">[4]</ref>. We trained the network for 100 epochs with the Adam optimization algorithm. The learning rate was 0.001 initially and was decreased with a multiplier of 0.96 every 4 epochs.</p><p>The training of our network was performed in two steps: first the depth estimator was fixed and only the 3D PoseNet was trained using Adam and a learning rate of 0.001 for 100 epochs. The learning rate was decreased the same way as with the baseline. The batch size was 256. In the second step the depth prediction network was trained as well. Since the network is much larger, only the top convolutional layer was updated, the rest remained fixed. Due to memory reasons, the batch size was decreased to 30 and the training ran for 5 epochs. Again, the Adam optimization algorithm was used with a learning rate of 10 −5 . During training we augmented the dataset by cropping and zooming the input images. This augmentation was used in the baseline experiments as well to have comparable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pose estimation performance</head><p>In this section we review the absolute and relative pose estimation performance of our network compared to the base-  <ref type="figure">Fig. 3</ref>. Histogram of the error distributions of the baseline and our method (note the logarithmic scale for the number of poses). Our method has much fewer predictions with large error, particularly above 500mm</p><p>. line algorithm. Quantitative results are presented in <ref type="table" target="#tab_1">Table I</ref>. First note that in relative pose estimation (R-MPJPE metric), our baseline algorithm already beats the state-of-the-art on the MuPoTS-3D dataset by 10 mm (7.6%), signaling the strength of the method. Our end-to-end trained method achieved the best overall results both on the A-MPJPE and R-MPJPE metrics. In absolute pose estimation the improvement is 28mm (8.7%). The relative error also decreased to 120mm (1.6%). In the case of the relative pose estimation, the baseline and our method differ only in the depth features that we included. Thus the improvement of R-MPJPE comes from the depth estimator solely.</p><p>To gain further insights on how the baseline and our method compares we present a histogram of the error distributions in <ref type="figure">Fig. 3</ref>. Our method produces fewer large errors, as the shorter tail of the graph indicates. On the other hand, the baseline algorithm performs a bit better on the low-error range; it has more input samples where the prediction error was under 100mm. However, this does not compensate for the larger errors produced on other poses.</p><p>Our algorithm detects slightly less (2%) poses than that of Mehta et al. <ref type="bibr" target="#b10">[11]</ref> but 5% more than LCR-Net <ref type="bibr" target="#b32">[33]</ref>. However, the baseline and our method have the same performance in this regard as the difference is only in the pose estimation part and not in the detection part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Baseline Ours <ref type="figure">Fig. 4</ref>. Typical error of baseline. For clarity, only one person is shown and from a different angle. Dark skeleton is the ground truth, light colored skeletons are the estimates. Since the predicted (root relative) 3D pose is incorrect, the baseline's reprojection error minimisation step places the pose far away from the ground truth. In contrast, due to the direct estimation of absolute coordinates, our model places the person at the correct position, even when the pose is incorrect.</p><p>To summarize, our end-to-end trained method achieves new state of the art relative pose estimation results on the MuPoTS-3D dataset and improves on the commonly used baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative results</head><p>We present sample outputs of our network on <ref type="figure" target="#fig_0">Fig. 5</ref>. One can see that the relative pose is similar to the baseline's output. On the other hand, the skeletons are placed closer to the ground truth by our model. If two people are very close to each other (e.g. hugging), the 2D detector often fails to find one of the persons due to heavy occlusion, see <ref type="figure" target="#fig_0">Fig. 5</ref> fifth row second column. Here one of the subjects is nearly fully occluded.</p><p>The left column of the bottom row shows a failure case where the detected (relative) pose is wrong. Note that our method still places the skeleton closer to the ground truth, while the baseline is unable to do that.</p><p>The right column of the bottom row shows an example where a person was undetected due to hidden hip. Most of the sitting person is occluded and it would be hard to make a good prediction of his pose.</p><p>Additionally, <ref type="figure">Fig. 4</ref> shows how our direct estimation solves a common problem of the two-step approach. The figure shows an input with a difficult pose for which both our and the baseline method returns an incorrect root-relative estimation. In the baseline method, not only the root-relative pose is faulty but the location of that pose is erroneous as well. This is due to the fact that no 2D reprojection of a bad 3D pose is close to the original detected 2D pose. However, our model correctly places the person in the space. In other words, the incorrect pose prediction does not prevent finding the correct location of the person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study</head><p>We validate our design decisions by an ablation study. The results are presented in <ref type="table" target="#tab_1">Table II</ref>. Changing from L2 loss to L1 improves the performance by 42 mm (10%). The large drop can be attributed to the robustness of L1 against outliers. The inclusion of depth features from MegaDepth further decreases the error by 12 mm. Additional improvements can be achieved</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Baseline</head><p>Ours Input Baseline Ours  by predicting the logarithm of the z coordinate of the hip. This choice was motivated by two facts: first, MegaDepth predicts logarithms of depth values, second, the distribution of depth coordinates has a long-tail distribution and the logarithm function essentially converts it back to a more symmetric one <ref type="bibr" target="#b14">[15]</ref>.</p><p>Augmenting the data with crops and zoom leads to another significant drop of 44mm or 12%. Finally, the end-to-end finetuning improves the results by an additional 22mm (7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We have introduced a single-step solution for absolute pose estimation on multi-person scenes. Unlike previous approaches, this does not require any post-processing. We also showed that depth estimators are good source of additional features for absolute pose estimation. This results in improved performance and state-of-the-art results on the MuPoTS-3D dataset. The dataset is different from the training set so it indicates a good generalization ability.</p><p>Although the decrease in error metrics is significant, there is still space for further improvements. The 2D Pose Estimator can be trained end-to-end, together with the 3D PoseNet and depth estimator networks. Also, for many applications, such as detecting interactions between the subjects, estimates are fine to be given scale independently, as long as all the persons on an image are represented in the same scale. Thus instead of predicting absolute coordinates, one could estimate scale invariant ones. This removes the ambiguity from the problem statement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results. Dark skeletons are ground truth values, light ones are the network predictions. Bottom left: an erroneous result, bottom right: a case when the hip is hidden. For more information, see text. Note: viewing angles differ from those of the images for visualization purposes. Also, not all people visible in the scene have ground-truth annotations, those are not displayed in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Our network architecture. The 2D pose estimator detects body keypoints in the image while the depth estimator calculates the depth for each pixel. The depth is read out at the detected keypoint coordinates. The predicted 2D coordinates are further normalized with the inverse of the camera intrinsic matrix. The depth values and the focus normalized 2D coordinates are fed to the 3D PoseNet regressor that calculates the final 3D output.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3D Pose</cell></row><row><cell></cell><cell cols="2">2D Pose Estimator</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Normalization</cell></row><row><cell></cell><cell cols="2">Depth Estimator</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3D PoseNet</cell></row><row><cell>Input Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 1. FC</cell><cell>BN</cell><cell>ReLU</cell><cell>Drop.</cell><cell>FC</cell><cell>BN</cell><cell>ReLU</cell><cell>Drop.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I POSE</head><label>I</label><figDesc>ESTIMATION PERFORMANCE ON THE MUPOTS-3D TEST SET. BEST RESULT SELECTED IN BOLD. ERRORS ARE IN MM.</figDesc><table><row><cell></cell><cell cols="3">A-MPJPE R-MPJPE Detection Rate</cell></row><row><cell>LCR-Net [33]</cell><cell>-</cell><cell>146</cell><cell>86%</cell></row><row><cell>Mehta et al. [11]</cell><cell>-</cell><cell>132</cell><cell>93%</cell></row><row><cell>Baseline</cell><cell>320</cell><cell>122</cell><cell>91%</cell></row><row><cell>Ours</cell><cell>292</cell><cell>120</cell><cell>91%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY RESULTS. THE TABLE SHOWS THE CHANGE IN A-MPJPE (UNITS IN MM) WHILE TURNING ON COMPONENTS OF OUR</figDesc><table><row><cell>NETWORK SEQUENTIALLY.</cell><cell></cell></row><row><cell cols="2">A-MPJPE</cell></row><row><cell>L2 loss</cell><cell>421</cell></row><row><cell>w/ L1 loss</cell><cell>379</cell></row><row><cell>w/ Depth features</cell><cell>367</cell></row><row><cell>Predicting log of hip z coordinate</cell><cell>358</cell></row><row><cell>Augmentation</cell><cell>314</cell></row><row><cell>End-to-end training</cell><cell>292</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2353" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Drpose3d: Depth ranking in 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligen</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligen</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="978" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fourth International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fbi-pose: Towards bridging the gap between 2d images and 3d human poses using forwardor-backward information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09241</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adding the third dimension to spatial relation detection in 2d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Birmingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muscat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="146" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d human pose estimation with siamese equivariant embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Véges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lőrincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">print</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3d human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="248" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Synthetic occlusion augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sarandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04987</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lcr-net: Localizationclassification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1216" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
