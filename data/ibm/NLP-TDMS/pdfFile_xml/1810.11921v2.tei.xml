<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoInt: Automatic Feature Interaction Learn-ing via Self-Attentive Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>November 3-7, 2019. 2019. No-vember 3-7, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
							<email>weiping.song@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
							<email>chenceshi@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
							<email>zjduan@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
							<email>xuyewen@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Mila-Quebec AI Institute, HEC Montreal &amp; CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoInt: Automatic Feature Interaction Learn-ing via Self-Attentive Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">The 28th ACM International Conference on Information and Knowledge Management (CIKM &apos;19)</title>
						<meeting> <address><addrLine>Beijing, China; Beijing, China; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">10</biblScope>
							<date type="published">November 3-7, 2019. 2019. No-vember 3-7, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3357925</idno>
					<note>Code is available at: https://github.com/DeepGraphLearning/RecommenderSystems. * Part of this work was performed when the first author was visiting Mila. † Corresponding authors. ACM ISBN 978-1-4503-6976-3/19/11. . . $15.00 ACM Reference Format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Recommender systems</term>
					<term>• Comput- ing methodologies → Neural networks</term>
					<term>Learning latent repre- sentations</term>
					<term>KEYWORDS High-order feature interactions, Self attention, CTR prediction, Explainable recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on highorder combinatorial features (a.k.a. cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding lowdimensional representations of the sparse and high-dimensional raw features and their meaningful combinations.</p><p>In this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multihead self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the lowdimensional space. With different layers of the multi-head selfattentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Predicting the probabilities of users clicking on ads or items (a.k.a., click-through rate prediction) is a critical problem for many applications such as online advertising and recommender systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b15">15]</ref>. The performance of the prediction has a direct impact on the final revenue of the business providers. Due to its importance, it has attracted growing interest in both academia and industry communities.</p><p>Machine learning has been playing a key role in click-through rate prediction, which is usually formulated as supervised learning with user profiles and item attributes as input features. The problem is very challenging for several reasons. First, the input features are extremely sparse and high-dimensional <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b32">32]</ref>. In real-world applications, a considerable percentage of user's demographics and item's attributes are usually discrete and/or categorical. To make supervised learning methods applicable, these features are first converted to a one-hot encoding vector, which can easily result in features with millions of dimensions. Taking the well-known CTR prediction data Criteo 1 as an example, the feature dimension is approximately 30 million with sparsity over 99.99%. With such sparse and high-dimensional input features, the machine learning models are easily overfitted. Second, as shown in extensive literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b32">32]</ref>, high-order feature interactions 2 are crucial for a good performance. For example, it is reasonable to recommend Mario Bros., a famous video game, to David, who is a ten-year-old boy. In this case, the third-order combinatorial feature &lt;Gender=Male, Age=10, ProductCategory=VideoGame&gt; is very informative for prediction. However, finding such meaningful high-order combinatorial features heavily relies on domain experts. Moreover, it is almost impossible to hand-craft all the meaningful combinations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">26]</ref>. One may ask that we can enumerate all the possible high-order features and let machine learning models select the meaningful ones. However, enumerating all the possible high-order features will exponentially increase the dimension and sparsity of the input features, leading to a more serious problem of model overfitting. Therefore, there has been extensive efforts in the communities in finding low-dimensional representations of the sparse and high-dimensional input features and meanwhile modeling different orders of feature combinations.</p><p>For example, Factorization Machines (FM) <ref type="bibr" target="#b26">[26]</ref>, which combine polynomial regression models with factorization techniques, are developed to model feature interactions and have been proved effective for various tasks <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>. However, limited by its polynomial fitting time, it is only effective for modeling low-order feature interactions and impractical to capture high-order feature interactions. Recently, many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b38">38]</ref> based on deep neural networks have been proposed to model the high-order feature interactions. Specifically, multiple layers of non-linear neural networks are usually used to capture the high-order feature interactions. However, such kinds of methods suffer from two limitations. First, fully-connected neural networks have been shown inefficient in learning multiplicative feature interactions <ref type="bibr" target="#b3">[4]</ref>. Second, since these models learn the feature interactions in an implicit way, they lack good explanation on which feature combinations are meaningful. Therefore, we are looking for an approach that is able to explicitly model different orders of feature combinations, represent the entire features into low-dimensional spaces, and meanwhile offer good model explainability.</p><p>In this paper, we propose such an approach based on the multihead self-attention mechanism <ref type="bibr" target="#b36">[36]</ref>. Our proposed approach learns effective low-dimensional representations of the sparse and highdimensional input features and is applicable to both the categorical and/or numerical input features. Specifically, both the categorical and numerical features are first embedded into low-dimensional spaces, which reduces the dimension of the input features and meanwhile allows different types of features to interact with each other via vector arithmetic (e.g., summation and inner product). Afterwards, we propose a novel interacting layer to promote the interactions between different features. Within each interacting layer, each feature is allowed to interact with all the other features and is able to automatically identify relevant features to form meaningful higher-order features via the multi-head attention mechanism <ref type="bibr" target="#b36">[36]</ref>. Moreover, the multi-head mechanism projects a feature into multiple subspaces, and hence it can capture different feature interactions in different subspaces. Such an interacting layer models the one-step interaction between the features. By stacking multiple interacting layers, we are able to model different orders of feature interactions. In practice, the residual connection <ref type="bibr" target="#b12">[12]</ref> is added to the interacting layer, which allows combining different orders of feature combinations. We use the attention mechanism for measuring the correlations between features, which offers good model explainability.</p><p>To summarize, in this paper we make the following contributions:</p><p>• We propose to study the problem of explicitly learning highorder feature interactions and meanwhile finding models with good explainability for the problem. • We propose a novel approach based on self-attentive neural network, which can automatically learn high-order feature interactions and efficiently handle large-scale highdimensional sparse data. • We conducted extensive experiments on several real-world data sets. Experimental results on the task of CTR prediction show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good model explainability. Our work is organized as follows. In Section 2, we summarize the related work. Section 3 formally defines our problem. Section 4 presents the proposed approach to learn feature interactions. In Section 5, we present the experimental results and detailed analysis. We conclude this paper and point out the future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is relevant to three lines of work: 1) Click-through rate prediction in recommender systems and online advertising, 2) techniques for learning feature interactions, and 3) self-attention mechanism and residual networks in the literature of deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Click-through Rate Prediction</head><p>Predicting click-through rates is important to many Internet companies, and various systems have been developed by different companies <ref type="bibr">[8-10, 15, 21, 29, 43]</ref>. For example, Google developed the Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> learning system for recommender systems, which combines the advantages of both the linear shallow models and deep models. The system achieves remarkable performance in APP recommendation. The problem also receives a lot of attention in the academic communities. For example, Shan et al. <ref type="bibr" target="#b31">[31]</ref> proposed a context-aware CTR prediction method which factorized three-way &lt;user, ad, context&gt; tensor. Oentaryo et al. <ref type="bibr" target="#b24">[24]</ref> developed hierarchical importance-aware factorization machine to model dynamic impacts of ads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Feature Interactions</head><p>Learning feature interactions is a fundamental problem and therefore extensively studied in the literature. A well-known example is Factorization Machines (FM) <ref type="bibr" target="#b26">[26]</ref>, which were proposed to mainly capture the first-and second-order feature interactions and have been proved effective for many tasks in recommender systems <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>. Afterwards, different variants of factorization machines have been proposed. For example, Field-aware Factorization Machines (FFM) <ref type="bibr" target="#b16">[16]</ref> modeled fine-grained interactions between features from different fields. GBFM <ref type="bibr" target="#b6">[7]</ref> and AFM <ref type="bibr" target="#b40">[40]</ref> considered the importance of different second-order feature interactions. However, all these approaches focus on modeling low-order feature interactions.</p><p>There are some recent works that model high-order feature interactions. For example, NFM <ref type="bibr" target="#b13">[13]</ref> stacked deep neural networks on top of the output of the second-order feature interactions to model higher-order features. Similarly, PNN <ref type="bibr" target="#b25">[25]</ref>, FNN <ref type="bibr" target="#b41">[41]</ref>, DeepCrossing <ref type="bibr" target="#b32">[32]</ref>, Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref> and DeepFM <ref type="bibr" target="#b11">[11]</ref> utilized feed-forward neural networks to model high-order feature interactions. However, all these approaches learn the high-order feature interactions in an implicit way and therefore lack good model explainability. On the contrary, there are three lines of works that learn feature interactions in an explicit fashion. First, Deep&amp;Cross <ref type="bibr" target="#b38">[38]</ref> and xDeepFM <ref type="bibr" target="#b19">[19]</ref> took outer product of features at the bit-and vector-wise level respectively. Although they perform explicit feature interactions, it is not trivial to explain which combinations are useful. Second, some tree-based methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b44">44]</ref> combined the power of embedding-based models and tree-based models but had to break training procedure into multiple stages. Third, HOFM <ref type="bibr" target="#b4">[5]</ref> proposed efficient training algorithms for high-order factorization machines. However, HOFM requires too many parameters and only its low-order (usually less than 5) form can be practically used. Different from existing work, we explicitly model feature interactions with attention mechanism in an end-to-end manner, and probe the learned feature combinations via visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention and Residual Networks</head><p>Our proposed model makes use of the latest techniques in the literature of deep learning: attention <ref type="bibr" target="#b1">[2]</ref> and residual networks <ref type="bibr" target="#b12">[12]</ref>. Attention is first proposed in the context of neural machine translation <ref type="bibr" target="#b1">[2]</ref> and has been proved effective in a variety of tasks such as question answering <ref type="bibr" target="#b35">[35]</ref>, text summarization <ref type="bibr" target="#b30">[30]</ref>, and recommender systems <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43]</ref>. Vaswani et al. <ref type="bibr" target="#b36">[36]</ref> further proposed multi-head self-attention to model complicated dependencies between words in machine translation.</p><p>Residual networks <ref type="bibr" target="#b12">[12]</ref> achieved state-of-the-art performance in the ImageNet contest. Since the residual connection, which can be simply formalized as y = F (x) + x, encourages gradient flow through interval layers, it becomes a popular network structure for training very deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>We first formally define the problem of click-through rate (CTR) prediction as follows: DEFINITION 1. (CTR Prediction) Let x ∈ R n denotes the concatenation of user u's features and item v's features, where categorical features are represented with one-hot encoding, and n is the dimension of concatenated features. The problem of click-through rate prediction aims to predict the probability of user u clicking on item v according to the feature vector x.</p><p>A straightforward solution for CTR prediction is to treat x as the input features and deploy the off-the-shelf classifiers such as logistic regression. However, since the original feature vector x is very sparse and high-dimensional, the model will be easily overfitted. Therefore, it is desirable to represent the raw input features in lowdimensional continuous spaces. Moreover, as shown in existing literature, it is crucial to utilize the higher-order combinatorial features to yield good prediction performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32]</ref>. Specifically, we define the high-order combinatorial features as follows:</p><formula xml:id="formula_0">DEFINITION 2. (p-order Combinatorial Feature) Given input feature vector x ∈ R n , a p-order combinatorial feature is defined as д(x i 1 , ..., x i p ) ,</formula><p>where each feature comes from a distinct field, p is the number of involved feature fields, and д(·) is a non-additive combination function, such as multiplication <ref type="bibr" target="#b26">[26]</ref> and outer product <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b38">38]</ref>. For example, x i 1 × x i 2 is a second-order combinatorial feature involving x i 1 and x i 2 .</p><p>Traditionally, meaningful high-order combinatorial features are hand-crafted by domain experts. However, this is very time-consuming and hard to generalize to other domains. Besides, it is almost impossible to hand-craft all meaningful high-order features. Therefore, we aim to develop an approach that is able to automatically discover the meaningful high-order combinatorial features and meanwhile map all these features into low-dimensional continuous spaces. Formally, we define our problem as follows: DEFINITION 3. (Problem Definition) Given an input feature vector x ∈ R n for click-through rate prediction, our goal is to learn a low-dimensional representation of x, which models the high-order combinatorial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AUTOINT: AUTOMATIC FEATURE INTERACTION LEARNING</head><p>In this section, we first give an overview of the proposed approach AutoInt, which can automatically learn feature interactions for CTR prediction. Next, we present a comprehensive description of how to learn a low-dimensional representation that models high-order combinatorial features without manual feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The goal of our approach is to map the original sparse and highdimensional feature vector into low-dimensional spaces and meanwhile model the high-order feature interactions. As shown in <ref type="figure">Fig</ref> (i.e., both categorical and numerical features) into the same lowdimensional space. Next, we feed embeddings of all fields into a novel interacting layer, which is implemented as a multi-head selfattentive neural network. For each interacting layer, high-order features are combined through the attention mechanism, and different kinds of combinations can be evaluated with the multi-head mechanisms, which map the features into different subspaces. By stacking multiple interacting layers, different orders of combinatorial features can be modeled. The output of the final interacting layer is the low-dimensional representation of the input feature, which models the high-order combinatorial features and is further used for estimating the clickthrough rate through a sigmoid function. Next, we introduce the details of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Input Layer</head><p>We first represent user's profiles and item's attributes as a sparse vector, which is the concatenation of all fields. Specifically,</p><formula xml:id="formula_1">x = [x 1 ; x 2 ; ...; x M ],<label>(1)</label></formula><p>where M is the number of total feature fields, and x i is the feature representation of the i-th field. x i is a one-hot vector if the i-th field is categorical (e.g., x 1 in <ref type="figure">Figure 2</ref>). x i is a scalar value if the i-th field is numerical (e.g., x M in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding Layer</head><p>Since the feature representations of the categorical features are very sparse and high-dimensional, a common way is to represent them into low-dimensional spaces (e.g., word embeddings). Specifically, we represent each categorical feature with a low-dimensional vector, i.e.,</p><formula xml:id="formula_2">e i = V i x i ,<label>(2)</label></formula><p>where V i is an embedding matrix for field i, and x i is an one-hot vector. Often times categorical features can be multi-valued, i.e., x i is a multi-hot vector. Take movie watching prediction as an example, there could be a feature field Genre which describes the types of a movie and it may be multi-valued (e.g., Drama and Romance for movie "Titanic"). To be compatible with multi-valued inputs, we further modify the Equation 2 and represent the multi-valued feature field as the average of corresponding feature embedding vectors:</p><formula xml:id="formula_3">e i = 1 q V i x i ,<label>(3)</label></formula><p>where q is the number of values that a sample has for i-th field and x i is the multi-hot vector representation for this field. To allow the interaction between categorical and numerical features, we also represent the numerical features in the same lowdimensional feature space. Specifically, we represent the numerical feature as</p><formula xml:id="formula_4">e m = v m x m ,<label>(4)</label></formula><p>where v m is an embedding vector for field m, and x m is a scalar value. By doing this, the output of the embedding layer would be a concatenation of multiple embedding vectors, as presented in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Interacting Layer</head><p>Once the numerical and categorical features live in the same lowdimensional space, we move to model high-order combinatorial features in the space. The key problem is to determine which features should be combined to form meaningful high-order features. Traditionally, this is accomplished by domain experts who create meaningful combinations based on their knowledge. In this paper, we tackle this problem with a novel method, the multi-head self-attention mechanism <ref type="bibr" target="#b36">[36]</ref>.</p><p>Multi-head self-attentive network <ref type="bibr" target="#b36">[36]</ref> has recently achieved remarkable performance in modeling complicated relations. For example, it shows superiority for modeling arbitrary word dependency in machine translation <ref type="bibr" target="#b36">[36]</ref> and sentence embedding <ref type="bibr" target="#b20">[20]</ref>, and has been successfully applied to capturing node similarities in graph embedding <ref type="bibr" target="#b37">[37]</ref>. Here we extend this latest technique to model the correlations between different feature fields.</p><p>Specifically, we adopt the key-value attention mechanism <ref type="bibr" target="#b22">[22]</ref> to determine which feature combinations are meaningful. Taking the feature m as an example, next we explain how to identify multiple meaningful high-order features involving feature m. We first define the correlation between feature m and feature k under a specific attention head h as follows:</p><formula xml:id="formula_5">α (h) m,k = exp(ψ (h) (e m , e k )) M l =1 exp(ψ (h) (e m , e l ))</formula><p>,</p><formula xml:id="formula_6">ψ (h) (e m , e k ) = ⟨W (h) Query e m , W (h) Key e k ⟩,<label>(5)</label></formula><p>where ψ (h) (·, ·) is an attention function which defines the similarity between the feature m and k. It can be defined as a neural network or as simple as inner product, i.e., ⟨·, ·⟩. In this work, we use inner product due to its simplicity and effectiveness. W </p><formula xml:id="formula_7">e (h) m = M k =1 α (h) m,k (W (h) Value e k ),<label>(6)</label></formula><p>where W (h)</p><formula xml:id="formula_8">Value ∈ R d ′ ×d . Since e (h) m ∈ R d ′</formula><p>is a combination of feature m and its relevant features (under head h), it represents a new combinatorial feature learned by our method. Furthermore, a feature is also likely to be involved in different combinatorial features, and we achieve this by using multiple heads, which create different subspaces and learn distinct feature interactions separately. We collect combinatorial features learned in all subspaces as follows:  </p><formula xml:id="formula_9">e m = e (1) m ⊕ e (2) m ⊕ · · · ⊕ e (H) m ,<label>(7)</label></formula><p>where ⊕ is the concatenation operator, and H is the number of total heads. To preserve previously learned combinatorial features, including raw individual (i.e., first-order) features, we add standard residual connections in our network. Formally,</p><formula xml:id="formula_10">e Res m = ReLU( e m + W Res e m ),<label>(8)</label></formula><p>where W Res ∈ R d ′ H ×d is the projection matrix in case of dimension mismatching <ref type="bibr" target="#b12">[12]</ref>, and ReLU(z) = max(0, z) is a non-linear activation function.</p><p>With such an interacting layer, the representation of each feature e m will be updated into a new feature representation e Res m , which is a representation of high-order features. We can stack multiple such layers with the output of the previous interacting layer as the input of the next interacting layer. By doing this, we can model arbitrary-order combinatorial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Output Layer</head><p>The output of the interacting layer is a set of feature vectors {e Res m } M m=1 , which includes raw individual features reserved by residual block and combinatorial features learned via the multi-head self-attention mechanism. For final CTR prediction, we simply concatenate all of them and then apply a non-linear projection as follows:</p><formula xml:id="formula_11">y = σ (w T (e Res 1 ⊕ e Res 2 ⊕ · · · ⊕ e Res M ) + b),<label>(9)</label></formula><p>where w ∈ R d ′ H M is a column projection vector which linearly combines concatenated features, b is the bias, and σ (x) = 1/(1+e −x ) transforms the values to users clicking probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Training</head><p>Our loss function is Log loss, which is defined as follows:</p><formula xml:id="formula_12">Loдloss = − 1 N N j=1 (y j log(ŷ j ) + (1 − y j ) log(1 −ŷ j )),<label>(10)</label></formula><p>where y j andŷ j are ground truth of user clicks and estimated CTR respectively, j indexes the training samples, and N is the total number of training samples. The parameters to learn in our model</p><formula xml:id="formula_13">are {V i , v m , W (h) Query , W (h) Key , W (h)</formula><p>Value , W Res , w, b}, which are updated via minimizing the total Logloss using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis Of AutoInt</head><p>Modeling Arbitrary Order Combinatorial Features. Given feature interaction operator defined by Equation 5 -8, we now analyze how low-order and high-order combinatorial features are modeled in our proposed model.</p><p>For simplicity, let's assume there are four feature fields (i.e., M=4) denoted by x 1 , x 2 , x 3 and x 4 respectively. Within the first interacting layer, each individual feature interacts with any other features through attention mechanism (i.e. Equation 5) and therefore a set of second-order feature combinations such as д(x 1 , x 2 ), д(x 2 , x 3 ) and д(x 3 , x 4 ) are captured with distinct correlation weights, where the non-additive property of interaction function д(·) (in DEFINITION 2) can be ensured by the non-linearity of activation function ReLU(·). Ideally, combinatorial features that involve x 1 can be encoded into the updated representation of the first feature field e Res 1 . As the same can be derived for other feature fields, all second-order feature interactions can be encoded in the output of the first interacting layer, where attention weights distill useful feature combinations.</p><p>Next, we prove that higher-order feature interactions can be modeled within the second interacting layer. Given the representation of the first feature field e Res 1 and the representation of the third feature field e Res 3 generated by the first interacting layer, third-order combinatorial features that involve x 1 , x 2 and x 3 can be modeled by allowing e Res 1 to attend on e Res 3 because e Res 1 contains the interaction д(x 1 , x 2 ) and e Res 3 contains the individual feature x 3 (from residual connection). Moreover, the maximum order of combinatorial features grows exponentially with respect to the number of interacting layers. For example, fourth-order feature interaction д(x 1 , x 2 , x 3 , x 4 ) can be captured by the combination of e Res 1 and e Res 3 , which contain the second-order interactions д(x 1 , x 2 ) and д(x 3 , x 4 ) respectively. Therefore a few interacting layers will suffice to model high-order feature interactions.</p><p>Based on above analysis, we can see that AutoInt learns feature interactions with attention mechanism in a hierarchical manner, i.e., from low-order to high-order, and all low-order feature interactions are carried by residual connections. This is promising and reasonable because learning hierarchical representation has proven quite effective in computer vision and speech processing with deep neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>Space Complexity. The embedding layer, which is a shared component in neural network-based methods <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b32">32]</ref>, contains nd parameters, where n is the dimension of sparse representation of input feature and d is the embedding size. As an interacting layer contains following weight matrices: {W </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we move forward to evaluate the effectiveness of our proposed approach. We aim to answer the following questions: RQ1 How does our proposed AutoInt perform on the problem of CTR prediction? Is it efficient for large-scale sparse and high-dimensional data? RQ2 What are the influences of different model configurations? RQ3 What are the dependency structures between different features? Is our proposed model explainable? RQ4 Will integrating implicit feature interactions further improve the performance? We first describe the experimental settings before answering these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Data Sets.</head><p>We use four public real-world data sets. The statistics of the data sets are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Criteo 3 This is a benchmark dataset for CTR prediction, which has 45 million users' clicking records on displayed ads. It contains 26 categorical feature fields and 13 numerical feature fields. Avazu 4 This dataset contains users' mobile behaviors including whether a displayed mobile ad is clicked by a user or not. It has 23 feature fields spanning from user/device features to ad attributes. KDD12 5 This data set was released by KDDCup 2012, which originally aimed to predict the number of clicks. Since our work focuses on CTR prediction rather than the exact number of clicks, we treat this problem as a binary classification problem (1 for clicks&gt;0, 0 for without click), which is similar to FFM <ref type="bibr" target="#b16">[16]</ref>. MovieLens-1M 6 This dataset contains users' ratings on movies. During binarization, we treat samples with a rating less than 3 as negative samples because a low score indicates that the user does not like the movie. We treat samples with a rating greater than 3 as positive samples and remove neutral samples, i.e., a rating equal to 3. Data Preparation First, we remove the infrequent features (appearing in less than threshold instances) and treat them as a single feature "&lt;unknown&gt;", where threshold is set to {10, 5, 10} for Criteo, Avazu and KDD12 data sets respectively. Second, since numerical features may have large variance and hurt machine learning algorithms, we normalize numerical values by transforming a value z to loд 2 (z) if z &gt; 2, which is proposed by the winner of Criteo Competition 7 . Third, we randomly select 80% of all samples for training and randomly split the rest into validation and test sets of equal size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation Metrics.</head><p>We use two popular metrics to evaluate the performance of all methods. AUC Area Under the ROC Curve (AUC) measures the probability that a CTR predictor will assign a higher score to a randomly chosen positive item than a randomly chosen negative item. A higher AUC indicates a better performance.</p><p>Logloss Since all models attempt to minimize the Logloss defined by Equation 10, we use it as a straightforward metric.</p><p>It is noticeable that a slightly higher AUC or lower Logloss at 0.001-level is regarded significant for CTR prediction task, which has also been pointed out in existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b38">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Competing Models.</head><p>We compare the proposed approach with three classes of previous models. (A) the linear approach that only uses individual features. (B) factorization machines-based methods that take into account second-order combinatorial features. (C) techniques that can capture high-order feature interactions. We associate the model classes with model names accordingly.</p><p>LR (A). LR only models the linear combination of raw features. FM <ref type="bibr" target="#b26">[26]</ref> (B). FM uses factorization techniques to model secondorder feature interactions.</p><p>AFM <ref type="bibr" target="#b40">[40]</ref> (B). AFM is one of the state-of-the-art models that capture second-order feature interactions. It extends FM by using attention mechanism to distinguish the different importance of second-order combinatorial features.</p><p>DeepCrossing <ref type="bibr" target="#b32">[32]</ref> (C). DeepCrossing utilizes deep fully-connected neural networks with residual connections to learn non-linear feature interactions in an implicit fashion.</p><p>NFM <ref type="bibr" target="#b13">[13]</ref> (C). NFM stacks deep neural networks on top of second-order feature interaction layer. High-order feature interactions are implicitly captured by the nonlinearity of neural networks.</p><p>CrossNet <ref type="bibr" target="#b38">[38]</ref> (C). Cross Network, which is the core of Deep&amp;Cross model, takes outer product of concatenated feature vector at the bit-wise level to model feature interactions explicitly.</p><p>CIN <ref type="bibr" target="#b19">[19]</ref> (C). Compressed Interaction Network, which is the core of xDeepFM model, takes outer product of stacked feature matrix at vector-wise level.</p><p>HOFM <ref type="bibr" target="#b4">[5]</ref> (C). HOFM proposes efficient kernel-based algorithms for training high-order factorization machines. Follow settings in Blondel et al. <ref type="bibr" target="#b4">[5]</ref> and He and Chua <ref type="bibr" target="#b13">[13]</ref>, we build a third-order factorization machine using public implementation.</p><p>We will compare with the full models of CrossNet and CIN, i.e., Deep&amp;Cross and xDeepFM, under the setting of joint training with plain DNN later (i.e., Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Implementation Details.</head><p>All methods are implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref>. For AutoInt and all baseline methods, we empirically set embedding dimension d to 16 and batch size to 1024. AutoInt has three interacting layers and the number of hidden units d ′ is 32 in default setting. Within each interacting layer, the number of  attention head is two 8 . To prevent overfitting, we use grid search to select dropout rate <ref type="bibr" target="#b34">[34]</ref> from {0.1 -0.9} for MovieLens-1M data set, and we found dropout is not necessary for other three large data sets. For baseline methods, we use one hidden layer of size 200 on top of Bi-Interaction layer for NFM as recommended by their paper. For CN and CIN, we use three interaction layers following AutoInt. DeepCrossing has four feed-forward layers and the number of hidden units is 100, because it performs poorly when using three neural layers. Once all network structures are fixed, we also apply grid search to baseline methods for optimal hype-parameters. Finally, we use Adam <ref type="bibr" target="#b17">[17]</ref> to optimize all deep neural network-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L R F M A F M D C C N C IN H O F M N F M A u to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Results (RQ1)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Effectiveness</head><p>We summarize the results averaged over 10 different runs into Table 2. We have the following observations: (1) FM and AFM, which explore second-order feature interactions, consistently outperform LR by a large margin on all datasets, which indicates that individual features are insufficient in CTR prediction. (2) An interesting observation is the inferiority of some models which capture highorder feature interactions. For example, although DeepCrossing and NFM use the deep neural network as a core component to learning high-order feature interactions, they do not guarantee improvement over FM and AFM. This may attribute to the fact that they learn feature interactions in an implicit fashion. On the contrary, CIN does it explicitly and outperforms low-order models consistently. (3) HOFM significantly outperforms FM on Criteo and MovieLens-1M datasets, which indicates that modeling third-order feature interactions can be beneficial to prediction performance. (4) AutoInt achieves the best performance overall baseline methods on three of four real-world data sets. On Avazu data set, CIN performs a little better than AutoInt in AUC evaluation, but we get lower Logloss. Note that our proposed AutoInt shares the same structures as DeepCrossing except the feature interacting layer, which indicates using the attention mechanism to learn explicit combinatorial features is crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Model Efficiency</head><p>We present the runtime results of different algorithms on four data sets in <ref type="figure" target="#fig_6">Figure 4</ref>. Unsurprisingly, LR is the most efficient algorithm due to its simplicity. FM and NFM perform similarly in terms of runtime because NFM only stacks a single feed-forward hidden layer on top of the second-order interaction layer. Among all listed methods, CIN, which achieves the best performance for prediction among all the baselines, is much more time-consuming due to its complicated crossing layer. This may make it impractical in the  We also compare the sizes of different models (i.e., the number of parameters) as another criterion for efficiency evaluation. As shown in <ref type="table" target="#tab_2">Table 3</ref>, comparing to the best model CIN in the baseline models, the number of parameters in AutoInt is much smaller.</p><p>To summarize, our proposed AutoInt achieves the best performance among all the compared models. Compared to the most competitive baseline model CIN, AutoInt requires much fewer parameters and is much more efficient during online inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis (RQ2)</head><p>To further validate and gain deep insights into the proposed model, we conduct ablation study and compare several variants of AutoInt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.1</head><p>Influence of Residual Structure. The standard AutoInt utilizes residual connections, which carry through all learned combinatorial features and therefore allow modeling very high-order combinations. To justify the contribution of residual units, we tease apart them from our standard model and keep other structures as they are. As presented in <ref type="table" target="#tab_3">Table 4</ref>, we observe that the performance decrease on all datasets if residual connections are removed. Specifically, the full model outperforms the variant by a large margin on the KDD12 and MovieLens-1M data, which indicates residual connections are crucial to model high-order feature interactions in our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Influence of Network Depths.</head><p>Our model learns high-order feature combinations by stacking multiple interacting layers (introduced in Section 4). Therefore, we are interested in how the performance change w.r.t. the number of interacting layers, i.e., the order of combinatorial features. Note that when there is no interacting layer (i.e., Number of layers equals zero), our model  takes the weighted sum of raw individual features as input, i.e., no combinatorial features are considered. The results are summarized in <ref type="figure" target="#fig_7">Figure 5</ref>. We can see that if one interacting layer is used, i.e., feature interactions are taken into account, the performance increase dramatically on both data sets, showing that combinatorial features are very informative for prediction. As the number of interacting layers further increases, i.e., higher-order combinatorial features are taken into account, the performance of the model further increases. When the number of layers reaches three, the performance becomes stable, showing that adding extremely high-order features are not informative for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Influence of Different Dimensions.</head><p>Next, we investigate the performance w.r.t. the parameter d, which is the output dimension of the embedding layer. On the KDD12 dataset, we can see that the performance continuously increase as we increase the dimension size since larger models are used for prediction. The results are different on the MovieLens-1M dataset. When the dimension size reaches 24, the performance begins to decrease. The reason is that this data set is small, and the model is overfitted when too many parameters are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Explainable Recommendations (RQ3)</head><p>A good recommender system can not only provide good recommendations but also offer good explainability. Therefore, in this part, we present how our AutoInt is able to explain the recommendation results. We take the MovieLens-1M dataset as an example.</p><p>Let's look at a recommendation result suggested by our algorithm, i.e., a user likes an item. <ref type="figure" target="#fig_9">Figure 7</ref> (a) presents the correlations between different fields of input features, which are obtained by  the attention score. We can see that AutoInt is able to identify the meaningful combinatorial feature &lt;Gender=Male, Age= <ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref>, MovieGenre=Action&amp;Triller&gt; (i.e., red dotted rectangle). This is very reasonable since young men are very likely to prefer action&amp;triller movies.</p><p>We are also interested in what the correlations between different feature fields in the data are. Therefore, we measure the correlations between the feature fields according to their average attention score in the entire data. The correlations between different fields are summarized into <ref type="figure" target="#fig_9">Figure 7</ref> (b). We can see that &lt;Gender, Genre&gt;, &lt;Age, Genre&gt;, &lt;RequestTime, ReleaseTime&gt; and &lt;Gender, Age, Genre&gt; (i.e., solid green region) are strongly correlated, which are the explainable rules for recommendation in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Integrating Implicit Interactions (RQ4)</head><p>Feed-forward neural networks are capable of modeling implicit feature interactions and have been widely integrated into existing CTR prediction methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b19">19]</ref>. To investigate whether integrating implicit feature interactions further improves the performance, we combine AutoInt with a two-layer feed-forward neural network by joint training. We name the joint model AutoInt+ and compare it with the following algorithms:</p><p>• Wide&amp;Deep <ref type="bibr" target="#b7">[8]</ref>. Wide&amp;Deep integrates the outputs of logistic regression and feed-forward neural networks. • DeepFM <ref type="bibr" target="#b11">[11]</ref>. DeepFM combines trainditional second-order factorization machines and feed-forward neural network, with a shared embedding layer. • Deep&amp;Cross <ref type="bibr" target="#b38">[38]</ref>. Deep&amp;Cross is the extension of CrossNet by integrating feed-forward neural networks.</p><p>• xDeepFM <ref type="bibr" target="#b19">[19]</ref>. xDeepFM is the extension of CIN by integrating feed-forward neural networks. <ref type="table" target="#tab_4">Table 5</ref> presents the averaged results (over 10 runs) of jointtraining models. We have the following observations: 1) The performance of our method improves by joint training with feed-forward neural networks on all datasets. This indicates that integrating implicit feature interactions indeed boosts the predictive ability of our proposed model. However, as can be seen from last two columns, the magnitude of performance improvement is fairly small compared to other models, showing that our individual model AutoInt is quite powerful. 2) After integrating implicit feature interactions, AutoInt+ outperforms all competitive methods, and achieves new state-of-the-art performances on used CTR prediction data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose a novel CTR prediction model based on self-attention mechanism, which can automatically learn high-order feature interactions in an explicit fashion. The key to our method is the newly-introduced interacting layer, which allows each feature to interact with the others and to determine the relevance through learning. Experimental results on four real-world data sets demonstrate the effectiveness and efficiency of our proposed model. Besides, we provide good model explainability via visualizing the learned combinatorial features. When integrating with implicit feature interactions captured by feed-forward neural networks, we achieve better offline AUC and Logloss scores compared to the previous state-of-the-art methods.</p><p>For future work , we are interested in incorporating contextual information into our method and improving its performance for online recommender systems. Besides, we also plan to extend AutoInt for general machine learning tasks, such as regression, classification and ranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our proposed model AutoInt. The details of embedding layer and interacting layer are illustrated inFigure 2andFigure 3respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>- ure 1 ,Figure 2 :</head><label>12</label><figDesc>our proposed method takes the sparse feature vector x as input, followed by an embedding layer that projects all features Illustration of input and embedding layer, where both categorical and numerical fields are represented by lowdimensional dense vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Key ∈ R d ′ ×d in Equation 5 are transformation matrices which map the original embedding space R d into a new space R d ′ . Next, we update the representation of feature m in subspace h via combining all relevant features guided by coefficients α (h) m,k :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of interacting layer. Combinatorial features are conditioned on attention weights, i.e., α (h) m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Value , W Res }, the number of parameters in an L-layer network is L×(3dd ′ +d ′ Hd), which is independent of the number of feature fields M. Finally, there are d ′ HM + 1 parameters in the output layer. As far as interacting layers are concerned, the space complexity is O(Ldd ′ H ).Note that H and d ′ are usually small (e.g., H = 2 and d ′ = 32 in our experiments), which makes the interacting layer memory-efficient.Time Complexity. Within each interacting layer, the computation cost is two-fold. First, calculating attention weights for one head takes O(Mdd ′ + M 2 d ′ ) time. Afterwards, forming combinatorial features under one head also takes O(Mdd ′ + M 2 d ′ ) time. Because we have H heads, it takes O(MHd ′ (M + d)) time altogether. It is therefore efficient because H, d and d ′ are usually small. We provide running time of AutoInt in Section 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Efficiency Comparison of Different Algorithms in terms of Run Time. "DC" and "CN" are DeepCrossing and CrossNet for short, respectively. Since HOFM cannot be fit on one GPU card for the KDD12 dataset, extra communication cost makes it most time-consuming. Further analysis is presented in Section 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Performance w.r.t. the number of interacting layers. Results on Criteo and Avazu data sets are similar and hence omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Performance w.r.t. number of embedding dimensions. Results on Criteo and Avazu data sets are similar and hence omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>(a) Label=1, Predicted CTR=0.89 (b) Overall feature interactions Heat maps of attention weights for both caseand global-level feature interactions on MovieLens-1M. The axises represent feature fields &lt;Gender, Age, Occupation, Zipcode, RequestTime, RealeaseTime, Genre&gt;. We highlight some learned combinatorial features in rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of evaluation data sets.</figDesc><table><row><cell>Data</cell><cell>#Samples</cell><cell cols="2">#Fields #Features (Sparse)</cell></row><row><cell>Criteo</cell><cell>45,840,617</cell><cell>39</cell><cell>998,960</cell></row><row><cell>Avazu</cell><cell>40,428,967</cell><cell>23</cell><cell>1,544,488</cell></row><row><cell>KDD12</cell><cell>149,639,105</cell><cell>13</cell><cell>6,019,086</cell></row><row><cell>MovieLens-1M</cell><cell>739,012</cell><cell>7</cell><cell>3,529</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness Comparison of Different Algorithms. We highlight that our proposed model almost outperforms all baselines across four data sets and both metrics. Further analysis is provided in Section 5.2.</figDesc><table><row><cell>Model Class</cell><cell>Model</cell><cell>AUC</cell><cell cols="2">Criteo Logloss</cell><cell>AUC</cell><cell cols="2">Avazu Logloss</cell><cell>AUC</cell><cell cols="2">KDD12 Logloss</cell><cell>MovieLens-1M AUC Logloss</cell></row><row><cell>First-order</cell><cell>LR</cell><cell>0.7820</cell><cell></cell><cell>0.4695</cell><cell cols="2">0.7560</cell><cell>0.3964</cell><cell cols="2">0.7361</cell><cell>0.1684</cell><cell>0.7716</cell><cell>0.4424</cell></row><row><cell>Second-order</cell><cell>FM [26] AFM[40]</cell><cell>0.7836 0.7938</cell><cell></cell><cell>0.4700 0.4584</cell><cell cols="2">0.7706 0.7718</cell><cell>0.3856 0.3854</cell><cell cols="2">0.7759 0.7659</cell><cell>0.1573 0.1591</cell><cell>0.8252 0.8227</cell><cell>0.3998 0.4048</cell></row><row><cell></cell><cell>DeepCrossing [32]</cell><cell>0.8009</cell><cell></cell><cell>0.4513</cell><cell cols="2">0.7643</cell><cell>0.3889</cell><cell cols="2">0.7715</cell><cell>0.1591</cell><cell>0.8448</cell><cell>0.3814</cell></row><row><cell></cell><cell>NFM [13]</cell><cell>0.7957</cell><cell></cell><cell>0.4562</cell><cell cols="2">0.7708</cell><cell>0.3864</cell><cell cols="2">0.7515</cell><cell>0.1631</cell><cell>0.8357</cell><cell>0.3883</cell></row><row><cell>High-order</cell><cell>CrossNet [38] CIN [19]</cell><cell>0.7907 0.8009</cell><cell></cell><cell>0.4591 0.4517</cell><cell cols="2">0.7667 0.7758</cell><cell>0.3868 0.3829</cell><cell cols="2">0.7773 0.7799</cell><cell>0.1572 0.1566</cell><cell>0.7968 0.8286</cell><cell>0.4266 0.4108</cell></row><row><cell></cell><cell>HOFM [5]</cell><cell>0.8005</cell><cell></cell><cell>0.4508</cell><cell cols="2">0.7701</cell><cell>0.3854</cell><cell cols="2">0.7707</cell><cell>0.1586</cell><cell>0.8304</cell><cell>0.4013</cell></row><row><cell></cell><cell>AutoInt (ours)</cell><cell cols="2">0.8061**</cell><cell>0.4455**</cell><cell cols="2">0.7752</cell><cell>0.3824</cell><cell cols="2">0.7883**</cell><cell>0.1546**</cell><cell>0.8456*</cell><cell>0.3797**</cell></row><row><cell cols="11">AutoInt outperforms the strongest baseline w.r.t. Criteo, KDD12 and MovieLens-1M data at the: ** 0.01 and * 0.05 level, unpaired t-test.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Efficiency Comparison of Different Algorithms in terms of Model Size on Criteo data set. "DC" and "CN" are DeepCrossing and CrossNet for short, respectively. The counted parameters exclude the embedding layer.</figDesc><table><row><cell>Model</cell><cell>DC</cell><cell>CN</cell><cell>CIN</cell><cell>NFM</cell><cell>AutoInt</cell></row><row><cell cols="6">#Params 1.6 × 10 5 3 × 10 3 1.9 × 10 6 4 × 10 3 3.9 × 10 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study comparing the performance of Au-toInt with and without residual connections. AutoInt w/ is the complete model while the AutoInt w/o is the model without residual connection.</figDesc><table><row><cell>Data Sets</cell><cell>Models</cell><cell>AUC</cell><cell>Logloss</cell></row><row><cell>Criteo</cell><cell>AutoInt w/ AutoInt w/o</cell><cell>0.8061 0.8033</cell><cell>0.4454 0.4478</cell></row><row><cell>Avazu</cell><cell>AutoInt w/ AutoInt w/o</cell><cell>0.7752 0.7729</cell><cell>0.3823 0.3836</cell></row><row><cell>KDD12</cell><cell>AutoInt w/ AutoInt w/o</cell><cell>0.7888 0.7831</cell><cell>0.1545 0.1557</cell></row><row><cell>MovieLens-1M</cell><cell>AutoInt w/ AutoInt w/o</cell><cell>0.8460 0.8299</cell><cell>0.3784 0.3959</cell></row><row><cell cols="4">industrial scenarios. Note that AutoInt is sufficiently efficient, which</cell></row><row><cell cols="4">is comparable to the efficient algorithms DeepCrossing and NFM.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of Integrating Implicit Feature Interactions. We indicate the base model behind each method. The last two columns are average changes of AUC and Logloss compared to corresponding base models ("+": increase, "-": decrease).AutoInt+ outperforms the strongest baseline w.r.t. each data at the: ** 0.01 and * 0.05 level, unpaired t-test.</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell cols="2">Criteo Logloss</cell><cell>AUC</cell><cell cols="2">Avazu Logloss</cell><cell cols="2">KDD12 AUC Logloss</cell><cell cols="2">MovieLens-1M AUC Logloss</cell><cell>Avg. Changes AUC Logloss</cell></row><row><cell>Wide&amp;Deep (LR)</cell><cell cols="2">0.8026</cell><cell>0.4494</cell><cell cols="2">0.7749</cell><cell>0.3824</cell><cell>0.7549</cell><cell>0.1619</cell><cell>0.8300</cell><cell>0.3976</cell><cell>+0.0292 -0.0213</cell></row><row><cell>DeepFM (FM)</cell><cell cols="2">0.8066</cell><cell>0.4449</cell><cell cols="2">0.7751</cell><cell>0.3829</cell><cell>0.7867</cell><cell>0.1549</cell><cell>0.8437</cell><cell>0.3846</cell><cell>+0.0142 -0.0113</cell></row><row><cell>Deep&amp;Cross (CN)</cell><cell cols="2">0.8067</cell><cell>0.4447</cell><cell cols="2">0.7731</cell><cell>0.3836</cell><cell>0.7872</cell><cell>0.1549</cell><cell>0.8446</cell><cell>0.3809</cell><cell>+0.0200 -0.0164</cell></row><row><cell>xDeepFM (CIN)</cell><cell cols="2">0.8070</cell><cell>0.4447</cell><cell cols="2">0.7770</cell><cell>0.3823</cell><cell>0.7820</cell><cell>0.1560</cell><cell>0.8463</cell><cell>0.3808</cell><cell>+0.0068 -0.0096</cell></row><row><cell>AutoInt+ (ours)</cell><cell cols="10">0.8083** 0.4434** 0.7774* 0.3811** 0.7898** 0.1543** 0.8488** 0.3753** +0.0023 -0.0020</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://labs.criteo.com/2014/09/kaggle-contest-dataset-now-available-academic-use/ 2 In this paper, we will use "combinatorial feature" and "feature interaction" interchangeably as they are both used in the literature<ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b32">32]</ref> .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.kaggle.com/c/criteo-display-ad-challenge 4 https://www.kaggle.com/c/avazu-ctr-prediction 5 https://www.kaggle.com/c/kddcup2012-track2 6 https://grouplens.org/datasets/movielens/ 7 https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We also tried different number of attention heads. The performance of using one head is inferior to that of two heads, and the improvement of further increasing head number is not significant.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>The authors would like to thank all the anonymous reviewers for their insightful comments. We thank Xiao Xiao and Jianbo Dong for the discussion on recommendation mechanism in China University MOOC platform. We also thank Meng Qu for reviewing the initial version of this paper. Weiping Song and   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Cross: Making Use of Context in Recurrent Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Higher-order factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient boosting factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender systems</title>
		<meeting>the 8th ACM Conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Web-scale Bayesian Click-through Rate Prediction for Sponsored Search Advertising in Microsoft&apos;s Bing Search Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><forename type="middle">Quiñonero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-machine Based Neural Network for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NAIS: Neural attentive item similarity model for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fieldaware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of hierarchical representations with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: A View from the Trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Key-Value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03795</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Exponential machines. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting response in mobile advertising with hierarchical importanceaware factorization machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finegold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorizing personalized markov chains for next-basket recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting ad clickthrough rates via feature-based fully coupled interaction tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research and Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep crossing: Web-scale modeling without manually crafted combinatorial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Session-based Social Recommendation via Dynamic Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="555" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep &amp; Cross Network for Ad Click Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">TEM: Tree-enhanced Embedding Model for Explainable Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1543" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attentional factorization machines: learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3119" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1311" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Interest Network for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep embedding forest: Forest-based serving with deep embedding features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holakou</forename><surname>Rahmanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1703" to="1711" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
