<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
							<email>weiyuewa@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego Los Angeles</settlement>
									<region>California San Diego</region>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
							<email>qianguih@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation on 2D images have achieved promising results recently <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b23">23]</ref>. With the rise of autonomous driving and robotics applications, the demand for 3D scene understanding and the availability of 3D scene data has rapidly increased in recently. Unfortunately, the literature for 3D instance segmentation and object detection lags far behind its 2D counterpart; scene understanding with Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b10">11]</ref> on 3D volumetric data is limited by high memory and computation cost. Recently, deep learning frameworks Point-Net/Pointnet++ <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b36">35]</ref> on point clouds open up more efficient and flexible ways to handle 3D data.</p><p>Following the pioneering works in 2D scene understanding, our goal is to develop a novel deep learning framework trained end-to-end for 3D instance-aware semantic segmentation on point clouds that, like established baseline systems for 2D scene understanding tasks, is intuitive, simple, flexi- An important consideration for instance segmentation on a point cloud is how to represent output results. Inspired by the trend of predicting proposals for tasks with a variable number of outputs, we introduce a Similarity Group Proposal Network (SGPN), which formulates group proposals of object instances by learning a novel 3D instance segmentation representation in the form of a similarity matrix .</p><p>Our pipeline first uses PointNet/PointNet++ to extract a descriptive feature vector for each point in the point cloud. As a form of similarity metric learning, we enforce the idea that points belonging to the same object instance should have very similar features; hence we measure the distance between the features of each pair of points in order to form a similarity matrix that indicates whether any given pair of points belong to the same object instance.</p><p>The rows in our similarity matrix can be viewed as instance candidates, which we combine with learned confidence scores in order to generate plausible group proposals. We also learn a semantic segmentation map in order to classify each object instance obtained from our group proposals. We are also able to directly derive tight 3D bounding boxes for object detection.</p><p>By simply measuring the distance between overdetermined feature representations of each pair of points, our similarity matrix simplifies our pipeline in that we remain in the natural point cloud representation of defining our objects by the relationships between points.</p><p>In summary, SGPN has three output branches for instance segmentation on point clouds: a similarity matrix yielding point-wise group proposals, a confidence map for pruning these proposals, and a semantic segmentation map to give the class label for each group.</p><p>We evaluate our framework on both 3D shapes (ShapeNet <ref type="bibr" target="#b3">[4]</ref>) and real 3D scenes (Stanford Indoor Semantic Dataset <ref type="bibr" target="#b0">[1]</ref> and NYUV2 <ref type="bibr" target="#b43">[42]</ref>) and demonstrate that SGPN achieves state-of-the-art results on 3D instance segmentation. We also conduct comprehensive experiments to show the capability of SGPN on achieving high performance on 3D semantic segmentation and 3D object detection on point clouds. Although a minimalistic framework with no bells and whistles already gives visually pleasing results ( <ref type="figure" target="#fig_0">Figure 1</ref>), we also demonstrate the flexibility of SGPN as we boost performance even more by seamlessly integrating CNN features from RGBD images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Detection and Instance Segmentation</head><p>Recent advances in object detection <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">25]</ref> and instance segmentation <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b32">31]</ref> on 2D images have achieved promising results. R-CNN <ref type="bibr" target="#b14">[15]</ref> for 2D object detection established a baseline system by introducing region proposals as candidate object regions. Faster R-CNN <ref type="bibr" target="#b40">[39]</ref> leveraged a CNN learning scheme and proposed Region Proposal Networks(RPN). YOLO <ref type="bibr" target="#b38">[37]</ref> divided the image into grids and each grid cell produced an object proposal. Many 2D instance segmentation approaches are based on segment proposals. DeepMask <ref type="bibr" target="#b32">[31]</ref> learns to generate segment proposals each with a corresponding object score. Dai et al. <ref type="bibr" target="#b9">[10]</ref> predict segment candidates from bounding box proposals. Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> extended Faster R-CNN by adding a branch on top of RPN to produce object masks for instance segmentation.</p><p>Following these pioneering 2D works, 3D bounding box detection frameworks have emerged <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5]</ref>. Song and Xiao <ref type="bibr" target="#b46">[45]</ref> use a volumetric CNN to create 3D RPN on a voxelized 3D scene and then use both the color and depth data of the image in a joint 3D and 2D object recognition network on each proposal. Deng and Latecki <ref type="bibr" target="#b10">[11]</ref> regress class-wise 3D bounding box models based on RGBD image appearance features only. Armeni et al <ref type="bibr" target="#b0">[1]</ref> use a sliding shape method with CRF to perform 3D object detection on point cloud. To the best of our knowledge, no previous work exists that learns 3D instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Deep Learning</head><p>Convolutional neural networks generalize well to 3D by performing convolution on voxels for certain tasks such as object classification <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30]</ref>, shape reconstruction <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref> of simple objects, and 3D object detection as mentioned in Section 2.1. However, volumetric representation carry a high memory and computational cost and have strong limitations dealing with 3D scenes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b47">46]</ref>. Octree-based CNNs <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b49">48]</ref> have been introduced recently, but they are less flexible than volumetric CNNs and still suffer from memory efficiency problems.</p><p>A point cloud is an intuitive, memory-efficient 3D representation well-suited for representing detailed, large scenes for 3D instance segmentation using deep learning. Point-Net/Pointnet++ <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b36">35]</ref> recently introduce deep neural networks on 3D point clouds, learning successful results for tasks such as object classification and part and semantic scene segmentation. We base our network architecture off of PointNet/PointNet++, achieving a novel method that learns 3D instance segmentation on point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Similarity Metric Learning</head><p>Our work is also closely related to similarity metric learning, which has been widely used in deep learning on various tasks such as person re-identification <ref type="bibr" target="#b53">[52]</ref>, matching <ref type="bibr" target="#b15">[16]</ref>, image retrival <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">50]</ref> and face recognition <ref type="bibr" target="#b5">[6]</ref>. Siamese CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b2">3]</ref> are used on tasks such as tracking <ref type="bibr" target="#b22">[22]</ref> and one-shot learning <ref type="bibr" target="#b19">[20]</ref> by measuring the similarity of two input images. Alejandro et. al <ref type="bibr" target="#b29">[28]</ref> introduced an associative embedding method to group similar pixels for multi-person pose estimation and 2D instance segmentation by enforcing that pixels in the same group should have similar values in their embedding space without actually enforcing what those exact values should be. Our method exploits metric learning in a different way in that we regress the likelihood of two points belonging to the same group and formulate the similarity matrix as group proposals to handle variable number of instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The goal of this paper is to take a 3D point cloud as input and produce an object instance label for each point and a class label for each instance. Utilizing recent developments in deep learning on point clouds <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b36">35]</ref>, we introduce a Similarity Group Proposal Network (SGPN), which consumes a 3D point cloud and outputs a set of instance proposals that each contain the group of points inside the instance as well as its class label. Section 3.1 introduces the design and properties of SGPN. Section 3.2 proposes an algorithm to merge similar groups and give each point an instance label. Section 3.3 gives implementation details. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the overview of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Similarity Group Proposal Network</head><p>SGPN is a very simple and intuitive framework. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, it first passes a point cloud P of size N p through a feed-forward feature extraction network inspired by PointNets <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b36">35]</ref>, learning both global and local features in the point cloud. This feature extraction network produces a matrix F . SGPN then diverges into three branches that each pass F through a single PointNet layer to obtain sized N p × N f feature matrices F SIM , F CF , F SEM , which we respectively use to obtain a similarity matrix, a confidence map and a semantic segmentation map. The ith row in a N p ×N f feature matrix is a N f -dimensional vector that represents point P i in an embedded feature space. Our loss L is given by the sum of the losses from each of these three branches: L = L SIM + L CF + L SEM . Our network architecture can be found in the supplemental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Matrix</head><p>We propose a novel similarity matrix S from which we can formulate group proposals to directly recover accurate instance segmentation results. S is of dimensions N p × N p , and element S ij classifies whether or not points P i and P j belong to the same object instance. Each row of S can be viewed as a proposed grouping of points that form a candidate object instance.</p><p>We leverage that points belonging to the same object instance should have similar features and lie very close together in feature space. We obtain S by, for each pair of points {P i , P j }, simply subtracting their corresponding feature vectors {F SIMi , F SIMj } and taking the L 2 norm such that S ij = ||F SIMi − F SIMj || 2 . This reduces the problem of instance segmentation to learning an embedding space where points in the same instance are close together and those in different object instances are far apart.</p><p>For a better understanding of how SGPN captures correlation between points, in <ref type="figure">Figure 3</ref>(a) we visualize the similarity (euclidean distance in feature space) between a given point and the rest of the points in the point cloud. Points in different instances have greater euclidean distances in feature space and thus smaller similarities even though they have the same semantic labels. For example, in the bottomright image of <ref type="figure">Figure 3</ref>(a), although the given <ref type="table">table leg point  has greater similarity with the other table leg points than the  table top, it is still distinguishable from the other table leg.</ref> We believe that a similarity matrix is a more natural and simple representation for 3D instance segmentation on a point cloud compared to traditional 2D instance segmentation representations. Most state-of-the-art 2D deep learning methods for instance segmentation first localize the image into patches, which are then passed through a neural network and segment a binary mask of the object.</p><p>While learning a binary mask in a bounding box is a more natural representation for space-centric structures such as images or volumetric grids where features are largely defined by which positions in a grid have strong signals, point clouds can be viewed as shape-centric structures where information is encoded by the relationship between the points in the cloud, so we would prefer to also define instance segmentation output by the relationship between points without working too much in grid space.</p><p>Hence we expect that a deep neural network could better learn our similarity matrix, which compared to traditional representations is a more natural and straightforward representation for instance segmentation in a point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Double-Hinge Loss for Similarity Matrix</head><p>As is the case in <ref type="bibr" target="#b29">[28]</ref>, in our similarity matrix we do not need to precisely regress the exact values of our features; we only optimize the simpler objective that similar points should be close together in feature space. We define three potential similarity classes for each pair of points {P i , P j }: 1) P i and P j belong to the same object instance; 2) P i and P j share the same semantic class but do not belong to the same object instance; 3) P i and P j do not share the same semantic class. Pairs of points should lie progressively further away from each other in feature space as their similarity class increases. We define out loss as:</p><formula xml:id="formula_0">L SIM = Np i Np j l(i, j) l(i, j) =      ||F SIMi − F SIMj || 2 C ij = 1 α max(0, K 1 − ||F SIMi − F SIMj || 2 ) C ij = 2 max(0, K 2 − ||F SIMi − F SIMj || 2 ) C ij = 3</formula><p>where C ij indicates which of the similarity classes defined above does the pair of points ({P i , P j )} belong to and α, K 1 , K 2 are constants such that α &gt; 1,</p><formula xml:id="formula_1">K 2 &gt; K 1 .</formula><p>Although the second and third similarity class are treated equivalently for the purposes of instance segmentation, distinguishing between them in L SIM using our double-hinge loss allows our similarity matrix output branch and our semantic segmentation output branch to mutually assist each other for increased accuracy and convergence speed. Since the semantic segmentation network is actually wrongly trying to bring pairs of points in our second similarity class closer together in feature space, we also add an α &gt; 1 term to increase the weight of our loss to dominate the gradient from the semantic segmentation output branch.  At test time if S ij &lt; T h S where T h S &lt; K 1 , then points pair P i and P j are in the same instance group.</p><p>Similarity Confidence Map SGPN also feeds F CF through an additional PointNet layer to predict a N p × 1 confidence map CM reflecting how confidently the model believes that each grouping candidate is indeed a correct object instance. <ref type="figure">Figure 3</ref>(b) provides a visualization of the confidence map; points located in the boundary area between parts have lower confidence.</p><p>We regress confidence scores based on ground truth groups G represented as a N p × N p matrix identical in form to our similarity matrix. If P i is a background point that does not belong to any object in the ground truth then the row G i will be all zeros. For each row in S i , we expect the ground-truth value in the confidence map CM i to be the intersection over union (IoU) between the set of points in the predicted group S i and the ground truth group G i . Our loss L CF is the L2 loss between the inferred and expected CM .</p><p>Although the loss L CF depends on the similarity matrix output branch during training, at test time we run the branches in parallel and only groups with confidence greater than a threshold T h C are considered valid group proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation Network</head><p>The semantic segmentation map acts as a point-wise classifier. SGPN passes F SEM through an additional PointNet layer whose architecture depends on the number of possible semantic classes, yielding the final output M SEM , which is a N p × N C sized matrix where N C is the number of possible object categories. M SEMij corresponds to the probability that point P i belongs to class C j .</p><p>The loss L SEM is a weighted sum of the cross entropy softmax loss for each row in the matrix. We use median frequency balancing <ref type="bibr" target="#b1">[2]</ref> and the weight assigned to a category is ac = medianf req/f req(c), where f req(c) is the total number of points of class c divided by the total number of points in samples where c is present, and medianf req is the median of these f req(c).</p><p>At test time, the class label for a group instance is assigned by calculating the mode of the semantic labels of the points in that group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group Proposal Merging</head><p>The similarity matrix S produces N p group proposals, many of which are noisy or represent the same object. We first discard proposals with predicted confidence less than T h C or cardinality less than T h M 2 . We further prune our proposals into clean, non-overlapping object instances by applying Non-Maximum Suppression; groups with IoU greater than T h M 1 are merged together by selecting the group with the maximum cardinality.</p><p>Each point is then assigned to the group proposal that contains it. In the rare case (∼ 2%) that after the merging stage a point belongs to more than one final group proposal, this usually means that the point is at the boundary between two object instances, which means that the effectiveness of our network would be roughly the same regardless of which group proposal the point is assigned to. Hence, with minimal loss in accuracy we randomly assign the point to any one of the group proposals that contains it. We refer to this process as GroupMerging throughout the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>We use an ADAM <ref type="bibr" target="#b18">[19]</ref> optimizer with initial learning rate 0.0005, momentum 0.9 and batch size 4. The learning rate is divided by 2 every 20 epochs. The network is trained with only the L SIM loss for the first 5 epochs. In our experiment, α is set to 2 initially and is increased by 2 every 5 epochs. This design makes the network more focused on separating features of points that belong to different object instances but have the same semantic labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate SGPN on 3D instance segmentation on the following datasets:</p><p>• Stanford 3D Indoor Semantics Dataset (S3DIS) <ref type="bibr" target="#b0">[1]</ref>:</p><p>This dataset contains 3D scans in 6 areas including 271 rooms. The input is a complete point cloud generated from scans fused together from multiple views. Each point has semantic labels and instance annotations.</p><p>• NYUV2 <ref type="bibr" target="#b43">[42]</ref>: Partial point clouds are generated from single view RGBD images. The dataset is annotated with 3D bounding boxes and 2D semantic segmentation masks. We use the improved annotation in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Since both 3D bounding boxes and 2D segmentation masks annotations are given, ground truth 3D instance segmentation labels for point clouds can be easily generated We follow the standard split with 795 training images and 654 testing images.</p><p>• ShapeNet <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b54">53]</ref> Part Segmentation: ShapeNet contains 16, 881 shapes annotated with 50 types of parts in total. Most object categories are labeled with two to five parts. We use the official split of 795 training samples and 654 testinn percentageg samples in our experiments.</p><p>We also show the capability of SGPN to improve semantic segmentation and 3D object detection. To validate the flexibility of SGPN, we also seamlessly incorporate 2D CNN features into our network to boos performance on the NYUV2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">S3DIS Instance Segmentation and 3D Object Detection</head><p>We perform experiments on Stanford 3D Indoor Semantic Dataset to evaluate our performance on large real scene scans. Following experimental settings in PointNet <ref type="bibr" target="#b34">[33]</ref>, points are uniformly sampled into blocks of area 1m × 1m. Each point is labeled as one of 13 categories (chair, table, floor, wall, clutter etc.) and represented by a 9D vector (XYZ, RGB, and normalized location as to the room). At train time we uniformly sample 4096 points in each block, and at test time we use all points in the block as input.</p><p>SGPN uses PointNet as its baseline architecture for this experiment. <ref type="bibr" target="#b0">1</ref>  <ref type="figure">Figure 5</ref> shows instance segmentation results on S3DIS with SGPN. Different colors represent different instances. Point colors of the same group are not necessarily the same as their counterparts in the ground truth since object instances are unordered. To visualize instance classes, we also add semantic segmentation results. SGPN achieves good performance on various room types.</p><p>We also compare instance segmentation performance with the following method (which we call Seg-Cluster): Perform semantic segmentation using our network and then select all points as seeds. Starting from a seed point, BFS is used to search neighboring points with the same label. If a cluster with more than 200 points has been found, it is viewed as a valid group. Our GroupMerging algorithm is then used to merge these valid groups.</p><p>We calculate the IoU on points between each predicted and ground truth group. A detected instance is considered as true positive if the IoU score is greater than a threshold. The average precision (AP) is further calculated for instance segmentation performance evaluation. <ref type="table" target="#tab_1">Table 1</ref> shows the AP for every category with IoU threshold 0.5. To the best of our knowledge, there are no existing instance segmentation method on point clouds for arbitrary object categories, so we further demonstrate the capability of SGPN to handle various objects by adding the 3D detection results of Armeni et al. <ref type="bibr" target="#b0">[1]</ref> on S3DIS to <ref type="table" target="#tab_1">Table 1</ref>. The difference in evaluation metrics between our method and <ref type="bibr" target="#b0">[1]</ref> is that the IoU threshold of <ref type="bibr" target="#b0">[1]</ref> is 0.5 on a 3D bounding box and the IoU calculation of our method is on points. Despite this difference in metrics, we can still see our superior performance on both large and small objects.</p><p>We see that a naive method like Seg-Cluster tends to properly separate regions far away for large objects like the ceiling and floor. However for small object, Seg-Cluster fails to segment instances with the same label if they are close to each other. Mean APs with different IoU thresholds (0.25, 0.5, 0.75) are also evaluated in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> shows qualitative comparison results.</p><p>Once we have instance segmentation results, we can compute the bounding box for every instance and thus produce 3D object detection predictions. In <ref type="table" target="#tab_3">Table 3</ref>, we compare out method with the 3D object detection system introduced in PointNet <ref type="bibr" target="#b34">[33]</ref>, which to the best of our knowledge is the state-of-the-art method for 3D detection on S3DIS. Detection performance is evaluated over 4 categories AP with IoU threshold 0.5.</p><p>The method introduced in PointNet clusters points given semantic segmentation results and uses a binary classification network for each category to separate close objects with same categories. Our method outperforms it by a large margin, and unlike PointNet does not require an additional network, which unnecessarily introduces additional complexdoes not. To make fair comparison, we use PointNet as our baseline architecture for this experiment while using PointNet++ in Sections 4. <ref type="bibr" target="#b1">2</ref>      <ref type="figure" target="#fig_3">Figure 4</ref>) since points in different instances are far apart in feature space even though they have the same semantic label. We further compare our semantic segmentation results with PointNet in <ref type="table" target="#tab_4">Table 4</ref>. SGPN outperforms its baseline with the help of its similarity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NYUV2 Object Detection and Instance Segmentation Evaluation</head><p>We evaluate the effectiveness of our approach on partial 3D scans on the NYUV2 dataset. In this dataset, 3D point clouds are lifted from a single RGBD image. An image of size H × W can produce H × W points. We subsample this point cloud by resizing the image to H 4 × W 4 and get the corresponding points using a nearest neighbor search. Both our training and testing experiments are conducted on such a point cloud. PointNet++ is used as our baseline.</p><p>In <ref type="bibr" target="#b37">[36]</ref>, 2D CNN features are combined 3D point cloud for RGBD semantic segmentation. By leveraging the flexibility of SGPN, we also seamlessly integrate 2D CNN features from RGB images to boost performance. A 2D CNN consumes an RGBD map and extracts feature maps points for every image, a feature vector of size N f 2 can be extracted from F 2 at each pixel location. Every feature vector is concatenated to F (a N p × N F feature matrix produced by PointNet/PointNet++ as mentioned in Section 3.1) for each corresponding point, yielding a feature map of size N P × (N F + N F 2 ), which we then feed to our output branches. <ref type="figure" target="#fig_5">Figure 6</ref> illustrates this procedure; we call this pipeline SGPN-CNN. In our experiments, we use a pre-trained AlexNet model <ref type="bibr" target="#b21">[21]</ref> (with the first layer stride 1) and extract F 2 from the conv5 layer. We use H × W = 316 × 415 and N p = 8137. The 2D CNN and SGPN are trained jointly.</p><formula xml:id="formula_2">F 2 with size H 4 × W 4 × N F 2 . Since there are H 4 × W 4 sub-sampled (a) (b) (c) (d) (e)</formula><p>Evaluation is performed on 19 object categories. <ref type="figure" target="#fig_6">Figure 7</ref> shows qualitative results on instance segmentation of SGPN. <ref type="table">Table 5</ref> shows comparisons between Seg-Cluster and our SGPN and CNN-SGPN frameworks on instance segmentation. The evaluation metric is average precision (AP) with IoU threshold 0.5.</p><p>The margin of improvement for SGPN compared to Seg-Cluster is not as high as it is on S3DIS, because in this dataset objects with the same semantic label are usually far apart in Euclidean space. Additionally, naive methods like Seg-Cluster benefit since it is easy to separate a single instance into parts since the points are not connected due to  <ref type="table">Table 5</ref>: Results on instance segmentation in NYUV2. The metric is AP with IoU 0.5. <ref type="figure">Figure 5</ref>: SGPN instance segmentation results on S3DIS. The first row is the prediction results. The second row is groud truths. Different colors represent different instances. The third row is the predicted semantic segmentation results. The fourth row is the ground truths for semantic segmentation.  occlusion in partial scanning. <ref type="table">Table 5</ref> also illustrates that SGPN can effectively utilize CNN features. Instead of concatenating fully-connected layer of 2D and 3D networks as in <ref type="bibr" target="#b46">[45]</ref>, we combine 2D and 3D features by considering their geometric relationships.   <ref type="table">Table 6</ref>: Comparison results on 3D detection (AP with IoU 0.5) in NYUV2. Please note we use point groups as inference while <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b10">11]</ref> use large bounding box with invisible regions as ground truth. Our prediction is the tight bounding box on points which makes the IoU much smaller than <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>We further calculate bounding boxes with instance segmentation results. <ref type="table">Table 6</ref> compares our work with the state-of-the-art works <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b10">11]</ref> on NYUV2 3D object detection. Following the evaluation metric in <ref type="bibr" target="#b45">[44]</ref>, AP is calculated with IoU threshold 0.25 on 3D bounding boxes. The NYUV2 dataset provides ground truth 3D bounding boxes that encapsulate the whole object including the part that is invisible in the depth image. Both <ref type="bibr" target="#b46">[45]</ref> and <ref type="bibr" target="#b10">[11]</ref> use these large ground truth bounding boxes for inference. In our method, we infer point groupings, which lack information of the invisible part of the object. Our output is the derived tight bounding box around the grouped points in the partial scan, which makes our IoUs much smaller than <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b10">11]</ref>. However, we can still see the effectiveness of SGPN on the   task of 3D object detection on partial scans as our method achieves better performance on small objects.</p><p>Computation Speed To benchmark the testing time with <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b10">11]</ref> and make fair comparison, we run our framework on an Nvidia K40 GPU. SGPN takes 170ms and around 400M GPU memory per sample. CNN-SGPN takes 300ms and 1.4G GPU memory per sample. GroupMerging takes 180ms on an Intel i7 CPU. However, the detection net in <ref type="bibr" target="#b10">[11]</ref> takes 739ms on an Nvidia Titan X GPU. In <ref type="bibr" target="#b46">[45]</ref>, RPN takes 5.62s and ORN takes 13.93s per image on an Nvidia K40 GPU. Our model improves the efficiency and reduces GPU memory usage by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ShapeNet Part Instance Segmentation</head><p>Following the settings in <ref type="bibr" target="#b36">[35]</ref>, point clouds are generated by uniformly sampling shapes from Shapenet <ref type="bibr" target="#b3">[4]</ref>. In our experiments we sample each shape into 2048 points. The XYZ of points are fed into network as input with size 2048 × 3. To generate ground truth labels for part instance segmentation from semantic segmentation results, we perform DBSCAN clustering on each part category of an object to group points into instances. This experiment is conducted as a toy example to demonstrate the effectiveness of our approach on instance segmentation for pointclouds.</p><p>We use Pointnet++ as our baseline. <ref type="figure" target="#fig_7">Figure 8(b)</ref> illustrates the instance segmentation results. For instance results, we again use different colors to represent different instances, and point colors of the same group are not necessarily the same as the ground truth. Since the generated ground truths are not "real" ground truths, only qualitative results are provided. SGPN achieves good results even under challenging conditions. As we can see from the <ref type="figure" target="#fig_7">Figure 8</ref>, SGPN is able to group the chair legs into four instances even though even in the ground truth DBSCAN can not separate the chair legs apart.</p><p>The similarity matrix can also help the semantic segmentation branch training. We compare SGPN to PointNet++ (i.e. our framework with solely a semantic segmentation branch) on semantic segmentation in <ref type="table" target="#tab_8">Table 7</ref>. The inputs of both networks are point clouds of size 2048. Evaluation metric is mIoU on points of each shape category. Our model performs better than PointNet++ due to the similarity matrix. Qualitative results are shown in <ref type="figure" target="#fig_7">Figure 8</ref>. Some false segmentation prediction is refined with the help of SGPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present SGPN, an intuitive, simple, and flexible framework for 3D instance segmentation on point clouds. With the introduction of the similarity matrix as our output representation, group proposals with class predictions can be easily generated from a single network. Experiments show that our algorithm can achieve good performance on instance segmentation for various 3D scenes and facilitate the tasks of 3D object detection and semantic segmentation.</p><p>Future Work While a similarity matrix provides an intuitive representation and an easily defined loss function, one limitation of SGPN is that the size of the similarity matrix scales quadratically as N p increases. Thus, although much more memory efficient than volumetric methods, SGPN cannot process extremely large scenes on the order 10 5 or more points. Future research directions can consider generating groups using seeds that are selected based on SGPN to reduce the size of the similarity matrix. SGPN can also be extended in future works to learn in a more unsupervised setting or to learn more different kinds of data representations beyond instance segementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>In our experiments, we use both PointNet and Point-Net++ as our baseline architectures. For the S3DIS dataset, we use PointNet as our baseline for fair comparison with the 3D object detection system described in the PointNet paper <ref type="bibr" target="#b34">[33]</ref>. The network architecture is the same as the semantic segmentation network as stated in PointNet except for the last two layers. Our F is the last 1 × 1 conv layer with BatchNorm and ReLU in PointNet with 256 output channels. F SIM , F CF , F SEM are 1 × 1 conv layers with output channels <ref type="figure" target="#fig_0">(128, 128, 128)</ref>, respectively.</p><p>For the NYUV2 dataset, we use PointNet++ as our baseline. We use the same notations as PointNet++ to describe our architecture:</p><p>SA(K, r, [l 1 , ..., l d ]) is a set abstraction (SA) level with K local regions of ball radius r using a PointNet architecture of d 1 × 1 conv layers with output channels l i (i = 1, ..., d). F P (l 1 , ..., l d ) is a feature propagation (F P ) level with d 1 × 1 conv layers. Our network architecture is: For our experiments on the ShapeNet part dataset, PointNet++ is used as our baseline. We use the same network architecture as in the PointNet++ paper <ref type="bibr" target="#b36">[35]</ref>. F SIM , F CF , F SEM are 1 × 1 conv layers with output channels (64, 64, 64), respectively.</p><formula xml:id="formula_3">SA(1024,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. S3DIS Dataset</head><p>Block Merging We divide each scene into 1m × 1m blocks with overlapping sliding windows in a snake pattern of stride 0.5m as is shown in <ref type="figure">Figure 9</ref>. The entire scene is also divided into a 400 × 400 × 400 grid V . V k is used to indicate the instance label of cell k where k ∈ [0, 400×400×400). Given V and point instance labels for each block P L where P L ij represents the instance label of jth point in block i, a BlockMerging algorithm (refer to Algorithm 1) is derived to merge object instances from different blocks.</p><p>In <ref type="figure" target="#fig_0">Figure 10</ref>, we show more qualitative results of instance segmentation with SGPN.   <ref type="table">Table 8</ref>: Instance segmentation results on ScanNet(v1). The metric is AP (%) with IoU threshold 0.25. We observe 0 percent AP on items that appear on the wall (door, window, picture) as they contain very little depth information and are almost all incorrectly semantically labeled as the wall. Future works can explore addressing this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. ScanNet</head><p>We provide more experimental results on ScanNet <ref type="bibr" target="#b6">[7]</ref>. This dataset contains 1513 scanned and reconstructed indoor scenes. We use the official split with 1201 scenes for training and 312 for testing. Following the same Block-Merging procedure, each scene is divided into 1.5m×1.5m blocks and each block is uniformly sampled into 4096 points for training. All points in the block are used at test time. Each point is represented by a 9D vector (XYZ, RGB, and normalized location with respect to the room scene). PointNet++ is used as the baseline. The network architecture is:</p><p>SA(1024, 0.1, <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr">64]</ref> And F SIM , F CF , F SEM are 1 × 1 conv layers with output channels (128, 128, 128) respectively. <ref type="table">Table 8</ref> illustrates the quantitative comparison results with Seg-Cluster. The metric is average precision (AP) with IoU threshold 0.25. <ref type="figure" target="#fig_0">Figure 11</ref> shows instance segmentation results on ScanNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Instance segmentation for point clouds using SGPN. Different colors represent different instances. (a) Instance segmentation on complete real scenes. (b) Single object part instance segmentation. (c) Instance segmentation on point clouds obtained from partial scans. ble, and effective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Pipeline of our system for point cloud instance segmentation.(a) (b)Figure 3: (a) Similarity (euclidean distance in feature space) between a given point (indicated by red arrow) and the rest of points. A darker color represents lower distance in feature space thus higher similarity. (b) Confidence map. A darker color represents higher confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>K 1 ,</head><label>1</label><figDesc>K 2 are set to 1.0 and 2.0, respectively. We use percategory histogram thresholding to get the threshold point T h s for each testing sample. T h M 1 is set to 0.6 and T h M 2 is set to 200. T h C is set to 0.1. Our network is implemented with Tensorflow and a single Nvidia GTX1080 Ti GPU. It takes 16-17 hours to converge. At test time, SGPN takes 40ms on an input point cloud with size 4096 × 9 with PointNet++ as our baseline architecture. Further runtime analysis can be found in Section 4.2. Code is availabel at github.com/laughtervv/SGPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison results on S3DIS. (a) Ground Truth for instance segmentation. Different colors represents different instances. (b) SGPN instance segmentation results. (c) Seg-Cluster instance segmentation results. (d) Ground Truth for semantic segmentation. (e) Semantic Segmentation and 3D detection results of SGPN. The color of the detected bounding box for each object category is the same as the semantic labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Incorporating CNN features in SGPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>SGPN instance segmentation results on NYUV2. (a) Input point clouds. (b) Ground truths for instance segmentation. (c) Instance segmentation results with SGPN. (d) Instance segmentation results with SGPN-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results on ShapeNet Part Dataset. (a) Generated ground truth for instance segmentation. (b) SGPN instance segmentation results. (c) Semantic segmentation results of PointNet++. (d) Semantic segmentation results of SGPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 : 4 if i is the 1st block then 5 for 7 V 10 for</head><label>945710</label><figDesc>Dividing scene into blocks with overlap (top view).Algorithm 1: BlockMerigingInput : V , P L Output: Point instance labels for the whole scene L 1 Initialize V with all elements −1; 2 GroupCount ← 0; 3 for every block i do every point P j in block i do<ref type="bibr" target="#b5">6</ref> Define k where P j is located in the kth cell of V ; k ← P L 1j ; every instance I j in block i do<ref type="bibr" target="#b10">11</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>), SA(256, 0.2, [64, 64, 128]), SA(64, 0.4, [128, 128, 256]), SA(16, 0.8, [256, 256, 512]), F P (256, 256), F P (256, 256), F P (256, 128), F P (128, 128, 128, 128).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Cluster 17.40 70.01 80.12 10.64 15.30 0.00 28.97 32.32 22.16 27.76 0.00 0.06 21.52 SGPN 36.30 58.42 83.67 42.24 25.64 7.15 42.73 45.23 38.25 47.05 0.00 13.57 31.68 Results on instance segmentation in S3DIS scenes. The metric is AP(%) with IoU threshold 0.5. To the best of our knowledge, there are no existing instance segmentation methods on point clouds for arbitrary object categories.</figDesc><table><row><cell>and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison results on instance segmentation with different IoU thresholds in S3DIS scenes. Metric is mean AP(%) over 13 categories.</figDesc><table><row><cell></cell><cell>Mean table chair sofa board</cell></row><row><cell cols="2">PointNet [33] 24.24 46.67 33.80 4.76 11.72</cell></row><row><cell cols="2">Seg-Cluster 18.72 33.44 22.8 5.38 13.07</cell></row><row><cell>SGPN</cell><cell>30.20 49.90 40.87 6.96 13.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison results on 3D detection in S3DIS scenes. SGPN uses PointNet as baseline. The metric is AP with IoU threshold 0.5.</figDesc><table><row><cell></cell><cell cols="2">Mean IoU Accuracy</cell></row><row><cell cols="2">PointNet [33] 49.76</cell><cell>79.66</cell></row><row><cell>SGPN</cell><cell>50.37</cell><cell>80.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on semantic segmentation in S3DIS scenes. SGPN uses PointNet as baseline. Metric is mean IoU(%) over 13 classes (including clutter). ity during both train and test time and local minima during train time. SGPN can effectively separate the difficult cases of objects of the same semantic class but different instances (c.f.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Mean Seg -</head><label>Seg</label><figDesc>Cluster 23.2 31.0 70.1 27.1 1.3 25.8 20.3 13.9 11.1 24.3 4.4 16.3 3.6 32.0 25.5 36.3 50.9 12.9 2.2 23.5 SGPN 26.5 55.9 53.3 27.8 0.0 27.4 59.6 28.9 6.1 33.9 2.0 19.7 2.0 29.4 30.7 39.1 43.6 17.6 1.2 25.9 SGPN-CNN 30.5 56.4 55.4 35.2 0.0 42.6 50.6 23.1 21.1 31.8 7.5 22.7 6.4 39.9 33.5 42.4 54.8 21.3 3.8 32.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>bag cap car chair head phone guitar knife lamp laptopmotor mug pistol rocket skate board table [35] 84.6 80.4 80.9 60.0 76.8 88.1 83.7 90.2 82.6 76.9 94.7 68.0 91.2 82.1 59.9 78.2 87.5 SGPN 85.8 80.4 78.6 78.8 71.5 88.6 78.0 90.9 83.0 78.8 95.8 77.8 93.8 87.4 60.1 92.3 89.4</figDesc><table><row><cell>Mean</cell><cell>air-</cell></row></table><note>plane</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Semantic segmentation results on ShapeNet part dataset. Metric is mean IoU(%) on points.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>SIM , F CF , F SEM are 1 × 1 conv layers with output channels (128, 128, 128) respectively.</figDesc><table><row><cell>0.1, [32, 32, 64]),</cell></row><row><cell>SA(256, 0.2, [64, 64, 128]),</cell></row><row><cell>SA(128, 0.4, [128, 128, 256]),</cell></row><row><cell>SA(64, 0.8, [256, 256, 256]),</cell></row><row><cell>SA(16, 1.2, [256, 256, 512]),</cell></row><row><cell>F P (512, 256),</cell></row><row><cell>F P (256, 256),</cell></row><row><cell>F P (256, 256),</cell></row><row><cell>F P (256, 128),</cell></row><row><cell>F P (128, 128, 128, 128).</cell></row></table><note>F</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Define V Ij points in I j are located in cells V Ij ; 12 V t ← the cells in V Ij that do not have value −1; 13 if the frequency of the mode in V t &lt; 30 then Ij ← the mode of V t ; for every point P j in the whole scene do 23 Define k where P j is located in the kth cell of V ; 24 L j ← V k ; Cluster 32.5 34.2 87.6 61.4 40.4 20.7 43.1 19.2 33.8 33.5 44.4 60.0 39.8 24.5 0 0 19.4 22.0 0 33.5 SGPN 35.1 55.5 86.7 64.4 41.1 40.7 42.7 36.1 39.6 38.6 45.1 50.3 15.9 27.0 0 0 19.3 28.6 0 35.1</figDesc><table><row><cell>Mean wall floor chair table desk bed</cell><cell>book shelf</cell><cell>sofa sink</cell><cell>bath tub</cell><cell>toilet</cell><cell>cur-tain</cell><cell>coun-ter</cell><cell>door</cell><cell>win-dow</cell><cell>shower curtain</cell><cell>fridge</cell><cell>pic-ture</cell><cell>cabi-net</cell></row><row><cell>Seg-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>18</cell><cell></cell><cell cols="2">end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>19</cell><cell></cell><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>20</cell><cell cols="2">end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">21 end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">25 end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>14 V Ij ← GroupCount;15 GroupCount ← GroupCount + 1;16 else17 V22</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PointNet<ref type="bibr" target="#b34">[33]</ref> proposed a 3D detection system while PointNet++<ref type="bibr" target="#b36">[35]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of largescale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning globallyconsistent local distance functions for shape-based image retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Highresolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Prediction Ground Truth Prediction Ground Truth Figure 10: Instance segmentation results on S3DIS with SGPN. Different colors represent different instances. The colors of the same object in ground truth and prediction are not necessarily the same</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning by tracking: siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR DeepVision Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focal Prediction Ground Truth Prediction Ground Truth Figure 11: Instance segmentation results on ScanNet with SGPN. Different colors represent different instances. The colors of the same object in ground truth and prediction are not necessarily the same. loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d point cloud object detection with multi-view convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic 3d industrial point cloud modeling and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MVA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep Sliding Shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08974</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Shape inpainting using 3d generative adversarial network and recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
