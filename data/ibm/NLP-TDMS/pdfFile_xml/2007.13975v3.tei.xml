<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Communication Engineering</orgName>
								<orgName type="institution">Jiangsu University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Communication Engineering</orgName>
								<orgName type="institution">Jiangsu University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Jiangsu Engineering Research Center of big data ubiquitous perception and intelligent agriculture applications</orgName>
								<address>
									<settlement>Zhenjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Communication Engineering</orgName>
								<orgName type="institution">Jiangsu University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: direct context-aware modeling</term>
					<term>transformer</term>
					<term>dual-path network</term>
					<term>speech separation</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dominant speech separation models are based on complex recurrent or convolution neural network that model speech sequences indirectly conditioning on context, such as passing information through many intermediate states in recurrent neural network, leading to suboptimal separation performance. In this paper, we propose a dual-path transformer network (DPT-Net) for end-to-end speech separation, which introduces direct context-awareness in the modeling for speech sequences. By introduces a improved transformer, elements in speech sequences can interact directly, which enables DPTNet can model for the speech sequences with direct context-awareness. The improved transformer in our approach learns the order information of the speech sequences without positional encodings by incorporating a recurrent neural network into the original transformer. In addition, the structure of dual paths makes our model efficient for extremely long speech sequence modeling. Extensive experiments on benchmark datasets show that our approach outperforms the current state-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech separation, often referred to as the cocktail party problem [1, 2], is a fundamental task in signal processing with a wide range of real-world applications, such as separating clean speech from noisy speech signals to improve the accuracy of automatic speech recognition. The human auditory system has the remarkable ability to extract separate sources from a complex mixture, while this task seems to be difficult for automatic calculation system, especially when only a monaural recording of mixed-speech is available.</p><p>Although there are many challenges in monaural speech separation, a lot of attempts have been made in previous works to deal with this problem over the decades. Before the deep learning era, many traditional methods are introduced for this task, such as non-negative matrix factorization (NMF) <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4]</ref>, computational auditory scene analysis (CASA) <ref type="bibr" target="#b3">[5]</ref> and probabilistic models <ref type="bibr" target="#b4">[6]</ref>. However, these models usually only work for closed-set speakers, which significantly restricts their practical applications. With the success of deep learning techniques on various domains <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8]</ref>, researches start to design data-based models to separate the mixture of unknown speakers, which overcomes the obstacles of the traditional methods. In general, deep learning techniques for monaural speech separation can be divided into two categories: time-frequency (T-F) domain methods and end-to-end time-domain approaches. Based on T-F features created by calculating the short-time Fourier transform (STFT), T-F methods separate the T-F features for each source and then reconstruct the source waveforms by inverse STFT <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b11">13]</ref>. They usually use the original phase of mixture to synthesize the estimated source waveforms, which retain the phase of the noisy mixture. This strategy imposes an upper limit on the separation performance. To overcome this problem, time-domain approach is proposed in paper <ref type="bibr" target="#b12">[14]</ref> , which directly model the mixture waveform using an encodedecoder framework and has made great progress in recent years <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21]</ref>.</p><p>However, the dominant speech separation models are usually based on recurrent neural network (RNN) or convolution neural network (CNN), which cannot model the speech sequences directly conditioning on context <ref type="bibr" target="#b20">[22]</ref>, leading to suboptimal separation performance. For example, RNN based models need to pass information through many intermediate states.</p><p>And the models based CNN suffer from the problem of limited receptive fields. Fortunately, the transformer based on selfattention mechanism can resolve this problem effectively <ref type="bibr" target="#b21">[23]</ref>, in which elements of the inputs can interact directly. Nevertheless, the transformer usually only deals with sequences with length of hundreds, while end-to-end time-domain speech separation systems often model extremely long input sequences, which can sometimes be tens of thousands. Dual-path network is an effective method to deal with this problem <ref type="bibr" target="#b18">[20]</ref>.</p><p>Inspired by the above, we propose a dual-path transformer network (DPTNet) for end-to-end monaural speech separation, which introduces a improved transformer to allow direct context-aware modeling on the speech sequences, leading to superior separation performance. The major contributions of our work are summarized as follows.</p><p>1. To the best of our knowledge, this is the first work that introduces direct context-aware modeling into speech separation. This method enables the elements in speech sequences can interact directly, which is beneficial to information transmission.</p><p>2. We integrate a recurrent neural network into original transformer to make it can learn the order information of the speech sequences without positional encodings. And we embed this improved transformer into a dualpath network, which makes our approach efficient for extremely long speech sequence modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extensive experiments on benchmark datasets show that</head><p>our approach outperforms the current state-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).</p><p>The remains of this paper are organized as follows. We introduces monaural speech separation with DPTNet in Section 2, present the experiment procedures in Section 3, analyze the experiment results in Section 4, conclude this paper and indicate future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Speech separation with dual-path transformer network</head><p>As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, our speech separation system consists of three stages: encoder, separation layer and decoder, which is similar to that of Conv-TasNet in paper <ref type="bibr" target="#b13">[15]</ref>. First, an encoder is used to convert segments of the mixture waveform into corresponding features in an intermediate feature space. Then the features are feed to the separation layer to construct a mask for each source. Finally, the decoder reconstructs the source waveforms by converting the masked features. In the following, we outline the encoder and decoder, and describe the separation layer, namely our dual-path transformer network, in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Encoder</head><p>If we denote the speech mixture by x ∈ R 1×T , then we can divide it into overlapping vectors x ∈ R L×I of length L samples, where I is the number of vectors. The encoder receive x and output the speech signal X ∈ R N ×I as follows:</p><formula xml:id="formula_0">X = ReLU (x * W )<label>(1)</label></formula><p>where the encoder can be characterized as a filter-bank W with N filters of length L, which is actually a 1-D convolution module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Separation layer: dual-path transformer network</head><p>The separation layer, namely dual-path transformer network, is composed of three stages: segmentation, dual-path transformer processing and overlap-add, which is inspired by the common dual-path network <ref type="bibr" target="#b18">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Segmentation</head><p>Firstly, the segmentation stage splits X into overlapped chunks of length K and hop size H. Then all the chunks are concatenated to be a 3-D tensor D ∈ R N ×K×P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Dual-path transformer processing</head><p>Broadly speaking, the transformer is composed of an encoder and a decoder <ref type="bibr" target="#b21">[23]</ref>. The encoder and decoder share the same model structure, except that the decoder is a left-context-only version for generation. To avoid confusions, the transformer in this paper refers specially to the encoder part, and it is comprised of three core modules: scaled dot-product attention, multi-head attention and position-wise feed-forward network. Scaled dot-product attention is an effective self-attention mechanism that associate different positions of input sequences to calculate representations for the inputs, which is shown in <ref type="figure">Figure 2</ref>(a). The final output of this module is computed as a weighted sum of the values, where the weight for each value is computed by a attention function of the query with the corresponding keys. Multi-head attention is composed of multiple scaled dot-product attention modules, as depicted in <ref type="figure" target="#fig_2">Figure 2(b)</ref>. First, it linearly maps the inputs h times with different, learnable linear projections to get parallel queries, keys and values respectively. Then the scaled dot-product attention is performed on these mapped queries, keys and values simultaneously. Position-wise feed-forward network is a fully connected feed-forward network. It is comprised of two linear transformations with a ReLU activation in between. Besides the three core modules, transformer also includes several residual and normalization layers. We present the overall structure of the transformer in <ref type="figure" target="#fig_3">Figure 3</ref>(a) and it can be formulated as follows:</p><formula xml:id="formula_1">Qi = ZW Q i , Ki = ZW K i , Vi = ZW V i i ∈ [1, h]<label>(2)</label></formula><p>headi = Attention(Qi, Ki, Vi)  <ref type="figure">Figure 2</ref>: Attention mechanism of the transformer.</p><formula xml:id="formula_2">= sof tmax( QiK T i √ d )Vi<label>(3)</label></formula><formula xml:id="formula_3">M ultiHead = Concat(head1,...,head h )W O (4) M id = LayerN orm(Z + M ultiHead) (5) F F N = ReLU (M idW1 + b1)W2 + b2 (6) Output = LayerN orm(M id + F F N )<label>(7)</label></formula><p>Here, Z ∈ R l×d is the input with length l and dimension d, and Qi, Ki, Vi ∈ R l×d/h are the mapped queries, keys and values.</p><formula xml:id="formula_4">W Q i , W K i , W V i ∈ R d×d/h and W O ∈ R d×d are parameter matrices. F F N denotes the output of the position-wise feed- forward network, in which W1 ∈ R d×d f f , W2 ∈ R d f f ×d , b1 ∈ R d f f , b2 ∈ R d , and d f f = 4 × d.</formula><p>The elements in speech sequences modeled by the transformer can contact directly without intermediate transmission, which introduces direct context-aware modeling into our method.</p><p>One thing missed by the above transformer is how to utilize the order information in the speech sequences. The origin transformer adds positional encodings to the input embeddings to represent order information, which is sine-and-cosine functions or learned parameters. However, we find that the positional encodings are not suitable for dual-path network and usually lead to model divergence in the training process. To learn the order information, we replace the first fully connected layer with a recurrent neural network in the feed-forward network, which is a interesting improvement from paper <ref type="bibr" target="#b20">[22]</ref>:</p><formula xml:id="formula_5">F F N = ReLU (RN N (M id))W2 + b2<label>(8)</label></formula><p>We show this improved transformer in <ref type="figure" target="#fig_3">Figure 3</ref>(b) and apply it in next dual-path transformer processing stage. In dual-path transformer processing stage, the output D of the segmentation stage is passed to a heap of B dual-path transformers (DPTs), as presented in <ref type="figure" target="#fig_0">Figure 1</ref>. Each DPT consists of intra-transformer and inter-transformer, which are committed to modeling local and global information respectively. The intra-transformer processing block first model the local chunk independently, which acts on the second dimension of D:</p><formula xml:id="formula_6">D intra b = IntraT ransf ormer b [D inter b−1 ] = [transf ormer(D inter b−1 [:, :, i]), i = 1, ..., P ]<label>(9)</label></formula><p>Then the inter-transformer is used to summarize the information from all chunks to learn global dependency with performing on the last dimension of D:  where b = 1, ..., B and D inter 0 = D. Note that the layer normalization in each sub-transformer is applied to all dimensions.</p><p>Indeed, this structure makes each element in speech sequences interact directly with only some elements and interact with the rest elements through an intermediate element. This fact imposes a slight negative impact on the direct contextaware modeling. However, the structure of dual paths allows our approach to model for extremely long speech sequences efficiently. In general, the small shortcoming caused by the dualpath structure is far less than the benefits it brings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Overlap-Add</head><p>The output of the last inter-transformer D inter B is used to learn a mask for each source by a 2-D convolution layer. The masks are transformed back into sequences Ms ∈ R N ×I by overlapadd, and masked encoder features for s-th source are obtained by the element-wise multiplication between X and Ms: Ys = X · Ms (11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Decoder</head><p>In decoder, a transposed convolution module is used to reconstruct separated speech signals ys ∈ R L×I for s-th source:</p><formula xml:id="formula_7">ys = Ys * V<label>(12)</label></formula><p>where values in V ∈ R N ×L are the parameters of the transposed convolution module. Then the overlap-add method is applied to obtain the final waveforms ys ∈ R 1×T . The structure and function of decoder are both symmetrical with those of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We evaluate our proposed model on two-speaker speech separation using the WSJ0-2mix <ref type="bibr" target="#b7">[9]</ref> and LS-2mix dataset <ref type="bibr" target="#b22">[24]</ref>. The WSJ0-2mix dataset is derived from the WSJ0 data corpus <ref type="bibr" target="#b23">[25]</ref>. The 30 hours of training data and 10 hours of validation data contain two-speaker mixtures generated by randomly selecting utterances from different speakers in the WSJ0 training set si tr s, and mixing them at random signal-to-noise ratios (SNR) between -5 dB and 5 dB. 5-hours test set is similarly generated using utterances from unseen speakers in WSJ0 validation set si dt 05 and evaluation set si et 05. <ref type="bibr" target="#b22">[24]</ref>, which is a new corpus of reading English speech. Two speakers are randomly selected from the train-100 set to generate training mixtures, at various SNRs uniformly sampled between 0 dB and 5 dB. The validation and test set are similarly generated using utterances from unseen speakers in the Librispeech validation and test set. Generated LS-2mix dataset contains 20000, 5000 and 3000 utterances in the train/validation/test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LS-2mix is created based on the Librispeech dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiment setup</head><p>In encoder and decoder, the window size is 2 samples and a 50% stride size is used. The number of filters is set to 64. As for the separation layer, the number of dual-path transformers, namely B, is set to 6, and h = 4 parallel attention layers are employed.</p><p>In the training stage, we train proposed model for 100 epochs on 4-second long segments, and the criteria for early stopping is no decrease in the loss function on validation set for 10 epochs. Adam <ref type="bibr" target="#b24">[26]</ref> is used as the optimizer and gradient clipping with maximum L2-norm of 5 is applied during training. We increase the learning rate linearly for the first warmup n training steps, and then decay it by 0.98 for every two epochs:</p><formula xml:id="formula_8">lrate = k1 · d −0.5</formula><p>model · n · warmup n −1.5 when n ≤ warmup n = k2 · 0.98 epoch//2 when n &gt; warmup n <ref type="bibr" target="#b11">(13)</ref> where n is the step number, k1, k2 are tunable scalars, and k1 = 0.2, k2 = 4e −4 , warmup n = 4000 in this paper. These hyper-parameters are selected empirically according to the setups in the dual-path network <ref type="bibr" target="#b18">[20]</ref> and transformer <ref type="bibr" target="#b21">[23]</ref>. A Pytorch implementation of our DPTNet model can be found at "https://github.com/ujscjj/DPTNet".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training objective</head><p>We train proposed model with utterance-level permutation invariant training (uPIT) <ref type="bibr" target="#b10">[12]</ref> to maximize scale-invariant sourceto-noise ratio (SI-SNR) <ref type="bibr" target="#b12">[14]</ref>. SI-SNR is defined as:</p><formula xml:id="formula_9">starget = x, x x x 2 (14) enoise =x − starget (15) SI − SN R := 10log10 starget 2 enoise 2<label>(16)</label></formula><p>where x,x are clean and estimated source respectively, both of which are normalized to zero-mean before the calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance evaluation</head><p>In all experiments, we report the scale-invariant signal-to-noise (SI-SNR) and signal-to-distortion ratio (SDR) to measure the separation performance of our DPTNet, both of which are often employed in various speech separation systems. We first report the SI-SNR and SDR scores on the WSJ0-2mix dataset obtained by our model and the well-known separation methods in recent years. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our DPTNet achieves 20.2 dB and 20.6 dB on the metrics of SI-SNR and SDR respectively, where a new state-of-the-art performance is achieved. Benefiting from the direct context-aware modeling, the elements in the speech sequences modeled by our DPT-Net can interact directly, which results in the optimal monaural speech separation performance. In addition, our model maintains a small model size.  To prove the generalization of our approach, we conduct related experiments on the LS-2mix dataset. Compared to those in the WSJ0-2mix data corpus, the mixtures in LS-2mix is difficult to separate, but this does not interfere with the comparison between our method and the baselines. We reproduce two classical methods, namely Conv-TasNet <ref type="bibr" target="#b13">[15]</ref> and DPRNN <ref type="bibr" target="#b18">[20]</ref>, as baselines. <ref type="table" target="#tab_1">Table 2</ref> lists the average SI-SNR and SDR obtained by our DPTNet and the two baselines, where our direct contextaware modeling is still significantly superior to the state-of-theart approach DPRNN. This presents the generalization of our method and further demonstrates the effectiveness of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>In this paper, we propose the dual-path transformer network for end-to-end multi-speaker monaural speech separation, which models the speech sequences directly conditioning on context. Our model can learn the order information in speech sequences without positional encodings and model effectively for extremely long sequences of speech signals. Experiments on two benchmark datasets demonstrate the effectiveness of proposed model, and we achieve a new state-of-the-art performance on the public WSJ0-2mix data corpus. In the future, we would like to extend this work by directly modeling long speech feature sequences without the dual-path structure. It is promising to further improve the separation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Framework of speech separation with dual-path transformer network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>D inter b =</head><label>b</label><figDesc>InterT ransf ormer b [D intra b ] = [transf ormer(D intra b [:, j, :]), j = 1, ..., K]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the origin and improved transformers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with other methods on WSJ0-2mix in SI-SNR (dB), SDR (dB) and Model Size</figDesc><table><row><cell>Method</cell><cell cols="3">SI-SNR SDR Model Size</cell></row><row><cell>DPCL++ [27]</cell><cell>10.8</cell><cell>-</cell><cell>13.6M</cell></row><row><cell>uPIT-BLSTM-ST [12]</cell><cell>-</cell><cell>10.0</cell><cell>92.7M</cell></row><row><cell>Deep Attractor [10]</cell><cell>10.5</cell><cell>-</cell><cell>-</cell></row><row><cell>ADANet [28]</cell><cell>10.4</cell><cell>10.8</cell><cell>9.1M</cell></row><row><cell>Grid LSTM PIT [29]</cell><cell>-</cell><cell>10.2</cell><cell>-</cell></row><row><cell>ConvLSTM-GAT [30]</cell><cell>-</cell><cell>11.0</cell><cell>-</cell></row><row><cell>Chimera++ [31]</cell><cell>11.5</cell><cell>12.0</cell><cell>-</cell></row><row><cell>WA-MISI-5 [32]</cell><cell>12.6</cell><cell>13.1</cell><cell>32.9M</cell></row><row><cell>BLSTM-TasNet [14]</cell><cell>13.2</cell><cell>13.6</cell><cell>-</cell></row><row><cell>Conv-TasNet-gLN [15]</cell><cell>15.3</cell><cell>15.6</cell><cell>5.1M</cell></row><row><cell>Conv-TasNet+MBT [33]</cell><cell>15.5</cell><cell>15.9</cell><cell>-</cell></row><row><cell>Deep CASA [34]</cell><cell>17.7</cell><cell>18.0</cell><cell>12.8M</cell></row><row><cell>FurcaNeXt [35]</cell><cell>-</cell><cell>18.4</cell><cell>51.4M</cell></row><row><cell>DPRNN [20]</cell><cell>18.8</cell><cell>19.0</cell><cell>2.6M</cell></row><row><cell>DPTNet</cell><cell>20.2</cell><cell>20.6</cell><cell>2.69M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with baselines on the LS-2mix dataset</figDesc><table><row><cell>Method</cell><cell cols="3">SI-SNR SDR Model Size</cell></row><row><cell>Conv-TasNet-gLN [15]</cell><cell>12.9</cell><cell>13.5</cell><cell>5.1M</cell></row><row><cell>DPRNN [20]</cell><cell>15.0</cell><cell>15.6</cell><cell>2.6M</cell></row><row><cell>DPTNet</cell><cell>16.2</cell><cell>16.8</cell><cell>2.69M</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The cocktail party problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1875" to="1902" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Single-channel speech separation using sparse non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Olsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep nmf for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computational auditory scene analysis: Principles, algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acoustical Society of America Journal</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech recognition using factorial hidden markov models for separation in the feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparsity and geometry preserving graph embedding for dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="75" to="748" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual exclusive attentive transfer for unsupervised deep convolutional domain adaptation in speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N N</forename><surname>Ocquaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="93" to="847" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved speech separation with time-and-frequency cross-domain joint embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1363" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mesgarani</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep attention gated dilated temporal convolutional networks with intra-parallel convolutional modules for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3183" to="3187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end monaural speech separation with multi-scale dynamic weighted gated dilated convolutional pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hayakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4614" to="4618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive speech separation for unknown number of speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasaarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1348" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A multi-phase gammatone filterbank for speech separation via tasnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ditter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11615</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-attentional acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="page" from="3723" to="3727" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speaker-independent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single channel speech separation with constrained utterance level permutation invariant training using grid lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cbldnn-based speakerindependent speech separation via generative adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="711" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="686" to="690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mixup-breakdown: a consistency training method for improving generalization of speech separation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13253</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2092" to="2102" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Furcanext: Endto-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
