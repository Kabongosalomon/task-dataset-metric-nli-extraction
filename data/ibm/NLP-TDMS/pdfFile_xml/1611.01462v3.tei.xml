<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TYING WORD VECTORS AND WORD CLASSIFIERS: A LOSS FRAMEWORK FOR LANGUAGE MODELING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-03-11">11 Mar 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Inan</surname></persName>
							<email>inanh@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Khosravi</surname></persName>
							<email>khosravi@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<email>rsocher@salesforce.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Salesforce Research Palo Alto</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TYING WORD VECTORS AND WORD CLASSIFIERS: A LOSS FRAMEWORK FOR LANGUAGE MODELING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-03-11">11 Mar 2017</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural network models have recently made tremendous progress in a variety of NLP applications such as speech recognition <ref type="bibr" target="#b10">(Irie et al., 2016)</ref>, sentiment analysis <ref type="bibr" target="#b25">(Socher et al., 2013)</ref>, text summarization <ref type="bibr" target="#b19">Nallapati et al., 2016)</ref>, and machine translation <ref type="bibr" target="#b4">(Firat et al., 2016</ref>).</p><p>Despite the overwhelming success achieved by recurrent neural networks in modeling long range dependencies between words, current recurrent neural network language models (RNNLM) are based on the conventional classification framework, which has two major drawbacks: First, there is no assumed metric on the output classes, whereas there is evidence suggesting that learning is improved when one can define a natural metric on the output space <ref type="bibr" target="#b5">(Frogner et al., 2015)</ref>. In language modeling, there is a well established metric space for the outputs (words in the language) based on word embeddings, with meaningful distances between words <ref type="bibr" target="#b17">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b22">Pennington et al., 2014)</ref>. Second, in the classical framework, inputs and outputs are considered as isolated entities with no semantic link between them. This is clearly not the case for language modeling, where inputs and outputs in fact live in identical spaces. Therefore, even for models with moderately sized vocabularies, the classical framework could be a vast source of inefficiency in terms of the number of variables in the model, and in terms of utilizing the information gathered by different parts of the model (e.g. inputs and outputs).</p><p>In this work, we introduce a novel loss framework for language modeling to remedy the above two problems. Our framework is comprised of two closely linked improvements. First, we augment the classical cross-entropy loss with an additional term which minimizes the KL-divergence between the model's prediction and an estimated target distribution based on the word embeddings space. This estimated distribution uses knowledge of word vector similarity. We then theoretically analyze this loss, and this leads to a second and synergistic improvement: tying together two large matrices by reusing the input word embedding matrix as the output classification matrix. We empirically validate our theory in a practical setting, with much milder assumptions than those in theory. We also find empirically that for large networks, most of the improvement could be achieved by only reusing the word embeddings.</p><p>We test our framework by performing extensive experiments on the Penn Treebank corpus, a dataset widely used for benchmarking language models <ref type="bibr" target="#b16">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b14">Merity et al., 2016)</ref>. We demonstrate that models trained using our proposed framework significantly outperform models trained using the conventional framework. We also perform experiments on the newly introduced Wikitext-2 dataset <ref type="bibr" target="#b14">(Merity et al., 2016)</ref>, and verify that the empirical performance of our proposed framework is consistent across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: RECURRENT NEURAL NETWORK LANGUAGE MODEL</head><p>In any variant of recurrent neural network language model (RNNLM), the goal is to predict the next word indexed by t in a sequence of one-hot word tokens (y * 1 , . . . y * N ) as follows:</p><formula xml:id="formula_0">x t = Ly * t−1 , (2.1) h t = f (x t , h t−1 ), (2.2) y t = softmax (W h t + b) .</formula><p>( <ref type="formula">2.</ref>3)</p><p>The matrix L ∈ R dx×|V | is the word embedding matrix, where d x is the word embedding dimension and |V | is the size of the vocabulary. The function f (., .) represents the recurrent neural network which takes in the current input and the previous hidden state and produces the next hidden state. W ∈ R |V |×d h and b ∈ R |V | are the the output projection matrix and the bias, respectively, and d h is the size of the RNN hidden state. The |V | dimensional y t models the discrete probability distribution for the next word.</p><p>Note that the above formulation does not make any assumptions about the specifics of the recurrent neural units, and f could be replaced with a standard recurrent unit, a gated recurrent unit (GRU) <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, a long-short term memory (LSTM) unit <ref type="bibr" target="#b8">(Hochreiter &amp; Schmidhuber, 1997)</ref>, etc.</p><p>For our experiments, we use LSTM units with two layers.</p><p>Given y t for the t th example, a loss is calculated for that example. The loss used in the RNNLMs is almost exclusively the cross-entropy between y t and the observed one-hot word token, y * t :</p><formula xml:id="formula_1">J t = CE(y * t y t ) = − i∈|V | y * t,i log y t,i . (2.4)</formula><p>We shall refer to y t as the model prediction distribution for the t th example, and y * t as the empirical target distribution (both are in fact conditional distributions given the history). Since crossentropy and Kullback-Leibler divergence are equivalent when the target distribution is one-hot, we can rewrite the loss for the t th example as</p><formula xml:id="formula_2">J t = D KL (y * t y t ).</formula><p>(2.5) Therefore, we can think of the optimization of the conventional loss in an RNNLM as trying to minimize the distance 1 between the model prediction distribution (y) and the empirical target distribution (y * ), which, with many training examples, will get close to minimizing distance to the actual target distribution. In the framework which we will introduce, we utilize Kullback-Leibler divergence as opposed to cross-entropy due to its intuitive interpretation as a distance between distributions, although the two are not equivalent in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AUGMENTING THE CROSS-ENTROPY LOSS</head><p>We propose to augment the conventional cross-entropy loss with an additional loss term as follows:</p><formula xml:id="formula_3">y t = softmax (W h t /τ ) , (3.1) J aug t = D KL (ỹ t ŷ t ), (3.2) J tot t = J t + αJ aug t . (3.3)</formula><p>In above, α is a hyperparameter to be adjusted, andŷ t is almost identical to the regular model prediction distribution y t with the exception that the logits are divided by a temperature parameter τ . We defineỹ t as some probability distribution that estimates the true data distribution (conditioned on the word history) which satisfies Eỹ t = Ey * t . The goal of this framework is to minimize the distribution distance between the prediction distribution and a more accurate estimate of the true data distribution.</p><p>To understand the effect of optimizing in this setting, let's focus on an ideal case in which we are given the true data distribution so thatỹ t = Ey * t , and we only use the augmented loss, J aug . We will carry out our investigation through stochastic gradient descent, which is the technique dominantly used for training neural networks. The gradient of J aug t with respect to the logits W h t is</p><formula xml:id="formula_4">∇ J aug t = 1 τ (ŷ t −ỹ t ). (3.4)</formula><p>Let's denote by e j ∈ R |V | the vector whose j th entry is 1, and others are zero. We can then rewrite (3.4) as</p><formula xml:id="formula_5">τ ∇ J aug t =ŷ t − e 1 , . . . , e |V | ỹ t = i∈Vỹ t,i (ŷ t − e i ).</formula><p>(3.5) Implication of (3.5) is the following: Every time the optimizer sees one training example, it takes a step not only on account of the label seen, but it proceeds taking into account all the class labels for which the conditional probability is not zero, and the relative step size for each step is given by the conditional probability for that label,ỹ t,i . Furthermore, this is a much less noisy update since the target distribution is exact and deterministic. Therefore, unless all the examples exclusively belong to a specific class with probability 1, the optimization will act much differently and train with greatly improved supervision.</p><p>The idea proposed in the recent work by <ref type="bibr" target="#b7">Hinton et al. (2015)</ref> might be considered as an application of this framework, where they try to obtain a good set ofỹ's by training very large models and using the model prediction distributions of those.</p><p>Although finding a goodỹ in general is rather nontrivial, in the context of language modeling we can hope to achieve this by exploiting the inherent metric space of classes encoded into the model, namely the space of word embeddings. Specifically, we propose the following forỹ:</p><formula xml:id="formula_6">u t = Ly * t , (3.6) y t = softmax L T u t τ .</formula><p>(3.7)</p><p>In words, we first find the target word vector which corresponds to the target word token (resulting in u t ), and then take the inner product of the target word vector with all the other word vectors to get an unnormalized probability distribution. We adjust this with the same temperature parameter τ used for obtainingŷ t and apply softmax. The target distribution estimate,ỹ, therefore measures the similarity between the word vectors and assigns similar probability masses to words that the language model deems close. Note that the estimation ofỹ with this procedure is iterative, and the estimates ofỹ in the initial phase of the training are not necessarily informative. However, as training procedes, we expectỹ to capture the word statistics better and yield a consistently more accurate estimate of the true data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICALLY DRIVEN REUSE OF WORD EMBEDDINGS</head><p>We now theoretically motivate and introduce a second modification to improve learning in the language model. We do this by analyzing the proposed augmented loss in a particular setting, and observe an implicit core mechanism of this loss. We then make our proposition by making this mechanism explicit.</p><p>We start by introducing our setting for the analysis. We restrict our attention to the case where the input embedding dimension is equal to the dimension of the RNN hidden state, i.e. d d x = d h . We also set b = 0 in (2.3) so that y t = W h t . We only use the augmented loss, i.e. J tot = J aug , and we assume that we can achieve zero training loss. Finally, we set the temperature parameter τ to be large.</p><p>We first show that when the temperature parameter, τ , is high enough, J aug t acts to match the logits of the prediction distribution to the logits of the the more informative labels,ỹ. We proceed in the same way as was done in <ref type="bibr" target="#b7">Hinton et al. (2015)</ref> to make an identical argument. Particularly, we consider the derivative of J aug t with respect to the entries of the logits produced by the neural network.</p><p>Let's denote by l i the i th column of L. Using the first order approximation of exponential function around zero (exp (x) ≈ 1 + x), we can approximateỹ t (same holds forŷ t ) at high temperatures as follows:ỹ</p><formula xml:id="formula_7">t,i = exp ( u t , l i /τ ) j∈V exp ( u t , l j /τ ) ≈ 1 + u t , l i /τ |V | + j∈V u t , l j /τ . (4.1)</formula><p>We can further simplify (4.1) if we assume that u t , l j = 0 on average:</p><formula xml:id="formula_8">y t,i ≈ 1 + u t , l i /τ |V | . (4.2)</formula><p>By replacingỹ t andŷ t in (3.4) with their simplified forms according to (4.2), we get</p><formula xml:id="formula_9">∂J aug t ∂ (W h t ) i → 1 τ 2 |V | W h t − L T u t i as τ → ∞, (4.3)</formula><p>which is the desired result that augmented loss tries to match the logits of the model to the logits of y's. Since the training loss is zero by assumption, we necessarily have</p><formula xml:id="formula_10">W h t = L T u t (4.4)</formula><p>for each training example, i.e., gradient contributed by each example is zero. Provided that W and L are full rank matrices and there are more linearly independent examples of h t 's than the embedding dimension d, we get that the space spanned by the columns of L T is equivalent to that spanned by the columns of W . Let's now introduce a square matrix A such that W = L T A. (We know A exists since L T and W span the same column space). In this case, we can rewrite</p><formula xml:id="formula_11">W h t = L T Ah t L Th t . (4.5)</formula><p>In other words, by reusing the embedding matrix in the output projection layer (with a transpose) and letting the neural network do the necessary linear mapping h → Ah, we get the same result as we would have in the first place.</p><p>Although the above scenario could be difficult to exactly replicate in practice, it uncovers a mechanism through which our proposed loss augmentation acts, which is trying to constrain the output (unnormalized) probability space to a small subspace governed by the embedding matrix. This suggests that we can make this mechanism explicit and constrain W = L T during training while setting the output bias, b, to zero. Doing so would not only eliminate a big matrix which dominates the network size for models with even moderately sized vocabularies, but it would also be optimal in our setting of loss augmentation as it would eliminate much work to be done by the augmented loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Since their introduction in <ref type="bibr" target="#b16">Mikolov et al. (2010)</ref>, many improvements have been proposed for RNNLMs , including different dropout methods <ref type="bibr" target="#b26">(Zaremba et al., 2014;</ref><ref type="bibr" target="#b6">Gal, 2015)</ref>, novel recurrent units <ref type="bibr" target="#b27">(Zilly et al., 2016)</ref>, and use of pointer networks to complement the recurrent neural network <ref type="bibr" target="#b14">(Merity et al., 2016)</ref>. However, none of the improvements dealt with the loss structure, and to the best of our knowledge, our work is the first to offer a new loss framework.</p><p>Our technique is closely related to the one in <ref type="bibr" target="#b7">Hinton et al. (2015)</ref>, where they also try to estimate a more informed data distribution and augment the conventional loss with KL divergence between model prediction distribution and the estimated data distribution. However, they estimate their data distribution by training large networks on the data and then use it to improve learning in smaller networks. This is fundamentally different from our approach, where we improve learning by transferring knowledge between different parts of the same network, in a self contained manner.</p><p>The work we present in this paper is based on a report which was made public in <ref type="bibr" target="#b9">Inan &amp; Khosravi (2016)</ref>. We have recently come across a concurrent preprint <ref type="bibr" target="#b23">(Press &amp; Wolf, 2016)</ref> where the authors reuse the word embedding matrix in the output projection to improve language modeling. However, their work is purely empirical, and they do not provide any theoretical justification for their approach. Finally, we would like to note that the idea of using the same representation for input and output words has been explored in the past, and there exists language models which could be interpreted as simple neural networks with shared input and output embeddings <ref type="bibr" target="#b0">(Bengio et al., 2001;</ref><ref type="bibr" target="#b18">Mnih &amp; Hinton, 2007)</ref>. However, shared input and output representations were implicitly built into these models, rather than proposed as a supplement to a baseline. Consequently, possibility of improvement was not particularly pursued by sharing input and output representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In our experiments, we use the Penn Treebank corpus (PTB) <ref type="bibr" target="#b13">(Marcus et al., 1993)</ref>, and the Wikitext-2 dataset <ref type="bibr" target="#b14">(Merity et al., 2016)</ref>. PTB has been a standard dataset used for benchmarking language models. It consists of 923k training, 73k validation, and 82k test words. The version of this dataset which we use is the one processed in <ref type="bibr" target="#b16">Mikolov et al. (2010)</ref>, with the most frequent 10k words selected to be in the vocabulary and rest replaced with a an &lt;unk&gt; token 2 . Wikitext-2 is a dataset released recently as an alternative to PTB 3 . It contains 2, 088k training, 217k validation, and 245k test tokens, and has a vocabulary of 33, 278 words; therefore, in comparison to PTB, it is roughly 2 times larger in dataset size, and 3 times larger in vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MODEL AND TRAINING HIGHLIGHTS</head><p>We closely follow the LSTM based language model proposed in <ref type="bibr" target="#b26">Zaremba et al. (2014)</ref> for constructing our baseline model. Specifically, we use a 2-layer LSTM with the same number of hidden units in each layer, and we use 3 different network sizes: small (200 units), medium (650 units), and large (1500 units). We train our models using stochastic gradient descent, and we use a variant of the dropout method proposed in <ref type="bibr" target="#b6">Gal (2015)</ref>. We defer further details regarding training the models to section A of the appendix. We refer to our baseline network as variational dropout LSTM, or VD-LSTM in short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EMPIRICAL VALIDATION FOR THE THEORY OF REUSING WORD EMBEDDINGS</head><p>In Section 4, we showed that the particular loss augmentation scheme we choose constrains the output projection matrix to be close to the input embedding matrix, without explicitly doing so by reusing the input embedding matrix. As a first experiment, we set out to validate this theoretical result. To do this, we try to simulate the setting in Section 4 by doing the following: We select a randomly chosen 20, 000 contiguous word sequence in the PTB training set, and train a 2-layer LSTM language model with 300 units in each layer with loss augmentation by minimizing the following loss:</p><formula xml:id="formula_12">J tot = βJ aug τ 2 |V | + (1 − β)J. (6.1)</formula><p>Here, β is the proportion of the augmented loss used in the total loss, and J aug is scaled by τ 2 |V | to approximately match the magnitudes of the derivatives of J and J aug (see (4.3)). Since we aim to achieve the minimum training loss possible, and the goal is to show a particular result rather than to achieve good generalization, we do not use any kind of regularization in the neural network (e.g. weight decay, dropout). For this set of experiments, we also constrain each row of the input embedding matrix to have a norm of 1 because training becomes difficult without this constraint when only augmented loss is used. After training, we compute a metric that measures distance between the subspace spanned by the rows of the input embedding matrix, L, and that spanned by the columns of the output projection matrix, W . For this, we use a common metric based on the relative residual norm from projection of one matrix onto another <ref type="bibr" target="#b1">(Björck &amp; Golub, 1973)</ref>. The computed distance between the subspaces is 1 when they are orthogonal, and 0 when they are the same. Interested reader may refer to section B in the appendix for the details of this metric. <ref type="figure" target="#fig_0">Figure 1</ref> shows the results from two tests. In one (panel a), we test the effect of using the augmented loss by sweeping β in (6.1) from 0 to 1 at a reasonably high temperature (τ = 10). With no loss augmentation (β = 0), the distance is almost 1, and as more and more augmented loss is used the distance decreases rapidly, and eventually reaches around 0.06 when only augmented loss is used.</p><p>In the second test (panel b), we set β = 1, and try to see the effect of the temperature on the subspace distance (remember the theory predicts low distance when τ → ∞). Notably, the augmented loss causes W to approach L T sufficiently even at temperatures as low as 2, although higher temperatures still lead to smaller subspace distances.</p><p>These results confirm the mechanism through which our proposed loss pushes W to learn the same column space as L T , and it suggests that reusing the input embedding matrix by explicitly constraining W = L T is not simply a kind of regularization, but is in fact an optimal choice in our framework. What can be achieved separately with each of the two proposed improvements as well as with the two of them combined is a question of empirical nature, which we investigate in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">RESULTS ON PTB AND WIKITEXT-2 DATASETS</head><p>In order to investigate the extent to which each of our proposed improvements helps with learning, we train 4 different models for each network size: (1) 2-Layer LSTM with variational dropout (VD-LSTM) (2) 2-Layer LSTM with variational dropout and augmented loss (VD-LSTM +AL) (3) 2-Layer LSTM with variational dropout and reused embeddings (VD-LSTM +RE) (4) 2-Layer LSTM with variational dropout and both RE and AL (VD-LSTM +REAL). <ref type="figure" target="#fig_1">Figure 2</ref> shows the validation perplexities of the four models during training on the PTB corpus for small (panel a) and large (panel b) networks. All of AL, RE, and REAL networks significantly outperform the baseline in both cases. <ref type="table" target="#tab_0">Table 1</ref> compares the final validation and test perplexities of the four models on both PTB and Wikitext-2 for each network size. In both datasets, both AL and RE improve upon the baseline individually, and using RE and AL together leads to the best performance. Based on performance comparisons, we make the following notes on the two proposed improvements:</p><p>• AL provides better performance gains for smaller networks. This is not surprising given the fact that small models are rather inflexible, and one would expect to see improved learning by training against a more informative data distribution (contributed by the augmented loss) (see <ref type="bibr" target="#b7">Hinton et al. (2015)</ref>). For the smaller PTB dataset, performance with AL surpasses that with RE. In comparison, for the larger Wikitext-2 dataset, improvement by AL is more limited. This is expected given larger training sets better represent the true data distribution, mitigating the supervision problem. In fact, we set out to validate this reasoning in a direct manner, and additionally train the small networks separately on the first and second halves of the Wikitext-2 training set. This results in two distinct datasets which are each about the same size as PTB (1044K vs 929K). As can be seen in <ref type="table">Table 2</ref>, AL has significantly improved competitive performance against RE and REAL despite the fact that embedding size is 3 times larger compared to PTB. These results support our argument that the proposed augmented loss term acts to improve the amount of information gathered from the dataset.</p><p>• RE significantly outperforms AL for larger networks. This indicates that, for large models, the more effective mechanism of our proposed framework is the one which enforces proximity between the output projection space and the input embedding space. From a model complexity perspective, the nontrivial gains offered by RE for all network sizes and for both datasets could be largely attributed to its explicit function to reduce the model size while preserving the representational power according to our framework.</p><p>We list in <ref type="table" target="#tab_2">Table 3 the</ref>    <ref type="table">Table 2</ref>: Performance of the four different small models trained on the equally sized two partitions of Wikitext-2 training set. These results are consistent with those on PTB (see <ref type="table" target="#tab_0">Table 1</ref>), which has a similar training set size with each of these partitions, although its word embedding dimension is three times smaller.   <ref type="bibr" target="#b20">(Pascanu et al., 2013a)</ref> 6M -107.5 Sum-Prod Net <ref type="bibr" target="#b2">(Cheng et al., 2014)</ref> 5M -100.0 LSTM (medium) <ref type="bibr" target="#b26">(Zaremba et al., 2014)</ref> 20M 86.2 82.7 CharCNN <ref type="bibr" target="#b12">(Kim et al., 2015)</ref> 19M -78.9 LSTM (large) <ref type="bibr" target="#b26">(Zaremba et al., 2014)</ref> 66M 82.2 78.4 VD-LSTM (large, untied, MC) <ref type="bibr" target="#b6">(Gal, 2015)</ref> 66M -73.4 ± 0.0 Pointer Sentinel-LSTM(medium) <ref type="bibr" target="#b14">(Merity et al., 2016)</ref> 21M 72.4 70.9 38 Large LSTMs <ref type="bibr" target="#b26">(Zaremba et al., 2014)</ref> 2.51B 71.9 68.7 10 Large VD-LSTMs <ref type="bibr" target="#b6">(Gal, 2015)</ref> 660M </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">QUALITATIVE RESULTS</head><p>One important feature of our framework that leads to better word predictions is the explicit mechanism to assign probabilities to words not merely according to the observed output statistics, but also considering the metric similarity between words. We observe direct consequences of this mechanism qualitatively in the Penn Treebank in different ways: First, we notice that the probability of generating the &lt;unk&gt; token with our proposed network (VD-LSTM +REAL) is significantly lower compared to the baseline network (VD-LSTM) across many words. This could be explained by noting the fact that the &lt;unk&gt; token is an aggregated token rather than a specific word, and it is often not expected to be close to specific words in the word embedding space. We observe the same behavior with very frequent words such as "a", "an", and "the", owing to the same fact that they are not correlated with particular words. Second, we not only observe better probability assignments for the target words, but we also observe relatively higher probability weights associated with the words close to the targets. Sometimes this happens in the form of predicting words semantically close together which are plausible even when the target word is not successfully captured by the model. We provide a few examples from the PTB test set which compare the prediction performance of 1500 unit VD-LSTM and 1500 unit VD-LSTM +REAL in table 4. We would like to note that prediction performance of VD-LSTM +RE is similar to VD-LSTM +REAL for the large network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we introduced a novel loss framework for language modeling. Particularly, we showed that the metric encoded into the space of word embeddings could be used to generate a more informed data distribution than the one-hot targets, and that additionally training against this distribution improves learning. We also showed theoretically that this approach lends itself to a second improvement, which is simply reusing the input embedding matrix in the output projection layer. This has an additional benefit of reducing the number of trainable variables in the model. We empirically validated the theoretical link, and verified that both proposed changes do in fact belong to the same framework. In our experiments on the Penn Treebank corpus and Wikitext-2, we showed that our framework outperforms the conventional one, and that even the simple modification of reusing the word embedding in the output projection layer is sufficient for large networks.</p><p>The improvements achieved by our framework are not unique to vanilla language modeling, and are readily applicable to other tasks which utilize language models such as neural machine translation, speech recognition, and text summarization. This could lead to significant improvements in such models especially with large vocabularies, with the additional benefit of greatly reducing the number of parameters to be trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A MODEL AND TRAINING DETAILS</head><p>We begin training with a learning rate of 1 and start decaying it with a constant rate after a certain epoch. This is 5, 10, and 1 for the small, medium, and large networks respectively. The decay rate is 0.9 for the small and medium networks, and 0.97 for the large network.</p><p>For both PTB and Wikitext-2 datasets, we unroll the network for 35 steps for backpropagation.</p><p>We use gradient clipping <ref type="bibr" target="#b21">(Pascanu et al., 2013b)</ref>; i.e. we rescale the gradients using the global norm if it exceeds a certain value. For both datasets, this is 5 for the small and the medium network, and 6 for the large network.</p><p>We use the dropout method introduced in Gal <ref type="formula">(2015)</ref>; particularly, we use the same dropout mask for each example through the unrolled network. Differently from what was proposed in Gal <ref type="formula">(2015)</ref>, we tie the dropout weights for hidden states further, and we use the same mask when they are propagated as states in the current layer and when they are used as inputs for the next layer. We don't use dropout in the input embedding layer, and we use the same dropout probability for inputs and hidden states. For PTB, dropout probabilities are 0.7, 0.5 and 0.35 for small, medium and large networks respectively. For Wikitext-2, probabilities are 0.8 for the small and 0.6 for the medium networks.</p><p>When training the networks with the augmented loss (AL), we use a temperature τ = 20. We have empirically observed that setting α, the weight of the augmented loss, according to α = γτ for all the networks works satisfactorily. We set γ to values between 0.5 and 0.8 for the PTB dataset, and between 1.0 and 1.5 for the Wikitext-2 dataset. We would like to note that we have not observed sudden deteriorations in the performance with respect to moderate variations in either τ or α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B METRIC FOR CALCULATING SUBSPACE DISTANCES</head><p>In this section, we detail the metric used for computing the subspace distance between two matrices. The computed metric is closely related with the principle angles between subspaces, first defined in <ref type="bibr" target="#b11">Jordan (1875)</ref>.</p><p>Our aim is to compute a metric distance between two given matrices, X and Y . We do this in three steps:</p><p>(1) Obtain two matrices with orthonormal columns, U and V , such that span(U )=span(X) and span(V )=span(Y ). U and V could be obtained with a QR decomposition.</p><p>(2) Calculate the projection of either one of U and V onto the other; e.g. do S = U U T V , where S is the projection of V onto U . Then calculate the residual matrix as R = V − S.</p><p>(3) Let . F r denote the frobenious norm, and let C be the number of columns of R. Then the distance metric is found as d where d 2 = 1 C R 2 F r = 1 C Trace(R T R).</p><p>We note that d as calculated above is a valid metric up to the equivalence set of matrices which span the same column space, although we are not going to show it. Instead, we will mention some metric properties of d, and relate it to the principal angles between the subspaces. We first work out an expression for d:</p><p>Published as a conference paper at ICLR 2017</p><formula xml:id="formula_13">Cd 2 = Trace(R T R) = Trace (V − U U T V ) T (V − U U T V ) = Trace V T (I − U U T )(I − U U T )V = Trace V T (I − U U T )V = Trace (I − U U T )V V T = Trace(V T V ) − Trace U U T V V T = C − Trace U U T V V T = C − Trace (U T V ) T (U T V ) = C − U T V 2 F r = C i=1 1 − ρ 2 i , (B.1)</formula><p>where ρ i is the i th singular value of U T V , commonly referred to as the i th principle angle between the subspaces of X and Y , θ i . In above, we used the cyclic permutation property of the trace in the third and the fourth lines.</p><p>Since d 2 is 1 C Trace(R T R), it is always nonnegative, and it is only zero when the residual is zero, which is the case when span(X) = span(Y). Further, it is symmetric between U and V due to the form of (B.1) (singular values of V T U and V T U are the same). Also, d 2 = 1 C C i=1 sin 2 (θ i ), namely the average of the sines of the principle angles, which is a quantity between 0 and 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>distance at different temperatures when only J aug is used Subspace distance between L T and W for different experiment conditions for the validation experiments. Results are averaged over 10 independent runs. These results validate our theory under practical conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Progress of validation perplexities during training for the 4 different models for two (small (200) and large (1500)) network sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>comparison of models with and without our proposed modifications on the Penn Treebank Corpus. The best LSTM model (VD-LSTM+REAL) outperforms all previous work which uses conventional framework, including large ensembles. The recently proposed recurrent highway networks (Zilly et al., 2016) when trained with reused embeddings (VD-RHN +RE) achieves the best overall performance, improving on VD-RHN by a perplexity of 2.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the final word level perplexities on the validation and test set for the 4 different models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>PTB</cell><cell cols="2">Wikitext-2</cell></row><row><cell cols="2">Network Model</cell><cell cols="2">Valid Test</cell><cell>Valid</cell><cell>Test</cell></row><row><cell>Small 4 (200 units)</cell><cell>VD-LSTM VD-LSTM+AL VD-LSTM+RE VD-LSTM+REAL</cell><cell>92.6 86.3 89.9 86.3</cell><cell>87.3 82.9 85.1 82.7</cell><cell cols="2">112.2 105.9 110.3 103.8 106.1 100.5 105.6 98.9</cell></row><row><cell>Medium (650 units)</cell><cell>VD-LSTM VD-LSTM+AL VD-LSTM+RE</cell><cell>82.0 77.4 77.1</cell><cell>77.7 74.7 73.9</cell><cell>100.2 98.8 92.3</cell><cell>95.3 93.1 87.7</cell></row><row><cell></cell><cell>VD-LSTM+REAL</cell><cell>75.7</cell><cell>73.2</cell><cell>91.5</cell><cell>87.0</cell></row><row><cell>Large 5 (1500 units)</cell><cell>VD-LSTM VD-LSTM+AL VD-LSTM+RE VD-LSTM+REAL</cell><cell>76.8 74.5 72.5 71.1</cell><cell>72.6 71.2 69.0 68.5</cell><cell>----</cell><cell>----</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our work to previous state of the art on word-level validation and test perplexities on the Penn Treebank corpus. Models using our framework significantly outperform other models.</figDesc><table><row><cell>Model</cell><cell cols="2">Parameters Validation</cell><cell>Test</cell></row><row><cell>RNN (Mikolov &amp; Zweig)</cell><cell>6M</cell><cell>-</cell><cell>124.7</cell></row><row><cell>RNN+LDA (Mikolov &amp; Zweig)</cell><cell>7M</cell><cell>-</cell><cell>113.7</cell></row><row><cell>RNN+LDA+KN-5+Cache (Mikolov &amp; Zweig)</cell><cell>9M</cell><cell>-</cell><cell>92.0</cell></row><row><cell>Deep RNN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Prediction for the next word by the baseline (VD-LSTM) and proposed (VD-LSTM +REAL) networks for a few example phrases in the PTB test set. Top 10 word predictions are sorted in descending probability, and are arranged in column-major format.</figDesc><table><row><cell>Phrase + Next word(s)</cell><cell cols="2">Top 10 predicted words VD-LSTM</cell><cell cols="2">Top 10 predicted words VD-LSTM +REAL</cell></row><row><cell>information international said it believes that the complaints filed in + federal court</cell><cell cols="2">the 0.27 a 0.13 federal 0.13 new 0.01 an 0.03 august 0.01 N 0.09 response 0.01 unk 0.05 connection 0.01</cell><cell cols="2">federal 0.22 connection 0.03 the 0.1 august 0.03 a 0.08 july 0.03 N 0.06 an 0.03 state 0.04 september 0.03</cell></row><row><cell>oil company refineries</cell><cell>the 0.09</cell><cell>in 0.03</cell><cell>august 0.08</cell><cell>a 0.03</cell></row><row><cell>ran flat out to prepare</cell><cell>N 0.08</cell><cell>has 0.03</cell><cell>N 0.05</cell><cell>in 0.03</cell></row><row><cell>for a robust holiday</cell><cell>a 0.07</cell><cell>is 0.02</cell><cell>early 0.05</cell><cell>that 0.02</cell></row><row><cell>driving season in july and</cell><cell cols="2">unk 0.07 will 0.02</cell><cell cols="2">september 0.05 ended 0.02</cell></row><row><cell>+ august</cell><cell>was 0.04</cell><cell>its 0.02</cell><cell>the 0.03</cell><cell>its 0.02</cell></row><row><cell>southmark said it plans</cell><cell>the 0.06</cell><cell>to 0.03</cell><cell>expected 0.1</cell><cell>a 0.03</cell></row><row><cell>to unk its unk to</cell><cell cols="2">unk 0.05 likely 0.03</cell><cell cols="2">completed 0.04 scheduled 0.03</cell></row><row><cell>provide financial results</cell><cell>a 0.05</cell><cell>expected 0.03</cell><cell>unk 0.03</cell><cell>n't 0.03</cell></row><row><cell>as soon as its audit is</cell><cell>in 0.04</cell><cell>scheduled 0.01</cell><cell>the 0.03</cell><cell>due 0.02</cell></row><row><cell>+ completed</cell><cell>n't 0.04</cell><cell>completed 0.01</cell><cell>in 0.03</cell><cell>to 0.01</cell></row><row><cell>merieux said the government 's minister of industry science and + technology</cell><cell cols="2">unk 0.33 industry 0.01 the 0.06 commerce 0.01 a 0.01 planning 0.01 other 0.01 management 0.01 others 0.01 mail 0.01</cell><cell cols="2">unk 0.09 health 0.08 development 0.04 telecomm. 0.02 industry 0.03 business 0.02 the 0.04 human 0.02 a 0.03 other 0.01</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We note, however, that Kullback-Leibler divergence is not a valid distance metric.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">PTB can be downloaded at http://www.fit.vutbr.cz/ imikolov/rnnlm/simple-examples.tgz 3 Wikitext-2 can be downloaded at https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For PTB, small models were re-trained by initializing to their final configuration from the first training session. This did not change the final perplexity for baseline, but lead to improvements for the other models.5  Large network results on Wikitext-2 are not reported since computational resources were insufficient to run some of the configurations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This model was developed following our work in<ref type="bibr" target="#b9">Inan &amp; Khosravi (2016)</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="http://www.iro.umontreal.ca/˜lisa/pointeurs/nips00_lm.ps" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ake</forename><surname>Björck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<title level="m">Numerical methods for computing angles between linear subspaces. Mathematics of computation</title>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="579" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language modeling with sum-product networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<editor>Hoai Vu Pham, Hai Leong Chieu, and Kian Ming Adam Chai</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01073</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with a wasserstein loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Frogner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Araya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2053" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05287</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved learning through augmenting the loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Khosravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stanford CS 224D: Deep Learning for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lstm, gru, highway and a bit of attention: an empirical overview for language modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltán</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bulletin de la Société mathématique de France</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1875" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="103" to="174" />
		</imprint>
	</monogr>
	<note>Essai sur la géométrieà n dimensions</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06615</idno>
		<title level="m">Character-aware neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Aglar Gulçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1312.6026</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05859</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent highway networks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
