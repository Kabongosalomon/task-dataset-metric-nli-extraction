<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
							<email>melamuo@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering Bar-Ilan University</orgName>
								<orgName type="department" key="dep3">Computer Science Dept. Bar-Ilan University</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering Bar-Ilan University</orgName>
								<orgName type="department" key="dep3">Computer Science Dept. Bar-Ilan University</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering Bar-Ilan University</orgName>
								<orgName type="department" key="dep3">Computer Science Dept. Bar-Ilan University</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
						<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL) <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="51" to="61"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Context representations are central to various NLP tasks, such as word sense disam-biguation, named entity recognition, co-reference resolution, and many more. In this work we present a neural model for efficiently learning a generic context embedding function from large corpora, using bidirectional LSTM. With a very simple application of our context representations , we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word em-beddings. We release our code and pre-trained models, suggesting they could be useful in a wide variety of NLP tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generic word embeddings capture semantic and syntactic information about individual words in a compact low-dimensional representation. While they are trained to optimize a generic taskindependent objective function, word embeddings were found useful in a broad range of NLP tasks, making an overall huge impact in recent years. A major advancement in this field was the introduction of highly efficient models, such as word2vec ( <ref type="bibr" target="#b23">Mikolov et al., 2013a</ref>) and GloVe <ref type="bibr">(Pen- nington et al., 2014</ref>), for learning generic word embeddings from very large corpora. Capturing information from such corpora substantially increased the value of word embeddings to both unsupervised and semi-supervised NLP tasks.</p><p>To make inferences regarding a concrete target word instance, good representations of both the target word type and the given context are helpful. For example, in the sentence "I can't find <ref type="bibr">[April]</ref>", we need to consider both the target word April and its context "I can't find [ ]" to infer that April probably refers to a person. This principle applies to various tasks, including word sense disambiguation, co-reference resolution and named entity recognition (NER).</p><p>Like target words, contexts are commonly represented via word embeddings. In an unsupervised setting, such representations were found useful for measuring context-sensitive similarity <ref type="bibr" target="#b5">(Huang et al., 2012)</ref>, word sense disambiguation ( <ref type="bibr" target="#b2">Chen et al., 2014</ref>), word sense induction <ref type="bibr">(Kågebäck et al., 2015)</ref>, lexical substitution ( <ref type="bibr" target="#b20">Melamud et al., 2015b</ref>), sentence completion ( <ref type="bibr" target="#b17">Liu et al., 2015)</ref> and more. The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings. We note that such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole.</p><p>In supervised settings, various NLP systems use labeled data to learn how to consider context word representations in a more optimized task-specific way. This was done in tasks, such as chunking, NER, semantic role labeling, and co-reference resolution ( <ref type="bibr" target="#b31">Turian et al., 2010;</ref><ref type="bibr" target="#b3">Collobert et al., 2011;</ref><ref type="bibr" target="#b21">Melamud et al., 2016)</ref>, mostly by considering the embeddings of words in a window around the target of interest. More recently, bidirectional recurrent neural networks, and specifically bidirectional LSTMs, were used in such tasks to learn internal representations of wider sentential contexts ( <ref type="bibr" target="#b34">Zhou and Xu, 2015;</ref><ref type="bibr" target="#b12">Lample et al., 2016)</ref>. Since supervised data is usually limited in size, it has been shown that training such systems, using word embeddings that were pre-trained on large corpora, improves performance significantly. Yet, pre-trained word embeddings carry limited information regarding the inter-dependencies between target words and their sentential context as a whole. To model this (and more), the supervised systems still need to rely heavily on their albeit limited supervised data.</p><p>In this work we present context2vec, an unsupervised model and toolkit 1 for efficiently learning generic context embedding of wide sentential contexts, using bidirectional LSTM. Essentially, we use large plain text corpora to learn a neural model that embeds entire sentential contexts and target words in the same low-dimensional space, which is optimized to reflect inter-dependencies between targets and their entire sentential context as a whole. To demonstrate their high quality, we show that with a very simple application of our context representations, we are able to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the common average-of-word-embeddings representation (denoted AWE). We further hypothesize that both unsupervised and semi-supervised systems may benefit from using our pre-trained models, instead or in addition to individual pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Context2vec's Neural Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Overview</head><p>The main goal of our model is to learn a generic task-independent embedding function for variable-length sentential contexts around target words. To do this, we propose a neural network architecture, which is based on word2vec's CBOW architecture ( <ref type="bibr" target="#b23">Mikolov et al., 2013a</ref>), but replaces its naive context modeling of averaged word embeddings in a fixed window, with a much more powerful neural model, using bidirectional LSTM. Our proposed architecture is illustrated in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, together with the analogical word2vec architecture. Both models learn context and target word representations at the same time, by embedding them into the same low-dimensional space, with the objective of having the context predict the target word via a log linear model. However, we utilize a much more powerful parametric model to capture the essence of sentential context.</p><p>The left-hand side of <ref type="figure" target="#fig_0">Figure 1b</ref> illustrates how context2vec represents sentential context. We use a bidirectional LSTM recurrent neural network, feeding one LSTM network with the sentence words from left to right, and another from right to left. The parameters of these two networks are completely separate, including two separate sets of left-to-right and right-to-left context word embeddings. To represent the context of a target word in a sentence (e.g. for "John [submitted] a paper"), we first concatenate the LSTM output vector representing its left-to-right context ("John") with the one representing its right-to-left context ("a paper"). With this, we aim to capture the relevant information in the sentential context, even when it is remote from the target word. Next, we feed this concatenated vector into a multi-layer perceptron to be capable of representing non-trivial dependencies between the two sides of the context. We consider the output of this layer as the embedding of the entire joint sentential context around the target word. At the same time, the target word itself (right-hand side of <ref type="figure" target="#fig_0">Figure 1b)</ref> is represented with its own embedding, equal in dimensionality to that of the sentential context. We note that the only (yet crucial) difference between our model and word2vec's CBOW <ref type="figure" target="#fig_0">(Figure 1a)</ref> is that CBOW represents the context around a target word as a simple average of the embeddings of the context words in a window around it, while context2vec utilizes a full-sentence neural representation of context.</p><p>Finally, to learn the parameters of our network, we use word2vec's negative sampling objective function, with a positive pair being a target word and its entire sentential context, and respective k negative pairs as random target words, sampled from a (smoothed) unigram distribution over the vocabulary, paired with the same context. With this, we learn both the context embedding network parameters and the target word embeddings.</p><p>In contrast to word2vec and similar word embedding models that use context modeling mostly internally and consider the target word embeddings as their main output, our primary focus is the context representation. Our model achieves its objective by assigning similar embeddings to sentential contexts and their associated target words. Further, similar to the case in word2vec models, this indirectly results in assigning similar embeddings to target words that are associated with similar sentential contexts, and conversely to sentential contexts that are associated with similar target words. We will show in the following sections how these properties make our model useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Formal Specification and Analysis</head><p>We use a bidirectional LSTM recurrent neural network to obtain a sentence-level context representation. Let lLS be an LSTM reading the words of a given sentence from left to right, and let rLS be a reverse one reading the words from right to left. Given a sentence w 1:n , our 'shallow' bidirectional LSTM context representation for the target w i is defined as the following vector concatenation: biLS(w 1:n , i) = lLS(l 1:i−1 ) ⊕ rLS(r n:i+1 ) where l/r represent distinct left-to-right/right-toleft word embeddings of the sentence words. <ref type="bibr">2</ref> This definition is a bit different than standard bidirectional LSTM, as we do not feed the LSTMs with the target word itself (i.e. the word in position i). Next, we apply the following non-linear function on the concatenation of the left and right context representations:</p><formula xml:id="formula_0">MLP(x) = L 2 (ReLU(L 1 (x)))</formula><p>where MLP stands for Multi Layer Perceptron, ReLU is the Rectified Linear Unit activation function, and</p><formula xml:id="formula_1">L i (x) = W i x + b i is a fully connected linear operation. Let c = (w 1 , ..., w i−1 , −, w i+1 , .</formula><p>.., w n ) be the sentential context of the word in position i. We define context2vec's representation of c as:</p><formula xml:id="formula_2">c = MLP(biLS(w 1:n , i)).</formula><p>Next, we denote the embedding of a target word t as t. We use the same embedding dimensionality for target and sentential context representations. To learn target word and context representations, we use the word2vec negative sampling objective function ( <ref type="bibr" target="#b24">Mikolov et al., 2013b</ref>):</p><formula xml:id="formula_3">S = t,c log σ( t · c) + k i=1 log σ(− t i · c)<label>(1)</label></formula><p>where the summation goes over each word token t in the training corpus and its corresponding (single) sentential context c, and σ is the sigmoid function. t 1 , ..., t k are the negative samples, independently sampled from a smoothed version of the target words unigram distribution: p α (t) ∝ (#t) α , such that 0 ≤ α &lt; 1 is a smoothing factor, which increases the probability of rare words. <ref type="bibr" target="#b14">Levy and Goldberg (2014b)</ref> proved that when the objective function in Equation <ref type="formula" target="#formula_3">(1)</ref> is applied to single-word contexts, it is optimized when:</p><formula xml:id="formula_4">t · c = PMI α (t, c) − log(k)<label>(2)</label></formula><p>where PMI(t, c) = log p(t,c) pα(t)p(c) is the pointwise mutual information between the target word t and the context word c. The analysis presented in <ref type="bibr" target="#b14">Levy and Goldberg (2014b)</ref> is valid for every cooccurrence matrix that describes the joint distribution of two random variables. Specifically, it can be applied to our case, where the context is not just a single word but an entire sentential context of a target word. Accordingly, we can view the targetcontext embedding obtained by our algorithm as a factorization of the PMI matrix between all possible target words and all possible different sentential contexts. Unlike the case of single-word contexts, it is not feasible to explicitly compute here this PMI matrix due to the exponential number of possible sentential contexts. However, the objective function that we optimize still aims to best approximate it. Based on the above analysis, we can expect the inner-product of our target and context embeddings to approximate PMI α (c, t). We note that accordingly, with larger values of α, there will be more bias towards placing rare words closer to their associated contexts in this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Illustration</head><p>To demonstrate the qualities of the embedded space learned by context2vec, we illustrate three types of similarity metrics in that space: target-tocontext (t2c), context-to-context (c2c) and targetto-target (t2t). All these are measured by the vector cosine value between the respective embedding representations. Only the latter target-to-target metric is the one typically used when illustrating and evaluating word embedding models, such as word2vec. <ref type="figure" target="#fig_1">Figure 2</ref> provides a 2D illustration of such a space and respective metrics.</p><p>In <ref type="table">Table 1</ref> we show sentential contexts and the target words that are closest to them, using the target-to-context similarity metric with context2vec embeddings. As can be seen, the bidirectional LSTM modeling of context2vec is indeed capable in this case to capture long range dependencies, as well as to take both sides of the context into account. In <ref type="table">Table 2</ref> we show the closest target words to given contexts, using different context2vec models, each learned with a different negative sampling smoothing parameter α. This illustrates the bias that high α values introduce towards rare words, as predicted with the analysis in section 2.2.</p><p>Next, to illustrate the context-to-context similarity metric, we took the set of contexts for the target lemma add from the training set of <ref type="bibr">Senseval-3 (Mihalcea et al., 2004</ref>). In <ref type="table" target="#tab_0">Table 3</ref> we show an example for a 'query' context from that set and the other two most similar contexts to it, based on context2vec and AWE (average of Skip-gram word embeddings) context representations. <ref type="bibr" target="#b19">Melamud et al. (2015a)</ref> argues that since contexts induce meanings (or senses) for target words, a good context similarity measure should assign high similarity values to contexts that induce similar senses for the same target word. As can be seen in this example, AWE's similarity measure seems to be influenced by the presence of the location names in the contexts, even though they have little effect on the perceived meaning of add in the sentences. Indeed, the sense of add in the closest contexts retrieved by AWE is different than that in the 'query' context. In this case, context2vec's similarity measure was robust to this problem.</p><p>Finally, in <ref type="table" target="#tab_1">Table 4</ref>, we show the closest target words to a few given target words, based on the target-to-target similarity metric. We compare context2vec's target word embeddings to Skipgram word2vec embeddings, trained with 2-word and 10-word windows. As can be seen, our model seems to better preserve the function of the given target words including part-of-speech and even tense, in comparison to the 2-word window model, and even more so compared to the 10-word window one. The intuition for this behavior is that Skip-gram literally skips words in the context</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentential Context</head><p>Closest target words This [ ] is due item, fact-sheet, offer, pack, card This [ ] is due not just to mere luck offer, suggestion, announcement, item, prize This [ ] is due not just to mere luck, award, prize, turnabout, offer, gift but to outstanding work and dedication [ ] is due not just to mere luck, it, success, this, victory, prize-money but to outstanding work and dedication <ref type="table">Table 1</ref>: Closest target words to various sentential contexts, illustrating context2vec's sensitivity to long range dependencies, and both sides of the target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>α</head><p>John was [ ] last year 0.25 born, late, married, out, back 0.50 born, back, married, released, elected 0.75 born, interviewed, re-elected 1.00 starstruck, goal-less, unwed <ref type="table">Table 2</ref>: Closest target words to a given sentential context using different α values in context2vec.</p><p>around the target word and therefore may find, for instance, the contexts of san and francisco to be very similar. In contrast, our model considers only entire sentential contexts, taking context word order and position into consideration. <ref type="bibr" target="#b21">Melamud et al. (2016)</ref> showed that target word embeddings, learned from context representations that are generated using n-gram language models, also exhibit function-preserving similarities, which is consistent with our observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relation to Language Models</head><p>Our model is closely related to language models, as can be seen in section 2.2 and tables 1 and 2. In particular, it has a lot in common with LSTMbased language models, as both train LSTM neural networks with the objective to predict target words based on their (short and long range) context, and both use techniques, such as negative sampling, to address large vocabulary computational challenges during training ( <ref type="bibr" target="#b6">Jozefowicz et al., 2016</ref>). The main difference is that LSTM language models are mainly concerned with optimizing predictions of conditional probabilities for target words given their history, while our model is focused on deriving generally useful representations to whole history-and-future contexts of target words. We follow word2vec's learning framework as it is known to produce high-quality representations for single words. It does so by having t · c approximate PMI(t, c) rather than log p(t|c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Settings</head><p>We intend context2vec's generic context embedding function to be integrated into various more optimized task-specific systems. However, to demonstrate its qualities independently, we address three different types of tasks by the simple means of measuring cosine distances between its embedded representations. Yet, we compare our performance against the state-of-the-art results of highly competitive task-optimized systems on each task. In addition we use AWE as a baseline representing a commonly used generic context representation, which like ours, can represent variable-length contexts with a fixed-size vector. Our evaluation includes the following tasks: sentence completion, lexical substitution and supervised word sense disambiguation (WSD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning corpus</head><p>With the exception of the sentence completion task (MSCC), which comes with its own learning corpus, we used the two billion word ukWaC <ref type="bibr">(Fer- raresi et al., 2008</ref>) as our learning corpus. To speed-up the training of context2vec, we discarded all sentences that are longer than 64 words, reducing the size of the corpus by ∼10%. However, we train the embeddings used in the AWE baseline on the full corpus to not penalize it on account of our model. We lower-cased all text and considered any token with fewer than 100 occurrences as an unknown word. This yielded a vocabulary of a little over 180K words for the full corpus, and 160K words for the trimmed version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compared Methods</head><p>context2vec We implemented our model using the Chainer toolkit ( <ref type="bibr" target="#b30">Tokui et al., 2015)</ref>, and Adam ( <ref type="bibr" target="#b9">Kingma and Ba, 2014</ref>) for optimization. To speed-up the learning time we used mini-batch training, where only sentences of equal length are Query Furthermore our work in Uganda and Romania <ref type="bibr">[ adds ]</ref> a wider perspective. ... themes in art have a fascination , since they [ add ] a subject interest context2vec to a viewer's enjoyment of artistic qualities. closest Richard is joining us every month to pass on tips , ideas and news from the world of horticulture , and [ add ] a touch of humour too ... the foreign ministers said political and economic reforms in Poland and Hungary AWE had made considerable progress but <ref type="bibr">[ added ]</ref> : the process remains fragile ... closest ... Germany had announced the solution as a humanitarian act by the government, <ref type="bibr">[ adding ]</ref> that it hoped Bonn in future would run its embassies in normal manner...  AWE We learned word embeddings with the popular word2vec Skip-gram model using standard hyperparameters: 600 dimensions, 10 negative samples, window-size 10 and 3/5 iterations for the ukWaC/MSCC learning corpora, respectively. Then we used a simple average of these embeddings as our AWE context representation. <ref type="bibr">3</ref> In addition, we experimented with the following variations: (1) ignoring stopwords (2) performing a weighted average of the words in the context using tf-idf weights (3) considering just the 5-word window around the target word instead of the whole sentence. Specifically, in the WSD experiment the context provided for the target words is a full paragraph. Though it could be extended, context2vec is currently not designed to take advantage of such large context and therefore ignores all context out-side of the sentence of the target word. However, for AWE we also experimented with the option of generating the context representation based on the entire paragraph. In all cases, the size (dimensionality) of the AWE context representation was equal to that of context2vec, and the context-to-target and context-to-context similarities were computed using vector cosine between the respective embedding representations, as with context2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Completion Challenge</head><p>The Microsoft Sentence Completion Challenge (MSCC) (Zweig and Burges, 2011) includes 1,040 items. Each item is a sentence with one word replaced by a gap, and the challenge is to identify the word, out of five choices, that is most meaningful and coherent as the gap-filler. While there is no official dev/test split for this dataset, we followed previous work <ref type="bibr" target="#b25">(Mirowski and Vlachos, 2015)</ref> and used the first 520 sentences for parameter tuning and the rest as the test set. <ref type="bibr">4</ref> The MSCC includes a learning corpus of 50 million words. To use this corpus for training our models, we first discarded all sentences longer than 128 words, which resulted in a negligible reduction of ∼ 1% in the size of the corpus. Then, we converted all text to lowercase and considered all words with frequency less than 3 as unknown, yielding a vocabulary of about 100K word types.</p><p>Finally, as the gap-filler, we simply choose the word whose target word embedding is the most similar to the embedding of the given context using the target-to-context similarity metric. We report the accuracy achieved in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Lexical Substitution Task</head><p>The lexical substitution task requires finding a substitute word for a given target word in sentential context. The difference between this and the sentence completion task is that the substitute word needs not only to be coherent with the sentential context, but also preserve the meaning of the original word in that context. Most recent works evaluated their performance on a ranking variant of the lexical substitution task, which uses predefined candidate lists provided with the gold standard, and requires to rank them considering the sentential context. Performance in this task is reported with generalized average precision (GAP). <ref type="bibr">5</ref> As in MSCC, in this evaluation we rank lexical substitutes according to the measured similarity between their target word embeddings and the embedding of the given sentential context.</p><p>We used two lexical substitution datasets in our experiments. The first is the dataset introduced in the lexical substitution task of <ref type="bibr">SemEval 2007 (Mc- Carthy and</ref><ref type="bibr" target="#b18">Navigli, 2007)</ref>, denoted LST-07, split into 300 dev sentences and 1,710 test sentences. The second is a more recent 'all-words ' dataset ( <ref type="bibr" target="#b11">Kremer et al., 2014</ref>), denoted LST-14, with over 15K target word instances. It comes with a predefined 35%/65% split. We used the smaller set as the dev set for parameter tuning and the larger one as our test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Supervised WSD</head><p>In supervised WSD tasks, the goal is to determine the correct sense of words in context, based on a manually tagged training set. To classify a test word instance in context, we consider all of the 5 See Melamud et al. (2015a) for more of their setting details, which we followed here. <ref type="table" target="#tab_0">context word units  300  LSTM hidden/output units  600  MLP input units  1200  MLP hidden units  1200  sentential context units  600  target word units  600  negative samples  10   Table 5</ref>: context2vec hyperparameters tagged instances of the same word lemma in the training set, and find the instance whose context embedding is the most similar to the context embedding of the test instance using the context-tocontext similarity metric. Then, we use the tagged senses 6 of that instance. We note that this is essentially the simplest form of a k-nearest-neighbor algorithm, with k = 1.</p><p>As our supervised WSD dataset we used the Senseval-3 lexical sample dataset ( <ref type="bibr" target="#b22">Mihalcea et al., 2004</ref>), denoted SE-3, which includes 7,860 train and 3,944 test instances. We used the training set for parameter tuning and report accuracy results on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Development Experiments</head><p>The hyperparameters used in our reported experiments with context2vec are summarized in Table 5. In preliminary development experiments, we used only 200 units for representing sentential contexts, and then saw significant improvement in results, when moving to 600 units. Increasing the representation size to 1,000 units did not seem to further improve results.</p><p>With mini-batches of 1,000 sentences at a time, we started by training our models with a single iteration over the 2-billion-word ukWaC corpus. This took ∼30 hours, using a single Tesla K80 GPU. For the smaller 50-million-word MSCC learning corpus, a full iteration with a batch size of 100 took only about 3 hours. For this corpus, we started with 5 training iterations.</p><p>To explore the rare-word bias effect of the vocabulary smoothing factor α, we varied its value in our development experiments. The results appear in <ref type="table">Table 6</ref> on the left hand side. Since we preferred to keep our model as simple as possible, based on these results, we chose the single context2vec AWE neg sampling parameter α iters+ best best worst worst 0.25 0.50 0.75 1.00 0.75 config result config result MSCC-dev 52.5 56.5 60.0 52.7</p><p>66.2 sent+stop 51.0 W5 36.5 LST-07-dev 50.1 52.9 53.6 54.3 55.4 W5+stop 45.8 sent 40.0 LST-14-dev 48.2 48.9 48.0 46.1 48.3 sent+stop 40.4 sent 39.2 SE-3-dev 72.1 72.4 71.6 72.5 72.6 W5+tf-idf 62.4 sent 57.3 <ref type="table">Table 6</ref>: Development set results. iters+ denotes the best model found when running more training iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.</p><p>value α = 0.75 for all of our test sets experiments. With this choice, we also tried training our models with more iterations and found that with 3 iterations over the ukWaC corpus and 10 iterations over the MSCC corpus we can obtain some further improvement in results, see iters+ in <ref type="table">Table 6</ref>.</p><p>The results of our experiments with all of the AWE variants, described in section 3.2, appear on the right hand side of <ref type="table">Table 6</ref>. For brevity, we report only the best and worst configuration for each benchmark. As can be seen, in two out of four benchmarks, a window of 5 words yields better performance than a full sentential context, suggesting that the AWE representation is not very successful in leveraging effectively long range information. Removing stop words or using tf-idf weights improves performance significantly. However, the results are still much lower than the ones achieved with context2vec. To raise the bar, in each test-set experiment we used the best AWE configuration found for the corresponding development-set experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test Sets Results</head><p>The test set results are summarized in <ref type="table" target="#tab_3">Table 7</ref>. First, we see that context2vec substantially outperforms AWE across all benchmarks. This suggests that our context representations are much better optimized for capturing sentential context information than AWE, at least for these tasks. Further, we see that with context2vec we either surpass or almost reach the state-of-the-art on all benchmarks. This is quite impressive, considering that all we did was measure cosine distances between context2vec's representations to compete with more complex and task-optimized systems.</p><p>More specifically, in the sentence completion task (MSCC) the prior state-of-the-art result is due to <ref type="bibr" target="#b23">Mikolov et al. (2013a)</ref>   and iters+ denotes the model that was trained with more iterations. S-1/S-2 stand for the best/secondbest prior result reported for the benchmark.</p><p>weighted combination of scores from two different models: a recurrent neural network language model, and a Skip-gram model. The second-best result is due to <ref type="bibr" target="#b17">Liu et al. (2015)</ref> and is based on word embeddings that are learned based on both corpora and structured knowledge resources, such as WordNet. context2vec outperforms both of them. In the lexical substitution tasks, the best prior results are due to <ref type="bibr" target="#b19">Melamud et al. (2015a)</ref>. <ref type="bibr">7</ref> They employ an exemplar-based approach that requires keeping thousands of exemplar contexts for every target word type. The second-best is due to <ref type="bibr" target="#b20">Melamud et al. (2015b)</ref>. They propose a simple approach, but it requires dependency-parsed text as input. context2vec achieves comparable results with these works, using the same learning corpus.</p><p>In the Senseval-3 supervised WSD task, the best result is due to <ref type="bibr" target="#b0">Ando (2006)</ref> and the second-best to <ref type="bibr" target="#b27">Rothe and Schütze (2015)</ref>. context2vec is almost on par with these results, which were achieved with dedicated feature engineering and supervised machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Substitute vectors <ref type="bibr" target="#b33">(Yuret, 2012</ref>) represent contexts as a probabilistic distribution over the potential gap-filler words for the target slot, pruned to its top-k most probable words. While using this representation showed interesting potential ( <ref type="bibr" target="#b32">Yatbaz et al., 2012;</ref><ref type="bibr" target="#b19">Melamud et al., 2015a</ref>), it can currently be generated efficiently only with n-gram language models and hence is limited to fixed-size context windows. It is also high dimensional and sparse, in contrast to our proposed representations. Syntactic dependency context embeddings have been proposed recently ( <ref type="bibr" target="#b13">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b1">Bansal et al., 2014</ref>). They depend on the availability of a high-quality dependency parser, and can be viewed as a 'bag-of-dependencies' rather than a single representation for the entire sentential context. However, we believe that incorporating such dependency-based information in our model is an interesting future direction.</p><p>A couple of recent works extended word2vec's CBOW by replacing its internal context representation. <ref type="bibr" target="#b16">Ling et al. (2015b)</ref> proposed a continuous window, which is a simple linear projection of the context window embeddings into a low dimensional vector. <ref type="bibr" target="#b15">Ling et al. (2015a)</ref> proposed 'CBOW with attention', which is used for finding the relevant features in a context window. In contrast to our model, both approaches confine the context to a fixed-size window. Furthermore, they limit their scope to using these context representations only internally to improve the learning of target words embeddings, rather than evaluate the benefit of using them directly in NLP tasks, as we do.  represent words in context using bidirectional LSTMs and multilingual supervision. In contrast, our model is focused on representing the context alone. Yet, as shown in our lexical substitution and word sense disambiguation evaluations, it can easily be used for modeling the meaning of words in context as well.</p><p>Finally, there is considerable work on using recurrent neural networks to represent word sequences, such as phrases or sentences <ref type="bibr" target="#b28">(Socher et al., 2011;</ref><ref type="bibr" target="#b10">Kiros et al., 2015)</ref>. We note that the techniques used for learning sentence representations have much in common with those we use for sentential context representations. Yet, sentential context representations aim to reflect the information in the sentence only inasmuch as it is relevant to the target slot. Specifically, different target positions in the same sentence can yield completely different context representations. In contrast, sentence representations aim to reflect the entire contents of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Potential</head><p>We presented context2vec, a neural model that learns a generic embedding function for variablelength contexts of target words. We demonstrated that it can be trained in a reasonable time over billions of words and generate high quality context representations, which substantially outperform the traditional average-of-word-embeddings approach on three different tasks. As such, we hypothesize that it could contribute to various NLP systems that model context. Specifically, semisupervised systems may benefit from using our model, as it may carry more useful information learned from large corpora, than individual pretrained word embeddings do.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: word2vec and context2vec architectures.</figDesc><graphic url="image-2.png" coords="2,307.28,212.14,218.27,215.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A 2D illustration of context2vec's embedded space and similarity metrics. Triangles and circles denote sentential context embeddings and target word embeddings, respectively.</figDesc><graphic url="image-3.png" coords="4,72.00,62.81,218.27,187.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>An example for a given 'query' context and the two closest contexts to it, as 'retrieved' by 
context2vec similarity and AWE similarity. 

context2vec word2vec-w2 word2vec-w10 context2vec 
word2vec-w2 
word2vec-w10 
flying 
syntactically 
gliding 
flew 
flew 
semantically 
grammatically semantically 
sailing 
fly 
fly 
lexically 
phonologically grammatically 
diving 
aerobatics 
aeroplane 
grammatically semantically 
syntax 
flown 
low-flying 
flown 
phonologically ungrammatical syntactic 
travelling 
flown 
bi-plane 
topologically 
lexically 
lexically 
san 
prize 
agios 
francisco 
francisco 
prizes 
prizes 
prizes 
aghios 
diego 
diego 
award 
prize-winner 
winner 
los 
fransisco 
fransisco 
trophy 
prizewinner 
winners 
tanjung 
los 
bernardino 
medal 
prize 
prizewinner 
puerto 
obispo 
los 
prizewinner 
prizewinners 
prize. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Top-5 closest target words to a few given target words. 

assigned to the same batch. We discuss the hyper-
parameters tuning of our model in section 4.1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results on test sets. c2v is context2vec 
</table></figure>

			<note place="foot" n="1"> Source code and pre-trained models are available at: http://www.cs.biu.ac.il/nlp/resources/ downloads/context2vec/ (a) word2vec CBOW (b) context2vec</note>

			<note place="foot" n="2"> We pad every input sentence with special BOS and EOS words in positions 0 and n + 1, respectively.</note>

			<note place="foot" n="3"> We made some preliminary experiments using word embeddings learned with word2vec&apos;s CBOW model, instead of Skip-gram, but this yielded worse results.</note>

			<note place="foot" n="4"> Mikolov et al. (2013a) did not specify their dev/test split and all other works reported results only on the entire dataset.</note>

			<note place="foot" n="6"> There&apos;s one or more senses assigned to a each instance.</note>

			<note place="foot" n="7"> Szarvas et al. (2013) achieved almost the same result, but with a supervised model, not directly compared to ours.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our anonymous reviewers for their useful comments. This work was partially supported by the Israel Science Foundation grant 880/12 and the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying alternating structure optimization to word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ando</forename><surname>Rie Kubota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing and evaluating ukwac, a very large web-derived corpus of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Web as Corpus Workshop (WAC-4)</title>
		<meeting>the 4th Web as Corpus Workshop (WAC-4)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural context embeddings for automatic discovery of word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devdatt</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to represent words in context with multilingual supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop in ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What substitutes tell us-analysis of an all-words lexical substitution corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural word embeddings as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chu-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 10: English lexical substitution task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling word meaning in context with substitute vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple word embedding model for lexical substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Vector Space Modeling for NLP (VSM)</title>
		<meeting>Workshop on Vector Space Modeling for NLP (VSM)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The role of context types and dimensionality in learning word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The senseval-3 english lexical sample task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">Anatolievich</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dependency recurrent neural language models for sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01193</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to rank lexical substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Róbert</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning syntactic categories using paradigmatic representations of word context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Mehmet Ali Yatbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">FASTSUBS: An efficient and exact procedure for finding the most likely lexical substitutes based on an n-gram language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="725" to="728" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The microsoft research sentence completion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2011- 129</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Microsoft</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
