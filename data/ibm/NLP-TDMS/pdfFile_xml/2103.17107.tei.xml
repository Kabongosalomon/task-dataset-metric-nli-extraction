<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial expression and attributes recognition based on multi-task learning of lightweight neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><forename type="middle">V</forename><surname>Savchenko</surname></persName>
							<email>avsavchenko@hse.ru</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory of Algorithms and Technologies for Network Analysis</orgName>
								<orgName type="institution">HSE University</orgName>
								<address>
									<settlement>Nizhny Novgorod</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial expression and attributes recognition based on multi-task learning of lightweight neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial analytics · Mobile devices · Facial expression recogni- tion · Age</term>
					<term>gender</term>
					<term>race classification · AffectNet · AFEW (Acted Facial Expression In The Wild) · VGAF (Video-level Group AFfect)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we examine the multi-task training of lightweight convolutional neural networks for face identification and classification of facial attributes (age, gender, ethnicity) trained on cropped faces without margins. It is shown that it is still necessary to fine-tune these networks in order to predict facial expressions. Several models are presented based on MobileNet, EfficientNet and RexNet architectures. It was experimentally demonstrated that our models are characterized by the state-of-the-art emotion classification accuracy on AffectNet dataset and near state-ofthe-art results in age, gender and race recognition for UTKFace dataset. Moreover, it is shown that the usage of our neural network as a feature extractor of facial regions in video frames and concatenation of several statistical functions (mean, max, etc.) leads to 4.5% higher accuracy than the previously known state-of-the-art single models for AFEW and VGAF datasets from the EmotiW challenges. The models and source code are publicly available at https://github.com/HSE-asavchenko/ face-emotion-recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is no doubt that facial analytics in images and videos, e.g., age, gender, ethnicity and emotion prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, is one of the most widely studied tasks in image recognition. Thousands of papers appear every year to present all the more difficult techniques and models based on deep convolutional neural network (CNN) <ref type="bibr" target="#b2">[3]</ref>. Ensembles of complex models won the prestigious challenges and contests <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Unfortunately, such methods may be too complicated for their practical usage, e.g. in mobile applications <ref type="bibr" target="#b5">[6]</ref>. As a result, there is a huge demand in development of simple easy-to-use solutions, e.g., multi-task learning <ref type="bibr" target="#b6">[7]</ref> and/or sequential training of models on different problems of facial analysis <ref type="bibr" target="#b7">[8]</ref>, that preserve the state-of-the-art quality and do not require fine-tuning on every new task and/or dataset. In this paper it is proposed to further simplify the training procedure to obtain lightweight but very accurate models. In particular, the network is pretrained on large face identification dataset <ref type="bibr" target="#b8">[9]</ref>. In contrast to existing studies, we analyze more carefully cropped faces using precise regions at the output of face detectors without additional margins. It is demonstrated that though the face recognition quality becomes slightly worth when compared to training on larger facial regions, the fine-tuning of the resulted network leads to more accurate ethnicity classification on UTKFace (University of Tennessee, Knoxville Face Dataset) <ref type="bibr" target="#b9">[10]</ref> and emotion recognition on AffectNet dataset <ref type="bibr" target="#b1">[2]</ref>. Moreover, the features extracted by the latter network make it possible to achieve the state-of-the-art results among single models in video-based emotion recognition using AFEW (Acted Facial Expression In The Wild) <ref type="bibr" target="#b10">[11]</ref> and VGAF (Videolevel Group AFfect) <ref type="bibr" target="#b11">[12]</ref> datasets from EmotiW (Emotion Recognition in the Wild) 2019 and 2020 challenges. Details of implementation together with best snapshots of our models and Android demo application are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>Face recognition. The very large VGGFace2 dataset <ref type="bibr" target="#b8">[9]</ref> was used to pre-train the CNNs. The training set contained 3,067,564 photos of 9131 subjects, while the remaining 243,722 images were put into the testing set. The facial regions were detected using MTCNN. The recognition accuracy was measured on the LFW (Labeled Faces in-the-Wild) dataset using the protocol of face identification from <ref type="bibr" target="#b12">[13]</ref>: C = 596 subjects who have at least two images in the LFW and at least one video in the YTF (YouTube Faces) database. Training set contains exactly one facial image, all other images from LFW were put into the testing set. We used two pre-processing techniques, namely, the center crop of 224x224 region in each photo <ref type="figure" target="#fig_0">(Fig. 1a</ref>,c) that is tradition for this dataset, and MTCNNbased face detection without any margins ( <ref type="figure" target="#fig_0">Fig. 1b,d)</ref>.</p><p>Facial attributes prediction. The training dataset was populated by 300K frontal cropped facial images from the IMDB-Wiki dataset <ref type="bibr" target="#b13">[14]</ref> to predict age and gender <ref type="bibr" target="#b6">[7]</ref>. In order to test the quality of age and gender prediction, the images from complete ("In the Wild") set of UTKFace <ref type="bibr" target="#b9">[10]</ref> were pre-processed using the following procedure from the Agegendernet 2 : faces are detected and aligned with margin 0.4 using get face chip function from DLib. Only 23,060 images with single face were used to test age and gender prediction quality. There was no fine-tuning on the UTKFace, so that the testing set contains all images from UTKFace. To train ethnicity classifier, conventional set of 23,708 images from "Aligned &amp; cropped faces" UTKFace <ref type="bibr" target="#b9">[10]</ref> was divided into 20,149 training images and 3559 testing images. Each facial image is associated with one of 5 classes (White, Black, Asian, Indian and Latino/Middle Eastern).</p><p>Facial expression recognition. Rather large and complex AffectNet dataset <ref type="bibr" target="#b1">[2]</ref> was used to train the emotion CNN. The training set contains 287,651 and 283,901 images for 8 classes (Neutral, Happy, Sad, Surprise, Fear, Anger, Disgust, Contempt) and 7 primary expressions (the same without Contempt), respectively. The validation set consists of 500 images per each class, i.e. 4000 and 3500 images for 8 and 7 classes.</p><p>In addition, the video-based emotion classification task is studied on two datasets from the EmotiW challenges. At first, the AFEW 8.0 dataset <ref type="bibr" target="#b10">[11]</ref> of short video clips extracted from movies is examined. The results are reported on the validation set since the test set labels are unavailable. The training and validation set contain 773 and 383 video files, respectively. Every sample belongs to one of the C = 7 emotional categories (Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Neutral). The facial regions in each frame have been detected using MTCNN. If it detects multiple faces in a frame, the face with the largest bounding box is selected.</p><p>Finally, group-level video-based emotion classification is studied on the recently introduced VGAF dataset <ref type="bibr" target="#b11">[12]</ref>. It has only C = 3 emotion labels of a group of people, namely, Positive, Negative and Neutral. The labels of the testing set are unknown, so that the classification accuracy is measured on the validation set with 766 clips, while 2661 videos are available for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-task training.</head><p>Multi-task networks. In this paper a multi-task neural network <ref type="bibr" target="#b6">[7]</ref> is adapted to solve several facial attributes recognition problems <ref type="figure">(Fig. 2)</ref>. The disjoint features among the tasks are exploited to increase the accuracies <ref type="bibr" target="#b0">[1]</ref>. At first, traditional approach is used: the base CNN is pre-trained on face identification using very large dataset. As this paper is concentrated on lightweight CNNs, it was decided to use such architectures as MobileNet, EfficientNet <ref type="bibr" target="#b14">[15]</ref> and RexNet <ref type="bibr" target="#b15">[16]</ref> as a backbone face recognition network. The resulted neural net extracts facial features x that are suitable to discriminate one subject from another. These features can be used to predict the attributes that are stable for a given person, i.e., gender and ethnicity, using simple classifier, i.e., one fully connected layer. The age of the same subject is not constant but it is changed very slow. Hence, we believe that this attribute can be predicted based on the same feature vector x, but several new layers can be added before the final fully connected layer</p><formula xml:id="formula_0">!""#$%&amp;'(#$)% *'+,'&amp;%,-'.#% /$0"012% *1&amp;&amp;(%3044#+2#5% 65#42,2(%7#'21$#)%!% *1&amp;&amp;(%3044#+2#5% 8.#% "$09'9,&amp;,2,#)%!!% :'&amp;#%.#45#$% "$09'9,&amp;,2(%!"% ;2&lt;4,+,2(% "$09'9,&amp;,2,#)%!#% ;-02,04% "$09'9,&amp;,2,#)%!$% *1&amp;&amp;(%3044#+2#5% *1&amp;&amp;(%3044#+2#5% *1&amp;&amp;(%3044#+2#5% *,4#214#5%&amp;'(#$)% 3==%&amp;0&gt;#$%&amp;'(#$)% *'+#% $#+0.4,2,04% 3==% Fig. 2:</formula><p>Multi-task facial expression and attributes recognition.</p><p>( <ref type="figure">Fig. 2</ref>). Though the age prediction is a special case of regression problem, it is considered as a multi-class classification with C a different ages, so that it is required to predict if an observed person is 1, 2, . . . or C a years old <ref type="bibr" target="#b6">[7]</ref>. It is important to emphasize that many other facial attributes are changed rapidly, so that the facial features from face recognition should remain identical with respect to such changes. An example is the emotion recognition task: inter-class distance between face identification features of the same person with different emotions should remain much lower than the intra-class distance between different persons even with the same facial expressions. Hence, it is claimed in this paper that the facial features extracted by CNN trained on identification task cannot be directly used for emotion recognition. At the same time, lower layers of such CNN consist of feature such as edges and corners <ref type="bibr" target="#b0">[1]</ref> that may be better for the latter task when compared to CNN pre-trained on the dataset unrelated to faces, e.g., ImageNet. Hence, in this paper we fine-tune the face recognition CNN on emotion dataset to either use valuable information about facial features, or predict the facial attributes that are orthogonal to the identity.</p><p>Training details. In order to simplify the training procedure, the CNNs are trained sequentially starting from face identification problem and further tuning on different facial attribute recognition tasks <ref type="bibr" target="#b7">[8]</ref>. At first, the face recognition CNN was trained using VGGFace2 dataset as follows. The new head, i.e., fully connected layer with 9131 outputs and softmax activation, was added to the network pre-trained on ImageNet. The weights of the base net were frozen and the head was learned during 1 epoch. Conventional categorical cross-entropy loss function was optimized using contemporary SAM (Sharpness-Aware Minimization) <ref type="bibr" target="#b16">[17]</ref> and Adam with learning rate equal to 0.001. Next, the whole CNN was trained in 10 epochs in the same way but with learning rate 0.0001. The models with the highest accuracy on validation set, namely, 92.1%, 95.6% and 96.7% for MobileNet-v1, EfficientNet-B0 and RexNet-150, respectively, were further used.</p><p>Next, separate heads for age, gender and ethnicity prediction were added ( <ref type="figure">Fig. 2)</ref> and their weights were learned on the datasets described in Subsection 2.1. The weights of the base model were frozen so that only new heads were updated. Ethnicity classifier was trained on the subset of the UTKFace dataset with different class weights in order to achieve better performance for imbalanced classes. The binary cross-entropy loss was used for gender recognition. After 3 epochs, the resulted MobileNet-based model obtained 97% and 13% validation accuracies for gender and age classification, respectively. In order to make a final age prediction, only L ∈ {1, 2, ..., C a } indices {a 1 , ..., a L } with the maximal posterior probabilities p a l at the output of the CNN were chosen, and the expected mean a(X r ) <ref type="bibr" target="#b6">[7]</ref> is computed:</p><formula xml:id="formula_1">a = L l=1 a l · p a l L l=1 p a l .</formula><p>(1)</p><p>Finally, the emotion recognition network is trained on the AffectNet dataset. Though it contains at least C e = 8 expressions, it is typical to test the accuracy using only 7 emotions without Contempt category. Two ways to classify 7 emotions were studied, namely, 1) train the model on reduced training set with 7 classes; and 2) train the model on the whole training set with 8 classes, but use only 7 scores from the last (Softmax) layer. In both cases, the weighted categorical cross-entropy (softmax) loss was optimized:</p><formula xml:id="formula_2">L(X, y) = − log sof tmax(z y ) · max c∈{1,...,Ce} N c /N y ,<label>(2)</label></formula><p>where X is the training image, y ∈ {1, ..., C e } is its emotional class label, N y is the total number of training examples of the y-th class, z y is the y-th output of the penultimate (logits) layer, and sof tmax is the softmax activation function.</p><p>The training procedure remains similar to the initial pre-training of face recognition CNN. At first, the new head with C e outputs was added, the remaining weights were frozen and the weights of the new head was learned in 3 epochs using SAM <ref type="bibr" target="#b16">[17]</ref>. Finally, all weights were learned during 10 epochs. Video-based facial attribute recognition. The datasets for the video-based emotion classification tasks, i.e., AFEW and VGAF, contain video clips that can be used to train a classifier. Hence, we borrow the networks that have been previously fine-tuned on AffectNet dataset, as feature extractors. The largest facial region from each frame from AFEW is fed into such CNN, and the Ddimensional output of its penultimate layers is stored in a frame descriptor. The video descriptor with dimensionality 4D, i.e., 4 · 1024 = 4096 and 4 · 1280 = 5120 for MobileNet and EfficientNet-B0, respectively, is computed as a concatenation of statistical functions (mean, max, min and standard deviation) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> applied to their frame descriptors. As a result of mistakes in face detection, facial regions have not been detected in all frames of several videos. We decided to completely ignore them in the training set. However, though 4 validation videos does not have detected faces, they were assigned to zero descriptors with the same dimensionality as normal video descriptors, so that the validation accuracy is directly comparable with existing papers. In order to classify videos, we use either Lin-earSVC or Random Forests with 1000 trees trained on the L 2 -normed video descriptors extracted from the training set. The group-level video emotion recognition task is solved similarly, though each frame may contain several facial regions. Hence, the video descriptor is computed as follows. At first, statistical functions (mean and standard deviation) of emotion features of all faces in a single frame are concatenated to obtain a descriptor of this frame. Next, all frame descriptors are aggregated using the same mean and standard deviation functions. Maximum and minimum aggregation functions are not used here in order to reduce the dimensionality of the final descriptor. Unfortunately, the clips have rather low resolution, so that only 2,619 training and 741validation videos has at least one detected face in at least one frame. Hence, the same procedure as described for the AFEW dataset was used: the training videos without faces were ignored, and 28 validation images without faces were associated with zero descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Face identification</head><p>The rank-1 face identification accuracies of several CNN models pre-trained on the VGGFace2 dataset <ref type="bibr" target="#b8">[9]</ref> estimated by random 10-times repeated crossvalidation are shown in <ref type="table" target="#tab_0">Table 1</ref>. As one can notice, conventional SENet-based facial descriptor <ref type="bibr" target="#b8">[9]</ref> is more accurate in this challenging task with only one training image per subject, especially if loosely cropped faces are recognized <ref type="figure" target="#fig_0">(Fig. 1a,c)</ref>. However, its error rate for faces cropped by face detector without margins <ref type="figure" target="#fig_0">(Fig. 1b,d)</ref> is 0.5% higher. In contrast to this behavior, the accuracy of our descriptors is increased on 1-2% when detected faces are classified instead of the usage of center crop. As a result, it is expected that the facial regions without background are more meaningful for other facial analytics tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Facial Expression Recognition</head><p>Classification of single images. In this subsection we examine emotion classification models that have been fine-tuned on both 8 and 7 classes in the AffectNet <ref type="table">Table 2</ref>: Accuracy on the testing set of AffectNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy, % 8 classes 7 classes PSR (VGG-16) <ref type="bibr" target="#b19">[20]</ref> 60.68 -ARM (ResNet-18) <ref type="bibr" target="#b20">[21]</ref> 59.75 64.49 RAN <ref type="bibr" target="#b21">[22]</ref> 59. dataset. Moreover, we report the results of models after the first training stage, in which only the weights of the classification head were learned, while the other part of the model remains the same as in the pre-trained face recognition CNN (left part of <ref type="figure">Fig. 2)</ref>. As a result, the base network extracts features appropriate for face identification. In addition, several existing models have been trained on AffectNet similarly to our own models. In particular, the following architectures have been studied: MobileNet, Inception, EfficientNet and NFNet-F0 <ref type="bibr" target="#b18">[19]</ref> pre-trained on ImageNet-1000 and SENet pre-trained on VGGFace2 <ref type="bibr" target="#b8">[9]</ref>. <ref type="table">Table 2</ref> gives a summary of our best models compared with the known state-of-the-art methods. The best result in each column is marked by bold.</p><p>As one can notice, the usage of a model trained on complete AffectNet training set with 8 classes for prediction of 7 emotional categories has slightly lower accuracy, though it is more universal as the same model can be used to predict either 8 or 7 emotions. Second, the experiment supports our claim that the identity features from pre-trained CNNs are not suitable for reliable facial expression recognition, though are models trained on the faces cropped by MTCNN are noticeably better. The most important property is the much higher accuracy of the models trained by the proposed approach when compared to CNNs pretrained on ImageNet. Even the SENet model pre-trained on VGGFace2 dataset, that has significantly higher face identification accuracy when compared to our lightweight networks <ref type="table" target="#tab_0">(Table 1)</ref>, is characterized by much worth emotion classifi- cation error rate. It is very important to use the face detection procedure with choice of the predicted bounding box without addition of any margins <ref type="figure" target="#fig_0">(Fig. 1)</ref>. As a result, our EfficientNet-based models improved the known state-of-the-art accuracy on AffectNet for both 8 and 7 classes. In addition, the number of parameters and CPU inference time of CNNs from <ref type="table">Table 2</ref> were computed ( <ref type="table" target="#tab_2">Table 3</ref>). The running time to predict emotion of one facial image was measured on the MSI GP63 8RE laptop (Windows 10, CPU Intel Core i7-8750H 2.2GHz, RAM 16Gb). As expected, we used one of the most lightweight models with limited number of parameters. The running time is also rather small, though ResNet-18 has comparable inference speed.</p><p>Classification of videos. The video-based emotion classification problem is examined in the next experiments. The results are summarized in <ref type="table" target="#tab_3">Table 4</ref>. As one can notice, the proposed approach provides the best known accuracy among single models for AFEW dataset. Even our MobileNet is 0.18% more accurate than the ResNet-18 with attention and iterative pre-training on the body language dataset. The more powerful EfficientNet-B0 architecture has 4% higher accuracy. Our approach is much better than the original method <ref type="bibr" target="#b17">[18]</ref> with the same statistical functions applied for four large CNNs. In fact, our model practically reaches the best-known accuracy of ensemble model <ref type="bibr" target="#b3">[4]</ref>, though we even do not process the whole validation set due to mistakes in face detection. If only 779 validation videos with faces are analyzed, the overall accuracy of our EfficientNet is equal to 59.89%.</p><p>Finally, our models were applied to the VGAF <ref type="bibr" target="#b11">[12]</ref> dataset. The validation accuracies are presented in <ref type="table" target="#tab_4">Table 5</ref>. It is worth noting that all existing results are reported in the papers of the EmotiW 2020 participants. The accuracies of our models are higher when compared to all participants except the winner of this challenge <ref type="bibr" target="#b4">[5]</ref>. Nevertheless, our approach is the best known single model for this dataset. For example, it is 2-4% more accurate when compared to the DenseNet-121 facial model of the winner. Moreover, one can expect further improvements in group-level emotion classification by making face detection better. For example, if we test our models on only 741 validation videos with at least one detected face, the overall accuracy is increased to 70.31% and 68.29% for MobileNet and EfficientNet, respectively. Third, in contrast to all previous re-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Details</head><p>Modality Accuracy, % Hybrid Networks <ref type="bibr" target="#b4">[5]</ref> Ensemble audio, video 74.28 K-injection network <ref type="bibr" target="#b28">[29]</ref> Ensemble audio, video 66.19 DenseNet-121 (FER+) <ref type="bibr" target="#b4">[5]</ref> Single sults, the accuracy of MobileNet features here is 2% higher when compared to EfficientNet, so that we could claim that both models have their advantages in various emotion recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Facial Attributes Recognition</head><p>Age/gender recognition. The results <ref type="bibr" target="#b6">[7]</ref> for the UTKFace dataset are shown in Table 6. Here the proposed approach using MobileNet backbone was compared with existing age/gender prediction models, namely, MobileNet v2 (Agegendernet) 2 , FaceNet 3 , ResNet-50 model from InsightFace and Deep expectation (DEX) VGG16 networks trained on the IMDB-Wiki <ref type="bibr" target="#b13">[14]</ref>. In contrast to our approach, all these CNNs have been fine-tuned only on specific datasets with age and gender labels, i.e., they are do not use large-scale face recognition datasets for pre-training. As a result, the proposed model leads to the best results with at least 2.5% higher accuracy of gender classification and 0.6 lower MAE (mean absolute error) of age prediction than DEX due to exploitation of the potential of very large face recognition dataset to learn face representations. Moreover, our model has 80-times lower parameters when compared to two VGG-16 DEX models. Our results are  even comparable with the state-of-the-art quality for UTKFace dataset, which is achieved by training on the part of this dataset <ref type="bibr" target="#b0">[1]</ref>. For instance, if the testing set described in the paper [31] was used, namely, 3,287 photos of persons from the age ranges <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">60]</ref>, our MobileNet-based multi-task model achieves 97.5% gender recognition accuracy and age prediction MAE 5.39. It is lower than 5.47 MAE of the best CORAL-CNN [31] on the same testing set, which was additionally trained on other subset of UTKFace. Ethnicity Recognition. We used only UTKFace dataset for ethnicity recognition. Our MobileNet-based model was compared with traditional classification of such facial features as VGGFace (VGG-16), VGGFace-2 (ResNet-50) and FaceNet (InceptionResNet v1 trained on VGGFace-2 dataset). The validation accuracies are shown in <ref type="table" target="#tab_7">Table 7</ref>. Here one can notice that our model with new head (classifier) provides an appropriate quality even in comparison with very deep state-of-the-art facial embeddings. The worst recognized class is Latino/Middle Eastern, which is caused by very small number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we presented simple training pipeline <ref type="figure">(Fig. 2</ref>) that leads to the state-of-the-art accuracy of lightweight neural networks in facial expression recognition in images and videos for several datasets. It was shown that, in contrast to existing models, we provide additional robustness to face extraction and alignment, which can be explained by pre-training of facial feature extractor for face identification from very large VGGFace2 dataset. The cropped faces with regions returned by face detectors without adding margins <ref type="figure" target="#fig_0">(Fig. 1b,d)</ref> were used. As a result, we obtained not only high accuracy ( <ref type="table">Table 2,</ref>  <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5)</ref>, but also excellent speed and model size ( <ref type="table" target="#tab_2">Table 3)</ref>. As a result, our models can be used even for fast decision-making in embedded systems, e.g., in mobile applications <ref type="bibr" target="#b5">[6]</ref>. In future, it is necessary to examine more complex classifiers on top of the features extracted by our networks, e.g., by using transformers or frame/channel-level attention <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> instead of support vector machines and random forests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Sample images from LFW dataset: (a), (c) Center cropped; (b), (d) Cropped by MTCNN without margins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Rank-1 accuracy (%) in face identification for LFW dataset.</figDesc><table><row><cell>CNN</cell><cell cols="2">Center crop Cropped by MTCNN without margins</cell></row><row><cell>SENet-50 [9]</cell><cell>97.21± 4.19</cell><cell>96.61± 2.02</cell></row><row><cell cols="2">Our MobileNet-v1 90.80± 3.96</cell><cell>92.60± 4.01</cell></row><row><cell cols="2">Our EfficientNet-B0 92.71± 4.61</cell><cell>94.58± 4.58</cell></row><row><cell cols="2">Our RexNet-150 94.76± 4.45</cell><cell>96.59± 3.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of emotion recognition models.</figDesc><table><row><cell>CNN</cell><cell cols="2">Running time per one image, ms Param count, MB</cell></row><row><cell>VGG-16</cell><cell>224.7</cell><cell>134.3</cell></row><row><cell>ResNet-18</cell><cell>58.7</cell><cell>11.7</cell></row><row><cell>Inception-v3</cell><cell>160.4</cell><cell>19.7</cell></row><row><cell>NFNet-F0</cell><cell>621.1</cell><cell>66.4</cell></row><row><cell>SENet-50</cell><cell>128.4</cell><cell>25.5</cell></row><row><cell>Our MobileNet-v1</cell><cell>40.6</cell><cell>3.2</cell></row><row><cell>Our EfficientNet-B0</cell><cell>54.8</cell><cell>4.3</cell></row><row><cell>Our RexNet-150</cell><cell>104.0</cell><cell>8.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Validation accuracy for AFEW dataset.</figDesc><table><row><cell>Method</cell><cell>Details</cell><cell cols="2">Modality Accuracy, %</cell></row><row><cell>VGG13 + VGG16 + ResNet [4]</cell><cell>Ensemble</cell><cell>video</cell><cell>59.42</cell></row><row><cell>Hu et al. [25]</cell><cell cols="2">Ensemble audio, video</cell><cell>59.01</cell></row><row><cell cols="2">Four face recognition CNNs+STAT+FFT [18] Ensemble</cell><cell>video</cell><cell>56.7</cell></row><row><cell cols="2">Noisy student with iterative training [26] Single model</cell><cell>video</cell><cell>55.17</cell></row><row><cell>Noisy student w/o iterative training [26]</cell><cell>Single model</cell><cell>video</cell><cell>52.49</cell></row><row><cell>DenseNet-161 [27]</cell><cell>Single model</cell><cell>video</cell><cell>51.44</cell></row><row><cell>Face attention network (FAN) [28]</cell><cell>Single model</cell><cell>video</cell><cell>51.18</cell></row><row><cell>LBP-TOP (baseline) [11]</cell><cell>Single model</cell><cell>video</cell><cell>38.90</cell></row><row><cell>Our MobileNet-v1</cell><cell>Single model</cell><cell>video</cell><cell>55.35</cell></row><row><cell>Our EfficientNet-B0</cell><cell>Single model</cell><cell>video</cell><cell>59.27</cell></row><row><cell>Our RexNet-150</cell><cell>Single model</cell><cell>video</cell><cell>57.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Validation accuracy for VGAF dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Age and gender recognition results for UTKFace dataset.</figDesc><table><row><cell>Models</cell><cell cols="3">Gender accuracy, % Age MAE Param count, MB</cell></row><row><cell>FaceNet</cell><cell>89.54</cell><cell>8.58</cell><cell>12.3</cell></row><row><cell>MobileNet v2 (Agegendernet)</cell><cell>91.47</cell><cell>7.29</cell><cell>7.1</cell></row><row><cell>ResNet-50 (InsightFace)</cell><cell>87.52</cell><cell>8.57</cell><cell>60.1</cell></row><row><cell>DEX</cell><cell>91.05</cell><cell>6.48</cell><cell>262.5</cell></row><row><cell>Our MobileNet-v1</cell><cell>93.79</cell><cell>5.74</cell><cell>3.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Accuracy (%) of ethnicity recognition on UTKFace dataset.</figDesc><table><row><cell>Classifier</cell><cell cols="4">VGGFace VGGFace-2 FaceNet Our MobileNet-v1</cell></row><row><cell>Random Forest</cell><cell>83.5</cell><cell>87.8</cell><cell>84.3</cell><cell>83.8</cell></row><row><cell>k-NN</cell><cell>76.2</cell><cell>84.5</cell><cell>84.4</cell><cell>82.2</cell></row><row><cell>LinearSVC</cell><cell>79.5</cell><cell>83.1</cell><cell>85.6</cell><cell>85.6</cell></row><row><cell>New Fully Connected layer</cell><cell>80.4</cell><cell>86.4</cell><cell>84.4</cell><cell>87.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/HSE-asavchenko/face-emotion-recognition</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/dandynaufaldi/Agendernet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/BoyuanJiang/Age-Gender-Estimate-TF</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31">. Cao, W., Mirjalili, V., Raschka, S.: Consistent rank logits for ordinal regression with convolutional neural networks. arXiv preprint arXiv:1901.07884(2019)   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work is supported by RSF (Russian Science Foundation) grant 20-71-10010.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mitigating bias in gender, age and ethnicity classification: a multi-task convolution neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">AffectNet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10798</idno>
		<title level="m">Computational emotion analysis from images: Recent advances and future directions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild from videos using images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction (ICMI)</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction (ICMI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="433" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group level audio-video emotion recognition using hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimodal Interaction (ICMI)</title>
		<meeting>the ACM International Conference on Multimodal Interaction (ICMI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="807" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MobileEmotiFace: Efficient facial image representations in video-based emotion recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Demochkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Savchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPR International Workshops and Challenges, Part V</title>
		<meeting>ICPR International Workshops and Challenges, Part V</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output ConvNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Savchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">197</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Increasingly packing multiple facial-informatics modules in a unified deep-learning model via lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia Retrieval (ICMR)</title>
		<meeting>the International Conference on Multimedia Retrieval (ICMR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="339" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<meeting>International Conference on Automatic Face &amp; Gesture Recognition (FG)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic emotion, engagement and cohesion prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimodal Interaction (ICMI)</title>
		<meeting>the ACM International Conference on Multimodal Interaction (ICMI)</meeting>
		<imprint>
			<publisher>EmotiW</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="546" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic group level affect and cohesion prediction in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos</title>
		<meeting>the 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unconstrained face recognition: Identifying a person of interest from a media collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2144" to="2157" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DEX: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (CVPR) Workshops</title>
		<meeting>the International Conference on Computer Vision (CVPR) Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00992</idno>
		<title level="m">RexNet: Diminishing representational bottleneck on convolutional neural network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01412</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shvetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuharenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04598</idno>
		<title level="m">Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid with super resolution for in-the-wild facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="131988" to="132001" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to amend facial expression representation via de-albino and affinity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10189</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4057" to="4069" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facial expression recognition in the wild via deep attentive center loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Farzaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2402" to="2411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06562</idno>
		<title level="m">Compacting, picking and growing for unforgetting continual learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning supervised scoring ensemble for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction (ICMI)</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction (ICMI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Noisy student training using body language dataset improves facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="756" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-feature based emotion recognition for video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction (ICMI)</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction (ICMI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="630" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frame attention networks for facial expression recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing (ICIP)</title>
		<meeting>the International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3866" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Implicit knowledge injectable cross attention audiovisual model for group emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heracleous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kurihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimodal Interaction (ICMI)</title>
		<meeting>the ACM International Conference on Multimodal Interaction (ICMI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="827" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Audiovisual classification of group emotion valence using activity recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sanhudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Image Processing, Applications and Systems (IPAS)</title>
		<meeting>the 4th International Conference on Image Processing, Applications and Systems (IPAS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
