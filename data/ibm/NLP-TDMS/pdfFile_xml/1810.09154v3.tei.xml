<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhe</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Aberdeen</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
							<email>c.lin@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Collinson</surname></persName>
							<email>matthew.collinson@abdn.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Aberdeen</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
							<email>x.li@abdn.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Aberdeen</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyi</forename><surname>Chen</surname></persName>
							<email>g.chen@uu.nl</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognising dialogue acts (DA) is important for many natural language processing tasks such as dialogue generation and intention recognition. In this paper, we propose a dualattention hierarchical recurrent neural network for DA classification. Our model is partially inspired by the observation that conversational utterances are normally associated with both a DA and a topic, where the former captures the social act and the latter describes the subject matter. However, such a dependency between DAs and topics has not been utilised by most existing systems for DA classification. With a novel dual task-specific attention mechanism, our model is able, for utterances, to capture information about both DAs and topics, as well as information about the interactions between them. Experimental results show that by modelling topic as an auxiliary task, our model can significantly improve DA classification, yielding better or comparable performance to the state-of-the-art method on three public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue Acts (DA) are semantic labels of utterances, which are crucial to understanding communication: much of a speaker's intent is expressed, explicitly or implicitly, via social actions (e.g., questions or requests) associated with utterances <ref type="bibr" target="#b26">(Searle, 1969)</ref>. Recognising DA labels is important for many natural language processing tasks. For instance, in dialogue systems, knowing the DA label of an utterance supports its interpretation as well as the generation of an appropriate response <ref type="bibr" target="#b26">(Searle, 1969;</ref><ref type="bibr" target="#b4">Chen et al., 2018)</ref>. In the security domain, being able to detect intention in conversational texts can effectively support the recognition of sensitive information exchanged in emails or other communication channels, which is critical to timely security intervention <ref type="bibr" target="#b29">(Verma et al., 2012)</ref>.</p><p>A wide range of techniques have been investigated for DA classification. Early works on DA classification are mostly based on general machine learning techniques, framing the problem either as multi-class classification (e.g., using SVMs <ref type="bibr" target="#b18">(Liu, 2006)</ref> and dynamic Bayesian networks <ref type="bibr" target="#b6">(Dielmann and Renals, 2008)</ref>) or a structured prediction task (e.g., using Conditional Random Fields <ref type="bibr" target="#b12">(Kim et al., 2010;</ref><ref type="bibr" target="#b4">Chen et al., 2018;</ref><ref type="bibr">Raheja and Tetreault, 2019, CRF)</ref>). Recent studies to the problem of DA classification have seen an increasing uptake of deep learning techniques, where promising results have been obtained. Deep learning approaches typically model the dependency between adjacent utterances <ref type="bibr" target="#b15">Lee and Dernoncourt, 2016)</ref>. Some researchers further account for dependencies among both consecutive utterances and consecutive DAs, i.e., both are considered factors that influence natural dialogue <ref type="bibr" target="#b14">(Kumar et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2018)</ref>. There is also work exploring different deep learning architectures (e.g., hierarchical CNN or RNN/LSTM) for incorporating context information for DA classification <ref type="bibr" target="#b19">(Liu et al., 2017)</ref>.</p><p>It has been observed that conversational utterances are normally associated with both a DA and a topic, where the former captures the social act (e.g., promising) and the latter describes the subject matter <ref type="bibr" target="#b31">(Wallace et al., 2013)</ref>. It is also recognised that the types of DA associated with a conversation are likely to be influenced by the topic of the conversation <ref type="bibr" target="#b26">(Searle, 1969;</ref><ref type="bibr" target="#b31">Wallace et al., 2013)</ref>. For instance, conversations relating to topics about customer service might be more frequently associated with DAs of type Wh-question (e.g., Why my mobile is not working?) and a complaining statement <ref type="bibr" target="#b1">(Bhuiyan et al., 2018)</ref>; whereas meetings covering administrative topics about resource allocation are likely to exhibit significantly more defending statements and floor grabbers (e.g., Well I mean -is the handheld really any better?) <ref type="bibr" target="#b32">(Wrede and Shriberg, 2003)</ref>. However, such a reasonable source of information, surprisingly, has not been explored in the deep learning literature for DA classification. We assume that modelling the topics of utterances as additional contextual information may effectively support DA classification.</p><p>In this paper, we propose a dual-attention hierarchical recurrent neural network with a CRF (DAH-CRF) for DA classification. Our model is able to account for rich context information with the developed dual-attention mechanism, which, in addition to accounting for the dependencies between utterances, can further capture, for utterances, information about both topics and DAs. Topic is a useful source of context information which has not previously been explored in existing deep learning models for DA classification. Second, compared to the flat structure employed by existing models <ref type="bibr" target="#b11">(Khanpour et al., 2016;</ref><ref type="bibr" target="#b15">Ji et al., 2016)</ref>, our hierarchical recurrent neural network can represent the input at the character, word, utterance, and conversation levels, preserving the natural hierarchical structure of a conversation. To capture the topic information of conversations, we propose a simple automatic utterance-level topic labelling mechanism based on LDA <ref type="bibr" target="#b2">(Blei et al., 2003)</ref>, which avoids expensive human annotation and improves the generalisability of our model. We evaluate our model against several strong baselines <ref type="bibr" target="#b31">(Wallace et al., 2013;</ref><ref type="bibr" target="#b15">Ji et al., 2016;</ref><ref type="bibr" target="#b14">Kumar et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2018;</ref><ref type="bibr" target="#b23">Raheja and Tetreault, 2019)</ref> on the task of DA classification. Extensive experiments conducted on three public datasets (i.e., Switchboard Dialog Act Corpus (SWDA), DailyDialog (DyDA), and the Meeting Recorder Dialogue Act corpus (MRDA)) show that by modelling the topic information of utterances as an auxiliary task, our model can significantly improve DA classification for all datasets compared to a base model without modelling topic information. Our model also yields better or comparable performance to state-of-the-art deep learning method <ref type="bibr" target="#b23">(Raheja and Tetreault, 2019)</ref> in classification accuracy.</p><p>To summarise, the contributions of our paper are three-fold: (1) we propose to leverage topic information of utterances, a useful source of con-textual information which has not previously been explored in existing deep learning models for DA classification; (2) we propose a dual-attention hierarchical recurrent neural network with a CRF which respects the natural hierarchical structure of a conversation, and is able to incorporate rich context information for DA classification, achieving better or comparable performance to the stateof-the-art; (3) we develop a simple topic labelling mechanism, showing that using the automatically acquired topic information for utterances can effectively improve DA classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Broadly speaking, methods for DA classification can be divided into two categories: multiclass classification (e.g., SVMs <ref type="bibr" target="#b18">(Liu, 2006)</ref> and dynamic Bayesian networks <ref type="bibr" target="#b6">(Dielmann and Renals, 2008)</ref>) and structured prediction tasks including HMM <ref type="bibr" target="#b28">(Stolcke et al., 2000)</ref> and <ref type="bibr">CRF (Kim et al., 2010)</ref>. Recently, deep learning has been widely applied in many NLP tasks, including DA classification. <ref type="bibr" target="#b10">Kalchbrenner and Blunsom (2013)</ref> proposed to model a DA sequence with a RNN where sentence representations were constructed by means of a convolutional neural network (CNN). <ref type="bibr" target="#b15">Lee and Dernoncourt (2016)</ref> tackled DA classification with a model built upon RNNs and CNNs. Specifically, their model can leverage the information of preceding texts, which can effectively help improve the DA classification accuracy. A latent variable recurrent neural network was developed for jointly modelling sequences of words and discourse relations between adjacent sentences . In their work, the shallow discourse structure is represented as a latent variable and the contextual information from preceding utterances are modelled with a RNN. <ref type="bibr" target="#b14">Kumar et al. (2018)</ref> proposed a hierarchical Bi-LSTM model with a CRF for DA classification, where the inter-utterance and intra-utterance information are encoded by a hierarchical Bi-LSTM and the dependency between DA labels is captured by a CRF. <ref type="bibr" target="#b4">Chen et al. (2018)</ref> developed a CRF-Attentive Structured Network (CRF-ASN) for DA classification. They applied structured attention network to the CRF layer in order to model contextual utterances and corresponding DAs together. <ref type="bibr" target="#b23">Raheja and Tetreault (2019)</ref> achieved the state-of-the-art performance on the SWDA dataset by employing a self-attention mechanism, a CRF layer and character-level embeddings.</p><p>In addition to modelling dependency between utterances, various contexts have also been explored for improving DA classification or joint modelling DA under multi-task learning. For instance, <ref type="bibr" target="#b31">Wallace et al. (2013)</ref> proposed a generative joint sequential model to classify both DA and topics of patient-doctor conversations. Their model is similar to the factorial LDA model <ref type="bibr" target="#b21">(Paul and Dredze, 2012)</ref>, which generalises LDA to assign each token a K-dimensional vector of latent variables. We would like to emphasise that the model of <ref type="bibr" target="#b31">Wallace et al. (2013)</ref>, only assumed that each utterance is generated conditioned on the previous and current topic/DA pairs. In contrast, our model is able to model the dependencies of all preceding utterances of a conversation, and hence can better capture the effect between DAs and topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a training corpus</head><formula xml:id="formula_0">D = (C n , Y n , Z n ) N n=1 , where C n = u n t T t=1 is a conversation contain- ing a sequence of T utterances, Y n = y n t T t=1 and Z n = z n t T t=1</formula><p>are the corresponding labels of DA and topics for C n , respectively. Each utterance</p><formula xml:id="formula_1">u t = w i t K i=1</formula><p>of C n is a sequence of K words. Our goal is to learn a model from D, such that, given an unseen conversation C u , the model can predict the DA labels of the utterances of C u . <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of the proposed Dual-Attention Hierarchical recurrent neural network with a CRF (DAH-CRF). A shared utterance encoder encodes each word w i t of an utterance u t into a vector h i t . The DA attention and topic attention mechanisms capture DA and topic information as well as the interactions between them. The outputs of the dual-attention are then encoded in the conversation-level sequence taggers (i.e., g t and s t ), based on the corresponding utterance representations (i.e., l t and v t ). Finally, the target labels (i.e., y t and z t ) are predicted in the CRF layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shared Utterance Encoder</head><p>In our model, we adopt a shared utterance encoder to encode the input utterances. Such a design is based on the rationale that the shared encoder can transfer parameters between two tasks and reduce the risk of overfitting <ref type="bibr" target="#b25">(Ruder, 2017)</ref>. Specifically, the shared utterance encoder is implemented using the bidirectional gated recurrent unit <ref type="bibr">(Cho et al., 2014, BiGRU)</ref>, which encodes each utter-</p><formula xml:id="formula_2">ance u t = w i t K i=1</formula><p>of a conversation C n as a series of hidden states h i t K i=1 . Here, i indicates the timestamp of a sequence, and we define h i t as follows</p><formula xml:id="formula_3">h i t = − → h i t ⊕ ← − h i t (1)</formula><p>where ⊕ is an operation for concatenating two vectors, and − → h i t and ← − h i t are the i-th hidden state of the forward gated recurrent unit <ref type="bibr">(Cho et al., 2014, GRU)</ref> and backward GRU for w i t , respectively. Formally, the forward GRU</p><formula xml:id="formula_4">− → h i t is com- puted as follows − → h i t = GRU( − → h i−1 t , e i t )<label>(2)</label></formula><p>where e i t is the concatenation of the word embedding and the character embedding of word w i t . Finally, the backward GRU encodes u t from the reverse direction</p><formula xml:id="formula_5">(i.e. w K t → w 1 t ) and generates ← − h i t K i=1</formula><p>following the same formulation as the forward GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task-specific Attention</head><p>Recall that one of the key challenges of our model is to capture for each utterance, information about both DAs and topics, as well as information about the interactions between them. We address this challenge by incorporating into our model a novel task-specific dual-attention mechanism, which accounts for both DA and topic information extracted from utterances. In addition, DAs and topics are semantically relevant to different words in an utterance. With the proposed attention mechanism, our model can also assign different weights to the words of an utterance by learning the degree of importance of the words to the DA or topic labelling task, i.e., promoting the words which are important to the task and reducing the noise introduced by less important words.</p><p>For each utterance u t , the DA attention calculates a weight vector</p><formula xml:id="formula_6">α i t K i=1 for h i t K i=1</formula><p>, the hidden states of u t . u t can then be represented as an attention vector l t computed as follows</p><formula xml:id="formula_7">l t = K i=1 α i t h i t<label>(3)</label></formula><p>In contrast to the traditional attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, which only depends on one set of hidden vectors from the Seq2Seq decoder, the DA attention of our model relies on two sets of hidden vectors, i.e., g t−1 of the conversation-level DA tagger and s t−1 of the conversation-level topic tagger, where dual attention mechanism can capture, for utterances, information about both DAs and topics as well as the interaction between them. Specifically, the weights</p><formula xml:id="formula_8">α i t K i=1</formula><p>for the DA attention are calculated as follows:</p><formula xml:id="formula_9">α i t = softmax(o i t ) (4) o i t = w a tanh W (act) (s t−1 ⊕ g t−1 ⊕ h i t ) + b (act)<label>(5)</label></formula><p>The topic attention layer has a similar architecture to the DA attention layer, which takes as input both s t−1 and g t−1 . The weight vector β i</p><formula xml:id="formula_10">t K i=1</formula><p>for the topic attention output v t can be calculated similar to Eq. 3 and Eq. 4. Note that w a , W (act) , and b (act) are vectors of parameters that need to be learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conversational Sequence Tagger</head><p>CRF sequence tagger for DA. The conversational CRF sequence tagger for DA predicts the next DA y t conditioned on the conversational hidden state g t and adjacent DAs (c.f. <ref type="figure" target="#fig_0">Figure 1)</ref>. Formally, this conditional probability of the whole conversation can be formulated as</p><formula xml:id="formula_11">p (y 1:T |C; θ) = T t=1 Ψ (y t−1 , y t , g t ; θ) Y T t=1 Ψ (y t−1 , y t , g t ; θ) (6) Ψ (y t−1 , y t , g t ; θ) = Ψ emi (y t , g t ) Ψ tran (y t−1 , y t ) = g t [y t ] P yt,y t−1<label>(7)</label></formula><p>Here the feature function Ψ(·) includes two score potentials: emission and transition. The emission potential Ψ emi regards utterance representation g t as the unary feature. The transition potential Ψ tran is a pairwise feature constructed from a T ×T state transition matrix P, where T is the number of DA classes, and P yt,y t−1 is the probability of transiting from state y t−1 to y t . C = u t T t=1 is the sequence of all utterances seen so far, θ is the parameters of the CRF layer. g t is calculated in a BiGRU similar to Eq. 1 and Eq. 2:</p><formula xml:id="formula_12">g t = − → g t ⊕ ← − g t (8) − → g t = GRU( − → g t−1 , l t )<label>(9)</label></formula><p>CRF sequence tagger for topic. The conversational CRF sequence tagger for topic is designed to predict topic z t conditioned on v t and adjacent topics, which can be calculated similar to the formulation of the CRF tagger for DA.</p><p>Training the model. Let Θ be all the model parameters that need to be estimated for DAH-CRF. Θ then is estimated based on D = (C n , Y n , Z n ) N n=1 (i.e., a corpus with N conversations) by maximising the following objective function</p><formula xml:id="formula_13">L = N n=1</formula><p>[log (p (y n 1:T |C n ; Θ)) +α log (p (z n 1:T |C n ; Θ))] (10)</p><p>The hyper-parameter α controls the contribution of the conversational topic tagger towards the objective function. In our experiments, α = 0.5 is determined using the validation datasets. During the test, the optimal DA or topic sequence is calculated using the Viterbi algorithm <ref type="bibr" target="#b30">(Viterbi, 1967)</ref>.</p><formula xml:id="formula_14">Y = arg max y 1:T ∈Y p(y 1:T |C, Θ)<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Automatically Acquiring Topic Labels</head><p>To avoid expensive human annotation and to improve the generalisability of our model, we propose to label the topic of each utterance of the datasets using LDA <ref type="bibr" target="#b2">(Blei et al., 2003)</ref>. While perplexity has been widely used for model selection for LDA <ref type="bibr" target="#b17">(Lin, 2011;</ref><ref type="bibr" target="#b7">He et al., 2012)</ref>, we employ a topic coherence measure proposed by <ref type="bibr" target="#b24">(Röder et al., 2015)</ref> to determine the optimal topic number for each dataset, which combines the indirect cosine measure with the normalised pointwise mutual information <ref type="bibr">(Bouma, 2009, NPMI)</ref> and the Boolean sliding window. Empirically, we found the latter yields much better topic clusters than perplexity for supporting DA classification. We treat each conversation as a document and train topic models using Gensim with topic number settings ranging from 10 to 100 (using an increment step of 10). Gibbs sampling is used to estimate the model posterior and for each model we run 1,000 iterations. For each trained model, we calculate the averaged coherence score of the extracted topics using Gensim 1 , an implementation following <ref type="bibr" target="#b24">(Röder et al., 2015)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows the topic coherence score for each topic number setting for all datasets, from which we determine that the optimal topic number setting for SWDA, DyDA, and MRDA are 60, 30, and 30, respectively.</p><p>Based on the optimal models (i.e., a trained LDA model using the optimal topic number setting), we assign topic labels to the datasets with two different strategies, i.e., conversation-level labelling (conv) and utterance-level labelling (utt).  For conversation-level labelling, we assign the topic label with the highest marginal probability to the conversation based on the corresponding per-document topic proportion estimated by LDA. Every utterance of the conversation then shares the same topic label of the conversation.</p><p>For utterance-level labelling, there is an additional step to perform inference on every utterance based on corresponding optimal model (e.g., for every utterance of SWDA, we do inference using the LDA trained on SWDA with 60 topics), and assign the topic label with the highest marginal probability to the utterance. Therefore, the topic labels of the utterances of the same conversation could be different for utterance-level labelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the performance of our model on three public DA datasets with different characteristics, namely, Switchboard Dialog Act Corpus <ref type="bibr">(Jurafsky, 1997, SWDA)</ref>, Dailydialog <ref type="bibr">(Li et al., 2017, DyDA)</ref>, and the Meeting Recorder Dialogue Act corpus <ref type="bibr">(Shriberg et al., 2004, MRDA)</ref>. SWDA 2 consists of 1,155 two-sided telephone conversations manually labelled with 66 conversation-level topics (e.g., taxes, music, etc.) and 42 utterance-level DAs (e.g., statementopinion, statement-non-opinion, wh-question). DyDA 3 contains 13,118 human-written daily conversations, manually labelled with 10 conversation-level topics (e.g., tourism, politics, finance) as well as four utterance-level DA classes, i.e., inform, question, directive and commissive. The former two classes are information transfer acts, while the latter two are action discussion acts. MRDA 4 contains 75 meeting conversations anno-tated with 5 DAs, i.e., Statement (S), Question (Q), Floorgrabber (F), Backchannel (B), and Disruption (D). The average number of utterances per conversation is 1,496. There are no manually annotated topic labels available for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For all experimental datasets, the top 85% highest frequency words were indexed. For SWDA and MRDA, we split training/validation/testing datasets following <ref type="bibr" target="#b28">(Stolcke et al., 2000;</ref><ref type="bibr" target="#b15">Lee and Dernoncourt, 2016)</ref>. For DyDA, we used the standard split from the original dataset <ref type="bibr" target="#b16">(Li et al., 2017)</ref>. The statistics of the experimental datasets are summarised in <ref type="table" target="#tab_1">Table 1</ref>. We represented input data with 300-dimensional Glove word embeddings <ref type="bibr" target="#b22">(Pennington et al., 2014)</ref> and 50-dimensional character embeddings <ref type="bibr" target="#b20">(Ma and Hovy, 2016)</ref>. We set the dimension of the hidden layers (i.e., h i t , g t and s t ) to 256 and applied a dropout layer to both the shared encoder and the sequence tagger at a rate of 0.2. The Adam optimiser (Kingma and Ba, 2015) was used for training with an initial learning rate of 0.001 and a weight decay of 0.0001. Each utterance in a minibatch was padded to the maximum length for that batch, and the maximum batch-size allowed was 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare the proposed DAH-CRF model incorporating utterance-level topic labels extracted by LDA (denoted as DAH-CRF+LDA utt ) against five strong baselines and two variants of our own models: JAS 5 : A generative joint, additive, sequential model of topics and speech acts in patient-doctor communication <ref type="figure" target="#fig_0">(Wallace et al., 2013)</ref>; DRLM-Cond 6 : A latent variable recurrent neural network for DA classification ; Bi-LSTM-CRF 7 : A hierarchical Bi-LSTM with a CRF to classify DAs <ref type="bibr" target="#b14">(Kumar et al., 2018)</ref>; CRF-ASN: An attentive structured network with a CRF for DA classification <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>; SelfAtt-CRF: A hierarchical Bi-GRU with selfattention and CRF <ref type="bibr" target="#b23">(Raheja and Tetreault, 2019)</ref>; DAH-CRF+MANUAL conv : Use the manually annotated conversation-level topic labels (i.e., each utterance of the conversation shares the same 5 https://github.com/bwallace/JAS 6 https://github.com/jiyfeng/drlm 7 https://github.com/YanWenqiang/HBLSTM-CRF   <ref type="table" target="#tab_3">Table 2</ref> shows the DA classification accuracy of our models and the baselines on three experimental datasets. We fine-tuned the model parameters for JAS, DRLM-Cond and Bi-LSTM-CRF in order to make the comparison as fair as possible. The implementation of CRF-ASN and SelfAtt-CRF are not available so we can only report their results for SWDA and MRDA based on the original papers <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b23">Raheja and Tetreault, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dialogue Acts Classification</head><p>It can be observed that by jointly modelling DA and topics, DAH-CRF+LDA utt outperforms the two best baseline models SelfAtt-CRF and CRF-ASN around 1% on the MRDA dataset. Our model also gives similar performance to SelfAtt-CRF, the baseline which achieved the state-ofthe-art performance on the SWDA dataset (i.e., 82.3% vs. 82.9%). While both manually annotated and automatically acquired topic labels are effective, we see that DAH-CRF+LDA utt outperforms both DAH-CRF+MANUAL conv and DAH-CRF+LDA conv , i.e., with over 1.6% gain on DyDA and over 1.4% on SWDA (significant; paired t-test p &lt; .01). It is also ob-  served that DAH-CRF+MANUAL conv and DAH-CRF+LDA conv perform very similar to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study Results</head><p>We conducted ablation studies (see <ref type="table" target="#tab_5">Table 3</ref>) in order to evaluate the contribution of the components of our DAH-CRF+LDA utt model, and more importantly, the effectiveness of leveraging topic information for supporting DA classification. DAH-CRF+LDA utt (without Dual-Att) removes the dual-attention component from DAH-CRF+LDA utt , and DAH+LDA utt removes the CRF from DAH-CRF+LDA utt but retaining the dual-attention component. SAH is a Single-Attention Hierarchical RNN model without a CRF, i.e., a simplified version of DAH+LDA utt that only models DAs with topical information omitted. As can be seen in <ref type="table" target="#tab_5">Table 3</ref>, DAH+LDA utt achieves over 3% averaged gain on all datasets when compared to SAH, which clearly shows that leveraging topic information can effectively support DA classification. It is also observed that both the dual-attention mechanism and the CRF component are beneficial, but are more effective on the SWDA and DyDA datasets than MRDA.</p><p>In summary, while all the analysed model components are beneficial, the biggest gain is obtained by jointly modelling DAs and topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysing the Effectiveness of Joint</head><p>Modelling Dialogue Act and Topic</p><p>In this section, we provide detailed analysis on why DAH-CRF+LDA utt can yield better performance than SAH-CRF by jointly modelling DAs and topics. Due to the page limit, our discussion focuses on SWDA and DyDA datasets. <ref type="figure" target="#fig_3">Figure 4</ref> shows the normalized confusion matrix derived from 10 DA classes of SWDA for both SAH-CRF and DAH-CRF+LDA utt models. It can be observed that DAH-CRF+LDA utt yields improvement on recall for many DA classes compared to SAH-CRF, e.g., 23.8% improvement on bk and 11.7% on sv. For bk (Response Acknowledge) which has the highest improvement level, we see that the improvement largely comes from the reduction of misclassifing bk to b (Acknowledge Backchannel). The key difference between bk and b is that an utterance labelled with bk has to be produced within a question-answer context, whereas b is a "continuer" simply representing a response to the speaker <ref type="bibr" target="#b9">(Jurafsky, 1997)</ref>. It is not surprising that SAH-CRF makes poor prediction on the utterances of these two DAs: they share many syntactic cues, e.g., indicator words such 'okay', 'oh', and 'uh-huh', which can easily confuse the model. When comparing the topic distribution of the utterances under the bk and b categories (cf. <ref type="figure" target="#fig_2">Figure 3)</ref>, we found topics relating to personal leisure (e.g., buying cars, music, and exercise) are much more prominent in bk than b. By leveraging the topic information, DAH-CRF+LDA utt can better handle the confusion cases and hence improve the prediction for bk significantly.</p><p>There are also cases where DAH-CRF+LDA utt performs worse than SAH-CRF. Take the DA pair of qo (Open Question) and qw (wh-questions) as an example. qo refers to questions like 'How about you?' and its variations (e.g., 'What do you think?'), whereas qw represents wh-questions which are much  more specific in general (e.g. 'What other long range goals do you have?'). SAH-CRF gives quite decent performance in distinguishing qw and qo classes. This is somewhat reasonable, as linguistically the utterances of these two classes are quite different, i.e., the qw utterance expresses very specific question and is relatively lengthy, whereas qo utterances tends to be very brief. We see that DAH-CRF+LDA utt performs worse than SAH-CRF: a greater number of qw utterances are misclassified by DAH-CRF+LDA utt as qo. This might be attributed to the fact that topic distributions of qw and qo are similar to each other (see <ref type="figure" target="#fig_2">Figure 3</ref>), i.e., incorporating the topic information into DAH-CRF may cause these two DAs to be less distinguishable for the model.</p><p>We also conducted a similar analysis on the DyDA dataset.</p><p>As can be seen from the confusion matrices shown in <ref type="figure" target="#fig_3">Figure 4</ref>, DAH-CRF+LDA utt gives improvement over SAH-CRF for all the four DA classes of DyDA. In particular, Directives and Commissive achieve higher improvement margin compared to the other two classes, where the improvement are largely attributed to less number of instances of the Directives and Commissive classes being mis-classified into Inform and Questions. Examining the topic distributions in <ref type="figure" target="#fig_2">Figure 3</ref> reveals that Directives and Commissive classes are more relevant to the topics such as food, shopping, and credit card. In contrast, the topics of Inform and Questions classes are more about business, and weather.</p><p>Finally, <ref type="figure" target="#fig_4">Figure 5</ref> shows the DA attention visualisation examples of SAH-CRF and DAH-CRF+LDA utt for an utterance from SWDA and DyDA. For SWDA, it can be seen that SAH-CRF gives very high weight to the word "because" and de-emphasizes other words. However, DAH-CRF+LDA utt can capture more important words (e.g., "if", "reasonable", etc.) and correctly predicts the DA label as sd. For DyDA, SAH-CRF only focuses on "me" and "your", but DAH-CRF+LDA utt captures more words relevant to Directive, such as "please", "tell", etc. To summarise, DAH-CRF+LDA utt can capture more significant words related to the corresponding DA, by modelling both DAs and topic information with the dual-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we developed a dual-attention hierarchical recurrent neural network with a CRF for DA classification. With the proposed taskspecific dual-attention mechanism, our model is able to capture information about both DAs and topics, as well as information about the interactions between them. Moreover, our model is generalised by leveraging an unsupervised model to automatically acquire topic labels. Experimental results based on three public datasets show that modelling utterance-level topic information as an auxiliary task can effectively improve DA classification, and that our model is able to achieve better or comparable performance to the state-of-the-art deep learning methods for DA classification.</p><p>We envisage that our idea of modelling topic information for improving DA classification can be adapted to other DNN models, e.g., to encode topic labels into word embeddings and then concatenate with the utterance-level or conversationlevel hidden vectors of our baselines, e.g. SelfAtt-CRF. It will also be interesting to explicitly take into account speaker's role in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the dual-attention hierarchical recurrent neural network with a CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Coherence score of LDA on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>We highlight the prominent topics for some example DAs. The topic distribution of a topic k under a DA label d is calculated by averaging the marginal probability of topic k for all utterances with the DA label d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The normalized confusion matrix of DAs using SAH-CRF (left) and DAH-CRF+LDA utt (right) on SWDA (a) and DyDA (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>DA Attention visualisation using SAH-CRF and DAH-CRF+LDA utt on (a) SWDA and (b) DyDA datasets. The true labels of the utterances above are sd (statement-non-opinion) and Directive, respectively. SAH-CRF misclassified the DA as sv (statement-opinion) and Inform whereas DAH-CRF+LDA utt gives correct prediction for both cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>|C| is the number of DA classes, |T | is the number of manually labelled conversation-level topic classes, |V | is the vocabulary size. Training, Validation and Testing indicate the number of conversations/utterances in the respective splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>DA classification accuracy. † indicates the results which are reported from the prior publications.</figDesc><table><row><cell>topic) for DAH-CRF model training rather than</cell></row><row><cell>the topic labels automatically acquired from LDA;</cell></row><row><cell>DAH-CRF+LDA conv : Use conversation-level</cell></row><row><cell>topic labels automatically acquired from LDA for</cell></row><row><cell>DAH-CRF model training.</cell></row><row><cell>Note that only JAS (a non-deep-learning model)</cell></row><row><cell>has attempted to model both DAs and topics,</cell></row><row><cell>whereas all the deep learning baselines do not</cell></row><row><cell>model topic information as a source of context</cell></row><row><cell>for DA classification. All the baselines mentioned</cell></row><row><cell>above use the same test dataset as our models for</cell></row><row><cell>all experimental datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of DA classification.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://radimrehurek.com/gensim/models/ coherencemodel.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://web.stanford.edu/ ∼ jurafsky/ws97/manual. august1.html 3 http://yanran.li/dailydialog 4 http://www1.icsi.berkeley.edu/ ∼ ees/dadb/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is supported by the award made by the UK Engineering and Physical Sciences Research Council (Grant number: EP/P011829/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansurul</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jalal</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Akkiraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICWSM workshop on Chatbots</title>
		<meeting>of ICWSM workshop on Chatbots</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normalized (pointwise) mutual information in collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GSCL</title>
		<meeting>GSCL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dialogue act recognition via crf-attentive structured network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheqian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognition of dialogue acts in multiparty meetings using a switching DBN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Dielmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1303" to="1314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online sentiment and topic dynamics tracking over the streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><forename type="middle">Elizabeth</forename><surname>Cano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Confernece on Social Computing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="258" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent variable recurrent neural network for discourse-driven language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="332" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Switchboard swbd-damsl shallow-discourse-function annotation coders manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>Institute of Cognitive Science Technical Report</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dialogue act classification in domain-independent conversations using a deep recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Khanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishitha</forename><surname>Guntakandla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2012" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classifying dialogue acts in one-on-one live chats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="862" to="871" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dialogue Act Sequence Labeling Using Hierarchical Encoder With CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachindra</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="515" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="986" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probabilistic topic models for sentiment analysis on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>University of Exeter</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using SVM and error-correcting codes for multiclass dialog act classification in meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using Context Information for Dialog Act Classification in DNN Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2170" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorial LDA: Sparse multi-dimensional text models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2582" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dialogue act classification with context-aware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Raheja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3727" to="3733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM international conference on Web search and data mining</title>
		<meeting>the eighth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Speech acts: An essay in the philosophy of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John R Searle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">626</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The icsi meeting recorder dialog act (mrda) corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Carvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue at HLT-NAACL 2004</title>
		<meeting>the 5th SIGdial Workshop on Discourse and Dialogue at HLT-NAACL 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting phishing emails the natural language way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narasimha</forename><surname>Shashidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Research in Computer Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="824" to="841" />
		</imprint>
	</monogr>
	<note>and Nabil Hossain</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A generative joint, additive, sequential model of topics and speech acts in patient-doctor communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barton</forename><surname>Trikalinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><forename type="middle">B</forename><surname>Laws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1765" to="1775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relationship between dialogue acts and hot spots in meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Wrede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<biblScope unit="issue">03EX721</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
