<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CERN: Confidence-Energy Recurrent Network for Group Activity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianmin</forename><surname>Shu</surname></persName>
							<email>tianmin.shu@ucla.edusinisa@onid.orst.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<email>sczhu@stat.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CERN: Confidence-Energy Recurrent Network for Group Activity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is about recognizing human activities occurring in videos at distinct semantic levels, including individual actions, interactions, and group activities. The recognition is realized using a two-level hierarchy of Long Short-Term Memory (LSTM) networks, forming a feed-forward deep architecture, which can be trained end-to-end. In comparison with existing architectures of LSTMs, we make two key contributions giving the name to our approach as Confidence-Energy Recurrent Network -CERN. First, instead of using the common softmax layer for prediction, we specify a novel energy layer (EL) for estimating the energy of our predictions. Second, rather than finding the common minimum-energy class assignment, which may be numerically unstable under uncertainty, we specify that the EL additionally computes the p-values of the solutions, and in this way estimates the most confident energy minimum. The evaluation on the Collective Activity and Volleyball datasets demonstrates: (i) advantages of our two contributions relative to the common softmax and energy-minimization formulations and (ii) a superior performance relative to the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper addresses activity recognition in videos, each showing a group activity or event (e.g., spiking in volleyball) arising as a whole from a number of individual actions (e.g., jumping) and human interactions (e.g., passing the ball). Our goal is to recognize events, interactions, and individual actions, for settings where training examples of all these classes are annotated. When ground truth annotations of interactions are not provided in training data, we only pursue recognition of events and actions.</p><p>Recent deep architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, representing a multilevel cascade of Long Short-Term Memory (LSTM) networks <ref type="bibr" target="#b12">[13]</ref>, have shown great promise in recognizing video events. In these approaches, the LSTMs at the bottom layer are grounded onto individual human trajectories, initially obtained from tracking. These LSTMs are aimed at extracting deep visual representations and predicting individual actions of the respective human trajectories. Outputs of the bottom LSTMs are forwarded to a higher-level LSTM for predicting events. All predictions are made in a feed-forward way using the softmax layer at each LSTM. Such a hierarchy of LSTMs is trained end-to-end using backpropagation-through-time of the cross-entropy loss. Motivated by the success of these approaches, we start off with a similar two-level hierarchy of LSTMs for recognizing individual actions, interactions, and events. We extend this hierarchy for producing more reliable and accurate predictions in the face of the uncertainty of the visual input.</p><p>Ideally, the aforementioned cascade should be learned to overcome uncertainty in a given domain (e.g., occlusion, dynamic background clutter). However, our empirical evaluation suggests that existing benchmark datasets (e.g., the Collective Activity dataset <ref type="bibr" target="#b5">[6]</ref> and the Volleyball dataset <ref type="bibr" target="#b13">[14]</ref>) are relatively too small for a robust training of all LSTMs in the cascade. Hence, in cases that have not been seen in the training data, we observe that the feedforwarding of predictions is typically too brittle, as errors made at the bottom level are directly propagated to the higher level. One way to address this challenge is to augment the training set. But it may not be practical as collecting and annotating group activities is usually difficult.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we take another two-pronged strategy toward more robust activity recognition that includes:</p><p>1. Minimizing energy of all our predictions at the different semantic levels considered, and 2. Maximizing confidence (reliability) of the predictions. Hence the name of our approach -Confidence-Energy Recurrent Network (CERN).</p><p>Our first contribution is aimed at mitigating the brittleness of the direct cascading of predictions in previous work. We specify an energy function for capturing dependencies between all LSTM predictions within CERN, and in this way enable recognition by energy minimization. Specifically, we extend the aforementioned two-layer hierarchy of LSTMs with an additional energy layer (EL) for estimating the energy of our predictions. The EL replaces the common softmax layer at the output of LSTMs. Importantly, this extension allows for a robust, energy-based, and end-to-end training of the EL layer on top of all LSTMs in CERN.</p><p>Our second contribution is aimed at improving the numerical stability of CERN's predictions under perturbations in the input, and resolving ambiguous cases with multiple similar-valued local minima. Instead of directly minimizing the energy, we consider more reliable solutions, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The reliability or confidence of solutions is formalized using the classical tool of a statistical hypothesis test <ref type="bibr" target="#b10">[11]</ref> -namely, p-values of the corresponding LSTM's hypotheses (i.e., class predictions). Thus, we seek more confident solutions by regularizing energy minimization with constraints on the p-values. This effectively amounts to a joint maximization of confidence and minimization of energy of CERN outputs. Therefore, we specify the EL to estimate the minimum energy with certain confidence constraints, rather than just the energy.</p><p>We also use the energy regularized by p-values for robust deep learning. Specifically, we formulate an energy-based loss which not only accounts for the energy but also the pvalues of CERN predictions on the training data.</p><p>Our evaluation on the Collective Activity <ref type="bibr" target="#b5">[6]</ref> and Volleyball <ref type="bibr" target="#b13">[14]</ref> datasets demonstrates: (i) advantages of the above contributions compared with the common softmax and energy-based formulations and (ii) a superior performance relative to the state-of-the-art methods.</p><p>In the following, Sec. 2 reviews prior work, Sec. 3 specifies CERN, Sec. 4 and 5 formulate the energy and confidence, Sec. 6 describes the energy layer, Sec. 7 specifies our learning, and finally Sec. 8 presents our results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Group activity recognition. Group activity recognition often requires the explicit representation of spatiotemporal structures of group activities defined in terms of individual actions and pairwise interactions. Previous work typically used graphical models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref> or AND-OR grammar models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref> to learn the structures grounded on handcrafted features. Recent methods learn a graphical model, typically MRF <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> or CRF <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref>, using recurrent neural networks (RNNs). Also, work on group activity recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref> has demonstrated many advantages of using deep architectures of RNNs over the mentioned nondeep approaches. Our approach extends this work by replacing the RNN's softmax layer with a new energy layer, and by specifying a new energy-based model that takes into account p-values of the network's predictions.</p><p>Energy-based learning. While energy-based formulations of inference and learning are common in non-deep group activity recognition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>, they are seldom used for deep architectures. Recently, a few approaches have tried to learn an energy-based model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> using deep neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. They have demonstrated that energy-based objectives have great potential in improving the performance of structured predictions, especially when training data are limited. Our approach extends this work by regularizing the energy-based objective such that it additionally accounts for the confidence of predictions.</p><p>Reliability of Recognition. Most energy-based models in computer vision have only focused on the energy minimization for various recognition problems. Our ap-  proach additionally estimates and regularizes inference with p-values. The p-values are specified within the framework of conformal prediction <ref type="bibr" target="#b23">[24]</ref>. This allows the selection of more reliable and numerically stable predictions.</p><formula xml:id="formula_0">LSTM i LSTM ij i 2 V hi, ji 2 E</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Components of the CERN Architecture</head><p>For recognizing events, interactions, and individual actions, we use a deep architecture of LSTMs, called CERN, shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. CERN is similar to the deep networks presented in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, and can be viewed as a graph G = V, E, c, Y , where V = {i} is the set of nodes corresponding to individual human trajectories, and E = {(i, j)} is the set of edges corresponding to pairs of human trajectories. These human trajectories are extracted using an offthe-shelf tracker <ref type="bibr" target="#b7">[8]</ref>. Also, c ∈ {1, · · · , C} denotes an event class (or group activity), and</p><formula xml:id="formula_1">Y = Y V ∪ Y E is the union set of individual action classes Y V = {y i : y i ∈ Y V } and human interaction classes Y E = {y ij : y ij ∈ Y E }</formula><p>associated with nodes and edges.</p><p>In CERN, we assign an LSTM to every node and edge in G. All the node LSTMs share the same weights and all the edge LSTMs also have the same weights. These LSTMs use convolutional neural networks (CNNs) to compute deep features of the corresponding human trajectories, and output softmax distributions of individual action classes, ψ V (x i , y i ), or softmax distributions of human in-teraction classes, ψ E (x ij , y ij ). The LSTM outputs are then forwarded to an energy layer (EL) in CERN for computing the energy E(G). Finally, CERN outputs a structured predictionĜ whose energy has a high confidence:</p><formula xml:id="formula_2">G = arg min G E(G) − log p-val(G).<label>(1)</label></formula><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we specify and evaluate two versions of CERN. CERN-1 uses LSTMs for predicting individual actions and interactions, whereas the event class is predicted by the EL as in (1). CERN-2 has an additional event LSTM which takes features maxpooled from the outputs of the node and edge LSTMs, and then computes the distribution of event classes, ψ(c). The EL in CERN-2 takes all three types of class distributions as input -specif-</p><formula xml:id="formula_3">ically, {ψ V (x i , y i )} i∈V , {ψ E (x ij , y ij )} (i,j)∈E ,</formula><p>and ψ(c)and predicts an optimal class assignment as in <ref type="bibr" target="#b0">(1)</ref>.</p><p>In the following, we specify E(G) and p-val(G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Formulation of Energy</head><p>For CERN-1, the energy of G is defined as where w V c,yi and w E c,yij are parameters, ψ V (x i , y i ) denotes the softmax output of the corresponding node LSTM, and ψ E (x ij , y ij ) denotes the softmax output of the corresponding edge LSTM (see Sec. <ref type="bibr" target="#b2">3)</ref>, and x i and x ij denote visual cues extracted from respective human trajectories by a CNN as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>.</p><formula xml:id="formula_4">E(G) ∝ i∈V w V c,yi ψ V (x i , y i ) node potential + (i,j)∈E w E c,yij ψ E (x ij , y ij ) edge potential,<label>(2)</label></formula><formula xml:id="formula_5">↵ V (1) class 1 in V0(c) ↵ V (2)</formula><p>For CERN-2, the energy in <ref type="formula" target="#formula_4">(2)</ref> is augmented by the softmax output of the event LSTM, i.e., </p><formula xml:id="formula_6">E(G) ∝ i∈V w V c,yi ψ V (x i , y i ) node potential + (i,j)∈E w E c,yij ψ E (x ij , y ij ) edge potential + w c ψ(x, c) event potential,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Formulation of Confidence</head><p>There are several well-studied ways to define the pvalues <ref type="bibr" target="#b10">[11]</ref>. In this paper, we follow the framework of conformal prediction <ref type="bibr" target="#b23">[24]</ref>. Conformal prediction uses a nonconformity (dissimilarity) measure to estimate the extent to which a new prediction is different from the system's predictions made during training. Hence, it provides a formalism to estimate the confidence of new predictions based on the past experience on the training data. Below, we define the nonconformity measure, which is used to compute the p-values for LSTMs' predictions of individual actions, interactions, and events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Nonconformity Measure and P-values</head><p>Given the node potential ψ V (x i , y i ), we define a nonconformity measure for action predictions:</p><formula xml:id="formula_7">α V (y i ) = 1 − ψ V (x i , y i ) y∈Y V ψ V (x i , y) = 1 − ψ V (x i , y i ),<label>(4)</label></formula><p>where the above derivation step holds because ψ V (x i , y i ) is the softmax output normalized over action classes. α V (y i ) is used to estimate the p-value of predicting action class y i under the context of event class c as</p><formula xml:id="formula_8">p V i (c, y i ) = i ∈V0(c) 1(y i = y i )1(α V (y i ) ≥ α V (y i )) i ∈V0(c) 1(y i = y i )</formula><p>.</p><p>(5) where 1(·) is the indicator, and V 0 (c) denotes the set of all human trajectories in training videos with ground truth labels y i and belonging to the ground truth event class c. From (5), the LSTM prediction ψ V (x i , y i ) is reliable -i.e., has a high p-value -when many training examples i of the same class have larger nonconformity measures.</p><p>To better understand the relationship between the nonconformity measure and the p-value, let us consider a simple case illustrated in <ref type="figure">Fig. 4</ref>. The figure plots the two distributions of nonconformity measures of two action classes in the training examples (green: class 1, red: class 2). Suppose that we observe a new instance whose softmax output indicates that action class 2 has a higher probability to be the true label, i.e., ψ V ( <ref type="bibr" target="#b1">(2)</ref>. From the two curves, however, we see that this softmax output is very likely to be wrong. This is because from <ref type="figure">Fig. 4</ref> we have the p-values p V i (c, 1) &gt; p V i (c, 2), since a majority of training examples with the class 1 label have larger nonconformity measures than α V (1), and hence class 1 is a more confident solution.</p><formula xml:id="formula_9">x i , 1) &lt; ψ V (x i , 2), and α V (1) &gt; α V</formula><p>Similarly, given the softmax output of the edge LSTM, ψ E (x ij , y ij ), we specify a nonconformity measure of predicting interaction classes:</p><formula xml:id="formula_10">α E ij (y ij ) = 1 − ψ E (x ij , y ij ) y∈Y E ψ E (x ij , y) = 1 − ψ E (x ij , y ij ),<label>(6)</label></formula><p>which is then used to estimate the p-value of predicting interaction class y ij under the context of event class c as</p><formula xml:id="formula_11">p E ij (c, y ij ) = (i ,j )∈E0(c) 1(y i j = y ij )1(α E i j (y i j ) ≥ α E ij (y ij )) (i ,j )∈E0(c) 1(y i j = y ij ) ,<label>(7)</label></formula><p>where E 0 (c) denotes the set of all pairs of human trajectories in training videos with ground truth labels y i j and belonging to the ground truth event class c. From <ref type="bibr" target="#b6">(7)</ref>, the LSTM prediction ψ E (x ij , y ij ) has a high p-value when many training examples (i , j ) in E 0 (c) have larger nonconformity measures.</p><p>Finally, in CERN-2, we also have the LSTM softmax output ψ(x, c), which is used to define a nonconformity measure for event predictions:</p><formula xml:id="formula_12">α(c) = 1 − ψ(x, c) c∈C ψ(x, c) = 1 − ψ(x, c),<label>(8)</label></formula><p>and the p-value of predicting event class c as</p><formula xml:id="formula_13">p(c) = v∈V0 1(c v = c)1(α(c v ) ≥ α(c)) v∈V0 1(c v = c) .<label>(9)</label></formula><p>where V 0 denotes the set of all training videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Confidence of the Structured Prediction G</head><p>To define the statistical significance of the hypothesis G among other hypotheses (i.e., possible solutions), we need to combine the p-values of predictions assigned to nodes, edges and the event of G. More rigorously, for specifying the p-value of a compound statistical test, p-val(G), consisting of multiple hypotheses, we follow the Fisher's combined hypothesis test <ref type="bibr" target="#b10">[11]</ref>. The Fisher's theory states that N independent hypothesis tests, whose p-values are p 1 , · · · p N , can be characterized by a test statistic χ 2 2N as</p><formula xml:id="formula_14">χ 2 2N = −2 N n=1 log p n ,<label>(10)</label></formula><p>where the statistic χ 2 2N is proved to follow the χ 2 probability distribution with 2N degrees of freedom. From (10), it follows that minimization of the statistic χ 2 2N will yield the maximum p-value characterizing the Fisher's combined hypothesis test.</p><p>In the following section, we will use this theoretical result to specify the energy layer of our CERN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">The Energy Layer of CERN</head><p>We extend the deep architecture of LSTMs with an additional energy layer (EL) aimed at jointly minimizing the energy, given by (3), and maximizing a p-value of the Fisher's combined hypothesis test, given by <ref type="bibr" target="#b9">(10)</ref>. For CERN-2, this optimization problem can be expressed as</p><formula xml:id="formula_15">min c,Y E(G) s.t. − i∈V log p V i (c, y i ) ≤ τ V , − (i,j)∈E log p E ij (c, y ij ) ≤ τ E , − log p(c) &lt; τ c ,<label>(11)</label></formula><p>where τ V , τ E , and τ c are parameters that impose lowerbound constraints on the p-values. Recall that according to the Fisher's theory on a combined hypothesis test, decreasing the constraint parameters τ V , τ E , and τ c will enforce higher p-values of the solution.</p><p>From <ref type="formula" target="#formula_6">(3)</ref> and <ref type="formula" target="#formula_2">(11)</ref>, we derive the following Lagrangian, also referred to as regularized energyẼ(X, Y, c), which can</p><formula xml:id="formula_16">w E c E p V c p E c + E V &gt; p V c V w V c V w V c &gt; V + V &gt; p V c + w E c &gt; E + E &gt; p E c + Energy c + w c c + p c w c p c w V c &gt; V w E c &gt; E E &gt; p E c p c c w c c (a)</formula><p>The unit for computing the regularized energy of category c, given by <ref type="bibr" target="#b14">(15)</ref>.  then be readily implemented as the EL:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimum Energy Selector</head><formula xml:id="formula_17">V p V 1 p V 2 p V C E p E 1 p E 2 p E C Energy 1 … V E Energy 2 V E Energy C p C p 2 p 1 2 1 C</formula><formula xml:id="formula_18">E(X, Y, c) = i∈V w V c,yi ψ V (x i , y i )−λ V i∈V log p V i (c, y i ) + (i,j)∈E w E c,yij ψ E (x ij , y ij )−λ E (i,j)∈E log p E ij (c, y ij ) +w c ψ(x, c) − λ log p(c),<label>(12)</label></formula><p>Note that for CERN-1, we drop the last two terms in <ref type="bibr" target="#b11">(12)</ref>, w c ψ c and λ log p(c).Ẽ(X, Y, c) can be expressed in a more compact form as</p><formula xml:id="formula_19">E(X, Y, c) = w V c ψ V − λ V log p V c +w E c ψ E − λ E c log p E c +w c ψ c − λ log p c ,<label>(13)</label></formula><p>where all parameters, potentials, and p-values are grouped into corresponding vectors. For brevity, we defer the specification of these vectors to the appendix. <ref type="figure" target="#fig_6">Fig. 5a</ref> shows a unit in the EL which computes <ref type="bibr" target="#b14">(15)</ref>. After stacking these units, as shown in <ref type="figure" target="#fig_6">Fig. 5b</ref>, we select the solutionĜ with the minimumẼ(Ĝ).</p><p>In the following, we explain our energy-based end-toend training of the EL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Learning Regularized By Confidence</head><p>Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref>, we use an energy-based loss for a training instance X i and its ground truth labels (Y i , c i ) to learn parameters of the EL, i.e., the regularized energy, specified in <ref type="formula" target="#formula_2">(12)</ref>:</p><formula xml:id="formula_20">L(X i , Y i , c i ) = max 0,Ẽ(X i , Y i , c i ) −Ẽ(X i ,Ȳ ,c) + 1(c i =c) , (14) whereȲ ,c = argmin Y,c =c iẼ (X i , Y, c) − 1(c i = c)</formula><p>is the most violated case. Alternatively, this loss can be replaced by other energy-based loss functions also considered in <ref type="bibr" target="#b18">[19]</ref>. Here we treat Y as latent variables for simplicity and thus only consider accuracy of c. However, one can include a comparison between Y and its corresponding ground truth label Y i into the loss function. It is usually difficult to find the most violated case. However, as <ref type="bibr" target="#b19">[20]</ref> points out, the inference of the most violated case does not require a global minimum solution since the normalization term is not modeled in our energy-based model, so we can simply setȲ to be the output of the node and edge LSTMs.</p><p>In practice, one can first train a network using common losses such as cross-entropy to learn the representation excluding the EL, namely from the input layer to softmax layers. Then the p-value of a training instance can be computed by removing itself from the training sets V 0 and E 0 . Finally we train the weights in (12) by minimizing the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Results</head><p>Implementation details. We stack the node LSTMs and edge LSTMs on top of a VGG-16 model <ref type="bibr" target="#b25">[26]</ref> without the FC-1000 layer. The VGG-16 is pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, and fine-tuned with LSTMs jointly. We train the top layer of CERN by fixing the weights of the CNNs and the bottom layer LSTMs. The batch size for the joint training of the bottom LSTMs and VGG-16 is 6. The training converges within 20000 iterations. The event LSTM and the EL are trained using 10000 iterations with a batch size of 2000. For the mini-batch gradient descent, we use RM-Sprop <ref type="bibr" target="#b27">[28]</ref> with a learning rate ranging from 0.000001 to 0.001. We use Keras <ref type="bibr" target="#b6">[7]</ref> with Theano <ref type="bibr" target="#b26">[27]</ref> as the backend to implement CERN, and run training and testing with a single NVIDIA Titan X (Pascal) GPU. For a fair comparison with <ref type="bibr" target="#b13">[14]</ref>, we use the same tracker and its implementation as in <ref type="bibr" target="#b13">[14]</ref>. Specifically, we use the tracker of <ref type="bibr" target="#b7">[8]</ref> from the Dlib library <ref type="bibr" target="#b15">[16]</ref>. The cropped image sequences of persons and pairs of persons are used as the inputs to node LSTMs and edge LSTMs, respectively.</p><p>We compare our approach with the state-of-the-art methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. In addition, we evaluate the following reasonable baselines.</p><p>Baselines:</p><p>• 2-layer LSTMs (B1). We test a network of 2-layer LSTMs similar to <ref type="bibr" target="#b13">[14]</ref>. All other baselines below and our full models use B1 to compute their potentials and p-values. B1 does not have the energy layer, but only a feed-forward network. The event class is predicted by the softmax output of the event LSTM. • CERN-1 w/o p-values (B2). This baseline represents the CERN-1 network with the EL, however, the pvalues are not computed and not used for regularizing energy minimization. Hence, the event class prediction of B2 comes from the standard energy minimization. • CERN-2 w/o p-values (B3). Similar to B2, in this B3, we do not estimate and do not use the p-values in the EL of CERN-2. Datasets. We evaluate our method in two domains: collective activities and sport events using the Collective Activity dataset <ref type="bibr" target="#b5">[6]</ref> and the Volleyball dataset <ref type="bibr" target="#b13">[14]</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Collective Activity Dataset</head><p>The Collective Activity dataset consists of 44 videos, annotated with 5 activity categories (crossing, walking, waiting, talking, and queueing), 6 individual action labels (NA, crossing, walking, waiting, talking, and queueing), and 8 pairwise interaction labels (NA, approaching, leaving, passing-by, facing-each-other, walking-side-by-side, standing-in-a-row, standing-side-by-side). The interaction labels are provided by the extended annotation in <ref type="bibr" target="#b4">[5]</ref>.</p><p>For this dataset, we first train the node LSTMs and edge LSTMs with 10 time steps and 3000 nodes. Then, we concatenate the outputs of these two types of LSTMs at the bottom layer of CERN, along with their VGG-16 features, and pass the concatenation to the bidirectional event LSTM with 500 nodes and 10 time steps at the top layer of CERN. The concatenation is passed through a max pooling layer and a fully-connected layer with a output dimension of 4500.</p><p>For comparison with <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> and baselines B1-B3, we use the following performance metrics: (i) multi-class classification accuracy (MCA), and (ii) mean per-class accuracy (MPCA). Our split of training and testing sets is the same as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. Tab. 1 summarizes the performance of all methods on recognizing group activities. Note that in Tab. 1 only <ref type="bibr" target="#b11">[12]</ref> does not use deep neural nets. As can be seen, our energy layer significantly boosts the accuracy, outperforming the state-of-the-art by a large margin. Even when we only have the bottom layer of LSTMs, CERN-1 still outperforms the 2-layer LSTMs in <ref type="bibr" target="#b13">[14]</ref> thanks to the EL. Without the EL, the baseline B1 yields lower accuracy than <ref type="bibr" target="#b13">[14]</ref> even with additional LSTMs for the interactions.</p><p>Our accuracies of recognizing individual actions and interactions on the Collective Activity dataset are 72.7% and 59.9%, using the node LSTMs and edge LSTMs respectively. Note that B1, CERN-1 and CERN-2 share the same node and edge LSTMs.</p><p>For evaluating numerical stability of predicting group activity classes by CERN-2, we corrupt all human trajectories in the testing data, and control the amount of corruption with the corruption probability. For instance, for the cor-Method MCA MPCA Cardinality kernel <ref type="bibr" target="#b11">[12]</ref> 83.4 81.9 2-layer LSTMs <ref type="bibr" target="#b13">[14]</ref> 81.5 80.   ruption probability of 0.5, we corrupt one bounding box of a person in every video frame with a 0.5 chance. When the bounding box is selected, we randomly shift it with a horizontal and a vertical displacement ranging from 20% to 80% of the original bounding box's width and height respectively. As <ref type="figure" target="#fig_8">Fig. 6</ref> shows, CERN-2 consistently experiences a lower degradation in performance compared to the baselines without p-values. This indicates that incorporating the p-values into the energy model indeed benefits the inference stability. Such benefit becomes more significant as the amount of corruption in input data increases. <ref type="figure">Fig. 8</ref> shows an example of the crossing activity. As can be seen, although B1 and CERN-2 share the same individual action labels, where a majority of the people are assigned incorrect action labels, CERN-2 can still correctly recognize the activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Volleyball Dataset</head><p>The Volleyball dataset consists of 55 videos with 4830 annotated frames.   (including 5 preceding and 4 succeeding frames). The event LSTM in CERN-2 is a bidirectional LSTM with 1000 nodes and 10 time steps. In <ref type="bibr" target="#b13">[14]</ref>, the max pooling has two types: 1) pooling over the output of all node LSTMs, or 2) dividing the players into two groups (the left team and the right team) first and pooling over each group separately. We test both types of max pooling for our approach to rule out the effect of pooling type in the comparison. CERN-1 does not have the pooling layer, thus is categorized as 1 group style. Recognition accuracy of individual actions is 69.1% using node LSTMs, and the accuracies of recognizing group activities are summarized in Tab. 2. Cleary, the regularized energy minimization increases the accuracy compared to the conventional energy minimization (B2 and B3), and CERN-2 outperforms the state-of-the-art when using either of the pooling types. CERN-1 does not achieve accuracy that is comparable to that of CERN-2 on the Volleyball dataset. This is mainly because CERN-1 reasons the group activity based on individual actions, which may not provide sufficient information for recognizing complex group activities in sports videos. CERN-2 overcomes this problem by adding the event LSTM.</p><p>We also evaluate the stability of recognizing group activities by CERN-2 under corruption of input human trajec- tories. As <ref type="figure" target="#fig_9">Fig. 7</ref> indicates, the p-values in the EL indeed increase the inference reliability on the Volleyball dataset. The qualitative results (2 groups) of a right pass activity is depicted in <ref type="figure">Fig. 9</ref>, which demonstrates the advantage of the inference based on our regularized energy compared to the softmax output of the deep recurrent networks when the action predictions are not accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>We have addressed the problem of recognizing group activities, human interactions, and individual actions with a novel deep architecture, called Confidence-Energy Recurrent Network (CERN). CERN extends an existing two-level hierarchy of LSTMs by additionally incorporating a confidence measure and an energy-based model toward improving reliability and numerical stability of inference. Inference is formulated as a joint minimization of the energy and maximization of the confidence measure of predictions made by the LSTMs. This is realized through a new differentiable energy layer (EL) that computes the energy reg-ularized by a p-value of the Fisher's combined statistical test. We have defined an energy-based loss in terms of the regularized energy for learning the EL end-to-end. CERN has been evaluated on the Collective Activity dataset and Volleyball dataset. In comparison with previous approaches that predict group activities in a feed-forward manner using deep recurrent networks, CERN gives a superior performance, and also gives more numerically stable solutions under uncertainty. For collective activities, our simpler variant CERN-1 gives more accurate predictions than a strong baseline representing a two-level hierarchy of LSTMs with softmax outputs taken as predictions. Our variant CERN-2 increases complexity but yields better accuracy on challenging group activities which are not merely a sum of individual actions but a complex whole.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our CERN represents a two-level hierarchy of LSTMs grounded onto human trajectories, where the LSTMs predict individual actions {yi}, human interactions {yij}, or the event class c in a given video. CERN outputs an optimal configuration of LSTM predictions which jointly minimizes the energy of the predictions and maximizes their confidence, for addressing the brittleness of cascaded predictions under uncertainty. This is realized by extending the two-level hierarchy with an additional energy layer, which can be trained in an end-to-end fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>log p-val(G) Loss = L(E(G ⇤ ), E(Ĝ), p-val(Ĝ)) (top) An imaginary illustration of the solution space where each circle represents a candidate solution. The colors and sizes of the circles indicate the energy (red:high, blue:low) and confidence (the larger the radius the higher confidence) computed by the energy layer in CERN. A candidate solutionĜ1 has the minimum energy, but seems numerically unstable for small perturbations in input. A joint maximization of confidence and minimization of energy gives a different, more confident solutionĜ2. Confidence is specified in terms of p-values of the energy potentials. (bottom) We formulate an energy-based loss for end-to-end learning of CERN. The loss accounts for the energy and p-values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>We specify and evaluate two versions of CERN. CERN is a deep architecture of LSTMs, which are grounded via CNNs to video frames at the bottom. The LSTMs forward their class predictions to the energy layer (EL) at the top. CERN-1 has LSTMs only at the bottom level which compute distributions of individual action classes (colored boxes) or distributions of interaction classes (colored links between green boxes). CERN-2 has an additional LSTM for computing the distribution of event (or group activity) classes. The EL takes the LSTM outputs, and infers an energy minimum with the maximum confidence. The figure shows that CERN-1 and CERN-2 give different results for the group activity crossing. CERN-1 wrongly predicts walking. CERN-2 typically yields better results for group activities that can not be defined only by individual actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>class 2 inFigure 4 :</head><label>24</label><figDesc>V0(c)    ↵ A simple illustration of the relationship between the nonconformity measure α of individual actions and the p-value, where the ratio of the dashed region to the whole area under the curve indicates the p-value. Clearly, for the given instance, action class 2 has a larger softmax output but action class 1 has a higher confidence. V0(c) is the training set of videos showing event c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where x in ψ(x, c) is the visual representation of all actions and interactions maxpooled from the outputs of the node LSTMs and edge LSTMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>c</head><label></label><figDesc>Event class(b) Diagram of all units in the energy layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The EL takes the softmax outputs of all LSTMs along with estimated p-values as input, and outputs a solution that jointly minimizes the energy and maximizes a p-value of the Fisher's combined hypothesis test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Performance decrease of group activity recognition for a varying percentage of corruption of human trajectories in the Collective Activity dataset. We compare 2-layer LSTMs (B1), CERN-2 w/o p-values (B3) and CERN-2 using the same corrupted trajectories as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>The decrease of group activity recognition accuracy over different input distortion percentages on the Volleyball dataset (all use the 2 groups style). CERN-2 is compared with 2-layer LSTMs (B1) and CERN-2 w/o p-values (B3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>The qualitative results on the Collective Activity dataset. From left to right, we show the inference results from B1, CERN-2 and the ground truth (GT) labels respectively. The colors of the bounding boxes indicate the individual action labels (green: crossing, red: waiting, magenta: walking). The interaction labels are not shown here for simplicity. The qualitative results on the Volleyball dataset: results of B1 (top), results of CERN-2 (middle) and the ground truth (GT) labels (bottom). The colors of the bounding boxes indicate the individual action labels (green: waiting, yellow: digging, red: falling, magenta: standing), and the numbers are the frame IDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The actions labels are waiting, setting, digging, failing, spiking, blocking, jumping, moving, and standing; and the group activity classes include right set, right spike, right pass, right winpoint, left winpoint, left pass, left spike, and left set. Interactions are not annotated in this dataset, so we do not recognize interactions and remove the edge LSTMs.The node LSTMs have 3000 nodes and 10 time steps</figDesc><table><row><cell>Method</cell><cell cols="2">MCA MPCA</cell></row><row><cell>2-layer LSTMs [14] (1 group)</cell><cell>70.3</cell><cell>65.9</cell></row><row><cell>B1: 2-layer LSTMs (1 group)</cell><cell>71.3</cell><cell>69.5</cell></row><row><cell>B2: CERN-1 w/o p-values (1 group)</cell><cell>33.3</cell><cell>34.3</cell></row><row><cell>B3: CERN-2 w/o p-values (1 group)</cell><cell>71.7</cell><cell>69.8</cell></row><row><cell>CERN-1 (1 group)</cell><cell>34.4</cell><cell>34.9</cell></row><row><cell>CERN-2 (1 group)</cell><cell>73.5</cell><cell>72.2</cell></row><row><cell>2-layer LSTMs [14] (2 groups)</cell><cell>81.9</cell><cell>82.9</cell></row><row><cell>B1: 2-layer LSTMs (2 group)</cell><cell>80.3</cell><cell>80.5</cell></row><row><cell cols="2">B3: CERN-2 w/o p-values (2 groups) 82.2</cell><cell>82.3</cell></row><row><cell>CERN-2 (2 groups)</cell><cell>83.3</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different methods for group activity recognition on the Volleyball dataset. The first block is for the methods with 1 group and the second one is for those with 2 groups.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by grants DARPA MSEE project FA 8650-11-1-7149, ONR MURI project N00014-16-1-2007, and NSF IIS-1423305.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Energy Function</head><p>The regularized energyẼ(X, Y, c) can be reformulated in a compact form as</p><p>where weights of the EL are grouped into {w c } c=1,··· ,C , λ, and the following parameter vectors:</p><p>and the input to the EL is specified in terms of the LSTM softmax outputs and p-values: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hirf: Hierarchical random field for collective activity recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="572" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cost-sensitive top-down/bottom-up inference for multiscale activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding collective activitiesof people from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What are they doing? : Collective activity classification using spatio-temporal relationship among people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1282" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical Methods for Research Workers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<publisher>Oliver and Boyd</publisher>
			<biblScope unit="page">5</biblScope>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>11 edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual recognition by counting instances: A multi-instance cardinality potential kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hierarchical deep temporal models for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02643</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Social roles in hierarchical models for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4321" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative latent models for recognizing contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robinovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1549" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting Structured Data</title>
		<editor>G. Bakir, T. Hofman, B. Scholkopf, A. Smola, and B. Taskar, editors</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Loss functions for discriminative training of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics Conference (AISTATS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3043" to="3053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social role discovery in human events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2475" to="2482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A tutorial on conformal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint inference of groups, events and human roles in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4576" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs:1605.02688</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. Coursera: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep markov random field for image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="295" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
