<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DELTA A DEep learning based Language Technology plAtform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghu</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwei</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahao</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangli</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lyu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Labs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didi</forename><surname>Chuxing</surname></persName>
						</author>
						<title level="a" type="main">DELTA A DEep learning based Language Technology plAtform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present DELTA, a deep learning based language technology platform. DELTA is an end-to-end platform designed to solve industry level natural language and speech processing problems. It integrates most popular neural network models for training as well as comprehensive deployment tools for production. DELTA aims to provide easy and fast experiences for using, deploying, and developing natural language processing and speech models for both academia and industry use cases. We demonstrate the reliable performance with DELTA on several natural language processing and speech tasks, including text classification, named entity recognition, natural language inference, speech recognition, speaker verification, etc. DELTA has been used for developing several state-of-the-art algorithms for publications and delivering real production to serve millions of users.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, deep learning has been achieving tremendous success on numerous machine learning applications <ref type="bibr" target="#b23">(LeCun et al., 2015;</ref><ref type="bibr" target="#b33">Schmidhuber, 2015)</ref>. It firstly dramatically improved the state-ofthe-art in speech recognition  and image recognition <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref>, and then produced extremely promising results on natural language processing (NLP) <ref type="bibr" target="#b9">(Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Devlin et al., 2019;</ref><ref type="bibr" target="#b37">Sutskever et al., 2014)</ref>.</p><p>In natural language and speech processing areas, various algorithms and models are emerging recently due to the rapid progress in both academia and industry. Recently, many open-sourced libraries are released for community development since the interest in applying deep learning approaches to NLP and speech processing is very high. However, some open-sourced libraries are implemented for specific tasks, and researchers and engineers have to adapt the code for their own problems. More importantly, most existing implementations are designed for research purposes and do not consider model serving. Although TensorFlow provides interfaces for model serving, it is not trivial to deploy the models for complex production usage.</p><p>We introduce DELTA, 2 a deep learning based natural language and speech processing platform, as an end-to-end open-sourced library, which integrates most popular NLP and speech processing models and deployment tools, forming with uniform code structures and input/output (I/O) interfaces. DELTA is implemented using TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>, featuring easy-to-use, easy-to-deploy, and easy-to-develop:</p><p>• Easy to use -It supports most common NLP and speech tasks that works out of the box. The users may directly complete model training through one line of command. -It is of highly-customized configuration. The users can easily customize model architectures and parameters. -It supports multimodal training using textual, acoustic, and numeric features, which is very useful for the multimodal scenarios especially in real applications. -It optimizes the front-end data preprocessing and parallelizes model training so it is easy to train models on a huge amount of data.</p><p>• Easy to deploy -What you see in training is what you get in serving. All data processing and feature extraction components are implemented as TensorFlow operators (OPs) and integrated into a graph for deployment. -All models have the uniform I/O interfaces and transparent to outside. One can deploy a new model with no changes on the serving code.</p><p>• Easy to develop -The models and components are modularized. The developers can easily build new models on top of them. -All modules are fully-tested and provide reliable and efficient performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Platform architecture</head><p>This section describes the overall architecture and code structures for DELTA. In high-level, DELTA includes two parts:</p><p>• Modeling: This is the main package for offline modeling, including data processing, model building, training, and evaluation, etc. The code is organized in delta directory. We discuss the details for this part in this section.</p><p>• Deployment: This is the package for online serving using existing model. DELTA provides flexible online serving approaches for various production environments such as GPU machines, mobile devices, embedded devices. The code is organized in deltann directory. We will discuss this part in the next section.   <ref type="figure" target="#fig_0">Fig. 1</ref> shows a high-level overview for the modeling part in DELTA. An user is responsible for providing data and a user-defined configuration. Based on the configuration, DELTA can automatically generate a training pipeline including the corresponding data processing and modeling modules. The pipeline will start model training processing and generate a model once complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data processing</head><p>Acoustic and textual information are the two most important ways for human communication, both of which are presented as sequences but in different modalities. Therefore most sequential models are applicable to both text and speech. In DELTA, we use Dataset API to load and process data, and the code is modularized in the directory delta/data 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Text data</head><p>DELTA supports text inputs in the raw text format. Preprocessing transforms the text inputs from the original format to a model-readable format. For English, we provide standard text processing functions using TensorFlow operations, including splitting, punctuation conversion, casing, etc. For Chinese, word segmentation is usually used to organize Chinese characters into words. We implement several Chinese word segmentation algorithms as TensorFlow OPs, such as jieba 4 . A tokenizer is then applied to transform a text string into a list of indices. In DELTA, SentenceToIdsOp is applied to index the tokens based on a vocabulary look-up table. All these processing operations are implemented as customized OPs, and some are introduced from the third-party libraries such as Lingvo 5 <ref type="bibr" target="#b35">(Shen et al., 2019)</ref>. The OPs are organized in delta/layers/ops/py_x_ops.py and can be compiled as a dynamic library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Speech data</head><p>Speech tasks, unlike NLP tasks, usually come with complex feature extraction on raw speech signals. TensorFlow has provided several functions for speech processing, such as MFCC and spectrogram. In DELTA, we also implement speech processing operations for broader applications, including filter-bank feature, perceptual linear prediction feature (PLP), zero crossing rate, frame power, pitch, first order and seconder order derivatives between frames (i.e., deltas and accelerations). To perform high-level transformation, we implemented AnalysisFilterBank and SynthesisFilterBank to generate spectrogram and phase features from waveforms and reconstruct waveforms from both, respectively. We reuse some OPs from speech processing libraries. All these OPs are also included in delta/layers/ops/py_x_ops.py and can be used to either build the model graph or process data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Numeric data</head><p>In addition to textual and acoustic features, the numeric feature (a.k.a., dense feature) is commonly used in machine learning. In fact, the numeric feature is a generic feature representation which can be directly fed into the model and most features are converted into numeric features for model training.</p><p>In practise, a user may need to train a model using numeric features from different domains, e.g., click-through-rate, user profile, etc. We implement model training with numeric features in DELTA, which can be used in general machine learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Multimodal data</head><p>We also want to highlight that DELTA supports multimodal training from different data sources. Multimodal training is very common in real applications. For example, in search engine, both textual features and user historic features (numeric features) are helpful for produce accurate search results. One may need to build a multimodal model to learn from textual features and numeric features simultaneously. Another example is that, in human speech emotion recognition, both acoustic features and textual features can be combined together to build a multimodal emotion recognizer <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>.</p><p>To utilize multimodal data, there are many approaches to combine the features <ref type="bibr" target="#b27">(Ngiam et al., 2011)</ref>. In DELTA, we use direct concatenation to combine the features due to the simplicity. Our implementation includes multimodal training for textual+numeric features, acoustic+numeric features and textual+acoustic features. A user only need to define the training mode in the configuration file to start a multimodal training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models</head><p>To build models in DELTA, we wrap the low-level neural network models as basic components, which can be directly used in high-level models. The basic components include multi-layer perceptrons (MLPs) <ref type="bibr" target="#b30">(Rosenblatt, 1958;</ref><ref type="bibr" target="#b31">Rumelhart et al., 1988)</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b22">(LeCun et al., 1990)</ref>, long short-term memory (LSTM) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997)</ref>, attention mechanism <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, transformers <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref>, etc. These components are widely used in NLP models and speech models for different tasks and they are implemented in delta/layers directory.</p><p>Note that, we have implemented the most popular high-level models in DELTA, meaning that a user can directly use the provided configuration files for model training. For most high-level models, our implementation refers to publications, and some modification may be applied. If you want to develop a new high-level model, you build a new model in delta/models with a python register.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">NLP models</head><p>In DELTA, we organize the NLP models based on the input/output data format. For different NLP tasks, if the input and output data formats are the same, they usually can be addressed by similar high-level architecture. For example, in high-level, a sequence-to-sequence architecture can be used to address both machine translation and text summarization, with some modification for each task. We currently implement the following model architectures:</p><p>• Sequence classification This model deals with tasks like sentence classification and document classification. Here, the input is a sequence of text and the target is a label. For document classification, you can define a sentence separator to divide the documents into a sequence of sentences for hierarchical modeling. We provide the sequence classification models using CNN <ref type="bibr" target="#b9">(Collobert et al., 2011;</ref><ref type="bibr" target="#b19">Kim, 2014)</ref>, LSTM <ref type="bibr" target="#b38">(Tai et al., 2015)</ref>, hierarchical attention networks <ref type="bibr" target="#b44">(Yang et al., 2016)</ref>, transformer <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref>, etc.</p><p>• Sequence labeling For this model, both the input and the output are sequences and the model assigns a label to each step in the input sequence, so the input sequence and the output sequence should have the same length. This model is usually used in named entity recognition (NER), part-of-speech tagging, etc. We have provided the LSTM based sequence labeling (Chiu and Nichols, 2016) and an LSTM with CRF based method <ref type="bibr" target="#b18">(Huang et al., 2015;</ref><ref type="bibr" target="#b21">Lample et al., 2016;</ref><ref type="bibr" target="#b26">Ma and Hovy, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Pairwise modeling</head><p>The input for this model is a pair of two documents. The model is trained to learn the relationship between them. In this model, each document is modeled separately to generate its document embedding, then the two embedding vectors are either concatenated together and fed into a classifier or compared to compute the cosine similarity. This type of models has been widely used on document similarity, query-based question-answering (QA), natural language inference (NLI), etc. The key for this approach is to supervisedly train the networks for document embedding <ref type="bibr" target="#b4">(Bowman et al., 2015;</ref><ref type="bibr" target="#b10">Conneau et al., 2017;</ref><ref type="bibr" target="#b17">Huang et al., 2013)</ref>.</p><p>• Sequence-to-sequence (seq2seq) modeling The seq2seq model takes input as a sequence of text and generate a sequence of text. But the lengths of two sequences are not necessarily the same. This structure usually has an encoder to learn the information from the input sequence and then uses a decoder to autoregressively generate the output. It has been widely used in many text generation tasks, for example, machine translation, text summarization, dialogue generation, etc. We implement the standard seq2seq models using LSTM with attention <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref> and transformers <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref>. Note that, the seq2seq structure is also used for speech recognition. In DELTA, this part is shared between NLP and ASR tasks.</p><p>• Multi-task modeling Since the models in DELTA are modularized, it is easy to support multi-task learning using the existing modules. As an example, we implement a multi-task model for sequence classification and labeling, where the sequence level loss and the step level loss are computed simultaneously. This model is used to jointly train an intent recognizer and named entity recognizer together. (Junwen add more details and citations) • Pretraining integration Recent NLP studies have been showing significant improvements on using pretrained language models on unlabeled data, e.g., ELMO <ref type="bibr" target="#b28">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>. We implement an interface to integrate a pretrained model into a DELTA model, where the pretrained model is used to dynamically generate embedding which is concatenated with the word embedding for the different task. To be specific, a user can pretrain an ELMO or BERT model first and then build a DELTA model with the prertained model. Both model will be combined into a TensorFlow graph for training and inference. The ELMO or BERT models trained from the official open-sourced libraries can be directly used in DELTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Speech models</head><p>For speech processing, the model usually couple with the feature extraction because different task may have different features extracted using signal processing. In order to better organize the model, we implement the speech models as a task-specific fashion.</p><p>• Automatic speech recognition (ASR) ASR is one of the most important task in speech processing, where the input is a raw speech waveform in time series and the output is the corresponding text. Recent trend in ASR is to build an end-to-end ASR system using seq2seq framework. We provide an attention based seq2seq ASR model <ref type="bibr" target="#b2">(Bahdanau et al., 2016;</ref><ref type="bibr" target="#b5">Chan et al., 2016)</ref>. We also implement another popular type of ASR model using connectionist temporal classification (CTC) <ref type="bibr" target="#b12">(Graves et al., 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Speaker verification</head><p>Speaker verification is the process of verifying whether an utterance belongs to a specific speaker, based on that speaker's known utterances (i.e., enrollment utterances). It has applications on voice match, identification check, etc. We provide an X-vector text-independent model <ref type="bibr" target="#b36">(Snyder et al., 2018)</ref> and an end-to-end model <ref type="bibr" target="#b41">(Wan et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Speech emotion recognition</head><p>From the machine learning perspective, speech emotion recognition is similar to speaker identification, because both are speech sentence classification. However, the features used in both tasks can be very different, therefore we provide both models separately. Recently several deep learning based approaches have been successfully used in speech emotion recognition <ref type="bibr" target="#b13">(Han et al., 2014;</ref><ref type="bibr" target="#b32">Satt et al., 2017)</ref> and we implement some models the in DELTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Multimodal models</head><p>Multimodal modeling is paramount to practical applications because real data usually comes from different sources and available in multiple modalities. Since it is still a research topic to how to model multimodal data, we provide a straightforward approach to concatenate the multimodal data in the input feature stage for modeling <ref type="bibr" target="#b27">(Ngiam et al., 2011)</ref>. This is a framework to use multimodal data for general purpose. More sophisticated data fusion approaches are usually task dependent and we provide an example using aligned textual and acoustic features for emotion recognition <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>.</p><p>• Textual+acoustic A common applications for multimodal learning with text and speech is speech emotion recognition, where the model takes input as speech signals and the corresponding transcripts and output the emotion states. In our implementation, we use two sequential models (e.g., CNNs or LSTMs) to learn the sequence embedding for speech and text separately, and then concatenates the learned embedding vectors for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Textual+numeric</head><p>Multimodal learning with textual and numeric features was widely used in real application. In DELTA, we implement the direct concatenation data fusion in data processing stage, therefore this type of multimodal training can be directly used for existing models in DELTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training pipeline</head><p>In this subsection, we introduce the whole training pipeline in DELTA. With the existing models in DELTA, a user only need to prepare the dataset and provide a configuration file in the format of yaml. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the configuration contains the parameters used in model training. The task configuration indicates the model or task types including several NLP and speech tasks as well as the multimodal tasks. The model configuration defines the model parameters such as types of neural networks, number of hidden layers, number of neural units. The training configuration mainly specify the parameters for optimization engines and evaluation metrics. Once the configuration files are defined, you can simply start a training pipeline with one line of command. To deep dive into the training pipeline, we now discuss the detailed implementation methods in DETLA. From the aspect of code structure, a model training process uses a uniform training pipeline, which consists of three components: task, model, and solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training config</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model config</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Task</head><p>Task is an abstract class for data processing as mentioned in Section 2.1, including feature processing and data formatting. It applies online or offline data processing, and the processed data can be fed into model for training, evaluation, or inference. It includes several abstract methods for generating, batching, feature preprocessing and shuffling the data examples. After data processing, it wraps the data examples by tf.data.dataset, which supports several functions such as batching, padding, shuffling, multi-threading processing. This is similar to the design in Tensor2Tensor <ref type="bibr" target="#b40">(Vaswani et al., 2018)</ref> and compatible with both TensorFlow 1.x and 2.x. The abstract methods in Task are brief described as follows:</p><p>• generate_data is a python generator to yield one example in every iteration.</p><p>• feature_spec is a specification of the types and shapes of the data example generated by generate_data.</p><p>• preprocess_batch is the main class for feature processing of batched examples. You can apply different feature processing in this class, such as text tokenization, Chinese word segmentation, audio feature extraction, speech feature normalization. Note that, in DELTA, each feature preprocessing operations have been implemented as a TensorFlow OP, therefore it is straightforward to use it as a model component and build an end-to-end model graph.</p><p>To develop new feature processing OP, one can also program customized TensorFlow OPs in DELTA.</p><p>• dataset is the main class of data pipeline, and a wrapper for tf.data.dataset. All data processing of examples are organized in this class.</p><p>• input_fn generates a function without parameters, which is also recommended by Tensor-Flow Estimator 6 .</p><p>For speech processing tasks, e.g., automatic speech recognition and speaker identification, the inputs are usually raw waveform files and you may need to apply extra offline feature processing, such as speech feature extraction and global feature normalization. These methods mainly follow the feature processing part in the speech recognition library Kaldi <ref type="bibr" target="#b29">(Povey et al., 2011)</ref>.</p><p>• generate_feat is responsible for generating speech feature from raw speech data, such as spectrogram feature or Mel-frequency cepstral coefficients (MFCCs).</p><p>• generate_cmvn compute global cepstral mean and variance normalization (CMVN) on generated speech feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Model building</head><p>For model building, to compatible with both TensorFlow 1.x and 2.x, we introduce two types of models in DELTA: Keras model and raw model. Keras <ref type="bibr" target="#b8">(Chollet et al., 2015)</ref> is a popular high-level API for building deep learning models, which has been adopted into TensorFlow 2.x, where the high-level API is changed from tf.layers to tf.keras.layers. To build a new model in DELTA, we recommend using Keras API because it is user-friendly and easy to extend. To compatible with legacy system, DELTA also supports the raw model type as in TensorFlow 1.x, which can be deployed into production for TensorFlow 1.x system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Keras model</head><p>Keras models supports object-oriented design, including variants, layers, models and checkpoint management. This is the default model type in DELTA and we use the subclass style paradigm of Keras to construct sublayers or submodels. In DETLA, a model is initialized in __init__() method and called in call() method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Raw model</head><p>For the sake of compatibility, we also use raw models. The interface of raw models is similar to that of Keras models, so we use the unified paradigm for model building. We use tf.variable_scope to wrapper the scope of the model implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Solver</head><p>A Solver is a module for composing the pipeline containing model construction, training and evaluation process. When building a model, you can choose a solver based on the model or task types. For example, you may want to choose RawClassSolver if the model you build is a subclass of TextClassModel. The most recommended solver base clase are EstimatorSolver and RawSolver. The main abstract methods of Solver are described as follows:</p><p>• train() builds a model for the training mode. This process include training data fetching, iteration on datasets, optimization, and parameters updating. The process is usually implemented with tf.train.MonitoredTrainingSession. This function saves the model to a TensorFlow checkpoint for every specified training steps.</p><p>• eval() builds the model for the evaluation mode, including evaluation data fetching with labels, inference, and measurement. The metrics are defined in delta.utils.metrics.</p><p>• infer() is similar to eval(), but it takes unlabeled data as inputs for inference. In addition, for some tasks, e.g., keyword spotting <ref type="bibr" target="#b6">(Chen et al., 2014)</ref>, we need to add post processing after the model inference, so infer() runs model inference followed by a postproc_fn() function.</p><p>• train_and_eval() is the combination of train() and eval(). It applies the evaluation for every specified training steps.</p><p>• export_model() transforms the model checkpoint saved during training process to a TensorFlow SavedModel format 7 , which is supported by most deployment environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deployment</head><p>To our knowledge, most open-sourced deep learning platforms are designed for model training and do not focus on model serving. However, serving a model for industrial applications is not trivial. In order to facilitate productizing a trained model, we provide deployment features for model serving in DELTA.</p><p>The deployment part is referred as DELTA-NN and implementations are mainly organized under deltnn dpl. <ref type="figure">Fig. 3</ref> shows an overview of DELTA-NN. A trained model is optionally compressed or optimized in terms of computational efficiency. The deployment pipeline is able to convert the model into several formats as needed. As shown in the figure, TF Serving supports two interfaces, GRPC and RESTful API. They are widely used in online serving in production. For a bare metal environment, you can wrap DELTA-NN as a library with interfaces for model inference.  <ref type="figure">Figure 3</ref>: High-level overview of productization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Optimization</head><p>Optimizing the computational efficiency is important to real applications. Deploying model into mobile or edge devices has restricted requirement on the models, such as limited memory and low power consumption. Some embedded devices do not have floating-point units and can only support integer accelerator. In DELTA-NN, we integrate a model distillation component which is used to learn a small model from an existing large model <ref type="bibr" target="#b15">(Hinton et al., 2014)</ref>. provide an interface for model optimization, such as model quantization, model pruning. We also</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model serving</head><p>For productization, DELTA-NN supports multi-graphs online and/or offline inference, with CPU, GPU and edge devices. There are several serving modes: TFModel, TFLite and TFServing. We can also combine these serving modes as a customized mode.</p><p>• TFModel This mode is provided by the standard TensorFlow library. TensorFlow provides several model formating, including CheckPoint, GraphDef, and SavedModel, etc. We recommend using SavedModel as the model format for standard model serving.</p><p>• TFLite TensorFlow Lite 8 is a lightweight solution for mobile and embedded devices for TensorFlow models. It enables low-latency inference of on-device machine learning models with a small binary size and fast performance supporting hardware acceleration. DELTA-NN supports using TensorFlow Lite as an engine for mobile and embedded devices.</p><p>• TFServing TensorFlow Serving 9 is a flexible, high-performance serving system for machine learning models, designed for production environments. It deals with the inference aspect of machine learning, taking models after training and managing their lifetimes, providing clients with versioned access via a high-performance, reference-counted look-up table. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, and can be easily extended to serve other types of models. DELTA-NN implements an HTTP/HTTPS Client to request remote TensorFlow Serving model results. Combining with TFLite, you can easily deploy offline serving solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deployment pipeline</head><p>The deployment pipeline is organized as scripts and put into the folder dpl.</p><p>A user is responsible for providing TensorFlow SavedModel and a user-defined model configuration.</p><p>Based on the configuration, DELTA-NN can automatically generate a deployment pipeline, including the following steps:</p><p>1. Model optimization This is a place-holder for model distillation, quantization, pruning, compression. You can choose to use a third-party library for model optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model graph transformation</head><p>This step converts a SavedModel to a proper model format for serving on cloud, mobile and/or embedded devices, e.g., frozen graph, saved model, TFLite model, etc. Scripts are under dpl/gadpter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compiling library</head><p>This step compiles the model inference engine library. It will compile models on different platforms and devices with the corresponding engine libraries, e.g., TensorFlow C++ library for CPU and GPU, TFLite library for Android and iOS, etc. After compiling the engine library, the step will compile DELTA-NN library. DELTA-NN abstracts low-level engine interface as a uniform interface, which supports multi-graphs inference, offline and online mixed inference, and multiple devices inference. These features are very useful in production. The engine library and DELTA-NN library are then combined into a composed binary for application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Testing</head><p>Testing is critical to an industrial level application. We release a Docker image 10 to perform unit test, integration test and system test. This process is automated through continuous integration (CI). A user can perform A/B test as part of CI. You can set up some metrics as trigger signals for continuous deployment when deploying a new model. We recommend using Docker to maintain the deployment process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarks</head><p>In order to evaluate the performance of the released models in DELTA, we provide experimental results for each task on public datasets as benchmarks. Note that, the purpose for the experiments is to provide solid and reliable benchmarks, therefore most experiments use those commonly-used models rather than the state-of-the-art algorithms. For each task, we compare our model with a similar model chosen from a highly-cited publication.</p><p>In DELTA, we organize all experiment settings together with the data processing scripts in the directory egs, similar to the usage in Kaldi <ref type="bibr" target="#b29">(Povey et al., 2011)</ref> or ESPnet <ref type="bibr" target="#b42">(Watanabe et al., 2018)</ref>. As a DELTA user, you can easily reproduce the experimental results by using the configure files under corresponding dataset directories. You can also extend the algorithms based on these models for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NLP benchmark</head><p>We present the comparison results of our implementation on NLP models. For each task, we choose a model from a popular reference as the baseline. We use DELTA to build the similar models as in the publications and use the same datasets and metrics for comparison. The baseline results are the number provided in the reference. <ref type="table" target="#tab_2">Table 1</ref> shows the comparison results on several different NLP tasks and our results are on a par with the baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Speech benchmark</head><p>We are working on benchmarks for speech tasks, and we will update as soon as available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present DELTA, a deep learning based NLP and speech processing platform. DELTA provides modularized components and simple pipelines for use and develop. It also contains many deployment supports for real production. We conducted experiments on public datasets to demonstrate the reliable and solid implementation.</p><p>For future work, we will implement more advanced models and present the state-of-the-art results for NLP and speech tasks using DELTA. We also interested in the implementation of parallel model training, automatic parameter tuning, production integration, etc. More importantly, DELTA is an open-sourced library and the contribution from the researchers and engineers in the community is highly appreciated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>High-level overview of DELTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Configuration of model training in DELTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performances of NLP models https://trec.nist.gov/data.html b https://webscope.sandbox.yahoo.com/#datasets c https://www.clips.uantwerpen.be/conll2003/ner/ d https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html e https://nlp.stanford.edu/projects/snli/ f https://github.com/deepmind/rc-data g Our model is slightly different from that of the reference, so a small performance gap is expected.</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell>Dataset</cell><cell>Metric</cell><cell cols="3">DELTA Baseline Reference</cell></row><row><cell cols="2">Sequence classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sentences</cell><cell>CNN</cell><cell>TREC a</cell><cell>Acc</cell><cell>92.2</cell><cell>91.2</cell><cell>Kim (2014)</cell></row><row><cell>Documents</cell><cell>HAN</cell><cell>Yahoo Answer b</cell><cell>Acc</cell><cell>75.1</cell><cell>75.8</cell><cell>Yang et al. (2016)</cell></row><row><cell>Sequence labeling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NER</cell><cell cols="2">BLSTM-CRF CoNLL2003 c</cell><cell>F1</cell><cell>84.6</cell><cell>84.7</cell><cell>Huang et al. (2015)</cell></row><row><cell>Multitask</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Intent</cell><cell cols="2">BLSTM-CRF ATIS d</cell><cell>Acc</cell><cell>97.4</cell><cell>98.2</cell><cell>Liu and Lane (2016)</cell></row><row><cell>Slot filling</cell><cell></cell><cell></cell><cell>F1</cell><cell>95.2</cell><cell>95.9</cell><cell></cell></row><row><cell cols="2">Pairwise modeling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NLI</cell><cell>LSTM</cell><cell>SNLI e</cell><cell>Acc</cell><cell>80.7</cell><cell>80.6</cell><cell>Bowman et al. (2016)</cell></row><row><cell>Seq2seq</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Summarization BLSTM</cell><cell cols="3">CNN/Daily Mail f RougeL 27.3</cell><cell>28.1 g</cell><cell>See et al. (2017)</cell></row><row><cell>Pretrained model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NER</cell><cell>ELMO</cell><cell>CoNLL2003</cell><cell>F1</cell><cell>92.2</cell><cell>92.2</cell><cell>Peters et al. (2018)</cell></row><row><cell>NER</cell><cell>BERT</cell><cell>CoNLL2003</cell><cell>F1</cell><cell>94.6</cell><cell>94.9</cell><cell>Devlin et al. (2019)</cell></row></table><note>a</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.tensorflow.org/api_docs/python/tf/data/Dataset 4 https://github.com/fxsjy/jieba 5 https://github.com/tensorflow/lingvo</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.tensorflow.org/guide/estimators 7 https://www.tensorflow.org/guide/saved_model</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.tensorflow.org/lite 9 https://github.com/tensorflow/serving 10 https://www.docker.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<editor>Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
	<note>Philemon Brakel, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the Conference on IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
	<note>ICASSP 2016</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
	<note>ICASSP 2014</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using deep neural network and extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international speech communication association</title>
		<meeting>the international speech communication association</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>INTERSPEECH 2014</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop of the Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information &amp; Knowledge Management (CIKM 2013)</title>
		<meeting>the ACM International Conference on Information &amp; Knowledge Management (CIKM 2013)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NIPS 2012)</title>
		<meeting>the Conference on Neural Information Processing Systems (NIPS 2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2016)</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NIPS 1990)</title>
		<meeting>the Conference on Neural Information Processing Systems (NIPS 1990)</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A self-attentive model with gate mechanism for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3824" to="3833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on International Speech Communication Association (INTERSPEECH 2016)</title>
		<meeting>the Conference on International Speech Communication Association (INTERSPEECH 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="685" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the Conference on Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient emotion recognition from speech using deep learning on spectrograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Satt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Rozenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Hoory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on International Speech Communication Association (INTERSPEECH 2017)</title>
		<meeting>the Conference on International Speech Communication Association (INTERSPEECH 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1089" to="1093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lingvo: a modular and scalable framework for sequence-to-sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">X-vectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NIPS 2014)</title>
		<meeting>the Conference on Neural Information Processing Systems (NIPS 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2015)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NIPS 2017)</title>
		<meeting>the Conference on Neural Information Processing Systems (NIPS 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
		<ptr target="http://arxiv.org/abs/1803.07416" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Generalized end-to-end loss for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Papir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.10467" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">Enrique</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsubasa</forename><surname>Ochiai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on International Speech Communication Association (INTERSPEECH 2018)</title>
		<meeting>the Conference on International Speech Communication Association (INTERSPEECH 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning alignment for multimodal emotion recognition from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on International Speech Communication Association</title>
		<meeting>the Conference on International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>INTERSPEECH 2019</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
