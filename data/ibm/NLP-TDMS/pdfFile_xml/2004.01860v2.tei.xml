<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deblurring by Realistic Blurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stenger</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rakuten Institute of Technology</orgName>
								<address>
									<addrLine>4 ACRV</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deblurring by Realistic Blurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing deep learning methods for image deblurring typically train models using pairs of sharp images and their blurred counterparts. However, synthetically blurring images do not necessarily model the genuine blurring process in real-world scenarios with sufficient accuracy. To address this problem, we propose a new method which combines two GAN models, i.e., a learning-to-Blur GAN (BGAN) and learning-to-DeBlur GAN (DBGAN), in order to learn a better model for image deblurring by primarily learning how to blur images. The first model, BGAN, learns how to blur sharp images with unpaired sharp and blurry image sets, and then guides the second model, DBGAN, to learn how to correctly deblur such images. In order to reduce the discrepancy between real blur and synthesized blur, a relativistic blur loss is leveraged. As an additional contribution, this paper also introduces a Real-World Blurred Image (RWBI) dataset including diverse blurry images. Our experiments show that the proposed method achieves consistently superior quantitative performance as well as higher perceptual quality on both the newly proposed dataset and the public GOPRO dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image deblurring is a classic problem in low-level computer vision, and it remains an active topic in the vision research community. Given a blurred image, which is corrupted by some unknown blur kernel or a spatially variant kernel, the task of (blind) image deblurring is to recover the sharp version of the original image, by reducing or removing the undesirable blur in the blurred image. Traditional deblurring methods handle this problem via estimating a blur kernel, through which a sharp version of the blurred input image can be recovered. Often, special characteristics of the blur kernel are assumed, and natural image priors are exploited in the deblurring process <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. However, estimating the optimal blur kernel is a difficult task and can therefore impair the overall performance.</p><p>Recently, deep learning methods, particularly convolutional neural networks (CNNs), have been applied to  <ref type="figure">Figure 1</ref>. (a) The differences between real and synthetic blurry images; (b) an illustration of learning to blur. Sharp images and random noises are fed into the BGAN G model to generate realistic blurry images via the RBL loss and the RWBI dataset.</p><p>tackle this task and obtained a remarkable success, e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. Existing deep learning methods focus on training deblurring models using paired blurry and sharp images. For example, Nah et al. <ref type="bibr" target="#b20">[21]</ref> propose a multiscale loss function to implement a coarse-to-fine processing pipeline. Tao et al. <ref type="bibr" target="#b35">[36]</ref> and Gao et al. <ref type="bibr" target="#b5">[6]</ref> improve the work by using shared network weights among different scales, achieving state-of-the-art performance.</p><p>However, many common effects are not adequately captured by the current deep learning models in the following sense. First, since in real-world scenarios, an image is captured during a time window (i.e., the exposure duration), the blurred image is in fact the integration of multi-frame instant and sharp snapshots <ref type="bibr" target="#b9">[10]</ref>. This can be formulated as</p><formula xml:id="formula_0">I B = g 1 T T t=0 I S(t) dt ,<label>(1)</label></formula><p>where I S is an instant sharp frame and I B is the blurry image. T is the exposure time period and g(Â·) is the Camera Response Function (CRF). In contrast, in conventional deblurring methods, blurry images used in the training set are often artificially synthesized by approximating the integration step with a simple averaging operation, as shown in Eq.</p><p>(2), where M is the number of frames:</p><formula xml:id="formula_1">I B g 1 M M t=1 I S[t] .<label>(2)</label></formula><p>Prior methods use M sharp frames I S[t] to replace the continuous sequence I S(t) and generate paired training data, avoiding the complexity of obtaining pairs of real blurry and sharp images. However, there is a clear gap between real blurry images and those artificially blurred images. <ref type="figure">Fig.  1</ref>(a) shows the generation of real and synthetic blurry images. Second, in real situations there are multi-fold factors (not limited to a single linear integration or summation) which can cause image blurs, for instance, camera shake, fast object motion, and small aperture with a wide depth of field. Many of these factors are very difficult to model precisely. To design a better deblurring algorithm, all these factors should be taken into consideration. If the real blurred images are different from the samples in the training set, the trained model may not perform well on the testing data. This observation inspires us to develop a new deblurring method which does not assume any particular blur type; rather such a method will be able to learn a blurring process in order to achieve better deblurring quality.</p><p>Specifically, in this paper we propose a method which contains a leaning-to-Blur GAN (BGAN) module and a learning-to-DeBlur GAN (DBGAN) module. BGAN and DBGAN are two complementary processes, in the sense that BGAN learns to mimic properties of real-world blurs by generating photo-realistic blurry images. This module is trained using unpaired sharp and blurry images, thus relaxing the requirement of needing paired data. Recently, Shaham et al. propose SinGAN <ref type="bibr" target="#b26">[27]</ref> to produce different images based on random noises, which inspires us to generate various blurry images given different noises. During the generation, sharp images are also fed into BGAN to make the generated blurry images bear the same content as the input images. The DBGAN module learns to recover sharp images from blurry images with real sharp and generated blurry images. We further employ a relativistic blur loss, which helps predict the probability that a real blurry image is relatively more realistic than a synthesized one. Finally, a Real-World Blurry Image (RWBI) dataset is created to help train the BGAN model and evaluate the performance of our proposed image deblurring model. <ref type="figure">Fig. 1(b)</ref> shows the process of learning realistic blur.</p><p>The contributions of this paper are three-fold: (1) We develop a new image deblurring framework which contains the process of image blurring and image deblurring. In contrast to previous deep learning methods which solely focus on image deblurring, our framework also considers image blurring, which generates realistic blurry images to help enhance the performance and robustness of image deblurring. <ref type="bibr" target="#b1">(2)</ref> In order to train the BGAN model and generate blurry images like those in the real world, a relativistic blur loss is introduced. We also contribute a real-world blurry dataset RWBI, which can be used for training an image blurring module and for evaluating deblurring models. (3) Experimental results show that the proposed method achieves not only the state-of-the-art quantitative performance on the public GOPRO benchmark, but also consistently superior perceptual quality on real-world blurry images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Our work in this paper is closely related to image blurring and image deblurring, which are briefly introduced as follows, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Blurring</head><p>Blur artifacts are caused by various factors. The blurring process can be mathematically formulated as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>,</p><formula xml:id="formula_2">I B = K * I S + N ,<label>(3)</label></formula><p>where I B and I S are blurry and sharp images, respectively. K is the unknown (blind) or known (non-blind) blur kernel and N is additive noise. For images with spatially varying blurs there are no camera response function (CRF) estimation techniques <ref type="bibr" target="#b34">[35]</ref>. Alternatively, the CRF can be approximated as the average of known CRFs as follows:</p><formula xml:id="formula_3">g(I S[i] ) = I S [i] 1 Î³ ,<label>(4)</label></formula><p>where Î³ is a parameter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image Deblurring</head><p>Early works use image priors, including total variation <ref type="bibr" target="#b2">[3]</ref>, a heavy-tailed gradient prior <ref type="bibr" target="#b27">[28]</ref>, or a hyper-Laplacian prior <ref type="bibr" target="#b14">[15]</ref>, which are typically applied to images in a coarse-to-fine manner. Recently, deep learning methods  <ref type="figure">Figure 2</ref>. The proposed framework and training process. This framework contains two main modules, a BGAN and a DBGAN. D and G denote discriminator and generator networks, respectively. The BGAN takes sharp images as input and outputs realistic blurry images, which are then fed into the DBGAN in order to learn to deblur. During the inference stage, only the DBGAN is applied. have achieved a great success in the areas of object recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref> and image reconstruction including video deblurring <ref type="bibr" target="#b44">[45]</ref>, video dehazing <ref type="bibr" target="#b24">[25]</ref>, and other GAN-based generation tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref>. For image deblurring, Sun et al. <ref type="bibr" target="#b33">[34]</ref> propose a CNN-based model to estimate a kernel and remove non-uniform motion blur. Chakrabarti <ref type="bibr" target="#b1">[2]</ref> uses a network to compute estimations of sharp images that are blurred by an unknown motion kernel. Nah et al. <ref type="bibr" target="#b20">[21]</ref> propose a multi-scale loss function to apply a coarse-to-fine strategy and an adversarial loss. Kupyn et al. propose DeblurGAN <ref type="bibr" target="#b15">[16]</ref> and DeblurGAN-v2 <ref type="bibr" target="#b16">[17]</ref> to remove blur kernels based on adversarial learning. Further, RNN-based methods have been proposed for image deblurring. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> propose a spatially variant neural network, which includes three CNNs and one RNN. Tao et al. <ref type="bibr" target="#b35">[36]</ref> propose an SRN-DeblurNet, which includes one LSTM and CNNs for multi-scale image deblurring. Shen et al. <ref type="bibr" target="#b29">[30]</ref> introduce a human-aware deblurring method to remove blur from foreground humans and background. Gao et al. <ref type="bibr" target="#b5">[6]</ref> propose a nested skip connection structure which achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat</head><p>All these above neural network based methods focus on solely recovering sharp images from blurry images (i.e., image deblurring), rather than better modeling the blurring process itself. Pan et al. <ref type="bibr" target="#b22">[23]</ref> try to generate blurry images in their algorithm based on deblurred results, and then calculate the difference between the generated blurry images and "GT" blurry images to update models. Therefore, these methods actually propose a new loss function, rather than data augmentation. The idea of data augmentation has been widely applied in different fields <ref type="bibr" target="#b30">[31]</ref>, like face verification <ref type="bibr" target="#b19">[20]</ref> and SR <ref type="bibr" target="#b0">[1]</ref>. For deblurring, one of the most relevant works is from the field of video deblurring <ref type="bibr" target="#b3">[4]</ref>. However, it generates blurred images based videos and it does not consider to generate realistic blurry images based on real blurred ones. More recently, a SinGAN <ref type="bibr" target="#b26">[27]</ref> model is proposed to learn how to generate different related images from one input image based on random noises. Inspired by this method, a GAN-based model is proposed to generate various blurry images based on different noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deblurring by Blurring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>Our framework contains two primary modules. Similar to prior image deblurring works, our framework includes a learning-to-DeBlur GAN (DBGAN) module, which is trained on paired sharp and blurry images to recover sharp images from blurry images. The paired sharp-blurry images are obtained from the BGAN module. The BGAN is trained on unpaired data, where sharp images come from a public dataset, while the blurry images come from a new real-world blurry dataset. <ref type="figure">Fig. 2</ref> shows the overall architecture of the proposed framework.</p><p>We further enhance the standard GAN model with a relativistic blur loss. In traditional GAN-based models for image deblurring, the discriminator D estimates the probability that the input data is real, and the generator G is trained to increase the probability that the generated data looks real. The developed relativistic blur loss estimates the probability that the given real-world blurry images are more realistic than the generated blurry images.</p><p>In the training stage, sharp images are input into the BGAN generator and its output is fed into the DBGAN to learn how to deblur. The generators in the DBGAN and BGAN modules generate corresponding images, and the discriminators conduct discrimination to create more realistic synthetic images. During the inference stage, only the DBGAN generator network is required for the image deblurring task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BGAN: Learning to Blur</head><p>The BGAN module is the primary difference from other neural network based methods for image deblurring. Similar to other GAN based models, the BGAN consists of a generator network and a discriminator network. In this section, we first discuss its architecture and loss functions.</p><p>BGAN Generator. The input to the BGAN generator is a sharp image from a public dataset. Given the numerous possible factors that can cause undesired blurring artifacts, we concatenate the input image with a noise map to model the different conditions. To obtain the noise map, we sample a noise vector of length 4 from a normal distribution and duplicate it 128 Ã 128 times in the spatial dimension to obtain a 4 Ã 128 Ã 128 noise map as in <ref type="bibr" target="#b45">[46]</ref>. In this way, we can generate various blurry images based on one sharp image. The network architecture consists of one convolutional layer, 9 residual blocks (ResBlocks) <ref type="bibr" target="#b8">[9]</ref> and extra two convolutional layers. Each ResBlock consists of 5 convolutional layers (64Ã3Ã3) and 4 ReLU activations. There is also a skip connection in each ResBlock, connecting the input and output features (refer to <ref type="figure">Fig. 2</ref>). The output of our BGAN generator is a blurry image of the same size as the sharp input image.</p><p>BGAN Discriminator. The input to the BGAN discriminator is the output of the BGAN generator. Its architecture is the same as the VGG19 network <ref type="bibr" target="#b31">[32]</ref>, and its output is the probability of the blurry image being classified as real.</p><p>BGAN Loss. The generator and discriminator of the BGAN are trained with a perceptual loss and an adversarial loss. Specifically, the perceptual loss is calculated based on the synthesized blurry images from the proposed BGAN and images taken from a public dataset. In this way, they can have similar contents. The adversarial loss is calculated between the synthesized and real blurry images. The real blurry images are taken from our newly created dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">DBGAN: Learning to Deblur</head><p>The BGAN module aims to mimic the real-world blurry images and cover as many blur cases as possible. Its goal is to drive the DBGAN module to be more effective in recovering sharp images from blurry images. In the following, we present the architecture and loss of the DBGAN.</p><p>DBGAN Generator. The input to the DBGAN generator is a blurry image. Many approaches have been proposed for this task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. When we design the DBGAN generator, we adopt their advantages. Specifically, we remove the batch normalization layers, which have been shown to increase the computational complexity and de-crease the performance on different tasks <ref type="bibr" target="#b20">[21]</ref>. Secondly, we use additive residual layers in each block, which combine multi-level residual networks and dense connections <ref type="bibr" target="#b10">[11]</ref>. The BGAN consists of one convolutional layer, 16 residual blocks (ResBlocks) <ref type="bibr" target="#b8">[9]</ref> and two more convolutional layers. The kernel size in ResBlocks is 63 Ã 3 Ã 3. The details can be referred to <ref type="figure">Fig. 2</ref>. The output of the DBGAN generator is the desired sharp image.</p><p>DBGAN Discriminator. Similar to the BGAN discriminator, the DBGAN also adopts the VGG19 network <ref type="bibr" target="#b31">[32]</ref> as its discriminator. The output of this model is the probability of the given sharp images looking realistic.</p><p>DBGAN Loss. Like the BGAN module, the proposed DBGAN model is trained using a perceptual loss and an adversarial loss. We also use an L 1 loss to update the DB-GAN. All the three types of loss functions are calculated based on the generated and real sharp images, so the DB-GAN is trained on paired images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relativistic Blur Loss</head><p>In this section, we describe a Relativistic Blur Loss (RBL) and other loss functions which are used to train our framework.</p><p>Perceptual Loss. In contrast to previous image deblurring methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref>, the proposed framework applies a perceptual loss L perceptual to update models. Note that Johnson et al. <ref type="bibr" target="#b12">[13]</ref> use a similar loss. However, in contrast to their work, we calculate the perceptual loss based on features before rather than after the ReLU activation layer.</p><p>Content Loss. The Mean Squared Error (MSE) is widely used as a loss function for image restoration methods. Based on the MSE, the content loss between groundtruth and generated images is calculated.</p><p>Relativistic Blur Loss. In order to drive the BGAN generator to produce blurry images similar to the real-world images, we develop a relativistic blur loss based on <ref type="bibr" target="#b13">[14]</ref> to update the model. The BGAN generator parameters are updated in order to fool the BGAN discriminator. The adversarial loss D is formulated as:</p><formula xml:id="formula_4">D(I real blurry ) = Ï(C(I real blurry )) â 1, D(I f ake blurry ) = D(G(I real sharp )) = Ï(C(G(I real sharp ))) â 0 ,<label>(5)</label></formula><p>where D(Â·) is the probability that the input is a real image. C(Â·) is the feature representation before activation and Ï(Â·) is the sigmoid function. The generator G is trained to increase the probability that synthesized images are real. Real and synthesized images are labeled as 0 or 1 by D, respectively. As <ref type="figure" target="#fig_1">Fig. 3 (a)</ref> shows, the effect of G is to transfer real sharp images to blurry images and "push" these generated images (label=0) closer to real blurry images (label=1). However, during the training stage, only the second part of Eq. <ref type="formula" target="#formula_4">(5)</ref> The RBL not only increases the probability that generated images look real (0 â 0.5, which is labeled as "Push"), but also simultaneously decreases the output probability that real images are real (1 â 0.5, which is labeled as "Pull"). (c) In order to increase the variations of blurry images, different blurry images are used to model the different types of blurs in the real world.</p><p>the parameters of generator G, while the first part is used to update the discriminator D model rather than the generator G <ref type="bibr" target="#b20">[21]</ref>. In fact, a powerful generator G should also decrease the probability that real blurry images are real. This is because a realistic synthesized image labeled as fake is similar to real one, and will thus fool the D model to learn to distinguish real and fake in the training stage. Based on this idea, we add D(I real blurry ) into the process of learning G in BGAN. Specially, a Relativistic Blur Loss (RBL) is developed to help calculate whether a real blurry image is more realistic than the synthesized blurry image. The formulation of Eq. (5) is modified to Ï(C(I real blurry ) â E(C(G(I input )))) â 1 , Ï(C(I f ake blurry ) â E(C(I real blurry ))) â 0 ,</p><p>where E(Â·) denotes the averaging operation over images in one batch. <ref type="figure" target="#fig_1">Fig. 3 (b)</ref> shows the aim of RBL. Although the goal is still to generate realistic blurry images which are similar to real-world ones, the optimization objective is different. RBL aims to update G to generate synthetic images which are near 0.5, and meanwhile to fool the D model, making it difficult to distinguish real images from fake ones. In this way, the probability of real blurry images predicted by D is also near to 0.5. We term the effects as "push" and "pull", respectively, which can complement each other to update the generator G. As <ref type="figure" target="#fig_1">Fig. 3</ref> shows, the sharp and blurry images can be regarded as two different domains. In order to rapidly generate blurry images and utilize prior research results of generating blurry images, we first train our BGAN model with artificially blurry images as <ref type="figure" target="#fig_1">Fig. 3(b)</ref> shows. We then add other types of blurry images to increase the variations of the produced blurry images based on Eq. (6) to cover different conditions in the real world, which is shown in <ref type="figure" target="#fig_1">Fig. 3(c)</ref>. Based on Eq. (6) and <ref type="figure" target="#fig_1">Fig. 3</ref>, our RBL, which is used in the BGAN generator, can be represented as</p><formula xml:id="formula_6">LRBL = â[log(Ï(C(I real blurry ) â E(C(G(I input ))))) + log(1 â (Ï(C(G(I input )) â E(C(I real blurry ))))].<label>(7)</label></formula><p>Based on the RBL, we apply a Relativistic Deblur loss (RDBL) in the DBGAN generator as LRDBL = â[log(Ï(C(I real sharp ) â E(C(G(I input ))))) + log(1 â (Ï(G(I input )) â E(C(I real sharp )))))].</p><p>Balance of Different Loss Functions. During the training stage, the loss functions for DBGAN and BGAN are combinations of different terms using a weighted fusion,</p><formula xml:id="formula_8">L BGAN = L perceptual + Î² Â· L RBL ,<label>(9)</label></formula><formula xml:id="formula_9">L DBGAN = L perceptual +Î±Â·L content +Î² Â·L RDBL .<label>(10)</label></formula><p>In order to balance the different kinds of losses, we use two hyper-parameters Î± and Î² to yield the final loss L for BGAN and DBGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>GOPRO Dataset. We evaluate the performance of our model on the public GOPRO dataset <ref type="bibr" target="#b20">[21]</ref>, which contains 3, 214 image pairs. The training and testing sets include 2, 103 and 1, 111 pairs, respectively. Existing methods convolve sharp images with a blur kernel <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> to synthesized blurry images. These synthetic blurry images are different from real ones captured by camera. In order to model more realistic blurry conditions, in the GOPRO dataset, sharp images with a high-speed camera and synthesize blurry images were collected by averaging these sharp images from videos.</p><p>RWBI Dataset. In order to train our BGAN model and evaluate the performance of deblurring models, we collect a Real-World Blurry Image dataset. The blurry images are captured with different hand-held devices, including an iPhone XS, a Samsung S9 Plus, a Huawei P30 Pro and a Go-Pro Hero 5 Black. Multiple devices are used to reduce the bias towards one specific device which may capture blurry images with unique characteristics. The dataset contains 22 different sequences of 3, 112 diverse blurry images.</p><p>We compare the performance of the proposed method with the state-of-the-art methods on the public GOPRO dataset quantitatively and qualitatively. As there is no ground truth of the developed RWBI dataset, we only conduct a qualitative comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>When training BGAN and DBGAN, we use a Gaussian distribution with zero mean and a standard deviation of 0.01 to initialize the weights. In each iteration, we update all the weights after learning a mini-batch of size 4. To augment the training set, we crop a 128Ã128 patch at any location of an image. To further increase the number of training samples, we also randomly flip frames. We use a learning rate annealing scheme, starting with a value of 10 â4 and reducing it to 10 â6 after the training loss gets converged. The hyper-parameters Î± and Î² are set as 0.005 and 0.01, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we conduct experiments to investigate the effectiveness of different components of our model. The proposed model has three variants:</p><p>(1) DBGAN is the model for learning to deblur. Its input is a blurry image and the output is a deblurred image. Similar to previous GAN-based deblurring methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>, this model contains generator and discriminator networks. Thus its loss function is a combination of L percetpual , L content and L RDBL with weights Î± and Î². The final loss function is shown in Eq. (10).</p><p>(2) DBGAN(-) has the same architecture as DBGAN. Differently, we replace the L RDBL with a traditional adversarial loss as <ref type="bibr" target="#b20">[21]</ref>. Namely, the training process does not contain the relativistic loss functions. It is trained based on L percetpual , L content and the traditional adversarial loss.</p><p>(3) DBGAN(+) is our full method. It has a similar architecture to DBGAN with the main difference of additionally employing the BGAN module during the fine-tuning stage. Specially, we firstly train a DBGAN model as above, and then blurry images generated by the BGAN model are randomly added into the training samples to enhance the learning performance of DBGAN. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the examples of different blurry images produced by the proposed BGAN. <ref type="figure" target="#fig_3">Fig. 5</ref> shows results of the qualitative comparison. The proposed DBGAN outperforms the DBGAN(-), which shows the effectiveness of the relativistic loss function for image deblurring. With the learning-to-blur module, DB-GAN(+) achieves a further improvement over DBGAN, suggesting the benefits of learning to deblur by learning to blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Existing Methods</head><p>To verify the effectiveness of our model, we compare its performance with several state-of-the-art approaches on the GOPRO dataset quantitatively and qualitatively. <ref type="bibr" target="#b11">[12]</ref> by Kim et al. is a traditional method to handle complex dynamic blurring images. For deep learning methods, Sun et al. <ref type="bibr" target="#b33">[34]</ref> use a CNN network to estimate blur kernels and apply traditional deconvolution methods to synthesize sharp images. Nah et al. <ref type="bibr" target="#b20">[21]</ref> propose a multi-scale function to model the coarse-to-fine approach. Similar to <ref type="bibr" target="#b20">[21]</ref>, Tao et al. <ref type="bibr" target="#b35">[36]</ref> propose a multi-scale network via sharing network weights between different scales to recover sharp images. In addition, Shen et al. <ref type="bibr" target="#b29">[30]</ref> introduce a human-aware deblurring method and Gao et al. <ref type="bibr" target="#b5">[6]</ref> propose a nested skip connection structure and achieve state-of-the-art performance. <ref type="table" target="#tab_2">Table 1</ref> shows the results of the quantitative comparison. DBGAN outperforms most of previous methods, while DB-GAN(+) achieves the state-of-the-art performance due to the framework of learning to deblur by learning to blur. For fair comparison, all values refer to the performance achieved by single model trained on the GOPRO dataset. Qualitative comparisons with some state-of-the-art methods are shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, demonstrating that our method consistently achieves better visual quality results. <ref type="figure">Fig. 7</ref> compare the proposed method with Gao et al. * <ref type="bibr" target="#b5">[6]</ref>. * means this model is trained with extra pairs of images.  <ref type="figure">Figure 7</ref>. Comparison with <ref type="bibr" target="#b5">[6]</ref>, which is trained with extra pairs of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance in Real-World Scenarios</head><p>To validate the effectiveness of our method, we compare the performance of our approach with several state-of-theart methods on the RWBI dataset of real-world blurry images. <ref type="figure" target="#fig_6">Fig. 8</ref> shows qualitative results of different models. The blurry images in the first column are from the RWBI dataset, and the images in the following columns are the results of Nah et al. <ref type="bibr" target="#b20">[21]</ref>, Tao et al. <ref type="bibr" target="#b35">[36]</ref> and the proposed DBGAN(+). <ref type="figure" target="#fig_6">Fig. 8</ref> shows that our method achieves better performance on real-world blurry images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has presented a new framework which firstly learns how to transfer sharp images to realistic blurry images via a learning-to-blur GAN (BGAN) module. This framework trains a learning-to-deblur GAN (DBGAN) module to learn how to recover a sharp image from a blurry image. In contrast to prior work which solely focuses on learning to deblur, our method learns to realistically synthesize blurring effects using unpaired sharp and blurry images. In order to generate more realistic blurred images, a relativistic blur loss is employed to help the BGAN module reduce the gap between synthesized blur and real blur. In addition, a RWBI dataset is built to help train and test deblurring models. The Experimental results have demonstrated that our method not only yields results of consistently superior perceptual quality, but also outperforms state-of-the-art methods quantitatively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>, i.e., D(I f ake blurry ) = D(G(I real sharp )) â 0 , updates An illustration of the Relativistic Blur Loss (RBL). Real and synthesized images are labeled as 1 and 0, respectively. (a) A traditional loss function is used to update the generator to create blurry images (label=0) which are similar to real ones (label=1). (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Synthesized blurry images. Examples of different blurry images created by the proposed BGAN. The first column shows input sharp images, and the next three columns are the produced blurred images used to train the DBGAN(+).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative ablation results. Examples of deblurred images generated by the proposed framework with different model structures. The first column shows input blurred images, and the next three columns are the deblurred images produced by DBGAN(-), DBGAN and DBGAN(+), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Comparison with state-of-the-art deblurring methods. From left to right: blurry images, results of Nah et al. [21], Tao et al. [36] and the proposed DBGAN(+) method. The improvement is clearly visible in the magnified patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) The blurry image (b) Gao et al. * [6] (c) Ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Performance comparison on real-world blurry images. From left to right: blurry images, results of Nah et al. [21], Tao et al. [36] and the proposed DBGAN(+) method. The improvement is clearly visible in the magnified patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the GOPRO Large dataset. Method Kim et al. Sun et al. Nah et al. Tao et al. Shen et al. Gao et al. DBGAN DBGAN(+)</figDesc><table><row><cell>PSNR</cell><cell>23.64</cell><cell>24.64</cell><cell>29.08</cell><cell>30.10</cell><cell>30.26</cell><cell>30.92</cell><cell>30.43</cell><cell>31.10</cell></row><row><cell>SSIM</cell><cell>0.8239</cell><cell>0.8429</cell><cell>0.9135</cell><cell>0.9323</cell><cell>0.940</cell><cell>0.9421</cell><cell>0.9372</cell><cell>0.9424</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is funded in part by the ARC Centre of Excellence for Robotics Vision (CE140100016), ARC-Discovery (DP 190102261) and ARC-LIEF (190100080) grants, as well as a research grant from Baidu on autonomous driving. The authors gratefully acknowledge the GPUs donated by NVIDIA Corporation. We thank all anonymous reviewers and ACs for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Total variation blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiu-Kwong</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reblur2deblur: Deblurring videos via self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast motion deblurring. TOG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blur-kernel estimation from spectral irregularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single image deblurring using motion density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongjoo</forename><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyper-laplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JiÅÃ­</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno>WACV, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Transferring cross-domain knowledge for video sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03703,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring disentangled feature representation beyond face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deblurring text images via l0-regularized intensity and gradient prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Physics-based generative adversarial models for image restoration and beyond. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep video dehazing with semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to deblur. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep semantic face deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nonlinear camera response functions and image deblurring: Theoretical analysis and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Foreground-aware image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cousin network guided sketch recognition via latent attribute warehouse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning joint gait representation via quintuplet loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial spatio-temporal learning for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
