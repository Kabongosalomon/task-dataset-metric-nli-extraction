<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-17">17 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-17">17 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Domain Adaptation</term>
					<term>Semantic segmentation</term>
					<term>Near IR Dataset</term>
					<term>VAE equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prashant Pandey [0000−0002−6594−9685] , Aayush Kumar Tyagi [0000−0002−3615−7283] , Sameer Ambekar [0000−0002−8650−3180] , and Prathosh AP [0000−0002−8699−5760]</p><p>Abstract. Segmentation of the pixels corresponding to human skin is an essential first step in multiple applications ranging from surveillance to heart-rate estimation from remote-photoplethysmography. However, the existing literature considers the problem only in the visible-range of the EM-spectrum which limits their utility in low or no light settings where the criticality of the application is higher. To alleviate this problem, we consider the problem of skin segmentation from the Near-infrared images. However, Deep learning based state-of-the-art segmentation techniques demands large amounts of labelled data that is unavailable for the current problem. Therefore we cast the skin segmentation problem as that of target-independent Unsupervised Domain Adaptation (UDA) where we use the data from the Red-channel of the visible-range to develop skin segmentation algorithm on NIR images. We propose a method for targetindependent segmentation where the 'nearest-clone' of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. We prove the existence of 'nearest-clone' and propose a method to find it through an optimization algorithm over the latent space of a Deep generative model based on variational inference. We demonstrate the efficacy of the proposed method for NIR skin segmentation over the state-of-the-art UDA segmentation methods on the two newly created skin segmentation datasets in NIR domain despite not having access to the target NIR data. Additionally, we report state-of-the-art results for adaption from Synthia to Cityscapes which is a popular setting in Unsupervised Domain Adaptation for semantic segmentation. The code and datasets are available at https://github.com/ambekarsameer96/GLSS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>Human skin segmentation is the task of finding pixels corresponding to skin from images or videos. It serves as a necessary pre-processing step for multiple applications like video surveillance, people tracking, human computer interaction, face detection and recognition, facial gesture detection and monitoring heart rate and respiratory rate <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr">39]</ref> using remote photoplethysmography. Most of the research efforts on skin detection have focused on visible spectrum images because of the challenges that it poses including, illumination change, ethnicity change and presence of background/clothes similar to skin colour. These factors adversely affect the applications where skin is used as conjugate information. Further, the algorithms that rely on visible spectrum images cannot be employed in the low/no light conditions especially during night times where the criticality of the application like human detection is higher. These problems which are encountered in visible spectrum domain can be overcome by considering the images taken in the Near-infrared (NIR) domain <ref type="bibr" target="#b24">[25]</ref> or hyper spectral imaging <ref type="bibr" target="#b35">[36]</ref>. The information about the skin pixels is invariant of factors such as illumination conditions, ethnicity etc., in these domains. Moreover, most of the surveillance cameras that are used world-wide are NIR imaging devices. Thus, it is meaningful to pursue the endeavour of detecting the skin pixels from the NIR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Problem setting and contributions</head><p>The task of detection of skin pixels from an image is typically cast as a segmentation problem. Most of the classical approaches relied on the fact that the skin-pixels have a distinctive color pattern <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> compared to other objects. In recent years, harnessing the power of Deep learning, skin segmentation problem has been dealt with using deep neural networks that show significant performance enhancement over the traditional methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr">41]</ref>, albeit generalization across different illuminations still remains a challenge. While there exists sufficient literature on skin segmentation in the visible-spectrum, there is very little work done on segmenting the skin pixels in the NIR domain. Further, all the state-of-the-art Deep learning based segmentation algorithms demand largescale annotated datasets to achieve good performance which is available in the case of visible-spectrum images but not the NIR images. Thus, building a fullysupervised skin segmentation network from scratch is not feasible for the NIR images because of the unavailability of the large-scale annotated data. However, the underlying concept of 'skin-pixels' is the same across the images irrespective of the band in which they were captured. Additionally, the NIR and the Redchannel of the visible-spectrum are close in terms of their wavelengths. Owing to these observations, we pose the following question in this paper -Can the labelled data (source) in the visible-spectrum (Red-channel) be used to perform skin segmentation in the NIR domain (target) <ref type="bibr">[38]</ref>?</p><p>We cast the problem of skin segmentation from NIR images as a targetindependent Unsupervised Domain Adaptation (UDA) task <ref type="bibr" target="#b36">[37]</ref> where we consider the Red-channel of the visible-spectrum images as the source domain and NIR images as the target domain. The state-of-the-art UDA techniques demand access to the target data, albeit unlabelled, to adapt the source domain features to the target domain. In the present case, we do not assume existence of any data from the target domain, even unlabelled. This is an important desired attribute which ensures that a model trained on the Red-channel does not need any retraining with the data from NIR domain. The core idea is to sample the 'nearest-clone' in the source domain to a given test image from the target domain. This is accomplished through a simultaneous sampling-cum-optimization procedure using a latent-variable deep neural generative network learned on the source distribution. Thus, given a target sample, its 'nearest-clone' from the source domain is sampled and used as a proxy in the segmentation network trained only on the samples of the source domain. Since the segmentation network performs well on the source domain, it is expected to give the correct segmentation mask on the 'nearest-clone' which is then assigned to the target image. Specifically, the core contributions of this work are listed as follows:</p><p>1. We cast the problem of skin segmentation from NIR images as a UDA segmentation task where we use the data from the Red-channel of the visiblerange of the EM-spectrum to develop skin segmentation algorithm on NIR images. 2. We propose a method for target-independent segmentation where the 'nearestclone' of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. 3. We theoretically prove the existence of the 'nearest-clone' given that it can be sampled from the source domain with infinite data points. 4. We develop a joint-sampling and optimization algorithm using variational inference based generative model to search for the 'nearest-clone' through implicit sampling in the source domain. 5. We demonstrate the efficacy of the proposed method for NIR skin segmentation over the state-of-the-art UDA segmentation methods on the two newly created skin segmentation datasets in NIR domain. The proposed method is also shown to reach SOTA performance on standard segmentation datasets like Synthia [42] and Cityscapes <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we first review the existing methods for skin segmentation in the visible-range followed by a review of UDA methods for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skin Segmentation in Visible-range</head><p>Methods for skin segmentation can be grouped into three categories, i.e. (i) Thresholding based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">40]</ref>, (ii) Traditional machine learning techniques to learn a skin color model <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">52]</ref>, (iii) Deep learning based methods to learn an end-to-end model for skin segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">43,</ref><ref type="bibr">51]</ref>. The thresholding based methods focus on defining a specified range in different color representation spaces like (HSV) <ref type="bibr" target="#b34">[35]</ref> and orthogonal color space (YCbCr) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref> to differentiate skin pixels. Traditional machine learning can be further divided into pixel based and region based methods. In pixel based methods, each pixel is classified as skin or non-skin without considering the neighbours [46] whereas region based approaches use spatial information to identify similar regions <ref type="bibr" target="#b8">[9]</ref>. In recent years, Fully convolutional neural networks (FCN) are employed to solve the problem <ref type="bibr" target="#b30">[31]</ref>.</p><p>[41] proposed a UNet architecture, consisting of an encoderdecoder structure with backbones like InceptionNet[44] and ResNet <ref type="bibr" target="#b14">[15]</ref>. Holistic skin segmentation <ref type="bibr" target="#b12">[13]</ref> combine inductive transfer learning and UDA. They term this technique as cross domain pseudo-labelling and use it in an iterative manner to train and fine tune the model on the target domain. <ref type="bibr" target="#b15">[16]</ref> propose mutual guidance to improve skin detection with the usage of body masks as guidance. They use dual task neural network for joint detection with shared encoder and two decoders for detecting skin and body simultaneously. While all these methods offer different advantages, they do not generalize to low-light settings with NIR images, which we aim to solve through UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain Adaptation for semantic segmentation</head><p>Unsupervised Domain Adaptation aims to improve the performance of deep neural networks on a target domain, using labels only from a source domain. UDA for segmentation task can be grouped into following categories:</p><p>Adversarial training based methods: These methods use the principles of adversarial learning <ref type="bibr" target="#b16">[17]</ref>, which generally consists of two networks. One predicts the segmentation mask of the input image coming from either source or target distribution while the other network acts as discriminator which tries to predict the domain of the images. AdaptSegNet [47] exploits structural similarity between the source and target domains in a multi-level adversarial network framework. ADVENT [48] introduce entropy-based loss to directly penalize low-confident predictions on target domain. Adversarial training is used for structural adaptation of the target domain to the source domain. CLAN <ref type="bibr" target="#b31">[32]</ref> considers category-level joint distribution and aligns each class with an adaptive adversarial loss. They reduce the weight of the adversarial loss for category-level aligned features while increasing the adversarial force for those that are poorly aligned. DADA [49] uses the geometry of the scene by simultaneously aligning the segmentation and depth-based information of source and target domains using adversarial training.</p><p>Feature-transformation based methods: These methods are based on the idea of learning image-level or feature-level transformations between the source and the target domains. CyCADA <ref type="bibr" target="#b0">[1]</ref> adapts between domains using both generative image space alignment and latent representation space alignment. Image level adaptation is achieved with cycle loss, semantic consistency loss and pixellevel GAN loss while feature level adaptation employs feature-level GAN loss and task loss between true and predicted labels. DISE <ref type="bibr" target="#b4">[5]</ref> aims to discover a domain-invariant structural feature by learning to disentangle domain-invariant structural information of an image from its domain-specific texture information. BDL <ref type="bibr" target="#b26">[27]</ref> involves two separated modules a) image-to-image translation model b) segmentation adaptation model, in two directions namely 'translationto-segmentation' and 'segmentation-to-translation'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>Most of the UDA methods assume access to the unlabelled target data which may not be available at all times. In this work, we propose a UDA segmentation technique by learning to find a data point from the source that is arbitrarily close (called the 'nearest-clone') to a given target point so that it can used as a proxy in the segmentation network trained only on the source data. In the subsequent sections, we describe the methodology used to find the 'nearest-clone' from the source distribution to a given target point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Existence of nearest source point</head><p>To start with, we show that for a given target data point, there exists a corresponding source data point, that is arbitrarily close to, provided that infinite data points can be sampled from the source distribution. Mathematically, let P s (x) denotes the source distribution and P t (x) denotes any target distribution that is similar but not exactly same as P s (Red-channel images are source and NIR images are target). Let the underlying random variable on which P s and P t are defined form a separable metric space {X, D} with D being some distance metric. Let S n = {x 1 , x 2 , x 3 , ...., x n } be i.i.d points drawn from P s (x) andx be a point from P t (x). With this, the following lemma shows the existence of the 'nearest-clone'. Lemma 1. Ifx S ∈ S n is the point such that D{x,x S } &lt; D{x, x} ∀x ∈ S n , as n → ∞ (in S n ),x S converges tox with probability 1.</p><p>Proof. Let B r (x) = {x : D{x, x} ≤ r} be a closed ball of radius r aroundx under the metric D. Since X is a separable metric space <ref type="bibr" target="#b11">[12]</ref>,</p><formula xml:id="formula_0">Prob B r (x) Br(x) P s (x) dx &gt; 0, ∀r &gt; 0,<label>(1)</label></formula><p>With this, for any δ &gt; 0, the probability that none of the points in S n are within the ball B δ (x) of radius δ is:</p><formula xml:id="formula_1">Prob min i=1,2..,n D{x i ,x} ≥ δ = 1 − Prob B δ (x) n<label>(2)</label></formula><p>Therefore, the probability ofx S (the closest point tox) lying within B δ (x) is:</p><formula xml:id="formula_2">Prob x S ∈ B δ (x) = 1 − 1 − Prob B δ (x) n (3) = 1 as n → ∞<label>(4)</label></formula><p>Thus, given any infinitesimal δ &gt; 0, with probability 1, ∃x S ∈ S n ('nearestclone') that is within δ distance fromx as n → ∞ While Lemma 1 guarantees the existence of a 'nearest-clone', it demands the following two conditions:</p><p>-It should be possible to sample infinitely from the source distribution P s .</p><p>-It should be possible to search for the 'nearest-clone' in the P s , for a target samplex under the distance metric D.</p><p>We propose to employ Variational Auto-encoding based sampling models on the source distribution to simultaneously sample and find the 'nearest-clone' through an optimization over the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational Auto-Encoder for source sampling</head><p>Variational Auto-Encoders (VAEs) <ref type="bibr" target="#b23">[24]</ref> are a class of latent-variable generative models that are based on the principles of variational inference where the variational distribution, Q φ (z|x) is used to approximate the intractable true posterior P θ (z|x). The log-likelihood of the observed data is decomposed into two terms, The latent vector z is initialized with a random sample drawn from N(0, 1). Iterations over the latent space z are performed to minimize the L ssim loss between the input target imagẽ x T and the predicted target imagex (blue dotted lines). After convergence of L ssim loss, the optimal latent vectorz S , generates the closest clonex S which is used to predict the mask ofx T using the segmentation network S ψ .</p><p>an irreducible non-negative KL-divergence between P θ (z|x) and Q φ (z|x) and the Evidence Lower Bound (ELBO) term which is given by Eq. 5.</p><formula xml:id="formula_3">ln P θ (x) = L(θ, φ) + D KL [Q φ (z|x)||P θ (z|x)]<label>(5)</label></formula><p>where,</p><formula xml:id="formula_4">L(θ, φ) = E Q φ (z|x) [ln (P θ (x|z))] − D KL [Q φ (z|x)||P θ (z)]<label>(6)</label></formula><p>The non-negative KL-term in Eq. 5 is irreducible and thus, L(θ, φ) serves as a lower bound on the data log-likelihood which is maximized in a VAE by parameterizing Q φ (z|x) and P φ (x|z) using probabilistic encoder g φ (that outputs the parameters µ z and σ z of a distribution) and decoder h θ neural networks. The latent prior P θ (z) is taken to be arbitrary prior on z which is usually a 0 mean and unit variance Gaussian distribution. After training, the decoder network is used as a sampler for P s (x) in a two-step process: (i) Sample z ∼ N(0, I), (ii) Sample x from P θ (x|z). The likelihood term in Eq. 5 is approximated using norm based losses and it is known to result in blurry images. Therefore, we use the perceptual loss <ref type="bibr" target="#b20">[21]</ref> along with the standard norm based losses. Further, since the edges in images are generally invariant across the source and target domains, we extract edge of the input image and use it in the decoder of the VAE via a skip connection, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. This is shown to reduce the blur in the generated images. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts the entire VAE architecture used for training on the source data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VAE Latent Search for finding the 'nearest-clone'</head><p>As described, the objective of the current work is to search for the nearest point in the source distribution, given a sample from target distribution. The decoder h θ of a VAE trained on the source distribution P s (x), outputs a new sample using the Normally distributed latent sample as input. That is,</p><formula xml:id="formula_5">∀z ∼ N(0, I),x = h θ (z) ∼ P s (x)<label>(7)</label></formula><p>With this, our goal is to find the 'nearest-clone' to a given target sample. That is, given ax ∼ P t (x), findx S as follows:</p><formula xml:id="formula_6">x S = h θ (z S ) : D{x,x S } &lt; D{x,x} ∀x = h θ (z) ∼ P s (x)<label>(8)</label></formula><p>Since D is pre-defined and h θ (z) is a deep neural network, findingx S can be cast as an optimization problem over z with minimization of D as the objective.</p><formula xml:id="formula_7">Mathematically,z S = arg min z D x, h θ (z) (9) x S = h θ (z S )<label>(10)</label></formula><p>The optimization problem is Eq. 9 can be solved using gradient-descent based techniques on the decoder network h θ * θ * are the parameters of the decoder network trained only on the source samples S n with respect to z. This implies that given any input target image, the optimization problem in Eq. 9 will be solved to find its 'nearest-clone' in the source distribution which is used as a proxy in the segmentation network trained only on S n . We call the iterative procedure of findingx S through optimization using h θ * as the Latent Search (LS). Finally, inspired by the observations made in <ref type="bibr" target="#b17">[18]</ref>, we propose to use structural similarity index (SSIM) [50] based loss L ssim for D to conduct the Latent Search. Unlike norm based losses, SSIM loss helps in preservation of structural information which is needed for segmentation. <ref type="figure" target="#fig_8">Fig. 6</ref> depicts the complete inference procedure employed in the proposed method named as the Generative Latent Search for Segmentation (GLSS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>Architectural details of the VAE used are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Sobel operator is used to extract the edge information of the input image which is concatenated with one of the layers of the Decoder via a tanh non linearity as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference</head><p>Once the VAE is trained on the source dataset, given an imagex T from the target distribution, the Latent Search algorithm searches for an optimal latent vectorz S that generates its 'nearest-clone'x S from P S . The search is performed by minimizing the SSIM loss L ssim between the input target imagex T and the VAE-reconstructed target image, using a gradient-descent based optimization procedure such as ADAM <ref type="bibr" target="#b22">[23]</ref> with α = 0.1, β 1 = 0.9 and β 2 = 0.99. The Latent Search is performed for K (hyper-parameter) iterations over the latent space of the source for a given target image. Finally, the segmentation mask for the input target sample is assigned the same as the one given by the segmentation network S ψ , which is trained on source data, on the 'nearest-clone'x S . Latent Search for one sample takes roughly 450 ms and 120 ms on SNV and Hand Gesture datasets respectively. Please refer supplementary material for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We consider the Red-channel of the COMPAQ dataset <ref type="bibr" target="#b21">[22]</ref> as our source data. It consists of 4675 RGB images with the corresponding annotations of the skin. Since there is no publicly available dataset with NIR images and corresponding skin segmentation labels, we create and use two NIR datasets (publicly available) as targets. The first one named as the Skin NIR Vision (SNV), consists of 800 images of multiple human subjects taken in different scenes, captured using a WANSVIEW 720P camera in the night-vision mode. The captured images cover wide range of scenarios for skin detection task like presence of multiple humans, backgrounds similar to skin color, different illuminations, saturation levels and different postures of subjects to ensure diversity. Additionally, we made use of the publicly available multi-modal Hand Gesture dataset 1 as another target dataset which we call as Hand Gesture dataset. This dataset covers 16 different handposes of multiple subjects. We randomly sampled 500 images in order to cover illumination changes and diversity in hand poses. Both SNV and Hand Gesture datasets are manually annotated with precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmarking on SNV and Hand Gesture datasets</head><p>To begin with, we performed supervised segmentation experiments on both SNV and Hand Gesture datasets with 80-20 train-test split using SOTA segmentation algorithms.  <ref type="bibr" target="#b6">[7]</ref> with Xception network <ref type="bibr" target="#b9">[10]</ref> as backbone. It is seen that SNV dataset (IoU ≈ 0.79) is slightly complex as compared to Hand Gesture dataset (IoU ≈ 0.90). are employed for the segmentation network (S ψ ). It can be seen that although all the UDA SOTA methods improve upon the Source Only performance, GLSS offers significantly better performance despite not using any data from the target distribution. Hence, it may be empirically inferred that GLSS is successful in producing the 'nearest-clone' through implicit sampling from the source distribution and thereby reducing the domain shift. It is also observed that the performance of the segmentation network S ψ does not degrade on the source data with GLSS. The predicted masks with DeepLabv3+ are shown in <ref type="figure">Fig. 3</ref> for SNV and Hand Gesture datasets, respectively. It can be seen that GLSS is able to capture fine facial details like eyes, lips and body parts like hands, better as compared to SOTA methods. It is also seen that the predicted masks for Hand Gesture dataset are sharper in comparison to other methods. Most of the methods work with the assumption of spatial and structural similarity between the source and target data. Since our source and target datasets do not have similar backgrounds, the methods that make such assumptions perform poorer on our datasets. We observed that for methods like BDL, the image translation between NIR images and Red channel images is not effective for skin segmentation task.</p><p>Standard UDA task: We use standard UDA methods along with GLSS on standard domain adaptation datasets such as Synthia [42] and Cityscapes <ref type="bibr" target="#b10">[11]</ref>. As observed from <ref type="table" target="#tab_3">Table 3</ref>, even with large domain shift, GLSS finds a clone for every target image that is sampled from the source distribution while preserving the structure of the target image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We have conducted several ablation experiments on GLSS using both SNV and Hand Gesture datasets using DeepLabv3+ as segmentation networks (S ψ ) to ascertain the utility of different design choices we have made in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of number of iterations on LS:</head><p>The inference of GLSS involves a gradient-based optimization through the decoder network h θ * to generate the 'nearest-clone' for a given target image. In <ref type="figure">Fig. 4</ref>  shown in <ref type="figure" target="#fig_2">Fig. 5</ref> where it is seen that it saturates around 90-100 iterations that are used for all the UDA experiments described in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Edge concatenation:</head><p>As discussed earlier, edges extracted using Sobel filter on input images are concatenated with one of the layers of decoder for both training and inference. It is seen from <ref type="table" target="#tab_5">Table 4</ref> that IoU improves for both the target datasets with concatenation of edges. It is observed that without the edge concatenation, the generated images ('nearest-clones') are blurry thus the segmentation network fails to predict sharper skin masks.  <ref type="figure" target="#fig_2">Fig. 5</ref>. On both the datasets, it is seen that SSIM is consistently better than the norm based losses at all iterations affirming the superiority of the SSIM loss in preserving the structures while finding the 'nearest-clone'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we addressed the problem of skin segmentation from NIR images.</p><p>Owing to the non-existence of large-scale labelled NIR datasets for skin segmentation, the problem is casted as Unsupervised Domain Adaptation where we use the segmentation network trained on the Red-channel images from a large-scale labelled visible-spectrum dataset for UDA on NIR data. We propose a novel method for UDA without the need for the access to the target data (even unlabelled). Given a target image, we sample an image from the source distribution that is 'closest' to it under a distance metric. We show that such a 'closest' sample exists and describe a procedure using an optimization algorithm over the latent space of a VAE trained on the source data. We demonstrate the utility of the proposed method along with the comparisons with SOTA UDA segmentation methods on the skin segmentation task on two NIR datasets that were created. Also, we reach SOTA performance on Synthia and Cityscapes datasets for semantic segmentation of urban scenes. from error visibility to structural similarity. IEEE transactions on image processing 13(4), 600-612 (2004) 51. Wu, Q., Cai, R., Fan, L., Ruan, C., Leng, G.: Skin detection using color processing mechanism inspired by the visual system (2012) 52. Zaidan, A., Ahmad, N.N., Karim, H.A., Larbani, M., Zaidan, B., Sali, A.: On the multi-agent learning neural and bayesian methods in skin detector and pornography classifier: An automated anti-pornography system. Neurocomputing 131, 397-418 (2014) 53. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:</p><p>Proceedings of the IEEE conference on computer vision and pattern recognition.  <ref type="figure" target="#fig_0">Fig. 1: a)</ref> shows samples of COMPAQ dataset <ref type="bibr" target="#b21">[22]</ref> images with only Red-channel present b) contains samples from SNV dataset c) contains samples from Hand Gesture dataset.</p><p>Each row of <ref type="figure" target="#fig_0">Fig. 1</ref> shows few images with the corresponding skin-mask pairs from COMPAQ, SNV and Hand Gesture datasets respectively.  </p><formula xml:id="formula_8">µ (i) z , σ (i) z ← g φ (x i ) 5: sample z i ∼ N(µ (i) z , σ (i) z 2 ) 6: L r ← B i=1 x i − h θ (z i ) 2 2 7: L p ← B i=1 P ψ (x i ) − P ψ (h θ (z i )) 2 2 8: L g ← L r + L p + B i=1 D KL N(µ (i) z , σ (i) z 2 ) || N(0, 1) 9: L h ← L r + L p 10: φ ← φ + η∇ φ L g 11: θ ← θ + η∇ θ L h 12: until convergence of φ, θ</formula><formula xml:id="formula_9">L s = L dice + L bce (1) L s = L f ocal<label>(2)</label></formula><p>L dice is the dice coefficient loss which calculates the overlap between the predicted and the ground truth mask whereas L bce is the binary cross-entropy loss. Binary focal loss (L f ocal ) tries to down-weight the contribution of examples that can be easily segmented so that the segmentation model focuses more on learning hard examples. P ψ is a perceptual model (as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> in the paper) that uses perceptual loss L p . The perceptual features are taken from the 6th layer of UNet and the last concatenation layer of DeepLabv3+. VAE along with perceptual loss L p is trained for 150-200 epochs. L p is weighted with a factor β (a hyper-parameter) as shown:</p><formula xml:id="formula_10">L total = L vae + βL p<label>(3)</label></formula><p>In order to improve the quality of VAE reconstructed images, we weighted the perceptual loss (L p ) with different values of β. For UNet, we have used β = 2 whereas β = 3 is used for DeepLabv3+. The first part of Algorithm 1 shows the steps involved in training VAE and second part shows the steps involved in inference procedure.</p><p>Using an Intel Xeon processor (6 Cores) with a base frequency of 2.0 GHz, 32GB RAM and NVIDIA® Tesla® K40 (12 GB Memory) GPU, Latent Search for one sample on SNV dataset takes 450 ms and 120 ms on Hand Gesture dataset. The time required is in the order of milliseconds on a basic GPU like K40 which is not very significant. However, this is the cost that is to be paid for being target independent which is a very significant advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Implementation details of UDA baseline methods for skin segmentation</head><p>DeepLabv3+ was used as the segmentation model for all the baselines with images and corresponding masks of size 128 × 128. AdaptsegNet [47] uses discriminative approach to predict the domain of the images. For discriminator, we used a model with 5 convolutional layers (default implementation). We performed a grid search over λ advtarget1 and λ advtarget2 and reported the best IoU score for AdaptsegNet. DISE <ref type="bibr" target="#b4">[5]</ref> uses image-to-image translation approach to translate one domain to another. It employs label transfer loss to optimize the segmentation model. Image-to-image translation based methods work well in cases where the structural similarity is more. We used 0.1, 0.25 and 0.5 for λ seg and reported the best IoU using λ seg = 0.1 while the learning rate was set to 2.5e-4. Advent <ref type="bibr">[48]</ref> proposes to leverage an entropy loss to directly penalize lowconfident predictions on target domain. If λ ent is large then the entropy drops too quickly and the model is strongly biased towards a few classes. We used 0.001 for λ ent as suggested by the authors regardless of the network and dataset. Also, for adversarial training, 0.001 was used for λ adv . We trained with AdvEnt as it performed better that minEnt as stated in the paper. SGD and Adam were used as optimizers for segmentation and discriminator networks respectively. In DADA [49], authors make use of an additional depth information in the source domain. We performed a grid search over λ seg using values 0.25, 0.5, 1. The learning rate was varied with values 2.5e-4, 1e-4 and 3e-4 and finally best IoU was reported with λ seg = 0.5 and learning rate = 2.5e-4. CLAN <ref type="bibr" target="#b31">[32]</ref> makes use of a category-level joint distribution and align each class with an adaptive adversarial loss, thus ensuring correct mapping of source and target. Compared to traditional adversarial training, CLAN introduces the discrepancy loss and the category-level adversarial loss. Hyperparameters like learning rate, weight decay, λ weight and λ adv were used with values 2.5e-4, 5e-4, 0.01 and 0.001 respectively during training. For training BDL <ref type="bibr" target="#b26">[27]</ref>, we set the learning rate to 2.5e-4 for the segmentation network and 1e-4 for the discriminator. Grid search was performed for λ advtarget with values 1e-3, 2e-3, 5e-3 and best IoU was reported with λ advtarget = 1e-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SSIM Loss</head><p>SSIM loss compares pixels and their corresponding neighbourhoods between two images, preserving the luminance, contrast and structural information. To perform Latent Search, we used distance metric as SSIM loss, that helps to sample the 'nearest-clone' in the source distribution for the target image from the generative latent space of VAE. Unlike norm-based losses, SSIM loss helps in the preservation of structural information as compared to discrete pixel-level information. We used 11x11 Gaussian filter in our experiments. SSIM is defined using the three aspects of similarities, luminance l(x,x) , contrast c(x,x) and structure s(x,x) that are measured for a pair of images {x,x} as follows:</p><formula xml:id="formula_11">l(x,x) = 2µ x µx + C 1 µ 2 x + µ 2 x + C 1 (4) c(x,x) = 2σ x σx + C 2 σ x 2 + σx 2 + C 2 (5) s(x,x) = σ xx + C 3 σ x σx + C 3<label>(6)</label></formula><p>where µ's denote sample means and σ's denote variances. C 1 , C 2 and C 3 are constants. With these, SSIM and the corresponding loss function L ssim , for a pair of images {x,x} are defined as:</p><formula xml:id="formula_12">SSIM(x,x) = l(x,x) α · c(x,x) β · s(x,x) γ<label>(7)</label></formula><p>where α &gt; 0, β &gt; 0 and γ &gt; 0 are parameters used to adjust the relative importance of the three components.</p><formula xml:id="formula_13">L ssim (x,x) = 1 − SSIM(x,x)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Target-Independence of GLSS</head><p>GLSS is a general-purpose target-independent UDA method. For UDA, target independence is a merit since a SINGLE source model can be used across multiple targets. However, even with target data (for VAE training) GLSS doesn't degrade while SOTA methods do, for skin segmentation on NIR images <ref type="table">(Table below</ref> compares IoU).    It is evident from the figure that the SSIM scores are higher for the 'nearest-clone'x S as compared to the scores withx T . It indicates thatx S is more closer to the source domain (COMPAQ) as compared tox T . Hence, the 'nearest-clone'x S generated by GLSS for targetx T is used as a proxy in the segmentation network S ψ which is trained only on COMPAQ dataset, thereby increasing the IoU forx T . It is evident from the figure that the SSIM scores are higher for the 'nearest-clone'x S as compared to the scores withx T . It indicates thatx S is more closer to the source domain (COMPAQ) as compared tox T . Hence, the 'nearest-clone'x S generated by GLSS for target x T is used as a proxy in the segmentation network S ψ which is trained only on COMPAQ dataset, thereby increasing the IoU forx T .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>DKL[Qφ(z|x)||Pθ(z)] VAE training. Edges of an input image are concatenated with the features from the decoder h θ . Encoder and decoder parameters φ, θ are optimized with reconstruction loss L r , KL-divergence loss D KL and perceptual loss L p . Perceptual model P ψ is trained on source samples. A zero mean and unit variance isotropic Gaussian prior is imposed on the latent space z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Latent Search procedure during inference with GLSS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Performance of gradient-based Latent Search during inference on target SNV and Hand Gesture images using different objective functions; MSE, MAE, SSIM loss. DeepLabv3+ is employed as segmentation network. It is evident that the losses saturate at around 90-100 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>38. Pandey, P., Prathosh, A., Kohli, M., Pritchard, J.: Guided weak supervision for action recognition with scarce data to assess skills of children with autism. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 463-470 (2020) 39. Prathosh, A., Praveena, P., Mestha, L.K., Bharadwaj, S.: Estimation of respiratory pattern from video using selective ensemble aggregation. IEEE Transactions on Signal Processing 65(11), 2902-2916 (2017) 40. Qiang-rong, J., Hua-lan, L.: Robust human face detection in complicated color images. In: 2010 2nd IEEE International Conference on Information Management and Engineering. pp. 218-221. IEEE (2010) 41. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234-241. Springer (2015) 42. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3234-3243 (2016) 43. Seow, M.J., Valaparla, D., Asari, V.K.: Neural network based skin color model for face detection. In: 32nd Applied Imagery Pattern Recognition Workshop, 2003. Proceedings. pp. 141-145. IEEE (2003) 44. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818-2826 (2016) 45. Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 (2019) 46. Taqa, A.Y., Jalab, H.A.: Increasing the reliability of skin detectors. Scientific Research and Essays 5(17), 2480-2490 (2010) 47. Tsai, Y.H., Hung, W.C., Schulter, S., Sohn, K., Yang, M.H., Chandraker, M.: Learning to adapt structured output space for semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7472-7481 (2018) 48. Vu, T.H., Jain, H., Bucher, M., Cord, M., Pérez, P.: Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In: CVPR (2019) 49. Vu, T.H., Jain, H., Bucher, M., Cord, M., Pérez, P.: Dada: Depth-aware domain adaptation in semantic segmentation. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 7364-7373 (2019) 50. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 :repeat 3 :</head><label>13</label><figDesc>on NIR images S ψ is the segmentation model (as shown in Fig. 2 in the paper) implemented using DeepLabv3+ (XceptionNet) and UNet (EfficientNet). S ψ is trained for equal contribution Algorithm 1 Generative Latent Search for Segmentation (GLSS) Training VAE on source samples Input: Source dataset S n = {x 1 , ..., x n }, Number of source samples n, Encoder g φ , Decoder h θ , Trained Perceptual Model P ψ , Learning rate η, Batchsize B. Output: Optimal parameters φ * , θ * . Initialize parameters φ, θ 2: sample batch {x i } from dataset S n , for i = 1, ..., B 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Inference -Latent Search during testing with Target Input: Target samplex T , Trained decoder h θ * , Learning rate η. Output: 'nearest-clone'x S for the target samplex T . 13: sample z from N(0, 1) 14: repeat 15: L ssim ← 1 − SSIM(x T , h θ * (z)) 16: z ← z + η∇ z L ssim 17: until convergence of L ssim 18:z S ← z 19:x S ← h θ * (z S ) 100-150 epochs with losses (L s ) as shown in Eq. 1 and Eq. 2 for UNet and DeepLabv3+ respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Illustration of Latent Search (LS) in GLSS for SNV dataset. Prior to the LS, VAE reconstructed target samples are obtained. It is evident that the 'nearest-clones' (images generated using LS) improve as the LS progresses. Also the quality (empirically) of 'nearest-clones' are better as compared to the VAE reconstructed images. The 'nearest-clones' are shown after every 30 iterations. Illustration of Latent Search (LS) in GLSS for Hand Gesture dataset. Prior to the LS, VAE reconstructed target samples are obtained. It is evident that the 'nearest-clones' (images generated using LS) improve as the LS progresses. Also the quality (empirically) of 'nearest-clones' are better as compared to the VAE reconstructed images. The 'nearest-clones' are shown after every 30 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>w/o edge (c) w/o L (d) w/o LS (e) GLSS (a) the ground truth mask for SNV and Hand Gesture datasets, (b) the predicted mask of VAE reconstructed image without edge concatenation, (c) the predicted mask of VAE reconstructed image without L p , (d) the predicted mask of VAE reconstructed with edge concatenation and perceptual loss when no Latent Search (LS) was performed, (e) the predicted mask with GLSS. It is evident from the predicted masks that with edge concatenation, perceptual loss and Latent Search (LS), quality of predicted masks improve. Each component plays a significant role in improving the IoU. Hence, when all the components are employed (as in GLSS) we get the best IoU. (a) an NIR imagex T from SNV dataset (target), (b) 'nearest-clone'x S generated from GLSS, (c) Structural Similarity Index (SSIM) scores calculated betweenx T and all the samples (having only Red-channel) of COMPAQ dataset (source) are shown with blue color in the plot. Similary, SSIM scores calculated betweenx S and all the samples (having only Red-channel) of COMPAQ dataset are shown with red color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>(a) an NIR imagex T from Hand Gesture dataset (target), (b) 'nearestclone'x S generated from GLSS, (c) Structural Similarity Index (SSIM) scores calculated betweenx T and all the samples (having only Red-channel) of COM-PAQ dataset (source) are shown with blue color in the plot. Similary, SSIM scores calculated betweenx S and all the samples (having only Red-channel) of COMPAQ dataset are shown with red color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. The VAE is trained using (i) the Mean squared error reconstruction loss L r and KL divergence D KL and (ii) the perceptual loss L p for which the features are extracted from the l th layer (a hyper-parameter) of the DeepLabv3+<ref type="bibr" target="#b6">[7]</ref> (Xception backbone<ref type="bibr" target="#b9">[10]</ref>) and the UNet [41] (EfficientNet backbone [45]) segmentation networks. The segmentation network (S ψ inFig. 6) is either DeepLabv3+ or UNet and is trained on the source dataset. For traning S ψ , we use combination of binary cross-entropy (L bce ) and dice coefficient loss (L dise ) for UNet with RM-SProp (lr = 0.001) as optimizer and binary focal loss (L f ocal )<ref type="bibr" target="#b28">[29]</ref> with γ = 2.0, α = 0.75 and RMSProp (lr=0.01) as optimizer for DeepLabv3+. For the VAE , the hidden layers of Encoder and Decoder networks use Leaky ReLU and tanh as activation functions with the dimensionality of the latent space being 64. VAE is trained using standard gradient descent procedure with RMSprop (α=0.0001) as optimizer. We train VAE for 100 to 150 epochs with batchsize 64.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Benchmarking Skin NIR Vision (SNV) dataset and Hand Gesture dataset on standard segmentation architectures with 80-20 train-test split.</figDesc><table><row><cell></cell><cell>SNV</cell><cell></cell><cell cols="2">Hand Gesture</cell></row><row><cell>Method</cell><cell>IoU</cell><cell>Dice</cell><cell>IoU</cell><cell>Dice</cell></row><row><cell>FPN [28]</cell><cell>0.792</cell><cell>0.895</cell><cell>0.902</cell><cell>0.950</cell></row><row><cell>UNet [41]</cell><cell>0.798</cell><cell>0.890</cell><cell>0.903</cell><cell>0.950</cell></row><row><cell>DeepLabv3+ [7]</cell><cell>0.750</cell><cell>0.850</cell><cell>0.860</cell><cell>0.924</cell></row><row><cell>Linknet [6]</cell><cell>0.768</cell><cell>0.872</cell><cell>0.907</cell><cell>0.952</cell></row><row><cell>PSPNet [53]</cell><cell>0.757</cell><cell>0.850</cell><cell>0.905</cell><cell>0.949</cell></row></table><note>Table 1 shows the standard performance metrics such as IoU and Dice- coefficient calculated using FPN [28], UNet [41], LinkNet [6], PSPNet [53], all with EfficientNet [45] as backbone and DeepLabv3+</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Empirical analysis of GLSS along with standard UDA methods. IoU and Dice-coefficient are computed for both SNV and Hand Gesture datasets using UNet and DeepLabv3+ as segmentation networks. We have performed the UDA experiments with the SOTA UDA algorithms using Red-channel of the COMPAQ Dataset<ref type="bibr" target="#b21">[22]</ref> as the source and SNV and Hand Gesture as the target.Table 2compares the performance of proposed GLSS algorithm with six SOTA baselines along with the Source Only case (without any UDA). We have used entire target dataset for IoU and Dice-coefficient evaluation. Two architectures, DeepLabv3+ and UNet,</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SNV</cell><cell></cell><cell></cell><cell cols="2">Hand Gesture</cell><cell></cell></row><row><cell></cell><cell cols="2">UNet</cell><cell cols="2">DeepLabv3+</cell><cell cols="2">UNet</cell><cell cols="2">DeepLabv3+</cell></row><row><cell>Models</cell><cell>IoU</cell><cell>Dice</cell><cell>IoU</cell><cell>Dice</cell><cell>IoU</cell><cell>Dice</cell><cell>IoU</cell><cell>Dice</cell></row><row><cell>Source Only</cell><cell cols="8">0.295 0.426 0.215 0.426 0.601 0.711 0.505 0.680</cell></row><row><cell cols="9">AdaptSegnet [47] 0.315 0.435 0.230 0.435 0.641 0.716 0.542 0.736</cell></row><row><cell>Advent [48]</cell><cell cols="8">0.341 0.571 0.332 0.540 0.612 0.729 0.508 0.689</cell></row><row><cell>CLAN [32]</cell><cell cols="8">0.248 0.442 0.225 0.426 0.625 0.732 0.513 0.692</cell></row><row><cell>BDL [27]</cell><cell cols="8">0.320 0.518 0.301 0.509 0.647 0.720 0.536 0.750</cell></row><row><cell>DISE [5]</cell><cell cols="8">0.341 0.557 0.339 0.532 0.672 0.789 0.563 0.769</cell></row><row><cell>DADA [49]</cell><cell cols="8">0.332 0.534 0.314 0.521 0.643 0.743 0.559 0.761</cell></row><row><cell>ours (GLSS)</cell><cell cols="8">0.406 0.597 0.385 0.597 0.736 0.844 0.698 0.824</cell></row></table><note>Fig. 3: Qualitative comparison of predicted segmentation skin masks on SNV and Hand Gesture datasets with standard UDA methods. Top four rows shows skin masks for SNV dataset and the last four are the masks for Hand Gesture dataset. It is evident that GLSS predicted masks are very close to the GT masks as compared to other UDA methods. (SO=Source Only, ASN=AdaptSegNet [47], GT=Ground Truth).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Models</cell><cell cols="2">mIoU mIoU*</cell></row><row><cell cols="2">AdaptsegNet [47] 46.7</cell><cell>-</cell></row><row><cell>Advent [48]</cell><cell cols="2">48.0 41.2</cell></row><row><cell>BDL [27]</cell><cell>51.4</cell><cell>-</cell></row><row><cell>CLAN [32]</cell><cell>47.8</cell><cell>-</cell></row><row><cell>DISE [5]</cell><cell cols="2">48.8 41.5</cell></row><row><cell>DADA [49]</cell><cell cols="2">49.8 42.6</cell></row><row><cell>ours(GLSS)</cell><cell cols="2">52.3 44.5</cell></row></table><note>Empirical analysis of GLSS on standard domain adaptaion task of adapting Synthia [42] to Cityscapes [11]. We calculate the mean IoU for 13 classes (mIoU) and 16 classes (mIoU*).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, we show the skin masks of the transformed target images after every 30 iterations. It is seen that with the increasing number of iterations, the predicted skin masks improves using GLSS as the 'nearest-clones' are optimized during the Latent Search procedure. We plot the IoU as a function of the number of iterations during Latent Search as</figDesc><table><row><cell>real target</cell><cell>Source Only</cell><cell>VAE reconstruction</cell><cell>after 30</cell><cell>after 60 nearest-clones</cell><cell>after 90</cell></row><row><cell></cell><cell></cell><cell cols="4">iterations over the latent space of source</cell></row><row><cell cols="6">Fig. 4: Illustration of Latent Search in GLSS. Real target is a ground truth mask.</cell></row><row><cell cols="6">Source Only masks are obtained from target samples by training segmentation</cell></row><row><cell>network S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ψ on source dataset. Prior to the LS, skin masks are obtained from VAE reconstructed target samples. It is evident that predicted skin masks improve as the LS progresses. The predicted masks for the 'nearest-clones' are shown after every 30 iterations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation of different components of GLSS during training and inference; Edge, perceptual loss L p and Latent Search (LS).</figDesc><table><row><cell>Edge</cell><cell>L p</cell><cell>LS</cell><cell>SNV IoU</cell><cell>Hand Gesture IoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.112</cell><cell>0.227</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.178</cell><cell>0.560</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.120</cell><cell>0.250</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.128</cell><cell>0.238</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.330</cell><cell>0.615</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.182</cell><cell>0.300</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.223</cell><cell>0.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.385</cell><cell>0.698</cell></row><row><cell cols="3">Effect of Perceptual loss L</cell><cell></cell><cell></cell></row></table><note>p : We have introduced a perceptual model P ψ , trained on source samples. It ensures that the VAE reconstructed image is se- mantically similar to the input image unlike the norm based losses. Table 4 clearly demonstrates the improvement offered by the use of perceptual loss while training the VAE. Effect of SSIM for Latent Search: Finally, to validate the effect of SSIM loss for Latent Search, we plot the IoU metric using two norm based losses MSE (Mean squared error) and MAE (Mean absolute error) for the Latent Search procedure as shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>IoU comparison for Target-Independence of GLSS with change in the amount of target data. GLSS performance is not affected by change in the amount of target data during training while other SOTA methods degrade.GLSS demands a generative model that has both generation and inference capabilities (mapping from latent to data space and vice versa), which is not the case with GANs. This leads to non-convergence of latent search. To validate this, we trained a SOTA BigGAN<ref type="bibr" target="#b3">[4]</ref> on COMPAQ Dataset<ref type="bibr" target="#b20">[21]</ref> and performed GLSS. Although GAN had better generation quality (FID of 29.7 with BigGAN vs. 44 with VAE), the final IoU was worse as shown inTable 2.</figDesc><table><row><cell>% of Target data</cell><cell>Adaptsegnet</cell><cell>BDL</cell><cell>CLAN</cell><cell>Advent</cell><cell>DADA</cell><cell>GLSS</cell></row><row><cell>60</cell><cell>0.23</cell><cell>0.30</cell><cell>0.22</cell><cell>0.33</cell><cell>0.31</cell><cell>0.37</cell></row><row><cell>40</cell><cell>0.22</cell><cell>0.26</cell><cell>0.22</cell><cell>0.29</cell><cell>0.28</cell><cell>0.37</cell></row><row><cell>20</cell><cell>0.21</cell><cell>0.22</cell><cell>0.21</cell><cell>0.24</cell><cell>0.23</cell><cell>0.38</cell></row><row><cell cols="2">2.5 GAN vs. VAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>IoU score comparison between BigGAN and VAE when trained on SNV and Hand Gesture datasets. VAE scores better in terms of IoU.</figDesc><table><row><cell>SNV/BigGAN</cell><cell>Hand Gesture/BigGAN</cell><cell>SNV/VAE</cell><cell>Hand Gesture/VAE</cell></row><row><cell>0.09</cell><cell>0.21</cell><cell>0.38</cell><cell>0.69</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.gti.ssr.upm.es/data/MultiModalHandGesture dataset</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cycada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Impact of color space on human skin color detection using an intelligent system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Al-Mohair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saundi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st WSEAS international conference on image processing and pattern recognition (IPPR&apos;13)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human skin detection through correlation rules between the ycb and ycr subspaces based on dynamic color clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brancati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1900" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skin color modeling for face detection and segmentation: a review and a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="839" to="862" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Region-based and content adaptive skin detection in color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of pattern recognition and artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="831" to="853" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Domain adaptation for holistic skin detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weigang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06969</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining haar feature and skin color based classifiers for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ulukaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karaali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1497" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semisupervised skin detection by network with mutual guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2111" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image quality metrics: Psnr vs. ssim</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face detection in color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="696" to="706" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skin-color-based image segmentation and its application in face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huynh-Thu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaneko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA. pp</title>
		<imprint>
			<biblScope unit="page" from="48" to="51" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical color models with application to skin detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="96" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recent advances in visual and infrared face recognition-a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="135" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Human skin color clustering for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kovac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10620</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A robust skin color based face detection algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="525" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">High performance novel skin segmentation algorithm for images with complex background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mahmoodi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05588</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comprehensive survey on human skin detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mahmoodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Sayedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image, Graphics &amp; Signal Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A novel fuzzy rule base system for pose independent faces detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moallem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Monadjemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1801" to="1810" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face recognition in hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tromberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1552" to="1560" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Target-independent domain adaptation for wbc classification using generative latent search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kyatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Dastidar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05432</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
