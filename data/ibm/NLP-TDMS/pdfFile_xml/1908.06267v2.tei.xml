<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Message Passing Attention Networks for Document Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
							<email>nikolentzos@lix.polytechnique.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Athens University of Economics and Business</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Athens University of Economics and Business</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tixier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Athens University of Economics and Business</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Athens University of Economics and Business</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><surname>1é</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Athens University of Economics and Business</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polytechnique</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Athens University of Economics and Business</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Message Passing Attention Networks for Document Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have recently emerged as a very effective framework for processing graph-structured data. These models have achieved state-of-the-art performance in many tasks. Most graph neural networks can be described in terms of message passing, vertex update, and readout functions. In this paper, we represent documents as word co-occurrence networks and propose an application of the message passing framework to NLP, the Message Passing Attention network for Document understanding (MPAD). We also propose several hierarchical variants of MPAD. Experiments conducted on 10 standard text classification datasets show that our architectures are competitive with the state-of-the-art. Ablation studies reveal further insights about the impact of the different components on performance. Code is publicly available at: https://github.com/giannisnik/mpad.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The concept of message passing over graphs has been around for many years <ref type="bibr" target="#b41">(Weisfeiler and Lehman 1968;</ref><ref type="bibr" target="#b27">Murphy, Weiss, and Jordan 1999)</ref>, as well as that of graph neural networks (GNNs) <ref type="bibr" target="#b6">(Gori, Monfardini, and Scarselli 2005;</ref><ref type="bibr" target="#b34">Scarselli et al. 2008)</ref>. However, GNNs have only recently started to be closely investigated, following the advent of deep learning. Some notable examples include <ref type="bibr">(Duvenaud et al. 2015;</ref><ref type="bibr" target="#b1">Battaglia et al. 2016;</ref><ref type="bibr" target="#b21">Li et al. 2016;</ref><ref type="bibr" target="#b4">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b13">Kearnes et al. 2016;</ref><ref type="bibr" target="#b17">Kipf and Welling 2016;</ref><ref type="bibr" target="#b8">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b41">Veličković et al. 2017;</ref><ref type="bibr" target="#b44">Xu et al. 2018b</ref>). These approaches are known as spectral. Their similarity with message passing (MP) was observed by <ref type="bibr" target="#b17">(Kipf and Welling 2016)</ref> and formalized by <ref type="bibr" target="#b5">(Gilmer et al. 2017)</ref> and <ref type="bibr" target="#b43">(Xu et al. 2018a</ref>).</p><p>The MP framework is based on the core idea of recursive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on messages received from its neighbors. The majority of the spectral GNNs can be described in terms of the MP framework.</p><p>GNNs have been applied with great success to bioinformatics and social network data, for node classification, link prediction, and graph classification. However, a few studies only have focused on the application of the MP framework to representation learning on text. This paper proposes one such application. More precisely, we represent documents as word co-occurrence networks, and develop an expressive MP GNN tailored to document understanding, the Message Passing Attention network for Document understanding <ref type="bibr">(MPAD)</ref>. We also propose several hierarchical variants of MPAD. Evaluation on 10 document classification datasets shows that our architectures learn representations that are competitive with the state-of-the-art. Furthermore, ablation experiments shed light on the impact of various architectural choices.</p><p>In what follows, we first provide some background about the MP framework (sec. 2), thoroughly describe and explain MPAD (sec. 3), present our experimental framework (sec. 4), report and interpret our results (sec. 5), and provide a review of the relevant literature (sec. 6).</p><p>2 Message Passing Neural Networks <ref type="bibr" target="#b5">(Gilmer et al. 2017</ref>) proposed a MP framework under which many of the recently introduced GNNs can be reformulated 1 . MP consists in an aggregation phase followed by a combination phase <ref type="bibr" target="#b43">(Xu et al. 2018a</ref>). More precisely, let G = (V, E) be a graph, and let us consider v ∈ V . At time t + 1, a message vector m t+1 v is computed from the representations of the neighbors N (v) of v:</p><formula xml:id="formula_0">m t+1 v = AGGREGATE t+1 h t w | w ∈ N (v)<label>(1)</label></formula><p>The new representation h t+1 v of v is then computed by combining its current feature vector h t v with the message vector</p><formula xml:id="formula_1">m t+1 v : h t+1 v = COMBINE t+1 h t v , m t+1 v</formula><p>(2) Messages are passed for T time steps. Each step is implemented by a different layer of the MP network. Hence, iterations correspond to network depth. The final feature vector h T v of v is based on messages propagated from all the nodes in the subtree of height T rooted at v. It captures both the topology of the neighborhood of v and the distribution of the vertex representations in it.</p><p>If a graph-level feature vector is needed, e.g., for classification or regression, a READOUT pooling function, that must be invariant to permutations, is applied:</p><formula xml:id="formula_2">h G = READOUT h T v | v ∈ V<label>(3)</label></formula><p>Next, we present the MP network we developed for document understanding.</p><p>3 Message Passing Attention network for Document understanding (MPAD)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word co-occurrence networks</head><p>We represent a document as a statistical word co-occurrence network <ref type="bibr" target="#b25">(Mihalcea and Tarau 2004)</ref> with a sliding window of size 2 overspanning sentences. Let us denote that graph by G = (V, E). Each unique word in the preprocessed document is represented by a node in G, and an edge is added between two nodes if they are found together in at least one instantiation of the window. G is directed and weighted: edge directions and weights respectively capture text flow and cooccurrence counts. G is a compact representation of its document. In G, immediate neighbors are consecutive words in the same sentence 2 . That is, paths of length 2 correspond to bigrams. Paths of length more than 2 can correspond either to traditional n-grams or to relaxed n-grams, that is, words that never appear in the same sentence but co-occur with the same word(s). Such nodes are linked through common neighbors. Master node. Inspired by <ref type="bibr" target="#b34">(Scarselli et al. 2008)</ref>, our graph G also includes a special document node, linked to all other nodes via unit weight bi-directional edges. In what follows, let us denote by n the number of nodes in G, including the master node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Message passing</head><p>We formulate our AGGREGATE function as:</p><formula xml:id="formula_3">M t+1 = MLP t+1 D −1 AH t<label>(4)</label></formula><p>where H t ∈ R n×d contains node features (d is a hyperparameter 3 ), and A ∈ R n×n is the adjacency matrix of G.</p><p>Since G is directed, A is asymmetric. Also, A has zero diagonal as we choose not to consider the feature of the node itself, only that of its incoming neighbors, when updating its representation 4 . Since G is weighted, the i th row of A contains the weights of the edges incoming on node v i . D ∈ R n×n is the diagonal in-degree matrix of G. MLP denotes a multi-layer perceptron, and M t+1 ∈ R n×d is the message matrix. The use of a MLP was motivated by the observation that for graph classification, MP neural nets with 1-layer perceptrons are inferior to their MLP counterparts <ref type="bibr" target="#b43">(Xu et al. 2018a</ref>). Indeed, 1-layer perceptrons are not universal approximators of multiset functions. Note that like in <ref type="bibr" target="#b43">(Xu et al. 2018a)</ref>, we use a different MLP at each layer. Renormalization. The rows of D −1 A sum to 1. This is equivalent to the renormalization trick of <ref type="bibr" target="#b17">(Kipf and Welling 2016)</ref>, but using only the in-degrees. That is, instead of computing a weighted sum of the incoming neighbors' feature vectors, we compute a weighted average of them. The coefficients are proportional to the strength of co-occurrence between words. One should note that by averaging, we lose the ability to distinguish between different neighborhood structures in some special cases, that is, we lose injectivity. Such cases include neighborhoods in which all nodes have the same representations, and neighborhoods of different sizes containing various representations in equal proportions <ref type="bibr" target="#b43">(Xu et al. 2018a)</ref>. As suggested by the results of an ablation experiment, averaging is better than summing in our application (see subsection 5.2). Note that instead of simply summing/averaging, we also tried using GAT-like attention <ref type="bibr" target="#b41">(Veličković et al. 2017)</ref> in early experiments, without obtaining better results.</p><p>As far as our COMBINE function, we use the Gated Recurrent Unit <ref type="bibr" target="#b4">Chung et al. 2014)</ref>:</p><formula xml:id="formula_4">H t+1 = GRU(H t , M t+1 )<label>(5)</label></formula><p>Omitting biases for readability, we have:</p><formula xml:id="formula_5">R t+1 = σ(W t+1 R M t+1 + U t+1 R H t ) Z t+1 = σ(W t+1 Z M t+1 + U t+1 Z H t ) H t+1 = tanh(W t+1 M t+1 + U t+1 (R t+1 H t )) H t+1 = (1 − Z t+1 ) H t + Z t+1 H t+1 (6)</formula><p>where the W and U are trainable weight matrices not shared across time steps, σ(x) = 1/(1 + exp(−x)) is the sigmoid function, and R and Z are the parameters of the reset and update gates. The reset gate controls the amount of information from the previous time step (in H t ) that should propagate to the candidate representations,H t+1 . The new representations H t+1 are finally obtained by linearly interpolating between the previous and the candidate ones, using the coefficients returned by the update gate. Interpretation. Updating node representations through a GRU should in principle allow nodes to encode a combination of local and global signals (low and high values of t, resp.), by allowing them to remember about past iterations. In addition, we also explicitly consider node representations at all iterations when reading out (see Eq. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Readout</head><p>After passing messages and performing updates for T iterations, we obtain a matrix H T ∈ R n×d containing the final vertex representations. LetĜ be graph G without the special document node and its adjacent edges, and matrix H T ∈ R (n−1)×d be the corresponding representation matrix (i.e., H T without the row of the document node).</p><p>We use as our READOUT function the concatenation of self-attention applied toĤ T with the final document node representation. More precisely, we apply a global selfattention mechanism <ref type="bibr" target="#b24">(Lin et al. 2017</ref>) to the rows ofĤ T .</p><p>As shown in Eq. 7,Ĥ T is first passed to a dense layer parameterized by matrix W T A ∈ R d×d . An alignment vector α is then derived by comparing, via dot products, the rows of the output of the dense layer Y T ∈ R (n−1)×d with a trainable vector v T ∈ R d (initialized randomly) and normalizing with a softmax. The normalized alignment coefficients are finally used to compute the attentional vector u T ∈ R d as a weighted sum of the final representationsĤ T .</p><formula xml:id="formula_6">Y T = tanh(Ĥ T W T A ) α T i = exp(Y i T · v T ) n−1 j=1 exp(Y j T · v T ) u T = n−1 i=1 α T iĤ T i<label>(7)</label></formula><p>Note that we tried with multiple context vectors, i.e., with a matrix V T instead of a vector v T , like in <ref type="bibr" target="#b24">(Lin et al. 2017</ref>), but results were not convincing, even when adding a regularization term to the loss to favor diversity among the rows of V T . Master node skip connection. h T G ∈ R 2d is obtained by concatenating u T and the final master node representation. That is, the master node vector bypasses the attention mechanism. This is equivalent to a skip or shortcut connection ). The reason behind this choice is that we expect the special document node to learn a high-level summary about the document, such as its size, vocabulary, etc. (more details are given in subsection 5.2). Therefore, by making the master node bypass the attention layer, we directly inject global information about the document into its final representation. Multi-readout. <ref type="bibr" target="#b43">(Xu et al. 2018a)</ref>, inspired by Jumping Knowledge Networks <ref type="bibr" target="#b44">(Xu et al. 2018b</ref>), recommend to not only use the final representations when performing readout, but also that of the earlier steps. Indeed, as one iterates, node features capture more and more global information. However, retaining more local, intermediary information might be useful too. Thus, instead of applying the readout function only to t = T , we apply it to all time steps and concatenate the results, finally obtaining h G ∈ R T ×2d :</p><formula xml:id="formula_7">h G = CONCAT READOUT H t | t = 1 . . . T<label>(8)</label></formula><p>In effect, with this modification, we take into account features based on information aggregated from subtrees of different heights (from 1 to T ), corresponding to local and global features.  <ref type="bibr" target="#b35">Shang et al. 2019)</ref>. Inspired by this line of research, we propose several hierarchical variants of MPAD, detailed in what follows. In all of them, we represent each sentence in the document as a word co-occurrence network, and obtain an embedding for it by applying MPAD as previously described. MPAD-sentence-att. Here, the sentence embeddings are simply combined through self-attention. MPAD-clique. In this variant, we build a complete graph where each node represents a sentence. We then feed that graph to MPAD, where the feature vectors of the nodes are initialized with the sentence embeddings previously obtained. MPAD-path. This variant, shown in <ref type="figure">Fig. 1</ref>, is similar to the clique one, except that instead of a complete graph, we build a path according to the natural flow of the text. That is, two nodes are linked by a directed edge if the two sentences they represent follow each other in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical variants of MPAD</head><p>Note that the sentence graphs in MPAD-clique and MPAD-path do not feature a master node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the quality of the document embeddings learned by MPAD on 10 document classification datasets, covering the topic identification, coarse and fine sentiment analysis and opinion mining, and subjectivity detection tasks. We briefly introduce the datasets next. Their statistics are reported in <ref type="table" target="#tab_2">Table 1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We evaluate MPAD against multiple state-of-the-art baseline models, including hierarchical ones, to enable fair comparison with the hierarchical MPAD variants. Doc2vec <ref type="bibr" target="#b19">(Le and Mikolov 2014)</ref> is an extension of word2vec that learns vectors for documents in a fully unsupervised manner. Document embeddings are then fed to a logistic regression classifier. CNN <ref type="bibr" target="#b14">(Kim 2014</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model configuration and training</head><p>We preprocess all datasets using the code of (Kim 2014). On Yelp2013, we also replace all tokens appearing strictly less than 6 times with a special UNK token, like in <ref type="bibr" target="#b45">(Yang et al. 2016)</ref>. We then build a directed word co-occurrence network from each document, with a window of size 2. We use two MP iterations (T =2) for the basic MPAD, and two MP iterations at each level, for the hierarchical variants. The output of the readout goes through a dense layer before reaching the final classification layer (or the next level, at the first level of MPAD-path and MPAD-clique). We set d to 64, except on IMDB and Yelp on which d = 128, and use a two-layer MLP. The final graph representations are passed through a softmax for classification. All our dense layers (except in self-attention) use ReLU activation. We train MPAD in an end-to-end fashion by minimizing the crossentropy loss function with the Adam optimizer (Kingma and Ba 2014) and an initial learning rate of 0.001.</p><p>To regulate potential differences in magnitude, we apply batch normalization after concatenating the feature vector of the master node with the self-attentional vector, that is, after the skip connection (see subsection 3.3). To prevent overfitting, we use dropout <ref type="bibr" target="#b37">(Srivastava et al. 2014</ref>) with a rate of 0.5. We select the best epoch, capped at 200, based on the validation accuracy. When cross-validation is used (see 3 rd column of <ref type="table" target="#tab_2">Table 1</ref>), we construct a validation set by randomly sampling 10% of the training set of each fold.</p><p>On all datasets except Yelp2013, we use the publicly available 5 300-dimensional pre-trained Google News vectors <ref type="bibr" target="#b26">(Mikolov et al. 2013)</ref> to initialize the node representations H 0 . On Yelp2013, we follow <ref type="bibr" target="#b45">(Yang et al. 2016</ref>) and learn our own word vectors from the training and validation sets with the gensim implementation of word2vec (Řehůřek and Sojka 2010). MPAD was implemented in Python 3.6 using the PyTorch library. All experiments were run on a single machine consisting of a 3.4 GHz Intel Core i7 CPU with 16 GB of RAM and an NVidia GeForce Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and ablations 5.1 Results</head><p>Experimental results are shown in <ref type="table" target="#tab_5">Table 2</ref>. For the baselines, we provide the scores reported in the original papers. Furthermore, we have evaluated some of the baselines on the rest of our benchmark datasets, and we also report these scores. MPAD reaches best performance on 5 out of 10 datasets, and is close second elsewhere. Moreover, the 5 datasets on which MPAD ranks first widely differ in training set size, number of categories, and prediction task (topic, sentiment, etc.), which indicates that MPAD can perform well in different settings. MPAD vs. hierarchical variants. On 9 datasets out of 10, one or more of the hierarchical variants outperform the vanilla MPAD architecture, highlighting the benefit of explicitly modeling the hierarchical nature of documents.</p><p>However, on Subjectivity, standard MPAD outperforms all hierarchical variants. On TREC, it reaches the same accuracy. We hypothesize that in some cases, using a different graph to separately encode each sentence might be worse than using one single graph to directly encode the document. Indeed, in the single document graph, some words that never appear in the same sentence can be connected through common neighbors, as was explained in subsection 3.1. So, this way, some notion of cross-sentence context is captured while learning representations of words, bigrams, etc. at each MP iteration. This creates better informed representations, resulting in a better document embedding. With the hierarchical variants, on the other hand, each sentence vector is produced in isolation, without any contextual information about the other sentences in the document. Therefore, the final sentence embeddings might be of lower quality, and as a group might also contain redundant/repeated information. When the sentence vectors are finally combined into a document representation, it is too late to take context into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation studies</head><p>To understand the impact of some hyperparameters on performance, we conducted additional experiments on 5 https://code.google.com/archive/p/word2vec the Reuters, Polarity, and IMDB datasets, with the nonhierarchical version of MPAD. Results are shown in <ref type="table">Table 3</ref>. Number of MP iterations. First, we varied the number of message passing iterations from 1 to 4. We can clearly see in <ref type="table">Table 3</ref> that having more iterations improves performance. We attribute this to the fact that we are reading out at each iteration from 1 to T (see Eq. 8), which enables the final graph representation to encode a mixture of low-level and highlevel features. Indeed, in initial experiments involving readout at t=T only, setting T ≥ 2 was always decreasing performance, despite the GRU-based updates (Eq. 5) 6 . These results were consistent with that of <ref type="bibr" target="#b46">(Yao, Mao, and Luo 2019)</ref> and <ref type="bibr" target="#b17">(Kipf and Welling 2016)</ref>, who both are reading out only at t=T too. We hypothesize that node features at T ≥ 2 are too diffuse to be entirely relied upon during readout. More precisely, initially at t=0, node representations capture information about words, at t=1, about their 1-hop neighborhood (bigrams), at t=2, about compositions of bigrams, etc. Thus, pretty quickly, node features become general and diffuse. In such cases, considering also the lower-level, more precise features of the earlier iterations when reading out may be necessary. Undirected edges. On Reuters, using an undirected graph leads to better performance, while on Polarity and IMDB, it is the opposite. This can be explained by the fact that Reuters is a topic classification task, for which the presence or absence of some patterns is important, but not necessarily the order in which they appear, while Polarity and IMDB are sentiment analysis tasks. To capture sentiment, modeling word order is crucial, e.g., in detecting negation. No master node. Removing the master node deteriorates performance across all datasets, clearly showing the value of having such a node. We hypothesize that since the special document node is connected to all other nodes, it is able to encode during message passing a summary of the document. No renormalization. Here, we do not use the renormalization trick of (Kipf and Welling 2016) during MP (see subsection 3.2). That is, Eq. 4 becomes M t+1 = MLP t+1 AH t . In other words, instead of computing a weighted average of the incoming neighbors' feature vectors, we compute a weighted sum of them 7 . Unlike the mean, which captures distributions, the sum captures structural information <ref type="bibr" target="#b43">(Xu et al. 2018a)</ref>. As shown in <ref type="table">Table 3</ref>, using sum instead of mean decreases performance everywhere, suggesting that in our application, capturing the distribution of neighbor representations is more important that capturing their structure. We hypothesize that this is the case because statistical word cooccurrence networks tend to have similar structural properties, regardless of the topic, polarity, sentiment, etc. of the corresponding documents. Neighbors-only. In this experiment, we replaced the GRU COMBINE function (see Eq. 5) with the identity function. That is, we simply have H t+1 =M t+1 . Since A has zero diagonal, by doing so, we completely ignore the previous feature of the node itself when updating its representation.   <ref type="table">Table 3</ref>: Ablation results. The n in nMP refers to the number of message passing iterations. *vanilla model (MPAD in <ref type="table" target="#tab_5">Table 2</ref>).</p><p>That is, the update is based entirely on its neighbors. Except on Reuters (almost no change), performance always suffers, stressing the need to take into account the root node during updates and not only its neighborhood. No master node skip connection. Here, the master node does not bypass the attention mechanism and is treated as a normal node. This leads to better performance on Polarity, but slightly worse performance on Reuters and IMDB.</p><p>6 Related work <ref type="bibr" target="#b17">(Kipf and Welling 2016;</ref><ref type="bibr" target="#b0">Atwood and Towsley 2016;</ref><ref type="bibr" target="#b41">Veličković et al. 2017;</ref><ref type="bibr" target="#b8">Hamilton, Ying, and Leskovec 2017)</ref> conduct some node classification experiments on citation networks, where nodes are scientific papers, i.e., textual data. However, text is only used to derive node feature vectors. The external graph structure, which plays a central role in determining node labels, is completely unrelated to text.</p><p>On the other hand, <ref type="bibr" target="#b10">(Henaff, Bruna, and LeCun 2015;</ref><ref type="bibr" target="#b4">Defferrard, Bresson, and Vandergheynst 2016</ref>) experiment on traditional document classification tasks. They both build k-nearest neighbor similarity graphs based on the Gaussian diffusion kernel. More precisely, <ref type="bibr" target="#b10">(Henaff, Bruna, and LeCun 2015)</ref> build one single graph where nodes are documents and distance is computed in the BoW space. Node features are then used for classification. Closer to our work, (Deffer-rard, Bresson, and Vandergheynst 2016) represent each document as a graph. All document graphs are derived from the same underlying structure. Only node features, corresponding to the entries of the documents' BoW vectors, vary. The underlying, shared structure is that of a k-NN graph where nodes are vocabulary terms and similarity is the cosine of the word embedding vectors. (Defferrard, Bresson, and Vandergheynst 2016) then perform graph classification. However they found performance to be lower than that of a naive Bayes classifier. <ref type="bibr" target="#b33">(Peng et al. 2018</ref>) use a GNN for hierarchical classification into a large taxonomy of topics. This task differs from traditional document classification. The authors represent documents as unweighted, undirected word co-occurrence networks with word embeddings as node features. They then use the spatial GNN of <ref type="bibr" target="#b28">(Niepert, Ahmed, and Kutzkov 2016)</ref> to perform graph classification.</p><p>The work closest to ours is probably that of <ref type="bibr" target="#b46">(Yao, Mao, and Luo 2019)</ref>. The authors adopt the semi-supervised node classification approach of <ref type="bibr" target="#b17">(Kipf and Welling 2016)</ref>. They build one single undirected graph from the entire dataset, with both word and document nodes. Document-word edges are weighted by TF-IDF and word-word edges are weighted by pointwise mutual information derived from cooccurrence within a sliding window. There are no documentdocument edges. The GNN is trained based on the crossentropy loss computed only for the labeled nodes, that is, the documents in the training set. When the final node representations are obtained, one can use that of the test documents to classify them and evaluate prediction performance.</p><p>There are significant differences between <ref type="bibr" target="#b46">(Yao, Mao, and Luo 2019)</ref> and our work. First, our approach is inductive 8 , not transductive. Indeed, while the node classification approach of <ref type="bibr" target="#b46">(Yao, Mao, and Luo 2019)</ref> requires all test documents at training time, our graph classification model is able to perform inference on new, never-seen documents. The downside of representing documents as separate graphs, however, is that we lose the ability to capture corpus-level dependencies. Also, our directed graphs capture word ordering, which is ignored by <ref type="bibr" target="#b46">(Yao, Mao, and Luo 2019)</ref>. Finally, the approach of (Yao, Mao, and Luo 2019) requires computing the PMI for every word pair in the vocabulary, which may be prohibitive on datasets with very large vocabularies. On the other hand, the complexity of MPAD does not depend on vocabulary size.</p><p>MPAD is also related to the Transformer's encoder stack <ref type="bibr" target="#b40">(Vaswani et al. 2017)</ref>. Specifically, the self-attention layer in each encoder updates the representation of each term based on the representations of all the other terms in the document, and can thus be thought of as a function performing the AGGREGATE and COMBINE steps. Stacking multiple encoders can also be thought of as performing multiple MP iterations. The main difference is that the Transformer's selfattention graph is complete, thus ignoring word order and proximity. Also, building that graph requires constructing an adjacency matrix that may become prohibitively large with long documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed an application of the message passing framework to NLP, the Message Passing Attention network for Document understanding (MPAD). Experiments show that our architecture is competitive with the state-of-the-art. By processing weighted, directed word co-occurrence networks, MPAD is sensitive to word order and word-word relationship strength. To capture the hierarchical structure of documents, we also proposed three hierarchical variants of MPAD, that bring improvements over the vanilla model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>GN is supported by the project "ESIGMA" (ANR-17-CE40-0028). We thank the NVidia corporation for the donation of a GPU as part of their GPU grant program.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 4 )</head><label>4</label><figDesc>Subjectivity<ref type="bibr" target="#b31">(Pang and Lee 2004)</ref> contains movie review snippets from Rotten Tomatoes (subjective sentences), and IMDB plot summaries (objective sentences).(5) MPQA (Wiebe, Wilson, and Cardie 2005) is made of positive and negative phrases, annotated as part of the summer 2002 NRRC Workshop on Multi-Perspective Question Answering. (6) IMDB (Maas et al. 2011) is a collection of highly polarized movie reviews (positive/negative). (7) TREC (Li and Roth 2002) consists of questions that are classified into 6 different categories. (8) SST-1 (Socher et al. 2013) contains the same snippets as Polarity, split into multiple sentences and annotated with fine-grained polarity (from very negative to very positive). (9) SST-2 (Socher et al. 2013) is the same as SST-1 but with neutral reviews removed and snippets classified as positive or negative. (10) Yelp2013 (Tang, Qin, and Liu 2015) features reviews obtained from the 2013 Yelp Dataset Challenge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Through the successive MP iterations, it could be argued that MPAD implicitly captures some soft notion of the hierarchical structure of documents (words → bigrams → compositions of bigrams, etc.). However, it might be beneficial to explicitly capture document hierarchy. Hierarchical architectures have brought significant improvements to many NLP tasks, such as language modeling and generation<ref type="bibr" target="#b23">(Lin et al. 2015;</ref><ref type="bibr" target="#b22">Li, Luong, and Jurafsky 2015)</ref>, sentiment and topic classification<ref type="bibr" target="#b39">(Tang, Qin, and Liu 2015;</ref>    </figDesc><table><row><cell></cell><cell cols="4">sentence encoder</cell><cell>doc encoder</cell></row><row><cell>[ readout</cell><cell>skip</cell><cell cols="2">; dense</cell><cell>self att. ]</cell><cell>readout MLP + softmax</cell></row><row><cell></cell><cell></cell><cell>MP</cell><cell></cell><cell></cell><cell>… MP</cell></row><row><cell></cell><cell></cell><cell cols="2">…</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sentence encoder</cell></row><row><cell></cell><cell></cell><cell>…</cell><cell></cell><cell></cell><cell>s 1</cell><cell>…</cell><cell>s D</cell></row><row><cell></cell><cell>w 1</cell><cell>…</cell><cell cols="2">w S</cell></row><row><cell cols="6">Figure 1: Illustration of MPAD-path ( : master node).</cell></row><row><cell cols="6">Yang et al. 2016), and spoken language understanding (Ra-</cell></row><row><cell cols="3">heja and Tetreault 2019;</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Reuters contains stories from the Reuters news agency. We used the ModApte split, removed documents belonging to multiple classes and considered only the 8 classes with the highest number of training examples. (2) BBCSport (Greene and Cunningham 2006) contains sports news articles from the BBC Sport website. (3) Polarity (Pang and Lee 2005) features positive and negative labeled snippets from Rotten Tomatoes.</figDesc><table><row><cell>. # classes av. # words max # words voc. size 8 102.3 964 23,585 8 18.0 50 29,257 5 380.5 1,818 14,340 2 20.3 56 18,777 2 23.3 120 21,335 2 3.0 36 6,248 2 254.3 2,633 141,655 6 10.0 37 9,593 5 7.4 53 17,833 2 9.5 53 17,237 (1) Dataset # training # test examples examples Reuters 5,485 2,189 Snippets 10,060 2,280 BBCSport 737 CV Polarity 10,662 CV Subjectivity 10,000 CV MPQA 10,606 CV IMDB 25,000 25,000 TREC 5,452 500 SST-1 157,918 2,210 SST-2 77,833 1,821 Yelp2013 301,514 33,504 5 143.7 1,184 48,212</cell><cell># pretrained words 15,587 17,142 13,390 16,416 17,896 6,085 104,391 9,125 16,262 15,756 48,212</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used in our experiments. CV indicates that cross-validation was used. # pretrained words refers to the number of words in the vocabulary having an entry in the Google News word vectors (except for Yelp2013).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). 1D convolutional neural network where the word embeddings are used as channels (depth dimensions). DAN<ref type="bibr" target="#b12">(Iyyer et al. 2015)</ref>. The Deep Averaging Network passes the unweighted average of the embeddings of the input words through multiple dense layers and a final softmax.</figDesc><table><row><cell>C-LSTM (Zhou et al. 2015) combines convolutional and re-</cell></row><row><cell>current neural networks. The region embeddings provided</cell></row><row><cell>by a CNN are fed to a LSTM.</cell></row><row><cell>SPGK (Nikolentzos et al. 2017) also models documents as</cell></row><row><cell>word co-occurrence networks. It computes a graph kernel</cell></row><row><cell>that compares shortest paths extracted from the word co-</cell></row><row><cell>occurrence networks and then relies on a SVM.</cell></row><row><cell>WMD (Kusner et al. 2015) is an application of the well-</cell></row><row><cell>known Earth Mover's Distance to text. A k-nearest neighbor</cell></row><row><cell>classifier is used.</cell></row><row><cell>DiSAN (Shen et al. 2018) uses directional self-attention</cell></row><row><cell>along with multi-dimensional attention to generate docu-</cell></row><row><cell>ment representations.</cell></row><row><cell>LSTM-GRNN (Tang, Qin, and Liu 2015) is a hierarchical</cell></row><row><cell>model where sentence embeddings are obtained with a CNN</cell></row><row><cell>and a GRU-RNN is fed the sentence representations to ob-</cell></row><row><cell>tain a document vector.</cell></row><row><cell>HN-ATT (Yang et al. 2016) is another hierarchical model,</cell></row><row><cell>where the same encoder architecture (a bidirectional GRU-</cell></row><row><cell>RNN) is used for both sentences and documents. Self-</cell></row><row><cell>attention is applied at each level.</cell></row><row><cell>Tree-LSTM (Tai, Socher, and Manning 2015) is a general-</cell></row><row><cell>ization of the standard LSTM architecture to constituency</cell></row><row><cell>and dependency parse trees.</cell></row><row><cell>DRNN (Irsoy and Cardie 2014). Recursive neural networks</cell></row><row><cell>are stacked and applied to parse trees.</cell></row></table><note>LSTMN (Cheng, Dong, and Lapata 2016) is an extension of the LSTM model where the memory cell is replaced by a memory network which stores word representations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies. Best performance per column in bold, *best MPAD variant. OOM: &gt;16GB RAM.</figDesc><table><row><cell>MPAD variant</cell><cell>Reut.</cell><cell>Pol.</cell><cell>IMDB</cell></row><row><cell>MPAD 1MP</cell><cell cols="3">96.57 79.91 90.57</cell></row><row><cell>MPAD 2MP*</cell><cell cols="3">97.07 80.24 91.30</cell></row><row><cell>MPAD 3MP</cell><cell cols="3">97.07 80.20 91.24</cell></row><row><cell>MPAD 4MP</cell><cell cols="3">97.48 80.52 91.30</cell></row><row><cell>MPAD 2MP undirected</cell><cell cols="3">97.35 80.05 90.97</cell></row><row><cell>MPAD 2MP no master node</cell><cell cols="3">96.66 79.15 91.09</cell></row><row><cell>MPAD 2MP no renormalization</cell><cell cols="3">96.02 79.84 91.16</cell></row><row><cell>MPAD 2MP neighbors-only</cell><cell cols="3">97.12 79.22 89.50</cell></row><row><cell cols="4">MPAD 2MP no master node skip connection 96.93 80.62 91.12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">except for words at the end/beginning of two successive sentences.3 at t=0, d is equal to the dimensionality of the pretrained word embeddings. 4 the feature of the node itself is already taken into account by our GRU-based COMBINE function (see Eq. 5).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The GRU should in principle enable nodes to retain locality in their representations, by remembering about early iterations.7  Weights are co-occurrence counts, as before.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Note that other GNNs used in inductive settings can be found<ref type="bibr" target="#b8">(Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b41">Veličković et al. 2017</ref>).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>and Towsley</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interaction Networks for Learning about Objects, Relations and Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory-Networks for Machine Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A New Model for Learning in Graph Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scarselli ; Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>the 2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep Convolutional Networks on Graph-Structured Data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Recursive Neural Networksfor Compositionality in Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
	<note>Iyyer et al. 2015</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kearnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Ba</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and Welling</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From Word Embeddings to Document Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kusner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolov ;</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Question Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Hierarchical Neural Autoencoder for Paragraphs and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical Recurrent Neural Network for Document Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<idno>Maas et al. 2011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning Word Vectors for Sentiment Analysis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tex-tRank: Bringing Order into Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
	<note>Mihalcea and Tarau</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Loopy Belief Propagation for Approximate Inference: An Empirical Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiss</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan ;</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 15th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kutzkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shortest-Path Graph Kernels for Document Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikolentzos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kernel Graph Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikolentzos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Artificial Neural Networks</title>
		<meeting>the 27th International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
	<note>Pang and Lee</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Pang and Lee</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02594.[ŘehůřekandSojka2010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">Dialogue Act Classification with Context-Aware Self-Attention. arXiv preprint</note>
	<note>Proceedings of the 2018 World Wide Web Conference</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09491</idno>
		<title level="m">Energy-based Self-attentive Learning of Abstractive Communities for Spoken Language Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>Proceedings of the 32nd AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Liu ; Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-P</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="578" to="593" />
		</imprint>
	</monogr>
	<note>Proceedings of the 28th International Conference on Artificial Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Annotating Expressions of Opinions and Emotions in Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="165" to="210" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How Powerful are Graph Neural Networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08630</idno>
		<title level="m">A C-LSTM Neural Network for Text Classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
