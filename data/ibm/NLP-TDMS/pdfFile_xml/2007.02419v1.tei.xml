<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Joint Detection of Object and Semantic Part</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Morabia</surname></persName>
							<email>morabia2@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Arora</surname></persName>
							<email>jatin2@illinois.edu</email>
						</author>
						<title level="a" type="main">Attention-based Joint Detection of Object and Semantic Part</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>University of Illinois at Urbana-Champaign Tara Vijaykumar</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of joint detection of objects like dog and its semantic parts like face, leg, etc. Our model is created on top of two Faster-RCNN models that share their features to perform a novel Attention-based feature fusion of related Object and Part features to get enhanced representations of both. These representations are used for final classification and bounding box regression separately for both models. Our experiments on the PASCAL-Part 2010 dataset show that joint detection can simultaneously improve both object detection and part detection in terms of mean Average Precision (mAP) at IoU=0.5.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advances in camera and imaging technology, the amount of available online content with images and videos is ever-increasing. This makes it impossible for any human to manually parse and make sense out of images and motivates the development of automated techniques to parse images. One such popular and immensely important task is automated object detection in images. Recently, there have been major developments in this domain ( <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b3">[3]</ref>) but the diversity of this problem poses new challenges motivating active research in this direction.</p><p>Humans identify objects not only by looking at the object as a whole, but looking at various parts of the object to gain confidence. Taking inspiration from this idea, there have been some recent researches ( <ref type="bibr" target="#b2">[2]</ref>, <ref type="bibr" target="#b7">[7]</ref>). A concrete example is, identifying a human in an image by identifying parts like hands, faces etc. to add confidence to classification. Another example is identifying an aeroplane by using information of its wings. <ref type="bibr" target="#b7">[7]</ref> propose a LSTM-based architecture for joint modelling of semantic-part information with full object detection. Recently, self-attention based transformer architectures <ref type="bibr" target="#b6">[6]</ref> have gained popularity which eliminated the use of Recurrent Model like RNN for modeling contextual information. We aim to borrow ideas of self-attention in our setting where semantic parts associated with an ob-ject will act as different contexts which will be weighted based on the attention score.</p><p>In this project, we propose a novel model for joint detection of an object as a whole and its semantic parts using an attention-based feature fusion of object and its related parts. We use the PASCAL-Part 2010 Dataset ([1]) that contains segmentation masks for object and its various semantic parts. From our limited experiments for Animal objects (bird, cat, cow, dog, horse, sheep) and part (face, leg, neck, tail, torso, wings) detection, we find that our joint detection model can give slight improvement for object detection and a reasonable improvement for part detection in terms of mean Average Precision (mAP) at Intersection over Union (IoU) threshold of 0.5. We believe a thorough hyperparameter tuning can lead to even better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model Architecture</head><p>As a starting point, we use pyTorch's Faster-RCNN implementation 1 (with Resnet50 backbone and Feature Pyramid Network) for object detection. Then we will use another Faster-RCNN model for semantic part detection which will be trained only to identify parts and not objects.</p><p>We believe that both these object detection and part detection model performances can be improved by using information of the other in a joint training architecture. Our model architecture is highly motivated from <ref type="bibr" target="#b1">[1]</ref> in that we replace the Relationship modeling and LSTM-based feature fusion with an Attention-based feature fusion architecture. We divide our model architecture in 3 parts: (i) Region Feature Extraction, (ii) Attention-based Feature Fusion, and (iii) Region Classification &amp; Regression as shown in <ref type="figure">Fig:1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Region Feature Extraction</head><p>In this part, we take the image as input and pass it to 2 different Region Proposal Networks (RPN) <ref type="bibr" target="#b5">[5]</ref>, one for object and another for parts, and produce object and part proposals respectively. From the feature map produced by the RPNs, we apply Multi-Scale RoI Align with a Feature Here green boxes are selected for fusion (thresh=0.9) where as red boxes are not selected since the intersection area of object and part is less than fusion thresh * part area for red boxes Pyramid Network to get a fixed sized feature for each object/part proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention-based Feature Fusion</head><p>Once we have all the object and part proposals from the previous step, we get an enhanced representation of object and part proposals that can be used in next step for final object and part detection. To get this new features for object and part boxes, we perform an attention-based feature fusion for object (or parts) and its related parts (or objects). We define a hyperparameter called f usion thresh(f ) that decides which object and part proposals boxes are related to each other and should undergo fusion. We consider those object and part boxes where: intersection area(object, part) ≥ f × area(part) <ref type="figure">Fig:2</ref> shows an example of related objects (or parts) that will be used for fusion to get fused part representation of related parts (or related objects).</p><p>Once we have related parts (or objects) for an object (or part), we learn an attention layer that gives a score for each object-part pair. This score will be used to get weighted average of features of all related parts (or objects) for the object (or part). This weighted average is the enhanced surrounding part (or object) representation for an object (or part). Note that the reason why we are weighing each related parts (or objects) for an object (or part) is because not all related object part pairs are relevant, for example a person can be standing next to a car as well, and not all nearby objects (or parts) are equally important. We expect this to give better performance than naive average of related parts (or objects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Region Classification &amp; Regression</head><p>In this step, we use the object features concatenated with fused related part features for final classification and bounding box regression for objects. Similarly, we use the part features concatenated with fused related object features for final classification and bounding box regression for parts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PASCAL-Part Dataset 2010</head><p>PASCAL VOC 2010 dataset 2 is a popular dataset used to benchmark Object Detection models in which each image has an annotation file containing bounding box coordinates and class labels for each object as shown in <ref type="figure" target="#fig_1">Fig:3</ref> The PASCAL-Part Dataset ( <ref type="bibr" target="#b1">[1]</ref>) is a set of additional annotations for PASCAL VOC 2010 as shown in <ref type="figure" target="#fig_2">Fig:4</ref>. It provides segmentation masks for each body part of the object and silhoutte annotation for categories that do not have a consistent set of parts (eg: boat). The part annotations were originally given in .mat format which we preprocessed using the scipy module to convert it into .json format. The  annotations only contain segmentation masks, so we use the rectangular box containing this segmentation as the bounding box coordinates that are used for Object Detection. This dataset can also be used for animal part detection and general part detection. Note that these part annotations are only given for train and val images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Statistics</head><p>For our study, we use the original train-validation splits provided with the dataset. The training data has 4,998 images and validation set has 5,105 images.   Similarly, WINGS includes lwing, rwing. Since, out overall task is to jointly model object and part detection, we remove those samples from consideration where for an object, the part information is not available. In <ref type="table" target="#tab_2">Table:</ref> 5, we present the distribution of part samples in training and validation sets for the animals category which we are working with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Metric</head><p>Object Detection models are evaluated using the metric called mean Average Precision (mAP). There are many components involved in computing mAP, which are Average Precision, and Intersection over Union (IoU). IoU measures the overlap between ground truth boundaries and predicted boundaries for an object. A prediction is considered correct if IoU &gt; threshold. Average Precision (AP) measures the average of precision values where recall values ranges from 0 to 1. mAP is the mean of AP values for all different classes in the dataset. mAP values are generally reported at IoU threshold of 0.5, but it can also be reported at IoU thresholds [0.5, 0.55, 0.6, ..., 0.95] and taken average of all these mAP values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>For training all models, we use the initial model parameters that are pretrained on MS COCO dataset 3 , which is much larger than PASCAL VOC 2010 dataset and contains about 4 times the number of classes, and replace the last classification and regression layers to make predictions for objects and parts. This transfer learning helps model <ref type="bibr" target="#b3">3</ref>   <ref type="table">Table 7</ref>. Class-wise part detection AP at IOU=0.5 for animals in dataset converge training very quickly in less number of training epochs and achieve better performance. While training, we randomly flip images horizontally with probability 0.5 as a data augmentation technique to make the model more robust. The joint detection model has about 82m parameters and training takes 12 minutes per epoch. We train the model for 15 epochs using SGD optimizer with initial learning rate 1e-3 (decayed by 0.1 after every 5 epochs), momentum 0.9, weight decay 1e-6, batch size of 1 image, and a fusion threshold of 0.9. Note that for VOC2010, no annotated test data is available so all our results are evaluated on PASCAL VOC 2010 val dataset. For faster experiments, we train and report results only on Animal object (bird, cat, cow, dog, horse, sheep) and part (face, leg, neck, tail, torso, wings) classes. As shown in <ref type="table" target="#tab_2">Table:</ref>4, the joint detection model gives slight improvement in terms of mAP@IoU=0.5 for animal object detection, and reasonable improvement for animal part detection. <ref type="table" target="#tab_2">Table:</ref>6 and 7 provide a detailed analysis of Average Precision for each of the object and part classes. We believe a thorough hyperparameter tuning can lead to even better results. In <ref type="figure" target="#fig_4">Figure 6</ref>, we present some sample outputs from our jointly trained model. In (b) we can observe that although the parts of the dog are detected correctly, there are multiple overlapping detections. We present more details on the part detection performance and our experiments with non-max suppression to handle this, in the next subsection. Note: We also experimented with a naive average of related parts (or objects) instead of weighted average using attention scores for joint detection, but that did not lead to any improvement in performance over 2 separate object and part detection models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Discussion on Part Detection performance</head><p>As it can be seen in <ref type="table" target="#tab_2">Table:</ref>4, part detection performance for animal part classes is very less as compared to object detection performance for single part detection model as well as Joint detection model. The reason for this is because in animals sub-dataset that we used for all experiments, there are many persons as well. Just looking at a leg, the model might get confused between person leg and animal leg and hence would detect both legs. Hence, there will be lots of false part detections for animals class. This can be observed below in <ref type="figure">Fig:?</ref> where person leg and torso are also detected even when the annotations used for training are only for animals objects. We expect this issue to be absent when we train the model for detection of all object and part classes.</p><p>Varying Part detection NMS inference threshold: We observed that there is a large number of overlapping part detections and there is high variance in the areas of their detected bounding boxes. This may be a cause of reduced mAP scores. Hence, we tried varying the NMS threshold for part evaluation of our Joint Detection model and present our finding in <ref type="table" target="#tab_2">Table:</ref>8 showing no improvement in performance with varying the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Work</head><p>We expected more improvement from the Joint detection model for object and part detection. Hence, we plan to investigate the feature fusion part deeply. We also plan to create attention score visualizations to see if the scores are coming out to be as expected or not. For the above re-NMS Threshold mAP @I0U=0.5 0. sults, we performed all experiments on animal object and part classes, but we also want to see results on all object and part classes. Finally, we will also tune hyperparameters thoroughly for better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Architecture for Joint Detection of Object and Semantic Part using Attention-based feature fusion (a) Example of related parts for an object. (b) Example of related objects for a part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example images and ground truth object bounding boxes from PASCAL VOC 2010 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Example segmentation annotations from the PASCAL-Part Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Example of merged smaller part classes into a coarse-grained part class for PASCAL-Part Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Sample outputs from our joint trained model. For cleanliness, we have labelled objects and parts separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Example output where person parts are also detected when training and evaluating only for animal classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. There are 20 classes present in the dataset which can be categorized into 4 super categories namely Person, Animal, Vehicle, Indoor. Training and validation contain 10,103 images while testing contains 9,637 images. The twenty object classes in Pascal VOC 2010 dataset are: 1. Person: person 2 http://host.robots.ox.ac.uk/pascal/VOC/voc2010/ 2. Animal: bird, cat, cow, dog, horse, sheep 3. Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train 4. Indoor: bottle, chair, dining table, potted plant, sofa, tvmonitor</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table :</head><label>:</label><figDesc></figDesc><table><row><cell></cell><cell>Class</cell><cell># Samples</cell></row><row><cell></cell><cell>person.head</cell><cell>3960</cell></row><row><cell></cell><cell>person.lear</cell><cell>937</cell></row><row><cell></cell><cell>horse.lear</cell><cell>213</cell></row><row><cell></cell><cell>cow.tail</cell><cell>91</cell></row><row><cell></cell><cell>car.door 3</cell><cell>1</cell></row><row><cell cols="3">Table 2. Uneven part distribution in raw training data</cell></row><row><cell cols="3">Category # Train Samples # Validation Samples</cell></row><row><cell>horse</cell><cell>208</cell><cell>316</cell></row><row><cell>dog</cell><cell>706</cell><cell>708</cell></row><row><cell>cat</cell><cell>563</cell><cell>568</cell></row><row><cell>bird</cell><cell>484</cell><cell>484</cell></row><row><cell>sheep</cell><cell>350</cell><cell>358</cell></row><row><cell>cow</cell><cell>233</cell><cell>239</cell></row><row><cell>1 presents</cell><cell></cell><cell></cell></row><row><cell>the category-wise distribution of object classes in the train-</cell><cell></cell><cell></cell></row><row><cell>ing set.</cell><cell></cell><cell></cell></row><row><cell>For part detection, the training dataset has very fine-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Class-wise object distribution for animals in datasetTable 4. mean Average Precision at Intersection over Union threshold 0.5 for Animal object and part detection on PASCAL-Parts validation dataset</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell>Animal Object Detection Animal Part Detection</cell></row><row><cell></cell><cell cols="2">Object Detection</cell><cell>87.2</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Part Detection</cell><cell>-</cell><cell>51.3</cell></row><row><cell></cell><cell cols="2">Joint Object and Part Detection</cell><cell>87.5</cell><cell>52.0</cell></row><row><cell cols="3">Category # Train Samples # Validation Samples</cell></row><row><cell>FACE</cell><cell>2933</cell><cell>2909</cell></row><row><cell>LEG</cell><cell>5217</cell><cell>5324</cell></row><row><cell>NECK</cell><cell>1628</cell><cell>1651</cell></row><row><cell>TAIL</cell><cell>1148</cell><cell>1163</cell></row><row><cell>TORSO</cell><cell>2978</cell><cell>2974</cell></row><row><cell>WINGS</cell><cell>189</cell><cell>175</cell></row></table><note>grained labels spanning across 166 different classes, with many classes not having enough training samples. Some stats of the distribution of classes in the training data are shown in Table:2. To address the uneven distribution of part labels and for faster experimentation, we work on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Class-wise part distribution for animals in dataset</figDesc><table /><note>only objects in the animals category, and collate fine- grained object-part classes into coarser ones, for example, FACE includes beak, hair, head, nose, lear, lebrow, leye, mouth, rear, rebrow, reye.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/pytorch/vision/blob/master/torchvision/models/detection/ 1 arXiv:2007.02419v1 [cs.CV] 5 Jul 2020</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Code: github.com/kevalmorabia97/Object-and-Semantic-Part-Detection-pyTorch 2. Video Presentation: tinyurl.com/joint-detection References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting lstm for joint object and semantic part detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="498" to="512" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
