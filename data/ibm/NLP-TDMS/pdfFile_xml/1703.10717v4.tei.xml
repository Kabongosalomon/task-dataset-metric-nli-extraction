<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BEGAN: Boundary Equilibrium Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schumm</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz Google</surname></persName>
						</author>
						<title level="a" type="main">BEGAN: Boundary Equilibrium Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Deep Convolutional GANs [15](DCGANs) first introduced a convolutional architecture which led to improved visual quality. More recently, Energy Based GANs [21](EBGANs) were proposed</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks <ref type="bibr" target="#b6">[7]</ref>(GANs) are a class of methods for learning a data distribution p model (x) and realizing a model to sample from it. GANs are architectured around two functions: the generator G(z), which maps a sample z from a random uniform distribution to the data distribution, and the discriminator D(x) which determines if a sample x belongs to the data distribution. The generator and discriminator are typically learned jointly by alternating the training of D and G, based on game theory principles.</p><p>GANs can generate very convincing images, sharper than ones produced by auto-encoders using pixel-wise losses. However, GANs still face many unsolved difficulties: in general they are notoriously difficult to train, even with many tricks applied <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Correct hyper-parameter selection is critical. Controlling the image diversity of the generated samples is difficult. Balancing the convergence of the discriminator and of the generator is a challenge: frequently the discriminator wins too easily at the beginning of training <ref type="bibr" target="#b5">[6]</ref>. GANs easily suffer from modal collapse, a failure mode in which just one image is learned <ref type="bibr" target="#b4">[5]</ref>. Heuristic regularizers such as batch discrimination <ref type="bibr" target="#b15">[16]</ref> and the repelling regularizer <ref type="bibr" target="#b20">[21]</ref> have been proposed to alleviate this problem with varying degrees of success.</p><p>In this paper, we make the following contributions:</p><p>• A GAN with a simple yet robust architecture, standard training procedure with fast and stable convergence. • An equilibrium concept that balances the power of the discriminator against the generator.</p><p>• A new way to control the trade-off between image diversity and visual quality.</p><p>• An approximate measure of convergence. To our knowledge the only other published measure is from Wasserstein GAN <ref type="bibr" target="#b0">[1]</ref> (WGAN), which will be discussed in the next section.</p><p>as a class of GANs that aims to model the discriminator D(x) as an energy function. This variant converges more stably and is both easy to train and robust to hyper-parameter variations. The authors attribute some of these benefits to the larger number of targets in the discriminator. EBGAN likewise implements its discriminator as an auto-encoder with a per-pixel error.</p><p>While earlier GAN variants lacked a measure of convergence, Wasserstein GANs <ref type="bibr" target="#b0">[1]</ref> (WGANs) recently introduced a loss that also acts as a measure of convergence. In their implementation it comes at the expense of slow training, but with the benefit of stability and better mode coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>We use an auto-encoder as a discriminator as was first proposed in EBGAN <ref type="bibr" target="#b20">[21]</ref>. While typical GANs try to match data distributions directly, our method aims to match auto-encoder loss distributions using a loss derived from the Wasserstein distance. This is done using a typical GAN objective with the addition of an equilibrium term to balance the discriminator and the generator. Our method has an easier training procedure and uses a simpler neural network architecture compared to typical GAN techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wasserstein distance lower bound for auto-encoders</head><p>We wish to study the effect of matching the distribution of the errors instead of matching the distribution of the samples directly. We first introduce the auto-encoder loss, then we compute a lower bound to the Wasserstein distance between the auto-encoder loss distributions of real and generated samples.</p><p>We first introduce L : R Nx → R + the loss for training a pixel-wise autoencoder as:</p><formula xml:id="formula_0">L(v) = |v − D(v)| η where    D : R Nx → R Nx is the autoencoder function. η ∈ {1, 2} is the target norm. v ∈ R Nx is a sample of dimension N x .</formula><p>Let µ 1,2 be two distributions of auto-encoder losses, let Γ(µ 1 , µ 2 ) be the set all of couplings of µ 1 and µ 2 , and let m 1,2 ∈ R be their respective means. The Wasserstein distance can be expressed as:</p><formula xml:id="formula_1">W 1 (µ 1 , µ 2 ) = inf γ∈Γ(µ1,µ2) E (x1,x2)∼γ [|x 1 − x 2 |]</formula><p>Using Jensen's inequality, we can derive a lower bound to W 1 (µ 1 , µ 2 ):</p><formula xml:id="formula_2">inf E[|x 1 − x 2 |] inf |E[x 1 − x 2 ]| = |m 1 − m 2 |<label>(1)</label></formula><p>It is important to note that we are aiming to optimize a lower bound of the Wasserstein distance between auto-encoder loss distributions, not between sample distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GAN objective</head><p>We design the discriminator to maximize equation 1 between auto-encoder losses. Let µ 1 be the distribution of the loss L(x), where x are real samples. Let µ 2 be the distribution of the loss L(G(z)), where G : R Nz → R Nx is the generator function and z ∈ [−1, 1] Nz are uniform random samples of dimension N z .</p><p>Since m 1 , m 2 ∈ R + there are only two possible solutions to maximizing |m 1 − m 2 |:</p><formula xml:id="formula_3">(a)    W 1 (µ 1 , µ 2 ) m 1 − m 2 m 1 → ∞ m 2 → 0 or (b)    W 1 (µ 1 , µ 2 ) m 2 − m 1 m 1 → 0 m 2 → ∞</formula><p>We select solution (b) for our objective since minimizing m 1 leads naturally to auto-encoding the real images. Given the discriminator and generator parameters θ D and θ G , each updated by minimizing the losses L D and L G , we express the problem as the GAN objective, where z D and z G are samples from z:</p><formula xml:id="formula_4">L D = L(x; θ D ) − L(G(z D ; θ G ); θ D ) for θ D L G = −L D for θ G<label>(2)</label></formula><p>Note that in the following we use an abbreviated notation: G(·) = G(·, θ G ) and L(·) = L(·; θ D ).</p><p>This equation, while similar to the one from WGAN <ref type="bibr" target="#b0">[1]</ref>, has two important differences: First we match distributions between losses, not between samples. And second, we do not explicitly require the discriminator to be K-Lipschitz since we are not using the Kantorovich and Rubinstein duality theorem <ref type="bibr" target="#b17">[18]</ref>.</p><p>For function approximations, in our case deep neural networks, we must also consider the representational capacities of each function G and D. This is determined both by the model implementing the function and the number of parameters. It is typically the case that G and D are not well balanced and the discriminator D wins easily. To account for this situation we introduce an equilibrium concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Equilibrium</head><p>In practice it is crucial to maintain a balance between the generator and discriminator losses; we consider them to be at equilibrium when:</p><formula xml:id="formula_5">E [L(x)] = E [L(G(z))]<label>(3)</label></formula><p>If we generate samples that cannot be distinguished by the discriminator from real ones, the distribution of their errors should be the same, including their expected error. This concept allows us to balance the effort allocated to the generator and discriminator so that neither wins over the other.</p><p>We can relax the equilibrium with the introduction of a new hyper-parameter γ ∈ [0, 1] defined as</p><formula xml:id="formula_6">γ = E [L(G(z))] E [L(x)]<label>(4)</label></formula><p>In our model, the discriminator has two competing goals: auto-encode real images and discriminate real from generated images. The γ term lets us balance these two goals. Lower values of γ lead to lower image diversity because the discriminator focuses more heavily on auto-encoding real images.</p><p>We will refer to γ as the diversity ratio. There is a natural boundary for which images are sharp and have details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Boundary Equilibrium GAN</head><p>The BEGAN objective is:</p><formula xml:id="formula_7">   L D = L(x) − k t .L(G(z D )) for θ D L G = L(G(z G )) for θ G k t+1 = k t + λ k (γL(x) − L(G(z G ))) for each training step t</formula><p>We use Proportional Control Theory to maintain the equilibrium E [L(G(z))] = γE [L(x)]. This is implemented using a variable k t ∈ [0, 1] to control how much emphasis is put on L(G(z D )) during gradient descent. We initialize k 0 = 0. λ k is the proportional gain for k; in machine learning terms, it is the learning rate for k. We used 0.001 in our experiments. In essence, this can be thought of as a form of closed-loop feedback control in which k t is adjusted at each step to maintain equation 4.</p><p>In early training stages, G tends to generate easy-to-reconstruct data for the auto-encoder since generated data is close to 0 and the real data distribution has not been learned accurately yet. This  In contrast to traditional GANs which require alternating training D and G, or pretraining D, our proposed method BEGAN requires neither to train stably. Adam <ref type="bibr" target="#b9">[10]</ref> was used during training with the default hyper-parameters. θ D and θ G are updated independently based on their respective losses with separate Adam optimizers. We typically used a batch size of n = 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Convergence measure</head><p>Determining the convergence of GANs is generally a difficult task since the original formulation is defined as a zero-sum game. As a consequence, one loss goes up when the other goes down. The number of epochs or visual inspection are typically the only practical ways to get a sense of how training has progressed.</p><p>We derive a global measure of convergence by using the equilibrium concept: we can frame the convergence process as finding the closest reconstruction L(x) with the lowest absolute value of the instantaneous process error for the proportion control algorithm |γL(x) − L(G(z G ))|. This measure is formulated as the sum of these two terms:</p><formula xml:id="formula_8">M global = L(x) + |γL(x) − L(G(z G ))|</formula><p>This measure can be used to determine when the network has reached its final state or if the model has collapsed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model architecture</head><p>The discriminator D : R Nx → R Nx is a convolutional deep neural network architectured as an autoencoder. N x = H × W × C is shorthand for the dimensions of x where H, W, C are the height, width and colors. We use an auto-encoder with both a deep encoder and decoder. The intent is to be as simple as possible to avoid typical GAN tricks.</p><p>The structure is shown in <ref type="figure" target="#fig_1">figure 1</ref>. We used 3 × 3 convolutions with exponential linear units <ref type="bibr" target="#b3">[4]</ref> (ELUs) applied at their outputs. Each layer is repeated a number of times (typically 2). We observed that more repetitions led to even better visual results. The convolution filters are increased linearly with each down-sampling. Down-sampling is implemented as sub-sampling with stride 2 and upsampling is done by nearest neighbor. At the boundary between the encoder and the decoder, the tensor of processed data is mapped via fully connected layers, not followed by any non-linearities, to and from an embedding state h ∈ R N h where N h is the dimension of the auto-encoder's hidden state.</p><p>The generator G : R Nz → R Nx uses the same architecture (though not the same weights) as the discriminator decoder. We made this choice only for simplicity. The input state is z ∈ [−1, 1] Nz sampled uniformly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Optional improvements</head><p>This simple architecture achieves high quality results and demonstrates the robustness of our technique.</p><p>Further, optional, refinements aid gradient propagation and produce yet sharper images. Taking inspiration from deep residual networks <ref type="bibr" target="#b7">[8]</ref>, we initialize the network using vanishing residuals: for successive same sized layers, the layer's input is combined with its output: in x+1 = carry × in x + (1 − carry) × out x . In our experiments, we start with carry = 1 and progressively decrease it to 0 over 16000 steps (one epoch).</p><p>We also introduce skip connections <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref> to help gradient propagation <ref type="bibr" target="#b2">[3]</ref>. The first decoder tensor h0 is obtained from projecting h to an 8 × 8 × n tensor. After each upsampling step, the output is concatenated with h0 upsampled to the same dimensions. This creates a skip connection between the hidden state and each successive upsampling layer of the decoder.</p><p>We did not explore other techniques typically used in GANs, such as batch normalization, dropout, transpose convolutions or exponential growth for convolution filters, though they might further improve upon these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We trained our model using Adam with an initial learning rate of 0.0001, decaying by a factor of 2 when the measure of convergence stalls. Modal collapses or visual artifacts were observed sporadically with high initial learning rates, however simply reducing the learning rate was sufficient to avoid them. We trained models for varied resolutions from 32 to 256, adding or removing convolution layers to adjust for the image size, keeping a constant final down-sampled image size of 8 × 8. We used N h = N z = 64 in most of our experiments with this dataset.</p><p>Our biggest model for 128 × 128 images used a convolution with n = 128 filters and had a total of 17.3 × 10 6 trainable parameters. Training time was about 2.5 days on four P100 GPUs. Smaller models of size 32 × 32 could train in a few hours on a single GPU.</p><p>We use a dataset of 360K celebrity face images for training in place of CelebA <ref type="bibr" target="#b11">[12]</ref>. This dataset has a larger variety of facial poses, including rotations around the camera axis. These are more varied and potentially more difficult to model than the aligned faces from CelebA, presenting an interesting challenge. We preferred the use of faces as a visual estimator since humans excel at identifying flaws in faces. <ref type="figure" target="#fig_2">Figure 2b</ref> shows some representative samples drawn uniformly from z at resolutions of 128 × 128. Higher resolution images, while maintaining coherency, tend to lose sharpness, but this may be improved upon with additional hyper-parameter explorations. To our knowledge these are the first anatomically coherent high-resolution results except for Stacked GANs <ref type="bibr" target="#b19">[20]</ref> which has shown some promise for flowers and birds at up to 256 × 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image diversity and quality</head><p>We observe varied poses, expressions, genders, skin colors, light exposure, and facial hair. However we did not see glasses, we see few older people and there are more women than men. For comparison we also displayed some EBGAN <ref type="bibr" target="#b20">[21]</ref> results in figure 2a. We must keep in mind that these are trained on different datasets so direct comparison is difficult.</p><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we compared the effect of varying γ. The model appears well behaved, still maintaining a degree of image diversity across the range of values. At low values, the faces look overly uniform.  Variety increases with γ but so do artifacts. Our observations seem to contradict those of <ref type="bibr" target="#b13">[14]</ref> that diversity and quality were independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Space continuity</head><p>To estimate the modal coverage of our generator we take real images and find their corresponding z r embedding for the generator. This is done using Adam to find a value for z r that minimizes e r = |x r − G(z r )|. Mapping to real images is not the goal of the model but it provides a way of testing its ability to generalize. By interpolating the z r embeddings between two real images, we verify that the model generalized the image contents rather than simply memorizing them. <ref type="figure">Figure 4c</ref> displays interpolations on z r between real images at 128 × 128 resolution; these images were not part of the training data. The first and last columns contain the real images to be represented and interpolated. The images immediately next to them are their corresponding approximations while the images in-between are the results of linear interpolation in z r . For comparison with the current state of the art for generative models, we included ALI <ref type="bibr" target="#b4">[5]</ref> results at 64 × 64 (figure 4a) and conditional PixelCNN <ref type="bibr" target="#b12">[13]</ref> results at 32 × 32 (figure 4b) both trained on different data sets (higher resolutions were not available to us for these models). In addition <ref type="figure">figure 4d</ref> showcases interpolation between an image and its mirror.</p><p>Sample diversity, while not perfect, is convincing; the generated images look relatively close to the real ones. The interpolations show good continuity. On the first row, the hair transitions in a natural way and intermediate hairstyles are believable, showing good generalization. It is also worth noting that some features are not represented such as the cigarette in the left image. The second and last rows show simple rotations. While the rotations are smooth, we can see that profile pictures are not captured as well as camera facing ones. We assume this is due to profiles being less common in our dataset. Finally the mirror example demonstrates separation between identity and rotation. A surprisingly realistic camera-facing image is derived from a single profile image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Convergence measure and image quality</head><p>The convergence measure M global was conjectured earlier to measure the convergence of the BE-GAN model. As can be seen in <ref type="figure" target="#fig_5">figure 5</ref> this measure correlates well with image fidelity. We can also  <ref type="figure">Figure 4</ref>: Interpolations of real images in latent space see from this plot that the model converges quickly, just as was originally reported for EBGANs. This seems to confirm the fast convergence property comes from pixel-wise losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Equilibrium for unbalanced networks</head><p>To test the robustness of the equilibrium balancing technique, we performed an experiment advantaging the discriminator over the generator, and vice versa. <ref type="figure" target="#fig_6">Figure 6</ref> displays the results.</p><p>By maintaining the equilibrium the model remained stable and converged to meaningful results. The image quality suffered as expected with low dimensionality of h due to the reduced capacity of the discriminator. Surprisingly, reducing the dimensionality of z had relatively little effect on image diversity or quality.   To measure quality and diversity numerically, we computed the inception score <ref type="bibr" target="#b15">[16]</ref> on CIFAR-10 images. The inception score is a heuristic that has been used for GANs to measure single sample quality and diversity on the inception model. We train an unconditional version of our model and compare to previous unsupervised results. The goal is to generate a distribution that is representative of the original data.</p><p>A comparison to similar works on models trained entirely unsupervised is shown in table 1. With the exception of Denoising Feature Matching <ref type="bibr" target="#b18">[19]</ref> (DFM), our score is better than other GAN techniques that directly aim to match the data distribution. This seems to confirm experimentally that matching loss distributions of the auto-encoder is an effective indirect method of matching data distributions. DFM appears compatible with our method and combining them is a possible avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>There are still many unexplored avenues. Does the discriminator have to be an auto-encoder? Having pixel-level feedback seems to greatly help convergence, however using an auto-encoder has its drawbacks: what latent space size is best for a dataset? When should noise be added to the input and how much? What impact would using other varieties of auto-encoders such Variational Auto-Encoders <ref type="bibr" target="#b10">[11]</ref> (VAEs) have?</p><p>More fundamentally, we note that our objective bears a superficial resemblance to the WGAN <ref type="bibr" target="#b0">[1]</ref> objective. Is the auto-encoder combined with the equilibrium concept fulfilling a similar bounding functionality as the K-Lipschitz constraint in the WGAN formulation?</p><p>We introduced BEGAN, a GAN that uses an auto-encoder as the discriminator. Using proportional control theory, we proposed a novel equilibrium method for balancing adversarial networks. We believe this method has many potential applications such as dynamically weighing regularization terms or other heterogeneous objectives. Using this equilibrium method, the network converges to diverse and visually pleasing images. This remains true at higher resolutions with trivial modifications. Training is stable, fast and robust to parameter changes. It does not require a complex alternating training procedure. Our approach provides at least partial solutions to some outstanding GAN problems such as measuring convergence, controlling distributional diversity and maintaining the equilibrium between the discriminator and the generator. While we could partially control the diversity of generator by influencing the discriminator, there is clearly still room for improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Network architecture for the generator and discriminator. yields to L(x) &gt; L(G(z)) early on and this is maintained for the whole training process by the equilibrium constraint.The introductions of the approximation in equation 1 and γ in equation 4 have an impact on our modeling of the Wasserstein distance. Consequently, examination of samples generated from various γ values is of primary interest as will be shown in the results section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Random samples comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Random 64x64 samples at varying γ ∈ {0.3, 0.5, 0.7}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) ALI<ref type="bibr" target="#b4">[5]</ref> (64x64) (b) Conditional PixelCNN [13] (32x32) (c) Our results (128x128 with 128 filters) (d) Mirror interpolations (our results 128x128 with 128 filters)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Quality of the results w.r.t. the measure of convergence (128x128 with 128 filters) (a) Starved generator (z = 16 and h = 128) (b) Starved discriminator (z = 128 and h = 16)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Advantaging one network over the other</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jay Han, Llion Jones and Ankur Parikh for their help with the manuscript, Jakob Uszkoreit for his constant support, Wenze Hu, Aaron Sarna and Florian Schroff for technical support. Special thanks to Grant Reaber for his in-depth feedback on Wasserstein distance computation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00573</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Nips 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05328</idno>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02780</idno>
		<title level="m">Jascha Sohl-Dickstein, and Anelia Angelova. Improved generator objectives for gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving generative adversarial networks with denoising feature matching. ICLR submissions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">Metaxas</forename><surname>Stackgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03242</idno>
		<title level="m">Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
