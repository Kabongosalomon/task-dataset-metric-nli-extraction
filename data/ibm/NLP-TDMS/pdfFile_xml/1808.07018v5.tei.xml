<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hypernetwork Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-07-15">15 Jul 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
							<email>ivana.balazevic@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
							<email>carl.allen@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hypernetwork Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-07-15">15 Jul 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs are graphical representations of large databases of facts, which typically suffer from incompleteness. Inferring missing relations (links) between entities (nodes) is the task of link prediction. A recent state-of-the-art approach to link prediction, ConvE, implements a convolutional neural network to extract features from concatenated subject and relation vectors. Whilst results are impressive, the method is unintuitive and poorly understood. We propose a hypernetwork architecture that generates simplified relation-specific convolutional filters that (i) outperforms ConvE and all previous approaches across standard datasets; and (ii) can be framed as tensor factorization and thus set within a well established family of factorization models for link prediction. We thus demonstrate that convolution simply offers a convenient computational means of introducing sparsity and parameter tying to find an effective trade-off between non-linear expressiveness and the number of parameters to learn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs, such as WordNet, Freebase, and Google Knowledge Graph, are large graph-structured databases of facts, containing information in the form of triples (e 1 , r, e 2 ), with e 1 and e 2 representing subject and object entities and r a relation between them. They are considered important information resources, used for a wide variety of tasks ranging from question answering to information retrieval and text summarization. One of the main challenges with existing knowledge graphs is their incompleteness: many of the links between entities in the graph are missing. This has inspired substantial work in the field of link prediction, i.e. the task of inferring missing links in knowledge graphs.</p><p>Until recently, many approaches to link prediction have been based on different factorizations of a 3-moded binary tensor representation of the training triples <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref>. Such approaches are shallow and linear, with limited expressiveness. However, attempts to increase expressiveness with additional fully connected layers and non-linearities often lead to overfitting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. For this reason, <ref type="bibr">Dettmers et al.</ref> introduce ConvE, a model that uses 2D convolutions over reshaped and concatenated entity and relation embeddings <ref type="bibr" target="#b2">[3]</ref>. They motivate the use of convolutions by being parameter efficient and fast to compute on a GPU, as well as having various robust methods from computer vision to prevent overfitting. Even though results achieved by ConvE are impressive, it is highly unintuitive that convolution -particularly 2D convolution -should be effective for extracting information from 1D entity and relation embeddings.</p><p>In this paper, we introduce HypER, a model that uses a hypernetwork <ref type="bibr" target="#b4">[5]</ref> to generate convolutional filter weights for each relation. A hypernetwork is an approach by which one network generates weights for another network, that can be used to enable weight-sharing across layers and to dynamically synthesize weights given an input. In our context, we generate relation-specific filter weights to process input entities, and also achieve multi-task knowledge sharing across relations in the knowledge graph. Our proposed HypER model uses a hypernetwork to generate a set of 1D relation-specific filters to process the subject entity embeddings. This simplifies the interaction between subject entity and relation embeddings compared to ConvE, in which a global set of 2D filters are convolved over reshaped and concatenated subject entity and relation embeddings, which is unintuitive as it suggests the presence of 2D structure in word embeddings. Moreover, interaction between subject and relation in ConvE depends on an arbitrary choice about how they are reshaped and concatenated. In contrast, HypER's hypernetwork generates relation-specific filters, and thus extracts relation-specific features from the subject entity embedding. This necessitates no 2D reshaping, and allows entity and relation to interact more completely, rather than only around the concatenation boundary. We show that this simplified approach, in addition to improving link prediction performance, can be understood in terms of tensor factorization, thus placing HypER within a well established family of factorization models. The apparent obscurity of using convolution within word embeddings is thereby explained as simply a convenient computational means of introducing sparsity and parameter tying.</p><p>We evaluate HypER against several previously proposed link prediction models using standard datasets (FB15k-237, WN18RR, FB15k, WN18, YAGO3-10), across which it consistently achieves state-of-the-art performance. In summary, our key contributions are:</p><p>proposing a new model for link prediction (HypER) which achieves state-ofthe-art performance across all standard datasets; -showing that the benefit of using convolutional instead of fully connected layers is due to restricting the number of dimensions that interact (i.e. explicit regularization), rather than finding higher dimensional structure in the embeddings (as implied by ConvE); and showing that HypER in fact falls within a broad class of tensor factorization models despite the use of convolution, which serves to provide a good tradeoff between expressiveness and number of parameters to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Numerous matrix factorization approaches to link prediction have been proposed. An early model, RESCAL <ref type="bibr" target="#b11">[12]</ref>, tackles the link prediction task by optimizing a scoring function containing a bilinear product between vectors for each of the subject and object entities and a full rank matrix for each relation. Dist-Mult <ref type="bibr" target="#b22">[23]</ref> can be viewed as a special case of RESCAL with a diagonal matrix per relation type, which limits the linear transformation performed on entity vectors to a stretch. ComplEx <ref type="bibr" target="#b21">[22]</ref> extends DistMult to the complex domain. TransE <ref type="bibr" target="#b0">[1]</ref> is an affine model that represents a relation as a translation operation between subject and object entity vectors. A somewhat separate line of link prediction research introduces Relational Graph Convolutional Networks (R-GCNs) <ref type="bibr" target="#b14">[15]</ref>. R-GCNs use a convolution operator to capture locality information in graphs. The model closest to our own and which we draw inspiration from, is ConvE <ref type="bibr" target="#b2">[3]</ref>, where a convolution operation is performed on the subject entity vector and the relation vector, after they are each reshaped to a matrix and lengthwise concatenated. The obtained feature maps are flattened, put through a fully connected layer, and the inner product is taken with all object entity vectors to generate a score for each triple. Advantages of ConvE over previous approaches include its expressiveness, achieved by using multiple layers of non-linear features, its scalability to large knowledge graphs, and its robustness to overfitting. However, it is not intuitive why convolving across concatenated and reshaped subject entity and relation vectors should be effective.</p><p>The proposed HypER model does no such reshaping or concatenation and thus avoids both implying any inherent 2D structure in the embeddings and restricting interaction to the concatenation boundary. Instead, HypER convolves every dimension of the subject entity embedding with relation-specific convolutional filters generated by the hypernetwork. This way, entity and relation embeddings are combined in a non-linear (quadratic) manner, unlike the linear combination (weighted sum) in ConvE. This gives HypER more expressive power, while also reducing parameters.</p><p>Interestingly, we find that the differences in moving from ConvE to HypER in fact bring the factorization and convolutional approaches together, since the 1D convolution process is equivalent to multiplication by a highly sparse tensor with tied weights (see <ref type="figure" target="#fig_1">Figure 2</ref>). The multiplication of this "convolutional tensor" (defined by the relation embedding and hypernetwork) and other weights gives an implicit relation matrix, corresponding to those in e.g. RESCAL, DistMult and ComplEx. Other than the method of deriving these relation matrices, the key difference to existing factorization approaches is the ReLU non-linearity applied prior to interaction with the object embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Link Prediction</head><p>In link prediction, the aim is to learn a scoring function φ that assigns a score s = φ(e 1 , r, e 2 ) ∈ R to each input triple (e 1 , r, e 2 ), where e 1 , e 2 ∈ E are subject and object entities and r ∈ R a relation. The score indicates the strength of prediction that the given triple corresponds to a true fact, with positive scores meaning true and negative scores, false. Link prediction models typically map entity pair e 1 , e 2 to their corresponding distributed embedding representations e 1 , e 2 ∈ R de and a score is assigned using a relation-specific function, <ref type="table">Table 1</ref>. Scoring functions of state-of-the-art link prediction models, the dimensionality of their relation parameters, and their space complexity. de and dr are the dimensions of entity and relation embeddings respectively, e2 ∈ C de denotes the complex conjugate of e2, and e 1 , w r ∈ R dw ×d h denote a 2D reshaping of e1 and wr respectively.</p><p>* is the convolution operator, Fr = vec −1 (wrH) the matrix of relation specific convolutional filters, vec is a vectorization of a matrix and vec −1 its inverse, f is a non-linear function, and ne and nr respectively denote the number of entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Scoring Function</head><p>Relation Parameters Space Complexity</p><formula xml:id="formula_0">RESCAL [12] e ⊤ 1 Wre2 Wr ∈ R de 2 O(nede + nrd 2 e ) TransE [1] e1 + wr − e2 wr ∈ R de O(nede + nrde) NTN [17] u ⊤ r f (e1W [1..k] r e2 + Vr e1 e2 + br) Wr ∈ R de 2 k , Vr ∈ R 2dek , ur ∈ R k , br ∈ R k O(nede + nrde 2 k) DistMult [23] e1, wr, e2 wr ∈ R de O(nede + nrde) ComplEx [22] Re( e1, wr, e2 ) wr ∈ C de O(nede + nrde) ConvE [3] f (vec(f ([e 1 ; w r ] * w))W)e2 wr ∈ R dr O(nede + nrdr) HypER (ours) f (vec(e1 * vec −1 (wrH))W)e2 wr ∈ R dr O(nede + nrdr) s = φ r (e 1 , e 2 )</formula><p>. The majority of link prediction models apply the logistic sigmoid function σ(·) to the score to give a probabilistically interpretable prediction p = σ(s) ∈ [0, 1] as to whether the queried fact is true. The scoring functions for models from across the literature and HypER are summarized in <ref type="table">Table 1</ref>, together with the dimensionality of their relation parameters and the significant terms of their space complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hypernetwork Knowledge Graph Embeddings</head><p>In this work, we propose a novel hypernetwork model for link prediction in knowledge graphs. In summary, the hypernetwork projects a vector embedding of each relation via a fully connected layer, the result of which is reshaped to give a set of convolutional filter weight vectors for each relation. We explain this process in more detail below. The idea of using convolutions on entity and relation embeddings stems from computer vision, where feature maps reflect patterns in the image such as lines or edges. Their role in the text domain is harder to interpret, since little is known of the meaning of a single dimension in a word embedding. We believe convolutional filters have a regularizing effect when applied to word embeddings (compared to the corresponding full tensor), as the filter size restricts which dimensions of embeddings can interact. This allows nonlinear expressiveness while limiting overfitting by using few parameters. A visualization of HypER is given in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scoring Function and Model Architecture</head><p>The relation-specific scoring function for the HypER model is:</p><formula xml:id="formula_1">φ r (e 1 , e 2 ) = f (vec(e 1 * F r )W)e 2 = f (vec(e 1 * vec −1 (w r H))W)e 2 ,<label>(1)</label></formula><p>where the vec −1 operator reshapes a vector to a matrix, and non-linearity f is chosen to be a rectified linear unit (ReLU).   In the feed-forward pass, the model obtains embeddings for the input triple from the entity and relation embedding matrices E ∈ R ne×de and R ∈ R nr ×dr . The hypernetwork is a fully connected layer H ∈ R dr×l f n f (l f denotes filter length and n f the number of filters per relation, i.e. output channels of the convolution) that is applied to the relation embedding w r ∈ R dr . The result is reshaped to generate a matrix of convolutional filters F r = vec −1 (w r H) ∈ R l f ×n f . Whilst the overall dimensionality of the filter set is l f n f , the rank is restricted to d r to encourage parameter sharing between relations.</p><p>The subject entity embedding e 1 is convolved with the set of relation-specific filters F r to give a 2D feature map M r ∈ R lm×n f , where l m = d e − l f + 1 is the feature map length. The feature map is vectorized to vec(M r ) ∈ R lmn f , and projected to d e -dimensional space by the weight matrix W ∈ R lmn f ×de . After applying a ReLU activation function, the result is combined by way of inner product with each and every object entity embedding e 2 (i) , where i varies over all entities in the dataset (of size n e ), to give a vector of scores. The logistic sigmoid is applied element-wise to the score vector to obtain the predicted probability of each prospective triple being true p i = σ(φ r (e 1 , e 2 (i) )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Understanding HypER as Tensor Factorization</head><p>Having described the HypER architecture, we can view it as a series of tensor operations by considering the hypernetwork H and weight matrix W as tensors H ∈ R dr×l f ×n f and W ∈ R lm×n f ×de respectively. The act of convolving F r = w r ⊗ H over the subject entity embedding e 1 is equivalent to the multiplication of e 1 by a sparse tensor F r within which F r is diagonally duplicated with zeros elsewhere (see <ref type="figure" target="#fig_1">Figure 2</ref>). The result is multiplied by W to give a vector, which is subject to ReLU before the final dot product with e 2 . Linearity allows the product F r ⊗ W to be considered separately as generating a d e × d e matrix for each relation. Further, rather than duplicating entries of F r within F r , we can generalize F r to a relation-agnostic sparse 4 moded tensor F ∈ R dr×de×n f ×lm by replacing entries with d r -dimensional strands of H. Thus, the HypER model can be described explicitly as tensor multiplication of e 1 , e 2 and w r with a core tensor F ⊗W ∈ R de×de×dr , where F is heavily constrained in terms of its number of free variables. This insight allows HypER to be viewed in a very similar light to the family of factorization approaches to link prediction, such as RESCAL, DistMult and ComplEx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Procedure</head><p>Following the training procedure introduced by [3], we use 1-N scoring with the Adam optimizer <ref type="bibr" target="#b7">[8]</ref> to minimize the binary cross-entropy loss:</p><formula xml:id="formula_2">L(p, y) = − 1 n e i (y i log(p i ) + (1 − y i )log(1 − p i )),<label>(2)</label></formula><p>where y ∈ R ne is the label vector containing ones for true triples and zeros otherwise, subject to label smoothing. Label smoothing is a widely used technique shown to improve generalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>. Label smoothing changes the groundtruth label distribution by adding a uniform prior to encourage the model to be less confident, achieving a regularizing effect. 1-N scoring refers to simultaneously scoring (e 1 , r, E), i.e. for all entities e 2 ∈ E, in contrast to 1-1 scoring, the practice of training individual triples (e 1 , r, e 2 ) one at a time. As shown by <ref type="bibr" target="#b2">[3]</ref>, 1-N scoring offers a significant speedup (3x on train and 300x on test time) and improved accuracy compared to 1-1 scoring. A potential extension of the HypER model described above would be to apply convolutional filters to both subject and object entity embeddings. However, since this is not trivially implementable with 1-N scoring and wanting to keep its benefits, we leave this to future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Number of Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate our HypER model on the standard link prediction task using the following datasets (see <ref type="table" target="#tab_3">Table 3</ref>):</p><p>FB15k [1] a subset of Freebase, a large database of facts about the real world. WN18 [1] a subset of WordNet, containing lexical relations between words. FB15k-237 created by <ref type="bibr" target="#b20">[21]</ref>, noting that the validation and test sets of FB15k and WN18 contain the inverse of many relations present in the training set, making it easy for simple models to do well. FB15k-237 is a subset of FB15k with the inverse relations removed.</p><p>WN18RR <ref type="bibr" target="#b2">[3]</ref> a subset of WN18, created by removing the inverse relations. YAGO3-10 [3] a subset of YAGO3 <ref type="bibr" target="#b9">[10]</ref>, containing entities which have a minimum of 10 relations each. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We implement HypER in PyTorch <ref type="bibr" target="#b12">[13]</ref> and make our code publicly available. 1 To accelerate training and prevent overfitting, we use batch normalization <ref type="bibr" target="#b5">[6]</ref> and dropout <ref type="bibr" target="#b17">[18]</ref> on the input embeddings, feature maps and the hidden layer. We perform a hyperparameter search and select the best performing model by mean reciprocal rank (MRR) on the validation set. Having tested the values {0., 0.1, 0.2, 0.3}, we find that the following combination of parameters works well across all datasets: input dropout 0.2, feature map dropout 0.2, and hidden dropout 0.3, apart from FB15k-237, where we set input dropout to 0.3. We select the learning rate from {0.01, 0.005, 0.003, 0.001, 0.0005, 0.0001} and exponential learning rate decay from {1., 0.99, 0.995} for each dataset and find the best performing learning rate and learning rate decay to be dataset-specific. We set the convolution stride to 1, number of feature maps to 32 with the filter size 3 × 3 for ConvE and 1 × 9 for HypER, after testing different numbers of feature maps n f ∈ {16, 32, 64} and filter sizes l f ∈ {1 × 1, 1 × 2, 1 × 3, 1 × 6, 1 × 9, 1 × 12} (see <ref type="table" target="#tab_9">Table 9</ref>). We train all models using the Adam optimizer with batch size 128. One epoch on FB15k-237 takes approximately 12 seconds on a single GPU compared to 1 minute for e.g. RESCAL, largely due to 1-N scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Evaluation Results are obtained by iterating over all triples in the test set. A particular triple is evaluated by replacing the object entity e 2 with all entities E while keeping the subject entity e 1 fixed and vice versa, obtaining scores for each combination. These scores are then ranked using the "filtered" setting only, i.e. we remove all true cases other than the current test triple <ref type="bibr" target="#b0">[1]</ref>.</p><p>We evaluate HypER on five different metrics found throughout the link prediction literature: mean rank (MR), mean reciprocal rank (MRR), hits@10, hits@3, and hits@1. Mean rank is the average rank assigned to the true triple, over all test triples. Mean reciprocal rank takes the average of the reciprocal rank assigned to the true triple. Hits@k measures the percentage of cases in which the true triple appears in the top k ranked triples. Overall, the aim is to achieve high mean reciprocal rank and hits@k and low mean rank. For a more extensive description of how each of these metrics is calculated, we refer to <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Link prediction results for all models across the five datasets are shown in Tables 4, 5 and 6. Our key findings are:</p><p>whilst having fewer parameters than the closest comparator ConvE, Hy-pER consistently outperforms all other models across all datasets, thereby achieving state-of-the-art results on the link prediction task; and our filter dimension study suggests that no benefit is gained by convolving over reshaped 2D entity embeddings in comparison with 1D entity embedding vectors and that most information can be extracted with very small convolutional filters <ref type="table" target="#tab_9">(Table 9</ref>).</p><p>Overall, HypER outperforms all other models on all metrics apart from mean reciprocal rank on WN18 and mean rank on WN18RR, FB15k-237, WN18, and YAGO3-10. Given that mean rank is known to be highly sensitive to outliers <ref type="bibr" target="#b10">[11]</ref>, this suggests that HypER correctly ranks many true triples in the top 10, but makes larger ranking errors elsewhere.</p><p>Given that most models in the literature, with the exception of ConvE, were trained with 100 dimension embeddings and 1-1 scoring, we reimplement previous models (DistMult, ComplEx and ConvE) with 200 dimension embeddings and 1-N scoring for fair comparison and report the obtained results on WN18RR in <ref type="table" target="#tab_7">Table 7</ref>. We perform the same hyperparameter search for every model and present the mean and standard deviation of each result across five runs (different random seeds). This improves most previously published results, except for ConvE where we fail to replicate some values. Notwithstanding, HypER remains the best performing model overall despite better tuning of the competitors. <ref type="table">Table 4</ref>. Link prediction results on WN18RR and FB15k-237. The RotatE <ref type="bibr" target="#b18">[19]</ref> results are reported without their self-adversarial negative sampling (see Appendix H in the original paper) for fair comparison, given that it is not specific to that model only.   To ensure that the difference between reported results for HypER and ConvE is not simply due to HypER having a reduced number of parameters (implicit regularization), we trained ConvE reducing the number of feature maps to 16 instead of 32 to have a comparable number of parameters to HypER (explicit regularization). This showed no improvement in ConvE results, indicating Hy-pER's architecture does more than merely reducing the number of parameters. Hypernetwork Influence To test the influence of the hypernetwork and, thereby, knowledge sharing between relations, we compare HypER results on WN18RR and FB15k-237 with the hypernetwork component removed, i.e. without the first fully connected layer and with the relation embeddings directly corresponding to a set of convolutional filters. Results presented in <ref type="table" target="#tab_8">Table 8</ref> show that the hypernetwork component improves performance, demonstrating the value of multi-task learning across different relations. <ref type="table" target="#tab_9">Table 9</ref> shows results of our study investigating the influence of different convolutional filter sizes on the performance of HypER. The lower part of the table shows results for 2D filters convolved over reshaped <ref type="bibr">(10 × 20)</ref> 2D subject entity embeddings. It can be seen that reshaping the embeddings is of no benefit, especially on WN18RR. These results indicate that the purpose of convolution on word embeddings is not to find patterns in a 2D embedding (as with images), but perhaps to limit the number of dimensions that can interact with each other, thereby avoiding overfitting. In the upper part of the table, we vary the length of 1D filters, showing that comparable results can be achieved with filter sizes 1 × 6 and 1 × 9, with diminishing results for smaller (e.g. 1 × 1) and larger (e.g. 1 × 12) filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter Dimension Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Smoothing</head><p>Contrary to the ablation study of <ref type="bibr" target="#b2">[3]</ref>, showing the influence of hyperparameters on mean reciprocal rank for FB15k-237, from which they deem label smoothing unimportant, we find label smoothing to give a significant improvement in prediction scores for WN18RR. However, we find it does have a negative influence on the FB15k scores and as such, exclude label smoothing from our experiments on that dataset. We therefore recommend evaluating the influence of label smoothing on a per dataset basis and leave to future work analysis of the utility of label smoothing in the general case. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce HypER, a hypernetwork model for link prediction on knowledge graphs. HypER generates relation-specific convolutional filters and applies them to subject entity embeddings. The hypernetwork component allows information to be shared between relation vectors, enabling multi-task learning across relations. To our knowledge, HypER is the first link prediction model that creates non-linear interaction between entity and relation embeddings by convolving relation-specific filters over the entity embeddings. We show that no benefit is gained from 2D convolutional filters over 1D, dispelling the suggestion that 2D structure exists in entity embeddings implied by ConvE. We also recast HypER in terms of tensor operations showing that, despite the convolution operation, it is closely related to the established family of tensor factorization models. Our results suggest that convolution provides a good trade-off between expressiveness and parameter number compared to a dense network. HypER is fast, robust to overfitting, has relatively few parameters, and achieves state-of-the-art results across almost all metrics on multiple link prediction datasets.</p><p>Future work might include expanding the current architecture by applying convolutional filters to both subject and object entity embeddings. We may also analyze the influence of label smoothing and explore the interpretability of convolutional feature maps to gain insight and potentially improve the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Interpretation of the HypER model in terms of tensor operations. Each relation embedding wr generates a set of filters Fr via the hypernetwork H. The act of convolving Fr over e1 is equivalent to multiplication of e1 by a tensor Fr (in which Fr is diagonally duplicated and zero elsewhere). The tensor product Fr ⊗yz W gives a de × de matrix specific to each relation. Axes labels indicate the modes of tensor interaction (via inner product).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We train our model with 200 dimension entity and relation embeddings (d e = d r = 200) and 1-N scoring. Whilst the relation embedding dimension does not have to equal the entity embedding dimension, we set d r = 200 to match ConvE for fairness of comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Visualization of the HypER model architecture. Subject entity embedding e1 is convolved with filters Fr, created by the hypernetwork H from relation embedding wr. The obtained feature maps Mr are mapped to de-dimensional space via W and the non-linearity f applied before being combined with all object vectors e2 ∈ E through an inner product to give a score for each triple. Predictions are obtained by applying the logistic sigmoid function to each score.</figDesc><table><row><cell>e1</cell><cell></cell><cell></cell><cell>Mr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wr</cell><cell>H</cell><cell>Fr</cell><cell>Convolve</cell><cell>W</cell><cell>. . .</cell><cell>f</cell><cell>×</cell><cell>σ</cell><cell>0.3 0.1 0.9 0.4 0.8 0.2 0.1 0.5 0.2 0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>E</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>compares the number of parameters of ConvE and HypER (for the FB15k-237 dataset, which determines n e and n r ). It can be seen that, overall, HypER has fewer parameters (4.3M) than ConvE (5.1M) due to the way HypER directly transforms relations to convolutional filters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of number of parameters for ConvE and HypER on FB15k-237. hm and wm are height and width of the ConvE feature maps respectively.</figDesc><table><row><cell>Model</cell><cell>E</cell><cell>R</cell><cell>Filters</cell><cell>W</cell></row><row><cell>ConvE</cell><cell>ne × de 2.9M</cell><cell>nr × dr 0.1M</cell><cell>lf nf 0.0M</cell><cell>hmwmnf × de 2.1M</cell></row><row><cell>HypER</cell><cell>ne × de 2.9M</cell><cell>nr × dr 0.1M</cell><cell>dr × lf nf 0.1M</cell><cell>lmnf × de 1.2M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Summary of dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Entities (ne) Relations (nr)</cell></row><row><cell>FB15k</cell><cell>14,951</cell><cell>1,345</cell></row><row><cell>WN18</cell><cell>40,943</cell><cell>18</cell></row><row><cell>FB15k-237</cell><cell>14,541</cell><cell>237</cell></row><row><cell>WN18RR</cell><cell>40,943</cell><cell>11</cell></row><row><cell>YAGO3-10</cell><cell>123,182</cell><cell>37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>HypER (ours) 5798 .465 .522 .477 .436 250 .341 .520 .376 .252</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell></cell><cell></cell><cell cols="3">FB15k-237</cell></row><row><cell></cell><cell cols="9">MR MRR H@10 H@3 H@1 MR MRR H@10 H@3 H@1</cell></row><row><cell cols="10">DistMult [23] 5110 .430 .490 .440 .390 254 .241 .419 .263 .155</cell></row><row><cell cols="10">ComplEx [22] 5261 .440 .510 .460 .410 339 .247 .428 .275 .158</cell></row><row><cell cols="2">Neural LP [24] −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="4">− .250 .408 −</cell><cell>−</cell></row><row><cell>R-GCN [15]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="4">− .248 .417 .264 .151</cell></row><row><cell cols="2">MINERVA [2] −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="2">.456 −</cell><cell>−</cell></row><row><cell>ConvE [3]</cell><cell cols="9">4187 .430 .520 .440 .400 244 .325 .501 .356 .237</cell></row><row><cell>M-Walk [16]</cell><cell cols="2">− .437</cell><cell cols="4">− .445 .414 −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>RotatE [19]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell cols="5">− 185 .297 .480 .328 .205</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Link prediction results on WN18 and FB15k. DistMult [23] 902 .822 .936 .914 .728 97 .654 .824 .733 .546 ComplEx [22] − .941 .947 .936 .936 − .692 .840 .759 .599 ANALOGY [9] − .942 .947 .944 .939 − .725 .854 .785 .646 − .942 .947 .944 .939 − .727 .838 .773 .660 HypER (ours) 431 .951 .958 .955 .947 44 .790 .885 .829 .734</figDesc><table><row><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell>FB15k</cell></row><row><cell></cell><cell cols="6">MR MRR H@10 H@3 H@1 MR MRR H@10 H@3 H@1</cell></row><row><cell>TransE [1]</cell><cell>251 −</cell><cell>.892 −</cell><cell cols="2">− 125 −</cell><cell>.471 −</cell><cell>−</cell></row><row><cell cols="3">Neural LP [24] − .940 .945 −</cell><cell>−</cell><cell cols="2">− .760 .837 −</cell><cell>−</cell></row><row><cell>R-GCN [15]</cell><cell cols="6">− .819 .964 .929 .697 − .696 .842 .760 .601</cell></row><row><cell>TorusE [4]</cell><cell cols="6">− .947 .954 .950 .943 − .733 .832 .771 .674</cell></row><row><cell>ConvE [3]</cell><cell cols="6">374 .943 .956 .946 .935 51 .657 .831 .723 .558</cell></row><row><cell>SimplE [7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Link prediction results on YAGO3-10.</figDesc><table><row><cell></cell><cell>YAGO3-10</cell></row><row><cell></cell><cell>MR MRR H@10 H@3 H@1</cell></row><row><cell cols="2">DistMult [23] 5926 .340 .540 .380 .240</cell></row><row><cell cols="2">ComplEx [22] 6351 .360 .550 .400 .260</cell></row><row><cell>ConvE [3]</cell><cell>1676 .440 .620 .490 .350</cell></row><row><cell cols="2">HypER (ours) 2529 .533 .678 .580 .455</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Link prediction results on WN18RR; all models trained with 200 dimension embeddings and 1-N scoring. 4911 ± 109 .434 ± .002 .508 ± .002 .447 ± .001 .399 ± .002 ComplEx [22] 5930 ± 125 .446 ± .001 .523 ± .002 .462 ± .001 .409 ± .001 ConvE [3] 4997± 99 .431 ± .001 .504 ± .002 .443 ± .002 .396 ± .001 HypER (ours) 5798 ± 124 .465 ± .002 .522 ± .003 .477 ± .002 .436 ± .003</figDesc><table><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell></row><row><cell>MR</cell><cell>MRR</cell><cell>H@10</cell><cell>H@3</cell><cell>H@1</cell></row><row><cell>DistMult [23]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Results with and without hypernetwork on WN18RR and FB15k-237. .002 .522 ± .003 .341 ± .001 .520 ± .002 HypER (no H) .459 ± .002 .511 ± .002 .338 ± .001 .515 ± .001</figDesc><table><row><cell></cell><cell cols="2">WN18RR</cell><cell cols="2">FB15k-237</cell></row><row><cell></cell><cell>MRR</cell><cell>H@10</cell><cell>MRR</cell><cell>H@10</cell></row><row><cell>HypER</cell><cell>.465 ±</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Influence of different filter dimension choices on prediction results.</figDesc><table><row><cell></cell><cell>WN18RR FB15k-237</cell></row><row><cell cols="2">Filter Size MRR H@1 MRR H@1</cell></row><row><cell>1 × 1</cell><cell>.455 .422 .337 .248</cell></row><row><cell>1 × 2</cell><cell>.458 .428 .337 .248</cell></row><row><cell>1 × 3</cell><cell>.457 .427 .339 .250</cell></row><row><cell>1 × 6</cell><cell>.459 .429 .340 .251</cell></row><row><cell>1 × 9</cell><cell>.465 .436 .341 .252</cell></row><row><cell>1 × 12</cell><cell>.457 .428 .341 .252</cell></row><row><cell>2 × 2</cell><cell>.456 .429 .340 .250</cell></row><row><cell>3 × 3</cell><cell>.458 .430 .339 .250</cell></row><row><cell>5 × 5</cell><cell>.452 .423 .340 .252</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ibalazevic/HypER</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ivan Titov for helpful discussions on this work. Ivana Balažević and Carl Allen were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Go for a Walk and Arrive at the Answer: Reasoning over Paths in Knowledge Bases Using Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convolutional 2D Knowledge Graph Embeddings. In: Association for the Advancement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TorusE: Knowledge Graph Embedding on a Lie Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Hypernetworks. In: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SimplE Embedding for Link Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analogical Inference for Multi-relational Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Yago3: A Knowledge Base from Multilingual Wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Innovative Data Systems Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Holographic Embeddings of Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic Differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reasoning with Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
