<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Clustering with Measure Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhua</forename><surname>Chen</surname></persName>
							<email>mchen@interactions.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interactions LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badrinath</forename><surname>Jayakumar</surname></persName>
							<email>bjayakumar@interactions.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interactions LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmasundari</forename><surname>Gopalakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Interactions LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Huang</surname></persName>
							<email>qhuang@interactions.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interactions LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Johnston</surname></persName>
							<email>mjohnston@interactions.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interactions LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
							<email>phaffner@interactions.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interactions LLC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Clustering with Measure Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep models have improved state-of-the-art for both supervised and unsupervised learning. For example, deep embedded clustering (DEC) has greatly improved the unsupervised clustering performance, by using stacked autoencoders for representation learning. However, one weakness of deep modeling is that the local neighborhood structure in the original space is not necessarily preserved in the latent space. To preserve local geometry, various methods have been proposed in the supervised and semi-supervised learning literature (e.g., spectral clustering and label propagation) using graph Laplacian regularization. In this paper, we combine the strength of deep representation learning with measure propagation (MP), a KL-divergence based graph regularization method originally used in the semi-supervised scenario. The main assumption of MP is that if two data points are close in the original space, they are likely to belong to the same class, measured by KL-divergence of class membership distribution. By taking the same assumption in the unsupervised learning scenario, we propose our Deep Embedded Clustering Aided by Measure Propagation (DECAMP) model. We evaluate DECAMP on short text clustering tasks. On three public datasets, DECAMP performs competitively with other state-of-the-art baselines, including baselines using additional data to generate word embeddings used in the clustering process. As an example, on the Stackoverflow dataset, DECAMP achieved a clustering accuracy of 79%, which is about 5% higher than all existing baselines. These empirical results suggest that DECAMP is a very effective method for unsupervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Our society is generating a lot of short texts everyday. In social media such as Twitter and Facebook, short texts are posted on various aspects of everyday life. In the customer service domain, dialogues between customers and agents are transcribed and analyzed to find pattens in their interactions. More recently, virtual assistants such as Google Home have been responding to short inquiries from family users. The volume of the unlabeled texts is growing so huge that no human has the bandwidth to read through them, let alone This paper was accepted as a poster in 14th Annual Machine Learning Symposium in The New York Academy of Sciences label them. Hence it becomes necessary to use automated algorithms to analyze and organize short texts, documents and dialogues. As labeled data is often expensive to obtain, unsupervised learning methods which can cluster short texts in meaningful ways become very important.</p><p>In last decade, deep models have improved stateof-the-art for both supervised and unsupervised learning (Krizhevsky, Sutskever, and Hinton 2012) <ref type="bibr" target="#b4">(Goodfellow et al. 2014)</ref>. For example, deep embedded clustering (DEC) <ref type="bibr" target="#b15">(Xie, Girshick, and Farhadi 2016)</ref> has greatly improved unsupervised clustering performance over traditional methods such as K-means. By using stacked autoencoders <ref type="bibr" target="#b6">(Hinton and Zemel 1994)</ref> for representation learning, DEC can jointly learn the latent representation and the clustering, which is very attractive for analyzing high dimensional data such as image and text. Many deep clustering algorithms have been proposed since the initial demonstration of DEC. For example in <ref type="bibr">IDEC (Guo et al. 2017)</ref>, the autoencoder reconstruction loss is also included in the clustering process, while in the original DEC the decoder is discarded during clustering. In VaDE <ref type="bibr" target="#b9">(Jiang et al. 2016)</ref>, the autoencoder is replaced by a variational autoencoder (VAE) (Kingma and Welling 2013), and a GMM prior is imposed in the latent space for joint representation learning and clustering.</p><p>Consequently, it would be a natural idea to apply DEC to short text clustering. A very recent paper <ref type="bibr" target="#b5">(Hadifar et al. 2019)</ref> reported encouraging results in this direction. However, one weakness of deep modeling is that the local neighborhood structure in the original space is not necessarily preserved in the latent space. Two data points in the original space may be mapped far apart through the deep encoders in DEC. Precisely due to this reason, the Laplacian autoencoder <ref type="bibr" target="#b8">(Jia et al. 2015)</ref> was proposed in the literature to remedy this issue. As local structure preserving is important for representation learning and clustering, we will focus on improving it in this paper.</p><p>To preserve local geometry, various methods have been proposed in the supervised and semi-supervised learning literature (e.g., spectral clustering (Von Luxburg 2007) and label propagation <ref type="bibr" target="#b18">(Zhu and Ghahramani 2002)</ref>) using graph Laplacian regularization <ref type="bibr" target="#b0">(Belkin, Niyogi, and Sindhwani 2006)</ref>  <ref type="bibr" target="#b0">(Belkin and Niyogi 2003)</ref>. A sparse graph is first constructed from the input space, so that nearby data points are connected in the graph. Then this graph is applied to the latent space through regularization to preserve the neighborhood structure. Besides graph regularization, another approach <ref type="bibr" target="#b10">(Kipf and Welling 2016)</ref> which appeared recently uses the graph itself to generate latent representations through convolution and nonlinear operations. This graph convolutional network has been applied successfully to semi-supervised text classification problems. In the literature, there are some work on combining deep clustering with graph regularization, for example (Li, Zhang, and Ouyang 2019) <ref type="bibr" target="#b14">(Tzoreff, Kogan, and Choukroun 2018)</ref>, which we will review in the experiment section.</p><p>In this paper, we combine the strength of deep representation learning with measure propagation (MP) (Subramanya and Bilmes 2011), a KL divergence based graph regularization method originally used in a semi-supervised setting. The main assumption of MP is that if two data points are close in the original space, they are likely to belong to the same class, measured by KL divergence of class membership distribution. By taking the same assumption in the unsupervised learning scenario, we propose our Deep Embedded Clustering Aided by Measure Propagation (DECAMP) model. We evaluate DECAMP on short text clustering tasks. On three public datasets, DECAMP performs competitively with other state-of-the-art baselines, including baselines using additional data to generate word embeddings used in the clustering process. As an example, on the StackOverflow dataset, DECAMP achieves a clustering accuracy of 79%, which is about 5% higher than all existing baselines. These empirical results suggest that DECAMP is a very effective method for unsupervised learning.</p><p>Contributions in this work are summarized as follows:</p><p>1. We combine the strength of deep clustering with graph regularization to preserve local structure of the data.</p><p>2. We extend the application of measure propagation from semi-supervised to unsupervised learning.</p><p>3. A special case of our algorithms provides a theoretical justification for the choice of the target distribution in DEC, which previously was considered as an empirical choice.</p><p>4. Optimization of DEC with batch gradient algorithms could not be done end-to-end due to a frequency balancing term spanning across the entire data. We redefine this balancing constraint as a regularization term, and propose and new solution that can be fully optimized over a single mini-batch. This makes the DEC training more efficient.</p><p>5. We improve the clustering performance of DEC without using additional data or external information, achieving state-of-the-art performance on short text clustering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Embedded Clustering</head><p>Deep learning has improved both supervised and unsupervised learning greatly in the past decade. Recently there is a lot of work to apply deep models to clustering problems <ref type="bibr" target="#b11">(Min et al. 2018)</ref>. Instead of clustering directly in the input space, deep clustering encodes the data and clusters in the latent space simultaneously. Thanks to the superb representation learning ability, deep clustering <ref type="bibr" target="#b1">(Chen et al. 2016</ref>) <ref type="bibr" target="#b9">(Jiang et al. 2016</ref>) <ref type="bibr" target="#b17">(Yang et al. 2017)</ref> can yield substantial improvement over standard clustering methods such as K-means, Gaussian Mixture Models (GMM) and spectral clustering. One such deep clustering method is Deep Embedded Clustering (DEC) (Xie, Girshick, and Farhadi 2016) which we will focus on for this paper. In DEC, an autoencoder is pretrained on the input data {x i } n i=1 with a Mean-Squared Error (MSE) loss to reconstruct the input. This pretraining step is important, as early work on deep learning (Hinton and Salakhutdinov 2006) <ref type="bibr" target="#b7">(Hinton 2002)</ref> has shown that it provides informative embedding and good initialization for further training. Then a clustering layer maps the latent</p><formula xml:id="formula_0">representation {z i } n i=1 to the cluster predictive distribution {q i (θ)} n i=1 via the following Student-t likelihood: q ik (θ) = (1 + z i − µ k 2 ) −1 K k =1 (1 + z i − µ k 2 ) −1 (1)</formula><p>Here K is the total number of clusters, and the deep network parameter θ includes both the encoder and the cluster centers {µ k } K k=1 . The encoder is initialized from the autoencoder pretraining process mentioned above, and the cluster centers are initialized via K-means clustering in the latent space. If the ground truth labels {p i } n i=1 were available, we could fine-tune the deep network parameter θ using the KL divergence training loss (equivalent to cross-entropy loss) as follows:</p><formula xml:id="formula_1">min θ 1 n n i=1 D KL (p i ||q i (θ))<label>(2)</label></formula><p>However, for clustering problems, no such ground truth labels are available. In DEC, a self-training approach is taken to generate pseudo labels via the following equation:</p><formula xml:id="formula_2">p ik = q 2 ik (θ)/( 1 n n j=1 q jk (θ)) K k =1 q 2 ik (θ)/( 1 n n j=1 q jk (θ))<label>(3)</label></formula><p>This procedure will generate sharpened and balanced pseudo labels, due to the square operator and the normalization over 1 n n j=1 q jk (θ). The DEC workflow can be summarized as follows: 1. Initialize the deep network via autoencoder pretraining and K-means clustering. 2. Generate pseudo labels according to (3). 3. Train the deep network with fixed pseudo labels using loss in (2). 4. Repeat the above two steps until convergence. Output predictive distribution in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep clustering with Graph Regularization</head><p>One weakness of DEC is that the local structure in the original space is not necessarily preserved in the latent space.</p><p>To be more specific, if two points are close in the original space, we would hope that they are mapped to the same cluster in the latent space, which is a common assumption in manifold learning (Roweis and Saul 2000) <ref type="bibr" target="#b13">(Tenenbaum, De Silva, and Langford 2000)</ref>. However, this is not necessarily the case for DEC, as it could map them far apart through the multi-layer deep encoder. To preserve the local neighborhood geometry, we first construct a graph affinity matrix W = {w ij } n i,j=1 in the original space, where w ij is nonnegative, and it is nonzero only if x i and x j are neighbors according to some affinity metric. This sparse matrix W provides pair-wise similarity information which could guide the deep clustering process. Hence we propose to optimize the following objective function for deep clustering:</p><formula xml:id="formula_3">min p,θ C KL (p, θ) = 1 n n i=1 D KL (p i ||q i (θ)) − ξH( 1 n n j=1 q j (θ)) + λ n n i=1 H(p i ) + ν n n i=1 n j=1 w ij D KL (p i ||p j )<label>(4)</label></formula><p>where each p i is restricted to be a probability distribution over K clusters, and H(·) is the Shannon entropy. This objective function is inspired by the work of (Subramanya and Bilmes 2011), but a key difference is that here we are dealing with unsupervised instead of semi-supervised learning. We explain the functionality of each term in detail as follows:</p><p>1. The first term is the same as the KL divergence loss (2) in DEC.</p><p>2. The second term is a balancing regularization for clustering. We would like the average predictive distribution 1 n n j=1 q j (θ) close to a uniform distribution, so that all clusters can be occupied to avoid degenerate solutions.</p><p>3. The third term is the entropy regularization on the pseudo labels, to make the label distribution sharp and unequivocal, as low entropy implies high confidence.</p><p>4. The fourth term is the graph regularization to preserve local structure. If the affinity weight w ij is large (i.e., x i and x j are close to each other), the objective function will drive D KL (p i ||p j ) small, which means that they are likely to be mapped to the same cluster in the latent space. In this way, the graph affinity information in the original space will guide the deep clustering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization via Measure Propagation</head><p>We take an alternating minimization <ref type="bibr" target="#b2">(Csiszár and Tusnády 1984)</ref> approach to solve (4). First, given fixed deep network parameter θ we solve the pseudo labels {p i } n i=1 , and then given the generated pseudo labels we optimize the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Label Generation</head><p>As no analytical solution exists for p i , we make use of the Measure Propagation (MP) method proposed in (Subramanya and Bilmes 2011). We first relax (4) to the following form:</p><formula xml:id="formula_4">min p,r,θ C M P (p, r, θ) = 1 n n i=1 D KL (p i ||q i (θ)) − ξH( 1 n n j=1 q j (θ)) + λ n n i=1 H(p i )+ ν n n i=1 n j=1 w ij D KL (p i ||r j ) + ν n n j=1 αD KL (p j ||r j )<label>(5)</label></formula><p>Here we introduced auxiliary labels {r i } n i=1 to make the objective function more tractable. It is easy to verify that lim α→∞ min p,r,θ</p><formula xml:id="formula_5">C M P (p, r, θ) = min p,θ C KL (p, θ)<label>(6)</label></formula><p>which bridges the optimization in <ref type="formula" target="#formula_3">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref>. In practice we set a finite value for α. More theoretical analysis on measure propagation could be found in (Subramanya and Bilmes 2011), which could provide justifications for the above relaxation with a finite α. By adding self-linking edges to the graph W = W + αI n</p><p>we could rewrite <ref type="formula" target="#formula_5">(6)</ref> as</p><formula xml:id="formula_7">min p,r,θ C M P (p, r, θ) = 1 n n i=1 D KL (p i ||q i (θ)) − ξH( 1 n n j=1 q j (θ)) + λ n n i=1 H(p i ) + ν n n i=1 n j=1 w ij D KL (p i ||r j )<label>(8)</label></formula><p>Now we can resort to alternating minimization again to solve p i and r j analytically as</p><formula xml:id="formula_8">p ik = exp((log q ik (θ) + ν n j=1 w ij log r jk )/ λ i ) K k =1 exp((log q ik (θ) + ν n j=1 w ij log r jk )/ λ i )<label>(9)</label></formula><formula xml:id="formula_9">r jk = n i=1 w ij p ik / n i=1 w ij<label>(10)</label></formula><p>where λ i = (1 − λ) + ν n j=1 w ij . These two equations are iterated until convergence, which constitutes measure propagation for pseudo label generation. The first equation aggregates evidence from the deep network prediction and the auxiliary labels, and the second equation propagates the pseudo labels through the graph to update the auxiliary labels. Notice that each iteration is done simultaneously for all samples, with a computation complexity of O(n · m · K) where m is the number of nearest neighbors in the sparse graph. Since m &lt;&lt; n and K &lt;&lt; n, each iteration of measure propagation is of linear complexity with respect to the sample size. More details on constructing the W matrix can be found in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Network Optimization</head><p>Given the pseudo labels {p i } n i=1 from measure propagation, we would like to optimize the deep network. By focusing only on terms related to θ in (4), we obtain the following loss function:</p><formula xml:id="formula_10">min θ 1 n n i=1 D KL (p i ||q i (θ)) − ξH( 1 n n j=1 q j (θ)) (11)</formula><p>which is equivalent to a standard cross-entropy loss plus a balancing regularization to avoid degenerate solutions <ref type="bibr" target="#b3">(Ghasedi Dizaji et al. 2017)</ref>. As the regularization requires averaging over the full dataset, which is unavailable during the mini-batch based training, we approximate it within each mini-batch separately, i.e., replacing 1 n n j=1 q j (θ) with 1 |B| j∈B q j (θ). Similar approximation is used in <ref type="bibr" target="#b8">(Hu et al. 2017)</ref>.</p><p>We call our full algorithm Deep Embedded Clustering Aided by Measure Propagation (DECAMP), which is summarized as follows: 1. Initialize the deep network via autoencoder pretraining and K-means clustering. 2. Generate pseudo labels by measure propagation, iterating between <ref type="formula" target="#formula_8">(9)</ref> and <ref type="formula" target="#formula_9">(10)</ref>. 3. Train the deep network with fixed pseudo labels using loss in (11). 4. Repeat the above two steps until convergence. Output predictive distribution in (1). The reader can compare it with DEC in Section , to see the differences in workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Special Case of ν = 0</head><p>An an ablation study and sanity check, when ν = 0 in (4), our algorithm should reduce to one similar to the original DEC, as no graph affinity information is used any more. We make this connection in this section.</p><p>When ν = 0 in (4), the pseudo labels can be solved analytically as</p><formula xml:id="formula_11">p ik = q 1 1−λ ik (θ) K k =1 q 1 1−λ ik (θ)<label>(12)</label></formula><p>Instead of relying on alternating minimization to solve (4), as was done in Section , we can directly replace the above analytical solution back to (4) to eliminate the pesudo labels. The result is an end-to-end loss function for deep clustering:</p><formula xml:id="formula_12">min θ C E2E (θ) = − 1 − λ n n i=1 log K k=1 q 1 1−λ ik (θ) − ξH( 1 n n j=1 q j (θ)) = λ n n i=1 H 1 1−λ (q i (θ)) − ξH( 1 n n j=1 q j (θ))<label>(13)</label></formula><p>where H 1 1−λ (·) is the Rényi entropy (Principe 2010) with parameter 1 1−λ . This loss function is related to the maximum <ref type="figure">Figure 1</ref>: Illustration of the DECAMP algorithm. The first row is autoencoder pretraining, and the second row is the main clustering process using deep learning and measure propagation.</p><p>mutual information criterion proposed in <ref type="bibr" target="#b11">(Krause, Perona, and Gomes 2010)</ref>  <ref type="bibr" target="#b8">(Hu et al. 2017)</ref>, except that Rényi entropy instead of Shannon entropy appeared in our loss function above. We call this algorithm DECE2E, and would expect that it has similar clustering performance to the original DEC algorithm. However, a key difference is that in DECE2E we have a clear objective function which can be optimized end-to-end, while DEC relies on an empirical pseudo label generation equation in (3) and the algorithm has to be trained in an alternative manner. DECE2E is a byproduct we obtained along the way, and we will compare it to the main DECAMP algorithm in the result section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Data Description</head><p>We evaluate our DECAMP algorithm on short text clustering tasks. We consider three public datasets: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Configurations</head><p>We set the number of clusters K to be the ground truth number found in <ref type="table">Table .</ref> The decision of the number of clusters from the data itself is an important research topic <ref type="bibr" target="#b2">(Evanno, Regnaut, and Goudet 2005)</ref>, and we leave it for future study. We use feed-forward layers in the autoencoder, with dimensions d x − 500 − 500 − 2000 − d z for the encoder, and with reverse order for the decoder. From the data preprocessing decription, we have d x = 2000. For Stackoverflow and Biomedical we set d z = 10, and run 100 epochs for autoencoder pretraining; for Searchsnippets we set d z = 100, and run 1000 epochs for autoencoder pretraining. For both pretraining and the main clustering process, we use stochastic gradient descent (SGD) with step size 0.1 and momentum 0.9. The above settings are kept the same for both DEC and DECAMP to make the comparison fair. The hyper-parameters in DECAMP are set as follows for all experiments: ξ = 1.0, λ = 0.5, ν = 0.5. The graph W for measure propagation is constructed as follows. First we define the affinity graph W as w ij = 1 if x j is among the top m nearest neighbors of x i 0 else (14) and we set m = 50 in our experiments. The metric we use for nearest neighbor search is cosine similarity (i.e.,</p><p>x i xj xi 2· xj 2 ). Then we add the self-linking edges W = W + I n as in equation <ref type="formula" target="#formula_6">(7)</ref>. Finally we normalize it as W ← D − 1 2 W D − 1 2 according to suggestions in <ref type="bibr" target="#b10">(Kipf and Welling 2016)</ref>, where D = diag( W 1 n ) contains the row sums of the graph matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We evaluate the performance of DECAMP using three metrics: accuracy (ACC), normalized mutual information (NMI) and adjusted rand index (ARI). The score values of these metrics all belong to the range of [0, 1], and the higher the scores are, the better the clustering quality. All three metrics require access to the ground truth labels. Notice that the ground truth labels are not used in the clustering process for all the algorithms. They are only used in this evaluation phase when clustering is finished.</p><p>Suppose the ground truth labels are Y = {y i } n i=1 and the predicted labels are C = {c i )} n i=1 . In DEC and DECAMP we have c i = arg max k q ik (θ). The accuracy (ACC) mea-sure is defined as</p><formula xml:id="formula_13">ACC = 1 n n i=1 δ(y i = map(c i ))<label>(15)</label></formula><p>where δ(·) is an indicator function, and map(·) represents the best K × K permutation to match the clustering result to the ground truth labels, through the Hungarian algorithm <ref type="bibr" target="#b12">(Papadimitrou and Steiglitz 1982)</ref>. The normalized mutual information (NMI) is computed by</p><formula xml:id="formula_14">NMI = I(Y ; C) H(Y )H(C)<label>(16)</label></formula><p>where I(Y ; C) is the mutual information between Y and C, and H(·) is the Shannon entropy. The adjusted rand index (ARI) is the corrected-for-chance version of rand index, which is a similarity measure between two data clusterings. Detailed computation equations could be found in the literature, and we omit it here to save some space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result and Analysis</head><p>First we review three baseline algorithms: STC2 , DEC-SIF <ref type="bibr" target="#b5">(Hadifar et al. 2019)</ref> and LapDMM <ref type="bibr" target="#b11">(Li, Zhang, and Ouyang 2019)</ref>. STC2 is an early and important baseline for short text clustering. It uses additional data (generic Wikipedia data, or in-domain abstracts and post contents) to pretrain word embeddings, and use these word embeddings in the convolutional neural network (CNN) for clustering the data. DEC-SIF is a DEC-based clustering algorithm. Instead of using the original tf-idf features, DEC-SIF uses weighted pooling of the pretrained word embeddings as feature input to DEC. LapDMM is a recently proposed model with graph Laplacian regularization on the posterior cluster distribution of a Dirichlet mixture model. There are two realizations of LapDMM: LapDMM-T and LapDMM-W, with the former one relying on the tf-idf feature to construct the graph, and the latter one relying on the word embeddings.</p><p>As DECAMP relies only on the input data for graph construction and clustering, it would be unfair to directly compare it with baselines using additional information or word embeddings derived from in-domain data. Nevertheless, we list all baseline models in the results below, to make our comparison more complete. Results on the three public datasets are listed in the tables. The results for GMM, DEC, DECE2E and DECAMP are obtained over 10 independent experimental runs, and results for other baselines are obtained from the corresponding papers. As other papers did not report ARI scores, we only report ARI for the above four algorithms which we run.</p><p>We have the following observations from the result tables: 1. DECAMP performs better than DEC and DECE2E in all three datasets, across all three evaluation metrics. For example, we observed about 4% ACC improvement for DE-CAMP across all three datasets. We provide detailed metric trajectories along training epochs in <ref type="figure">Figure ?</ref> for one experiment. As can be seen, while DEC and DECE2E's performance saturate in an early stage, DECAMP continues to improve along iterations till a higher performance level.  2. DEC and DECE2E perform comparably across the datasets. This result was expected in our previous discuss, as neither of them use the graph affinity information in the original space. However, the main advantage of DECE2E is its theoretical elegance and training efficiency, as we can train it end-to-end through a unified loss function. 3. Comparing with other state-of-art methods published on these three datasets, DECAMP is also among the best performers. For example, on Stackoverflow, DECAMP's ACC is about 5% higher than all existing baselines, including methods using additional information during the clustering process. Notice that on the Biomedical dataset, DECAMP does not compete well with DEC-SIF, and we hypothesize that one reason might be the high-quality word embeddings in DEC-SIF derived from additional indomain data. For baselines with no access to additional information source (such as DMM-T), we do see a big gain of DECAMP across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper we proposed a new deep clustering method for short text analysis. By combining the strength of deep representation and measure propagation, our DECAMP algorithm uses the neighborhood affinity information to guide the clustering process, and achieves state-of-the-art performance on multiple public datasets. As a by-product, we obtained an end-to-end version of DEC which admits a unified objective function and allows more efficient training. As a future research direction, we will explore the possibility of encoding the measure propagation step as a neural network component, so that DECAMP can become a new kind of graph neural network (Zhou et al. 2018) admitting end-toend training. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 2: Searchsnippets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>NMI (mean +/-std)</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Information geometry and alternating minimization problems. Statistics &amp; Decision, Supplement Issue No 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csiszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tusnády</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Regnaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular ecology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2611" to="2620" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>Detecting the number of clusters of individuals using the software structure: a simulation study</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dizaji</forename><surname>[ghasedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5736" to="5745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1753" to="1759" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A self-training approach for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hadifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Repl4NLP, the 4th Workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
	<note>Reducing the dimensionality of data with neural networks</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="250" to="260" />
		</imprint>
	</monogr>
	<note>JMLR. org</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<idno>arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kingma and Welling</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<title level="m">Semi-supervised classification with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and Welling</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dirichlet multinomial mixture with variational manifold regularization: Topic modeling over short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perona</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomes ; Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinton ; Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="39501" to="39514" />
		</imprint>
	</monogr>
	<note>A survey of clustering with deep learning: From the perspective of network architecture</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steiglitz</surname></persName>
		</author>
		<title level="m">Combinatorial optimization: algorithms and complexity</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
	<note>Papadimitrou and Steiglitz</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Information theoretic learning: Renyi&apos;s entropy and kernel perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note>science</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep discriminative latent space for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kogan</forename><surname>Tzoreff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzoreff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choukroun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10795</idno>
	</analytic>
	<monogr>
		<title level="j">A tutorial on spectral clustering. Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Von Luxburg</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girshick</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-taught convolutional neural networks for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph neural networks: A review of methods and applications</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
