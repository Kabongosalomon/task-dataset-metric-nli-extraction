<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Direction Concentration Learning: Enhancing Congruency in Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>AUGUST 20XX 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
						</author>
						<title level="a" type="main">Direction Concentration Learning: Enhancing Congruency in Machine Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published">AUGUST 20XX 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Optimization</term>
					<term>Machine Learning</term>
					<term>Computer Vision</term>
					<term>Accumulated Gradient</term>
					<term>Congruency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency. Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency. She has published more than 50 journal and conference papers in top computer vision, machine learning, and cognitive neuroscience venues, and edited a book with Springer, titled Computational and Cognitive Neuroscience of Vision, that provides a systematic and comprehensive overview of vision from various perspectives, ranging from neuroscience to cognition, and from computational principles to engineering developments. She is a member of the IEEE since 2004.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D EEP learning has been receiving considerable attention due to its success in various computer vision tasks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref> and challenges <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>. To prevent model overfitting and enhance the generalization ability, a training process often sequentially updates the model with gradients w.r.t. a mini-batch of training samples, as opposed to using a larger batch <ref type="bibr" target="#b11">[12]</ref>. Due to the complexity and diversity in the nature of image data and taskspecific semantics, the discrepancy between current and previous observed mini-batches could result in a circuitous convergence path, which possibly hinders the convergence to a local minimum.</p><p>To better understand the circuitousness/straightforwardness in a learning process, we introduce congruency to quantify the agreement between new information used for an update and the knowledge learned from previous iterations. The word "congruency" is borrowed from a psychology study <ref type="bibr" target="#b50">[51]</ref> that inspects the influence of an object which is inconsistent with the scene in the visual attention perception task. In this work, we define congruency ν as the cosine similarity between the gradient g to be used for update and a referential gradientĝ that indicates a general descent direction resulting from previous updates, i.e., ν = cos α(g,ĝ),</p><p>(The detailed formulation is presented in Section 3). <ref type="figure">Figure 1</ref> presents an illustration of congruency in the saliency prediction task. Due to similar scene (i.e., dining) and similar fixations on faces and foods, the update of sample S2 (i.e., ∆wS 2 ) is congruent with ∆wS 1 . In contrast, the scene and fixations in sample S3 are different from sample S1 and S2. This leads to a large angle (&gt; 90 • ) between ∆wS 3 and ∆wS 2 (or ∆wS 1 ). Congruency reflects the diversity of task-specific semantics in training samples (i.e., images and the corresponding ground-Model Updates Image Fixation <ref type="figure">Figure 1</ref>: An illustration of congruency in the saliency prediction task. Assuming training samples are provided in a sequential manner, an incongruency occurs since the food item is related to different saliency values across these samples. Here, Sj stands for sample j = {1, 2, 3}, wi is the weight at time step i, ∆wS j is the weight update generated with Sj for wi, and the arrows indicate updates for the model. Specifically, ∆wS j = −ηgS j where η is the learning rate and gS j is the gradient w.r.t. Sj. The update of S2 (i.e., ∆wS 2 ) is congruent with ∆wS 1 , whereas ∆wS 3 is incongruent with ∆wS 1 and ∆wS 2 . truths). In the visual attention task, attention is explained by various hypotheses <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b49">[50]</ref> and can be affected by many factors, such as bottom-up feature, top-down feature guidance, scene structure, and meaning <ref type="bibr" target="#b54">[55]</ref>. As a result, objects in the same category may exhibit disagreements with each other in various images in terms of attracting attention. Therefore, there is a high variability in the mapping between visual appearance and the corresponding fixations. Another task that has a considerable amount of diversity is continual learning, which is able to learn continually from a stream of data that is related to new concepts (i.e., unseen labels) <ref type="bibr" target="#b32">[33]</ref>. The diversity of the data among multiple classification arXiv:1912.08136v2 [cs.</p><p>LG] 2 Jan 2020 subtasks may be so much discrepant such that learning from new data violates previously established knowledge (i.e., catastrophic forgetting) in the learning process. Moreover, congruency can also be found in the classification task. Compared to saliency prediction and continual learning, the source of diversity in classification task is relatively simple, namely, diverse visual appearances w.r.t. various labels in the real-world images. In summary, saliency prediction, continual learning, and classification are challenging scenarios susceptible to the effects of congruency. In machine learning, congruency can be considered as a factor that influences the convergence of optimization methods, such as stochastic gradient descent (SGD) <ref type="bibr" target="#b41">[42]</ref>, RMSProp <ref type="bibr" target="#b13">[14]</ref>, or Adam <ref type="bibr" target="#b22">[23]</ref>. Without specific rectification, the diversity among training samples is implicitly and passively involved in a learning process and affects the descent direction in convergence. To understand the effects of congruency on convergence, we explicitly formulate a direction concentration learning (DCL) method by sensing and restricting the angle of deviation between an update gradient and a referential gradient that indicates the descent direction according to the previous updates. Inspired by Nesterov's accelerated gradient <ref type="bibr" target="#b36">[37]</ref>, we consider the accumulated gradient as the referential gradient in the proposed DCL method.</p><p>We comprehensively evaluate the proposed DCL method with various models and optimizers in saliency prediction, continual learning, and classification tasks. The experimental results show that the constraints restricting the angle deviation between the gradient for an update and the accumulated referential gradient can help the learning process to converge efficiently, comparing to the approaches without such constraints. Furthermore, we present the congruency patterns to show how the task-specific semantics affect congruency in a learning process. Last but not least, our analysis shows that enhancing congruency in continual learning can improve backward transfer.</p><p>The main contributions in this work are as follows: <ref type="bibr">•</ref> We define congruency to quantify the agreement between new information and the learned knowledge in a learning process, which is useful to understand the model convergence in terms of tractability. <ref type="bibr">•</ref> We propose a direction concentration learning (DCL) method to enhance congruency so that the disagreement between new information and the learned knowledge can be alleviated. It also generally adapts to various optimizers (e.g., SGD, RMSProp and Adam) and various tasks (e.g., saliency prediction, continual learning and classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The experimental results from continual learning task demonstrate that enhancing congruency can improve backward transfer. Note that large negative backward transfer is known as catastrophic forgetting <ref type="bibr" target="#b32">[33]</ref>.</p><p>• A general method analyzing congruency is presented and it can be used within both conventional models and models with the proposed DCL method. Comprehensive analyses w.r.t saliency prediction and classification show that our DCL method generally enhances the congruencies of the corresponding learning processes.</p><p>The rest of the paper is organized as follows. We begin by highlighting related works in Section 2. Then, we formulate the problem of congruency and discuss its factors in Section 3. The proposed DCL method is introduced in Section 4. Moreover, the experiments and analyses are provided in Section 5 and 6, respectively. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">State-of-the-art Models for Classification</head><p>Convolutional networks (ConvNets) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b55">[56]</ref> have exhibited their powers in the classification task. AlexNet <ref type="bibr" target="#b26">[27]</ref> is a typical ConvNet and consists of a series of convolutional, pooling, activation, and fully-connected layers, it achieves the best performance on ILSVRC 2012 <ref type="bibr" target="#b5">[6]</ref>. Since then, there are more and more attempts to delve into the architecture of ConvNets. He et al.</p><p>proposed residual blocks to solve the vanishing gradient problem and the resulting model, i.e., ResNet <ref type="bibr" target="#b12">[13]</ref>, achieves best performance on ILSVRC 2015. Along with a similar line of ResNet, ResNeXt <ref type="bibr" target="#b55">[56]</ref> is proposed to extend residual blocks to multibranch architecture and DenseNet <ref type="bibr" target="#b15">[16]</ref> is devised to establish the connections between each layer and later layers in a feed-forward fashion. Both models achieve desirable performance. Recently, Tan and Le <ref type="bibr" target="#b46">[47]</ref> study how network depth, width, and resolution influence the classification performance and propose EfficientNet that achieves state-of-the-art performance on ImageNet. In this work, we use ResNet, ResNeXt, DenseNet, and EfficientNet in the image classification experiments.</p><p>Yang et al. <ref type="bibr" target="#b59">[60]</ref> introduce a regularized feature selection framework for multi-task classification. Specifically, the trace norm of a low rank matrix is used in the objective function to share common knowledge across multiple classification tasks. Congruency generally works with gradient based optimization methods, whereas trace norm works with a specific optimization method. Moreover, congruency measures the agreement (or disagreement) between new information learned from a sample and the established knowledge, whereas trace norm is based on the weights of multiple classifiers and only measures the correlation between established knowledge w.r.t. different classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computational Modelling of Visual Attention</head><p>Saliency prediction is an attentional mechanism that focuses limited perceptual and cognitive resources on the most pertinent subset of the available sensory data. Itti et al. <ref type="bibr" target="#b18">[19]</ref> implement the first computational model to predict saliency maps by integrating bottom-up features. Recently, Huang et al. <ref type="bibr" target="#b16">[17]</ref> propose a datadriven DNN model, named SALICON, to model visual attention. Cornia et al. <ref type="bibr" target="#b4">[5]</ref> propose a convolutional LSTM to iteratively refine the predictions and Kummerer et al. <ref type="bibr" target="#b27">[28]</ref> design a readout network that is fed with the output features of VGG <ref type="bibr" target="#b45">[46]</ref> to improve saliency prediction. Yang et al. <ref type="bibr" target="#b58">[59]</ref> introduce an endto-end Dilated Inception Network (DINet) to capture multi-scale contextual features for saliency prediction and achieves state-ofthe-art performance. In this work, we adopt the SALICON model and DINet in the saliency prediction experiments.</p><p>There are several insightful works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> exploring the effects of congruency/incongruency in visual attention. In particular, according to the perception experiments, Gordon finds that the object which is inconsistent with the scene, e.g., a live chicken standing on a kitchen table, has significant influence on attentive allocation <ref type="bibr" target="#b10">[11]</ref>. Underwood and Foulsham <ref type="bibr" target="#b50">[51]</ref> find an unexpected interaction between saliency and negative congruency in the search task, that is, the congruency of the conspicuous object does not influence the delay in its fixation, but it is fixated earlier when the other object in the scene is incongruent. Furthermore, Underwood et al. <ref type="bibr" target="#b51">[52]</ref> investigate whether the effects of semantic inconsistency appear in free viewing. In their studies, inconsistent objects were fixated for significantly longer duration than consistent objects. These works inspire us to explore the congruency between the current and previous updates. In saliency prediction, negative congruency may result from the disagreement among the training samples in terms of visual appearance and ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Catastrophic Forgetting</head><p>Catastrophic forgetting problem has been extensively studied in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b38">[39]</ref>. McCloskey and Cohen <ref type="bibr" target="#b34">[35]</ref> study the problem that new learning may interfere catastrophically with old learning when models are trained sequentially. New learning may alter weights that are involved in representing old learning, and this may lead to catastrophic interference. Along the same line, Ratcliff <ref type="bibr" target="#b38">[39]</ref> further investigates the causes of catastrophic forgetting, and two problems are observed: 1) sequential learning is prone to rapidly forget well-learned information as new information is learned; 2) discrimination between observed samples and unobserved samples either decreases or is non-monotonic as a function of learning. To address the catastrophic forgetting problem, there are several works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref> proposed to solve the problem by using episodic memory. Kirkpatrick et al. <ref type="bibr" target="#b23">[24]</ref> propose an algorithm named elastic weight consolidation (EWC), which can adjust learning to minimize changes in parameters important for previously seen task. Moreover, Lopez and Ranzato <ref type="bibr" target="#b32">[33]</ref> introduce the gradient episodic memory (GEM) method to alleviate catastrophic forgetting problem. However, there could exist incongruency in the training process of GEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONGRUENCY IN MACHINE LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>We first review the general goal in machine learning. Without loss of generality, given a training set D = {(Ii, yi)} n i=1 , where a pair (Ii, yi) represents a training sample composed of an image Ii ∈ R N I (N I is the dimension of images) and the corresponding ground-truth yi ∈ Y, the goal is to learn a model f :</p><formula xml:id="formula_1">R N I − → Y.</formula><p>Specifically, a Deep Neural Network (DNN) model has a trunk net to generate discriminative features xi ∈ X and a classifier fw : X w − → Y to fulfill the task, where w is the weights of classifier. Note that we consider that DNN is a classifier as whole and the input is raw RGB images.</p><p>To accomplish the learning process, the conventional approach is to first specify and initialize a model. Next, the empirical risk minimization (ERM) principle <ref type="bibr" target="#b52">[53]</ref> is employed to find a desirable w w.r.t. f by minimizing a loss function : Y ×Y → [0, ∞) penalizing prediction errors, i.e., minimize</p><formula xml:id="formula_2">w 1 |D| (x i ,y i )∈D (fw(xi), yi).</formula><p>At time step k, the gradient computed by the loss is used to update the model, i.e., w k+1 := w k + ∆w k , where ∆w k is an update as well as a function of gradient g(w k ; x k , y k ) = ∇w k (fw k (x k ), y k ). Optimizers, such as SGD <ref type="bibr" target="#b41">[42]</ref>, RMSProp <ref type="bibr" target="#b13">[14]</ref>, or Adam <ref type="bibr" target="#b22">[23]</ref>, determine ∆w k (g(w k ; x k , y k )). Without loss of generality, we assume the optimizer is SGD in the following for convenience.</p><p>There exist two challenges w.r.t. congruency for practical use. First, due to the dynamic nature of the learning process, how to find a stable referential direction which can quantify the agreement between current and previous updates. Second, how to guarantee the referential direction is beneficial to search for a local minimum.</p><p>As the gradient at a training step implies the direction towards a local minimum by the currently observed mini-batch, the accumulation of all previous gradients provides an overall direction towards a local minimum. Hence, it provides a good referential direction to measure the agreement between a specific update and its previous updates. We denote the accumulated gradient aŝ</p><formula xml:id="formula_3">g k|wm = k i=m g i ,<label>(2)</label></formula><p>where w m is the weights learned at time step m andĝ k|wm indicates that the accumulation starts from w m at time step k. If there is no explicit w m indicated,ĝ k =ĝ k|w 1 . <ref type="figure">Figure 2</ref> shows an example of accumulated gradient, where the gradient of S 3 deviates from the accumulated gradient of S 1 and S 2 . This also elicits our solution to measure congruency in a training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Definition</head><p>Congruency ν is a metric to measure the agreement of updates in a training process. In general, it is directly related to an angle between the gradient for an update and the accumulated gradient, i.e., α(ĝ k−1|wm , g k ) ∈ [0, π]. Smaller angle indicates higher congruency. Practically, we use cosine similarity to approximate the angle for computational simplicity. Mathematically, at time step k, ν k can be defined as follows</p><formula xml:id="formula_4">ν k|wm = cos α(g k ,ĝ k−1|wm ) =ĝ k−1|wm g k ĝ k−1|wm g k , m ≤ k (3)</formula><p>where w m is the weight learned at time step m and taken as a reference point in weight space. α(ĝ k , g k ) is the angle betweenĝ k and g k . Based on ν k−1|wm , the congruency of a training process that starts from w j to learn out w n can be defined as</p><formula xml:id="formula_5">ν wj →wn|wm = 1 n − j + 1 n i=j ν i|wm , m ≤ j &lt; n<label>(4)</label></formula><p>Since the concept of congruency is built upon cosine similarity, ν k|wm will range from [−1, 1]. Another advantage of using cosine similarity is the tractability. The gradient computed from the loss is considered as a vector in weight space. Hence, cosine similarity can take any pair of gradients, such as the accumulated gradient and the gradient computed by a training sample, or two gradients computed by two respective training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task-specific Factors</head><p>Congruency is semantics-aware. As congruency is based on the gradients which are computed with the images and the semantic ground-truth, such as class label in the classification task or human fixation in the saliency prediction task. Therefore, congruency reflects the task-specific semantics. We discuss congruency taskby-task in the following subsection. Saliency Prediction. Visual attention is attracted to visually salient stimuli and is affected by many factors, such as scale, spatial bias, context and scene composition, oculomotor constraints, and so on. These factors result in high variabilities over fixations across various persons. The variabilities of visual semantics imply that same class objects in two images may have different salience levels, i.e., one object is predicted as salient object while the other same class object is not. In this sense, negative congruency in learning for saliency prediction may result from both feature-level and label-label disagreement across the images. Continual Learning. In the continual learning setting <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, a classification model is learned with past observed classes ......</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCL module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualize</head><p>Weight space Optimizer Image Fixation <ref type="figure">Figure 2</ref>: An illustration of model training with the proposed DCL module. Here, 3 samples are observed in a sequential manner. The gradient generated by S 3 is expected to be different with the gradients generated by S 1 and S 2 . Hence, to tackle the expected violation between the update −ηg S3 and the accumulated update −η 2 i=1 g Si , the proposed DCL method finds a corrected update −ηg S3 (the pink arrow) by solving a quadratic programming problem <ref type="bibr" target="#b4">(5)</ref>. In this way, the angle between −ηg S3 and −η 2 i=1 g Si (the blue arrow), i.e.,α, is guaranteed to be equal to or less than α. Note that the gradient descent processes with or without the proposed DCL module is identical in the test phase. and samples. New samples w.r.t. the unobserved classes may be distinct from previously seen samples in terms of both visual appearance and label. This leads to negative congruency in learning. Classification. For classification, the class labels are usually deterministic to human. The factors that cause negative congruency in learning lie in visual appearances. Due to the variability of realworld images, visual appearance of samples from the same class may be very different from each other in different images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we first overview the proposed DCL method. Then, we introduce its formulation and properties in detail. Finally, we discuss the lower bound of congruency with gradient descent methods. For simplicity, we assume it is at time step k and omit underscored k in the following formulations unless we explicitly indicate it. <ref type="figure">Figure 2</ref> demonstrates the basic idea of the proposed DCL method. Given training sample (I, y), where I is an image and y is the ground-truth, the corresponding feature x are first generated by the sample before it is passed to the classifier for computing the predictionsŷ = f w (x). Conventionally, the derivatives g of the loss (ŷ, y) are computed to determine the update ∆w by an optimizer to back-propagate the error layer by layer. In the proposed DCL method, g is taken to estimate a corrected gradient g that is congruent with previous updates. For example, as shown in <ref type="figure">Figure 2</ref>, the gradient of S 3 is expected to have a large deviation angle α to the accumulated anti-gradient − 2 i=1 gsi because S 1 and S 2 share similar visual appearance, but S 3 is different from them. The proposed DCL method aims to estimate a correctedg which has a smaller deviation angleα to − 2 i=1 gS i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Direction Concentration Learning</head><p>The core idea of the proposed DCL method is to concentrate the current update to a certain search direction. The accumulated gradientĝ is the direction voted by previous updates which provides information towards the possible local minimum. Ideally, according to the definition of congruency, i.e., Eq. <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_5">(4)</ref>, cosine similarity should be considered in optimization. However, minimizing cosine similarity with constraints is complicated. Therefore, similar to GEM <ref type="bibr" target="#b32">[33]</ref>, we adopt an alternative that optimizes the inner product, instead of the cosine similarity. According to Eq. (3), g1, g2 ≥ 0 indicates that the angle between the two vectors is less than or equal to 90 • . As shown in <ref type="figure">Figure 2</ref>, the proposed DCL method uses the accumulated gradient as a referential direction to compute a corrected gradientg, i.e.,</p><formula xml:id="formula_6">minimizẽ g 1 2 g − g 2 2 s.t. −ĝ ri , −g ≥ 0, 1 ≤ i ≤ N r<label>(5)</label></formula><p>where r i is a reference point in weight space,ĝ ri is the accumulated gradient that starts the accumulation from r i , and N r is the number of reference points. The accumulated gradientĝ ri indicates that the accumulation starts from the reference r i to the current weights w. The proposed DCL method can take N r points as the references {r i |1 ≤ i ≤ N r }. Assume that the weights at time step t is taken as the reference r i , i.e., r i = w t , we denote sub(·) as a function to find the index of a point in weight space. For example, with t = sub(r i ) = sub(w t ), we can compute the accumulated gradientĝ ri = j=sub(ri) g j . On the other hand, the function 1 2 g − g 2 2 is widely used in gradientbased methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b47">[48]</ref> and forcesg to be close to g in Euclidean space as much as possible. The constraints −ĝ ri , −g ≥ 0 are to guarantee that the gradient that is used for an update should not substantially deviate from the accumulated gradient.</p><p>In practice, instead of directly computingĝ ri by its definition (2)), we compute it by subtracting the current point w with the reference point r i , i.e.,ĝ ri = w − r i = −η j=i g j . Hence, the constraints can be deformed in a matrix form</p><formula xml:id="formula_7">A(−g) = −1 ×      (w − r 1 ) (w − r 2 )</formula><p>. . .  region with regards to −ĝ ri , i ∈ {1, 2} as Constraint <ref type="formula" target="#formula_8">(6)</ref> forcesg to have an angle smaller than 90 • . Due to two references in this example, the intersection between two feasible regions, i.e., the shaded region, is the intersected feasible region for optimization. Note that an accumulated gradient determines half-plane (hyperplane) as feasible region, instead of the full plane (hyperplane) in conventional gradient descent case. The optimization (5) becomes a classic quadratic programming problem and we can easily solve it by off-the-shelf solvers like quadprog 1 or CVXOPT 2 . However, since the size ofg can be sufficiently large, straightforward solution may be computationally expensive in terms of both time and storage. As introduced by Dorn <ref type="bibr" target="#b6">[7]</ref>, we apply a primal-dual method for quadratic programs to solve it efficiently.</p><formula xml:id="formula_8">(w − r Nr )     g ≥ 0<label>(6)</label></formula><p>Given a general quadratic problem, it can be formulated as follows</p><formula xml:id="formula_9">minimize z 1 2 z Cz + q z s.t. Bz ≥ b,<label>(7)</label></formula><p>whereas the corresponding dual problem to Problem <ref type="formula" target="#formula_9">(7)</ref> is</p><formula xml:id="formula_10">minimize u,v 1 2 u Cu + b v s.t. B v − Cu = q, v ≥ 0.<label>(8)</label></formula><p>Dorn provides the proof of the connection between Problem <ref type="bibr" target="#b6">(7)</ref> and Problem <ref type="bibr" target="#b7">(8)</ref>. Due to the equality constraint B v − Cu = q, assume C is full rank, we can plug u = C −1 (B v − q) back to the objective 1. https://github.com/rmcgibbo/quadprog 2. https://cvxopt.org/ function to further simplify Problem (8), i.e.,</p><formula xml:id="formula_11">minimize v 1 2 v B(C −1 ) B v + (b − p B )v s.t. v ≥ 0.<label>(9)</label></formula><p>Now it turns out to be a quadratic problem w.r.t. v only.</p><p>The DCL quadratic problem can be solved by the aforementioned primal-dual method. Specifically, g−g 2 2 = (g−g) (g− g) =g g − 2g g + g g. By omitting the constant term g g, it turns to a quadratic problem formg g − 2g g. Since we know the primal problem <ref type="bibr" target="#b6">(7)</ref> can be converted to its dual problem <ref type="bibr" target="#b7">(8)</ref>, the related coefficient matrices/vectors are easily determined by</p><formula xml:id="formula_12">C = I, B = −A, b = 0, p = −g,</formula><p>where I is a unit matrix. With these coefficients at hand, we have the corresponding dual problem</p><formula xml:id="formula_13">minimize v 1 2 v AA v − g A v s.t. v ≥ 0.<label>(10)</label></formula><p>By solving <ref type="formula" target="#formula_0">(10)</ref>, we have v * . On the other hand, Cg = Cu * , C = I and we can have the solutiong * bỹ</p><formula xml:id="formula_14">g * = Cu * = B v − q = −A v + g (11) Note thatg, u ∈ R p , v ∈ R Nr , A ∈ R Nr ×p , and b ∈ R Nr</formula><p>where p is the size of w. If taking the fully-connected layer of ResNet as w, p = 2048. In contrast with p, N r is usually smaller, i.e., 1,2, or 3. As N r becomes larger, it increases the possibility that the constraints are inconsistent. Thus, Nr p. This implies that solving Problem (10) in R Nr is more efficient than solving Problem (5) in R p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Theoretical Lower Bound</head><p>Here, we discuss about the congruency lower bound with gradient descent methods. First, we recall the theoretical characteristics w.r.t. gradient descent methods.  <ref type="bibr" target="#b35">[36]</ref>). If the gradient of a function f : R n → R is Lipschitz continuous with Lipschitz constant L for any x, y ∈ R n , i.e.,</p><formula xml:id="formula_15">∇f (y) − ∇f (x) ≤ L y − x (12) then f (y) ≤ f (x) + ∇f (x) (y − x) + L 2 y − x 2<label>(13)</label></formula><p>On the other hand, there is a proved bound w.r.t. the loss.</p><p>Corollary 4.3 (The bound on the loss at one iteration <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b48">[49]</ref>). Let x k be the k-th iteration result of gradient descent and η k ≥ 0 the k-th step size. If ∇f is L-Lipschitz continuous, then</p><formula xml:id="formula_16">f (x k+1 ) ≤ f (x k ) − η k 1 − Lη k 2 ∇f (x k ) 2<label>(14)</label></formula><p>By adding up a collection of inequalities, we can move further along this line to have the following corollary. </p><formula xml:id="formula_17">f (x k ) ≤ f (x 0 ) − k−1 i=0 η i 1 − Lη i 2 ∇f (x i ) 2<label>(15)</label></formula><p>Theorem 4.5 (Congruency lower bound). Assume the gradient descent method uses a fixed step size η and the gradient of the loss function f : R n → R is Lipschitz continuous with Lipschitz constant L, the congruency ν k|x0 referring to the initial point x 0 at the k-th iteration has the following lower bound</p><formula xml:id="formula_18">ν k|x0 ≥ max (1 − Lη) k−1 i=0 ∇f (x i ) ∇f (x k ) − Lη k−1 i=0 ∇f (x i ) i−1 j=0 ∇f (x j ) ∇f (x k ) k−1 i=0 ∇f (x i ) , −1 (16)</formula><p>Proof: Given x k and x 0 , according to Proposition 4.2 we have</p><formula xml:id="formula_19">∇f (x k ) (x k − x0) ≤ f (x k ) − f (x0) + L 2 x k − x0 2 Since x k = x0 − η k−1 i=0 ∇f (xi) and ν k|x 0 = (−∇f (x k )) (− k−1 i=0 ∇f (xi))/( ∇f (x k ) k−1 i=0 ∇f (xi) ),</formula><p>we can have</p><formula xml:id="formula_20">∇f (x k ) (x k − x0) = − η(−∇f (x k )) (− k−1 i=0 ∇f (xi)) = − η ∇f (x k ) k−1 i=0 ∇f (xi) ν k|x 0</formula><p>Plugging this in the inequality, it yields</p><formula xml:id="formula_21">ν k|x 0 ≥ 1 η f (x0) − f (x k ) − Lη 2 2 k−1 i=0 ∇f (xi) 2 ∇f (x k ) k−1 i=0 ∇f (xi)</formula><p>According to Corollary 4.4, the inequality can be rewritten as</p><formula xml:id="formula_22">ν k|x 0 ≥ (1 − Lη 2 ) k−1 i=0 ∇f (x i ) 2 − Lη 2 k−1 i=0 ∇f (x i ) 2 ∇f (x k ) k−1 i=0 ∇f (x i )<label>(17)</label></formula><p>By using polynomial expansion and the Cauchy-Schwarz inequality, we can expand the term</p><formula xml:id="formula_23">k−1 i=0 ∇f (x i ) 2 as follows k−1 i=0 ∇f (xi) 2 = ∇f (x k−1 ) + k−2 i=0 ∇f (xi) 2 ≤ ∇f (x k−1 ) 2 + 2 ∇f (x k−1 ) k−2 i=0 ∇f (xi) + k−2 i=0 ∇f (xi) 2 Recursively, k−2 i=0 ∇f (x i ) 2 , k−3 i=0 ∇f (x i ) 2 , . . ., till 1 i=0 ∇f (x i ) 2 can be expanded, e.g., 1 i=0 ∇f (xi) 2 = ∇f (x1) + ∇f (x0) 2 ≤ ∇f (x1) 2 + 2 ∇f (x1) ∇f (x0) + ∇f (x0) 2</formula><p>The above inequalities yield</p><formula xml:id="formula_24">k−1 i=0 ∇f (xi) 2 ≤ k−1 i=0 ∇f (xi) 2 + 2 k−1 i=0 ∇f (xi) i−1 j=0 ∇f (xj)</formula><p>Plugging it into Inequality (17), we have</p><formula xml:id="formula_25">ν k|x 0 ≥(1 − Lη) k−1 i=0 ∇f (xi) 2 ∇f (x k ) k−1 i=0 ∇f (xi) − Lη k−1 i=0 ∇f (xi) i−1 j=0 ∇f (xj) ∇f (x k ) k−1 i=0 ∇f (xi)</formula><p>E ff e c ti v e W in d o w <ref type="figure">Figure 4</ref>: An illustration to demonstrate the concept of the effective window. Given the spiral convergence path, −ηĝ 10|w0 restricts the search direction and the minimum (i.e., the red star) and w 11 are unreachable according to the search direction. In contrast, w 11 can be reached along the search direction of −ηĝ 10|w7 . To adaptively yield appropriate accumulated gradients that converge to the minimum, we define an effective window to periodically update the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Due to</head><formula xml:id="formula_26">k−1 i=0 ∇f (x i ) k−1 i=0 ∇f (x i )</formula><p>≥ 1, the congruency lower bound can be further simplified as</p><formula xml:id="formula_27">ν k|x 0 ≥(1 − Lη) k−1 i=0 ∇f (xi) ∇f (x k ) − Lη k−1 i=0 ∇f (xi) i−1 j=0 ∇f (xj) ∇f (x k ) k−1 i=0 ∇f (xi)</formula><p>Combining with the fact ν k|x0 ≥ −1, we complete the proof.</p><p>Remark 4.6. Theorem 4.5 implies that when we apply gradient descent method to search a local minimum, the congruency lower bound at a certain iteration in the learning process is determined by the gradients at current iteration and previous iterations.</p><p>Remark 4.7. Theorem 4.5 implies that the lower bound of congruency with a small step size, i.e., η &lt; 1 L , is tighter than the one of congruency with a large step size, i.e., η ≥ 1 L . This is consistent with the fact the large step size could lead to a zigzag convergence path. The negative lower bound of congruency when η ≥ 1 L indicates the huge turnaround would possibly occur in the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Adaptivity to Learning</head><p>As the reference is used to compute the accumulated gradient for narrowing down the search direction, a desirable referential direction should orient to a local minimum. Conversely, an inappropriate referential direction could mislead the training and slow down the convergence. Therefore, it is important to update the references to adapt to the target optimization problem.</p><p>In this work, we update the references with a short temporal window so as to yield a locally stable and reliable referential direction. For instance, <ref type="figure">Figure 4</ref> shows an unfavorable case that takes w 0 as the reference, where the convergence path is spiral. Due to the circuitous manifold, w 0 results in a misleading direction −ηĝ 10|w0 . In contrast, if taking w 7 as a reference, it can yield the appropriate search direction to reach w 11 . Therefore, we introduce  <ref type="figure">Figure 5</ref>: An example demonstrating the effect of the proposed DCL method on three optimizers, i.e., gradient descent (GD), RMSProp, and Adam. Given a problem z = f (x, y), we use these optimization algorithms to compute the local minima, i.e., (x * , y * ) that yield the minimal z * . In the experiment, except the learning rate, the setting and hyperparameters are the same for ALGO and ALGO DCL, where ALGO={GD, RMSProp, Adam}. The proposed DCL method encourages the convergence paths to be as straight as possible.</p><p>an "effective window" to allow the proposed DCL method to find an appropriate search direction. The effective window forces the proposed DCL method to only accumulate the gradients within the window. In <ref type="figure">Figure 4</ref>, the proposed DCL method with a small window size would converge, whereas the one with a large window size would diverge. We denote the window size as β w and the reference offset as β o . When the time step t satisfies</p><formula xml:id="formula_28">t mod β w = β o ,<label>(18)</label></formula><p>where mod is the modulo operator, it would trigger the reset mechanism, i.e., starting over to set references ri ← wt, 1 ≤ i ≤ Nr. β o indicates the first reference weight point. Once the reset process starts, the proposed DCL method would use g, instead ofg, for update until all the N r references are reset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of DCL</head><p>To intuitively understand the effect of the proposed DCL method, we present visual comparisons of the convergence paths with three popular optimizers, i.e., SGD <ref type="bibr" target="#b41">[42]</ref>, RMSProp <ref type="bibr" target="#b13">[14]</ref>, and Adam <ref type="bibr" target="#b22">[23]</ref>, on a publicly available problem <ref type="bibr" target="#b2">3</ref> .</p><p>In particular, given the problem z = f (x, y), we apply the three optimizers to compute a local minimum (x * , y * ). Unlike image classification, the problem does not need randomized data sequence as input so there is no stochastic process. For a fair comparison, except the learning rate, we keep the settings and hyperparameters the same between ALGO and ALGO DCL, where ALGO={GD, RMSProp, Adam} and GD stands for gradient descent. The convergence paths w.r.t. the optimization algorithms are shown in <ref type="figure">Figure 5</ref> We can see that all the baseline curves are circuitous, i.e., a sharp turn at the ridge region between two local minima. Moreover, different learning rates lead to different local minima. It implies that the training process in this case is influenceable and fickle in terms of the direction of the convergence. The proposed DCL method noticeably improves the convergence direction by choosing a relatively straightforward path over the three optimization algorithms. Note that as the objective function <ref type="bibr" target="#b4">(5)</ref> implies, if we do not take any the accumulated gradients (i.e., no constraints), or take the gradient for the coming update as the accumulated gradient (i.e.,ĝ ri = g), the proposed DCL method would become the baseline (i.e.,g = g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">DCL in Continual Learning</head><p>In previous subsections, we introduce the proposed DCL method in mini-batch learning. By its very nature, it can also work in continual learning manner. GEM <ref type="bibr" target="#b32">[33]</ref> is a recent method proposed for continual learning. The objective function of GEM is the same as the proposed DCL method, whereas the constraints of GEM and the proposed DCL method are devised for respective purposes. To apply the proposed DCL method in continual learning, we can merge the constraints of the proposed method with the ones of GEM. Hence, we have a new A as follows</p><formula xml:id="formula_29">A =           (w − r 1 )</formula><p>. . .</p><p>(w − r Nr ) −g(x S1 , y S1 ) . . .</p><formula xml:id="formula_30">−g(x S Nm , y S Nm )           , S i ∈ M<label>(19)</label></formula><p>DCL GEM Memory Sample A, B <ref type="figure">Figure 6</ref>: An illustration demonstrating the difference between DCL (left) and GEM <ref type="bibr" target="#b32">[33]</ref> (right). The search direction in DCL is determined by the accumulated gradient while the adjusted gradient (solid line) of GEM is optimized by avoiding the violation between the gradient (dashed line) and memory samples' gradients (green line). Since the weights are iteratively updated and the memory samples are preserved, the direction of the adjusted gradient of the memory samples could be dynamically varying.</p><p>where M is the memory and N m is the size of the memory. With the proposed DCL constraints, the correctedg is forced to be consistent with both the accumulated gradients and the directions of gradients generated by the samples in memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Comparison with Memory-based Constraints</head><p>Now, we discuss the difference between the proposed DCL constraints and the memory-based constraints used in GEM <ref type="bibr" target="#b32">[33]</ref>. There are two main differences between the DCL constraints and the GEM constraints. First, as shown in <ref type="figure">Figure 6</ref>, the descent direction in the proposed DCL method is regulated by the accumulated gradient, whereas the gradient for an update in GEM is regulated to avoid the violation with the gradients of the memory samples (i.e., images and the corresponding ground-truths). Since the weights are iteratively updated and the memory samples are preserved, the gradients of the memory samples could be changed at each iteration so the direction of the adjusted gradient could be dynamically varying. Second, the proposed DCL method only needs to memorize the references, whereas GEM memorizes the images and the corresponding ground-truths. The proposed DCL constraints are efficiently computed by a subtraction in Eq. (6), other than by computing the corresponding gradients like GEM.</p><p>Although the proposed DCL constraints are different from GEM constraints in terms of definition, they are able to work with each other in continual learning. We will dive into the details in the following experiment section. Moreover, GEM computes the gradients on all the parameters of a DNN. This works in the situations that input image resolution is relatively small, e.g., 784 for MNIST <ref type="bibr" target="#b28">[29]</ref> or 3072 for CIFAR-10/100 <ref type="bibr" target="#b25">[26]</ref>. The networks used to classify these images have small number of weights like MLP and ResNet-18. However, the number of parameters in a DNN could be huge. For example, ResNeXt-29 (16 × 64) <ref type="bibr" target="#b55">[56]</ref> has 68 million parameters. Although GEM applies primal-dual method to reduce the computation in optimization, the overall computation is still considerably high. In this work, we instead compute the gradients on the highest-level layer to generalize the proposed DCL method to any general DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>To comprehensively evaluate the proposed DCL method, we conduct experiments on three tasks, i.e., saliency prediction, continual learning, and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>For saliency prediction task, we use SALICON <ref type="bibr" target="#b19">[20]</ref> (the 2017 version), MIT1003 <ref type="bibr" target="#b21">[22]</ref>, and OSIE <ref type="bibr" target="#b57">[58]</ref>. For continual learning task, we follow the same experimental settings in GEM <ref type="bibr" target="#b32">[33]</ref> to use MNIST Permutations (MNIST-P), MNIST Rotations (MNIST-R), and incremental CIFAR-100 (iCIFAR-100). For classification, we use CIFAR <ref type="bibr" target="#b25">[26]</ref>, Tiny ImageNet, and ImageNet <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Models</head><p>For saliency prediction, we adopt an improved SALICON saliency model <ref type="bibr" target="#b16">[17]</ref> and DINet <ref type="bibr" target="#b58">[59]</ref> as the baselines. Both the baseline models takes ResNet-50 <ref type="bibr" target="#b12">[13]</ref> as the backbone architecture.</p><p>For continual learning, we adopt the same models used in GEM, i.e., Multiple Layer Perceptron (MLP) and ResNet-18, as well as EfficientNet-B1 <ref type="bibr" target="#b46">[47]</ref> as the backbone architecture for evaluation. EWC <ref type="bibr" target="#b23">[24]</ref> and GEM are used for comparison.</p><p>For classification, we use the state-of-the-art model without any architecture modifications for a fair evaluation. ResNeXt <ref type="bibr" target="#b55">[56]</ref> (i.e., ResNeXt-29), DenseNet <ref type="bibr" target="#b15">[16]</ref> (i.e., DenseNet-100-12), and EfficientNet-B1 <ref type="bibr" target="#b46">[47]</ref> are used in the evaluation of CIFAR-10 and CIFAR-100. ResNet (i.e., ResNet-101), DenseNet (i.e., DenseNet-169-32), and EfficientNet-B1 <ref type="bibr" target="#b46">[47]</ref> are used in the experiments on Tiny ImageNet. ResNet (i.e., ResNet-34 and ResNet-50) is used in the experiments on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Notation</head><p>For convenience, we notate model name + optimizer name + DCLβ w -N r for key experimental details in <ref type="table">Table 1</ref>, 2, 6 and 7. β w = ∞ indicates it never resets the references when the initialization of references is finished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Evaluation Metrics</head><p>For saliency prediction, we report the performance using the commonly use metrics, namely area under curve (AUC) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>, shuffled AUC (sAUC) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b44">[45]</ref>, normalized scanpath saliency (NSS) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref>, and correlation coefficient (CC) <ref type="bibr" target="#b37">[38]</ref>. Human fixations are used to form the positive set while the points from the saliency map are sampled to form the negative set. With the two sets, an ROC curve of true positive rate v.s. false positive rate would be plotted by thresholding over the saliency map. If the points are sampled in a uniform distribution, it is AUC. If the points are sampled from the human fixation points, it is sAUC. NSS would average the response values at human eye positions in an predicted saliency map which has been normalized to be zeromean and with unit standard deviation. CC measures the strength of a linear correlation between a ground-truth map and a predicted saliency map. For continual learning, we use the same metrics used in GEM <ref type="bibr" target="#b32">[33]</ref>, i.e., accuracy, backward transfer (BWT), and forward transfer (FWT). For classification, we evaluate the proposed DCL method with top 1 error rate metric on the CIFAR experiments while both top 1 and top 5 error rate are reported in the experiments of Tiny ImageNet and ImageNet. 1: Saliency prediction performance of the models which are trained on SALICON 2017 training set and evaluated on SALICON 2017 validation set. Higher score is better in all the metrics. Each experiment is repeated for 3 times and the mean and std of the scores are reported. We follow <ref type="bibr" target="#b58">[59]</ref> to only use Adam as the optimizer for DINet.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Experimental &amp; Training Details</head><p>In the experiments of saliency prediction, we use Adam <ref type="bibr" target="#b22">[23]</ref> and RMSProp (RMSP) <ref type="bibr" target="#b13">[14]</ref> optimizers. In the setting with Adam, we use η = 0.0002, weight decay 1e-5 while η = 0.0005, weight decay 1e-5 are used within the setting of RMSP. The momentum is set to 0.9 for both Adam and RMSP. η would be adjusted along with the epochs, i.e., η k+1 ← η 0 × 0.5 k−1 , where k is the current epoch. The batch size is 8 by default. To fairly evaluate the performances of the models, we use cross-dataset validation technique, i.e., the models are trained on the SALICON 2017 training set and evaluated on the SALICON 2017 validation set, and trained on OSIE and evaluated on MIT1003. We follow the experimental settings in <ref type="bibr" target="#b32">[33]</ref> for continual learning. Specifically, MNIST-P and MNIST-R have 20 tasks and each task has 1000 examples from 10 different classes. On iCIFAR-100, there are 20 tasks and each task has 2500 examples from 5 different classes. For each task, the first 256 training samples will be selected and stored as the memory on MNIST-P, MNIST-R, and iCIFAR-100. In this work, GEM constraints are concatenated with the DCL constraints by Eq. <ref type="bibr" target="#b18">(19)</ref>. As the different concepts are learned across the episodes, i.e., the tasks, we only consider that the accumulation of gradients would take place in each episode.</p><p>In the classification task, we evaluate the models with SGD optimizer <ref type="bibr" target="#b41">[42]</ref>. The hyperparameters are kept by default, i.e., weight decay 5e-4, initial η = 0.1, the number of total epochs 300. η would be changed to 0.01 and 0.001 at epoch 150 and 225, respectively. For the Tiny ImageNet experiments, we will train the models in 30 epochs with weight decay 1e-4, initial η = 0.001. η would be changed to 1e-4 and 1e-5 at epoch 11 and 21, respectively. The momentum is 0.9 by default. The batch size is 128 in the CIFAR experiments and 64 in the Tiny ImageNet experiments. In the ImageNet experiments, we use batch size of 512 to train ResNet-50.</p><p>In addition, we present the performance of GEM for reference as well. Note that more samples in memory may lead to inconsistent constraints. We set memory size to 1 and reset the memory at each epoch beginning, which is analogous to the case that GEM for continual learning would reset the memory at each beginning of the episode. The implementations of this work are built upon PyTorch 4 and quadprog package is employed to solve quadratic programming problems. <ref type="table">Table 1</ref> reports the mean and standard deviation (std) of the scores in NSS, sAUC, AUC, and CC over 3 runs on the SALICON 2017 validation set. We can see that the proposed DCL method overall improves the saliency prediction performance with both ResNet-50 and DINet over all the metrics. Moreover, small values of stds w.r.t. the proposed DCL method show that the randomness caused by the stochastic process does not contribute much to the improvement. <ref type="table" target="#tab_2">Table 2</ref> shows that the proposed DCL method trained on OSIE consistently improves the saliency prediction performance on MIT1003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Saliency Prediction</head><p>Note that Adam and RMSP optimizer are different algorithms to compute effective step sizes based on the gradients. The consistency of the improvement with both optimizers shows that the proposed DCL method generally works with these optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Continual Learning</head><p>As introduced in Section 4, we apply the proposed DCL method to enhance the congruency of the learning process for continual <ref type="bibr" target="#b3">4</ref>. https://github.com/pytorch/pytorch    <ref type="bibr" target="#b32">[33]</ref>. As reported in <ref type="table" target="#tab_3">Table 3</ref>, the proposed DCL method improves the classification accuracy by 0.7% on MNIST-R. Similarly, the proposed DCL method improves the classification accuracy on MNIST-P as well (see <ref type="table" target="#tab_4">Table 4</ref>). The marginal improvement may results from the difference between MNIST-R and MNIST-P. Permuting the pixels of the digits is harder to recognize than rotating the digits by a fixed angle, and makes the accumulated gradient less informative in terms of leading to the solution. We observe that shorter effective window size is helpful to improve the accuracy in the continual learning task. This is because the training process of continual learning is one-off and a fast variation could be caused by the limited images with brand new labels in each episode. The experiments on iCIFAR-100 in <ref type="table" target="#tab_5">Table 5</ref> confirm this pattern. The proposed DCL method with ResNet and β w = 4 improves the accuracy by 1.25% on iCIFAR-100. There are another two metrics for continual learning, i.e., forward transfer (FWT) and backward transfer (BWT). FWT is that learning a task is helpful in learning for the future tasks. Particularly, positive FWT is correlated to n-shot learning. Since the proposed DCL method utilizes the directional information of the past updates, it has less influence/correlation to FWT. Hence, we will focus on BWT. BWT is the influence that learning a task has on the performance on the previous tasks. Positive BWT is correlated to congruency in the learning process, while large negative BWT is referred as catastrophic forgetting. <ref type="table" target="#tab_3">Table 3</ref> and 4 show that the proposed DCL method is useful in improving BWT on MNIST-R and MNIST-P. The BWT of GEM is negative (-0.0047) and the proposed DCL method improves it to 0.0238 on MNIST-R. Similarly, the BWT of GEM is 0.0224 and the proposed DCL method improves it to 0.0464 on MNIST-P. Similarly, in <ref type="table" target="#tab_5">Table 5</ref>, the proposed DCL method with ResNet improves BWT of GEM from 0.0001 to 0.0305, while the proposed DCL method with EfficientNet <ref type="bibr" target="#b46">[47]</ref> improves BWT to 0.0602. <ref type="table" target="#tab_6">Table 6</ref> reports the top 1 error rates on CIFAR-10 and CIFAR-100 with ResNeXt, DenseNet, and EfficientNet. In all cases, the proposed DCL method outperforms the baseline, i.e., ResNeXt-29 SGD, DenseNet-100-12 SGD, and EfficientNet-B1 SGD. Specifically, the proposed DCL method with ResNeXt decreases the error rate by 0.2% on CIFAR-10 and by 0.28% on CIFAR-100, while the proposed DCL method with EfficientNet decreases the error rate by 0.12% on CIFAR-10 and by 0.16% on CIFAR-100. Similar improvements can be found in the experiments with DenseNet and this shows that the proposed DCL method is generally able to work with various models. Moreover, it can be seen in <ref type="table" target="#tab_6">Table 6</ref> that GEM has a higher error rate than the baseline in the experiments with ResNeXt, DenseNet, and EfficientNet. Because of the dynamical update process in learning, the gradient of the samples in memory does not guarantee that the direction leads to the solution. The direction can be even worse, e.g., it is possible to go in an opposite way to the solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Classification</head><p>A consistent improvement w.r.t. the proposed DCL method can be found in the experiments on Tiny ImageNet (see <ref type="table" target="#tab_8">Table  7</ref>). The proposed DCL method decreases top 1 error rate by 0.45% with ResNet, by 0.69% with DenseNet, and by 0.12% with EfficientNet. Also, the performance degradation caused by GEM <ref type="bibr" target="#b32">[33]</ref> can be observed that top 1 error rate generated by GEM with ResNeXt is increased by almost 4.44%, comparing to the baseline ResNet.   <ref type="table" target="#tab_2">Table  2</ref>. The cosine similarities (Sim.) between referred images and sample images are provided for comparison purposes. Source images and the corresponding ground-truths, i.e., fixation maps, are displayed along with the congruencies. The first and second block are the results of subset that contains persons in various scenes. The third block is examples of food subset. The rightmost block shows subset with mixed image categories, i.e., contain objects of various categories in various scenes. images. Given such difficulties, the proposed DCL method reduces the mean of top 1 errors by 0.24% over three runs. In summary, the improvement gained by the proposed DCL method is benefited from the better solution searched by optimizing DCL quadratic programming problem (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ANALYSIS</head><p>In this section, we first validate the defined congruency by comparing through qualitative examples. Then, an ablation study w.r.t. β and N r is presented. Moreover, we provide a congruency analysis in the training processes for the three tasks. In the end, the comparison between training from scratch and fine-tuning, as well as the computational cost are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Validity of Congruency Metric</head><p>In this subsection, we conduct a sanity check on the validity of the defined congruency. To do this, we consider a simple case where we directly take the gradients (i.e., g S1 and g S2 ) of two samples (i.e., S 1 and S 2 ) to compute the corresponding congruency, i.e., ν =</p><formula xml:id="formula_31">g S 1 g S 2 g S 1 g S 2 .</formula><p>For comparison purposes, the cosine similarity, Sim, between raw image S 1 and S 2 is also computed by Sim = S 1 S 2 S 1 S 2 . Note that congruency is semanticsaware, whereas cosine similarity between the two raw images is semantics-blind. This is because the gradients are computed by images and its semantic ground truth, e.g., the class label in the classification task or human fixation in the saliency prediction task.</p><p>For the analysis in the saliency prediction task, we sample 3 subsets, where 20 training samples w.r.t. person, 20 training For the analysis in the classification task, 3 subsets were sampled from Tiny Ima-geNet, which comprised of 100 images of tabby cat and Egyptian cat to form a intra-similar-class subset, 100 images of tabby cat and German shepherd dog to form a inter-class subset, and 50 images from various classes to form a mixed subset. In this way, we can analyze the correlation between the samples in terms of congruency. With these subsets, we use the baselines, i.e., ResNet-50 for saliency prediction and ResNet-101 for classification, to yield the samples gradients without updating the model. <ref type="figure" target="#fig_7">Figure 7</ref> demonstrates the congruencies w.r.t. the references and various samples (image + fixation map). In contrast to the deterministic nature in the classification task, saliency is contextrelated and semantics-based. It implies that the same objects within two different scenarios may have different saliency labels. Hence, we select the examples of same/similar objects for this experiment. In <ref type="figure" target="#fig_7">Figure 7</ref>, the first and second block on left are based on the person subset within various scenarios. The first block consists of the images of person and dining table. Taking the first row sample as reference, the sample in the second row has higher congruency (0.4155) when compared to bottom row sample (0.3699). Although all the fixation maps of all the samples are different, pizza in the second image is more similar to the reference image whereas food in the bottom sample is inconspicuous. In the second block, both the portrait of the fisher (reference) and the portrait of the baseball player (second sample) are similar in terms of the layout, comparing to the persons in dining room (third sample). Their fixation maps are similar as well. Ref.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cong.</head><p>Sim.</p><p>Ref.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cong.</head><p>Sim.</p><p>-0.   <ref type="table" target="#tab_8">Table 7</ref>.</p><p>The images with its labels are displayed along with the congruencies. The cosine similarities (Sim.) between referred images and sample images are provided for comparison purposes. The first block is the results of the intra-similar-class subset consisting of images of tabby cat and Egyptian cat. The middle block is the results of the inter-class subset consisting of images of tabby cat and German shepherd dog. The value in bracket indicates number of images. The bottom block is the results of images of various labels.</p><p>are higher than the one of the reference and third sample (0.2731).</p><p>In the third block, the image of the reference is three hot dogs and its fixation maps is similar to the fixation maps of the second sample. The two hog dog samples have similar visual appearance and layout of fixations to yield a higher congruency (0.6696). In contrast, third sample is different from the reference in terms of visual appearance and layout of fixations, which yields a lower congruency (0.5075). The rightmost block shows an interesting fact that two outdoor samples yield a positive congruency 0.1667, whereas the outdoor reference and the indoor sample yield a negative congruency −0.1965. One possible reason is that the fixation pattern are different between the reference and the bottom indoor sample. In addition, the visual appearance like illumination may be the another factor causing such the discrepancy.</p><p>For classification, <ref type="figure" target="#fig_8">Figure 8</ref> shows the congruencies w.r.t. the references and given samples in each subset. In all cases, we first observe that images with same genuine class as references yield high congruency, i.e., larger than 0.94 for all cases. These show that the gradients of the same labels are similar in the direction of the updates. Another observation is that the congruency of pairs with different labels are significantly smaller than the matched label counterpart. In <ref type="figure" target="#fig_8">Figure 8</ref>, the congruencies of the reference (Tabby cat) and Egyptian cat images are below 0.03, while the congruencies of the reference and German shepherd dog images are below 0.016 in the middle block. These demonstrate that the gradients of inter-class samples are nearly perpendicular to each other. The reference of class 'Dugong' has positive congruencies w.r.t. all the images that fall in the category of animal, except for the image of chain, which falls into a non-animal category. Last but not least, given the images with different labels, similar visual appearance would lead to relatively higher congruency. For example, the congruencies between tabby cat and Egyptian cat are overall higher than the ones between tabby cat and German shepherd dog. In summary, the labels are an important factor to influence the direction of the gradient in the classification task. Secondly, the visual appearance is another factor for congruency.</p><p>In contrast with congruency, cosine similarity between two raw images make less sense in the context of a specific task. For example, two similar dining scenes in the first column in <ref type="figure" target="#fig_7">Figure 7</ref> yield a negative cosine similarity −0.0066 in the saliency prediction task. Similarly, the first two cat images in the first row in <ref type="figure" target="#fig_8">Figure 8</ref>, which are cast to the same category, yield a negative cosine similarity −0.0013. The negative cosine similarity between two images with the same or similar ground truth are counterintuitive. It results from the fact that cosine similarity between two images only focuses on the difference between two sets of pixels and ignores the semantics associated to the pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>In this subsection, we study the effects of effective window size β w and reference number N r on saliency prediction task (with SALICON) and classification task (with Tiny ImageNet).</p><p>In the saliency prediction experiment, <ref type="figure" target="#fig_10">Figure 9a</ref> shows the curve of sAUC vs. β w based on DCL-β w -1, while <ref type="figure" target="#fig_10">Figure 9b</ref> shows the curve of sAUC vs. N r based on DCL-∞-N r . Note that for the reference number study, the training process on SALICON consists of 12500 iterations so β w ≥ 12500 is equivalent to β w = ∞, which means that it never resets the references in the whole learning process. It can be observed that different β w and N r yield relatively similar performance in sAUC. This aligns with the nature of saliency prediction, where it maps features to the salient label and the non-salient label. The features w.r.t. the salient label  are highly related to each other so β w and N r would pervasively help the learning process make use of congruency.</p><p>In the classification experiment, <ref type="figure" target="#fig_10">Figure 9c</ref> shows the curve of top 1 error vs. β w based on DCL-β w -1. We can see that only a small β w range, i.e., between 20 and 70, yields relatively lower errors than the other β w values. On the other hand, <ref type="figure" target="#fig_10">Figure 9d</ref> shows that only using one reference is helpful in the learning process for classification. Different from the pattern shown in <ref type="figure" target="#fig_10">Figure 9a</ref> and 9b, where the curves are relatively flat, the pattern in <ref type="figure" target="#fig_10">Figure 9c</ref> and 9d implies that the gradients in the learning process for classification are dramatically changed in angle to satisfy the 200-way prediction. Hence, the learning process for classification does not prefer large βw and Nr.</p><p>In summary, the nature of the task should be taken into account to determine the values of βw and Nr. Both parameters can lead to significantly different performances if the task-specific semantics in the data are highly varying. Specifically, as Nr increases, the feasible region for searching a local minimum possibly becomes narrow as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. If the local minimum is not in the narrowed feasible region, large Nr could lead to a slower convergence or even a divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Congruency Analysis</head><p>In this section, we focus on analyzing the patterns of congruency on saliency prediction, continual learning, and classification. For saliency prediction and classification, to study how the gradients of GEM and the proposed DCL method vary in the training process, we compute the congruency of each epoch in the training process by Eq. (4). Specifically, it turns to be ν wes→wee|w1 , where w es and w ee is the weights at the first and last iteration of each epoch, respectively. Here, w 0 is randomly initialized and w 1 represents the starting point of the training. For convenience, we simplify the notation of the average congruency ν wes→wee|w1 for each epoch as ν w1 . Correspondingly, we define the average magnitude d w1 of the accumulated gradients over the iterations in an epoch, i.e., <ref type="bibr" target="#b19">(20)</ref> where d w1 indicates the measurement of magnitudes of the accumulated gradients takes w 1 as the reference. Note that Eq. <ref type="bibr" target="#b19">(20)</ref> does work not only with an absolute reference (e.g., w 1 ), but can work with a relative reference (e.g., w i−1 ) as well. Specifically, we can substitute w i−1 for w 1 in Eq. <ref type="bibr" target="#b19">(20)</ref> to compute d wi−1 . Eq. <ref type="bibr" target="#b19">(20)</ref> can allow us to peek into the convergence process in the high dimensional weight space, where it is difficult to visualize the convergence. By taking an absolute reference (e.g., w 1 ) as the reference, it is able to provide an overview about how the learning process converges to the local minimum from the fixed reference, while a relative reference (e.g., w i−1 ) is helpful to reveal the iterative pattern.</p><formula xml:id="formula_32">d w 1 = 1 sub(w ee )−sub(w es )+1 sub(w ee ) i=sub(w es ) w i − w 1 2</formula><p>For the experiments of continual learning, since GEM uses the samples in memory to regulate the optimization direction, we follow this setting to check the effect of the proposed DCL method on the cosine similarities between the corrected gradient and the gradients generated by the samples in memory for analysis. More concretely, the average cosine similarity is defined as</p><formula xml:id="formula_33">1 N it 1 M N it i=1</formula><p>s∈M cos(gs, gGEMi), where N it is the number of iterations in an epoch and g GEM i is the gradient of GEM at i-th iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Saliency Prediction</head><p>We analyze the models from <ref type="table" target="#tab_2">Table 2</ref>, i.e., ResNet-50 Adam (baseline), ResNet-50 Adam GEM (GEM), and ResNet-50 Adam DCL-∞-1 (DCL). As the training samples sequence is affected by the stochastic process and it may be a factor influencing the proposed DCL method, we present two settings, i.e., within the independent stochastic process and within the same stochastic process amid the training of the three models, to gauge the influence of the stochastic process on the proposed DCL method. Specifically, <ref type="figure" target="#fig_16">Figure 10a</ref> and 10b are the curves with the independent stochastic process on OSIE and SALICON, respectively, whereas the same permuted samples sequences are used in the trainings of the three models in <ref type="figure">Figure 10c</ref> and 10d. We can see that they are similar in pattern and it implies that the permutation of the training samples has less influence on the proposed DCL method. Moreover, the proposed DCL method consistently gives rise to a more congruent learning process than the baseline and GEM. <ref type="figure" target="#fig_12">Figure 11a</ref>,11b, and 11c shows the congruency along the tasks which are the episodes to learn the new classes. It can be seen that the proposed DCL method significantly enhances the cosine similarities between the gradients for updates and the gradients generated by the samples in memory on MNIST-R. There are improvements made by the proposed DCL method on early tasks on MNIST-P. Moreover, an overall consistent improvement of the proposed DCL method can be observed on iCIFAR-100. Overall, the corrected updates for the model are computed by proposed DCL method to be more congruent with its previous updates. This consistently results in the improvement of BWT in <ref type="table" target="#tab_3">Table 3</ref>, 4, and 5.  <ref type="figure">Figure 10</ref>: Congruencies along the epochs in saliency prediction learning, as defined in Eq. (4). The samples sequences for training models are determined by independent stochastic processes in <ref type="figure" target="#fig_16">Figure 10a</ref> and 10b, while the permuted samples sequences are predetermined and fixed for all models in <ref type="figure">Figure 10c</ref> and 10d. The baseline, GEM, and DCL are ResNeXt-29 SGD, ResNeXt-29 SGD GEM, and ResNeXt-29 SGD DCL-∞-1 (see <ref type="table" target="#tab_6">Table 6</ref>), respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Continual Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Classification</head><p>We analyze the models from  <ref type="table" target="#tab_8">Table 7</ref> are used for analysis on Tiny ImageNet. The curves of the average congruencies are shown in <ref type="figure" target="#fig_16">Figure 12a</ref>, 12d, and 12g, while <ref type="figure">Figure 12b</ref>, 12e, and 12h show the average magnitudes. As shown in <ref type="figure" target="#fig_16">Figure 12a</ref>, 12d, and 12g, the congruency of the proposed DCL method is significantly higher than the baseline and GEM along all epochs on CIFAR-10 and CIFAR-100. Higher congruency indicates the convergence path would be flatter and smoother. For example, if all the congruencies of each epoch are 0, the convergence path would be a straight line.</p><p>On the other hand, the average magnitudes of the proposed DCL method are relatively flat and smooth in <ref type="figure">Figure 12b</ref>, 12e, and 12h, comparing to the baseline and GEM. Connecting the magnitudes with the congruencies in <ref type="figure" target="#fig_16">Figure 12a</ref>, 12d, and 12g, we can infer two points. First, the proposed DCL method finds a nearer local minimum to its initialized weights on CIFAR-10 and CIFAR-100. Because the magnitudes of the proposed DCL method is the smallest among the three methods. Second, the convergence (g) Avg. cong. on Tiny ImageNet.</p><p>(h) dw 1 on Tiny Ima-geNet.</p><p>(i) dw i−1 on Tiny Ima-geNet. <ref type="figure">Figure 12</ref>: Analyses of the congruencies and magnitudes along the epochs in classification task, as defined in Eq. <ref type="formula" target="#formula_5">(4)</ref> and <ref type="bibr" target="#b19">(20)</ref>.</p><p>path of the proposed DCL method is the least oscillatory because its congruencies are overall higher than the other two methods and its magnitudes are the lowest among the three methods. We take a further look at the training error vs. iteration curves to better understand the convergences in <ref type="figure" target="#fig_0">Figure 13</ref>. To give an overview along all epochs, we compute the mean and standard deviation of the training errors at each epoch and plot them at a logarithm scale in <ref type="figure" target="#fig_0">Figure 13a</ref>   <ref type="figure" target="#fig_17">Figure 14</ref> shows the validation losses w.r.t. the three tasks, i.e., saliency prediction (a), continual learning (b), and classification (c). In general, the proposed DCL method achieves lower loss than the baseline and GEM, which is aligned with the fact that the proposed DCL method outperforms the baseline and GEM. Note that classification losses of GEM are above 1.0 so they are not shown in <ref type="figure" target="#fig_17">Figure 14c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Empirical Convergence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Training from Scratch vs. Fine-tuning</head><p>We analyze the proposed approach with two types of training scheme on the validation set of Tiny ImageNet. The first training scheme train the models from scratch using the training set of the target dataset, whereas the second training scheme fine-tunes the pre-trained ImageNet models on Tiny ImageNet. For ease of comparison, the experimental results of training the models from scratch on Tiny ImageNet as the fine-tuning results are shown in <ref type="table" target="#tab_13">Table 9</ref>. Similar to the results of fine-tuning, the proposed DCL method achieves lower top 1 error (i.e., 67.56%) and top 5 error (i.e., 40.74%) than the baseline and GEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Computational Cost</head><p>We report computational cost on Tiny ImageNet, SALICON and ImageNet in <ref type="table" target="#tab_14">Table 10</ref> and 11, respectively. Specifically, the number of parameters of the models and the corresponding processing time per image are presented. The processing time per image is computed by (batch time − data time)/batch size, where batch time is the time to complete the process of a batch of images, and data time is the time to load a batch of images. Note that the processes of gradient descent with or without the proposed DCL   method are the same in the testing phase. We train the models on 3 NVIDIA 1080 Ti graphics cards for the experiments on Tiny ImageNet and SALICON, and on 8 NVIDIA V100 graphics cards for the experiment on ImageNet. ResNet-101 DCL on Tiny ImageNet is with β w = 60 and N r = 1. ResNet-50 DCL is with β w = ∞ and N r = 1 on SALICON, and β w = 1 and N r = 1 on ImageNet. ResNet-50 GEM is with N r = 1 on all the datasets. The difference of the numbers of parameters between the baseline and the proposed DCL method (or GEM) lies in the final layer, i.e., 1 × 1 convolutional layer for saliency prediction and the fully connected layer for classification. The proposed DCL method has more parameters to store the weights of the final layer for the references.</p><p>In the experiment on Tiny ImageNet, the proposed DCL method with ResNet-101 takes 2 more milliseconds than the baseline to solve the constrained quadratic problem <ref type="bibr" target="#b4">(5)</ref>. Similarly, with ResNet-50, it takes 1 and 2 more milliseconds than the baseline on SALICON and ImageNet, respectively. This shows that quadratic problems with high dimensional input can be efficiently solved by the tool quadprog. Hence, the proposed DCL method is practically accessible. On the other hand, GEM <ref type="bibr" target="#b32">[33]</ref> is less efficient than the other two methods across the three datasets. This is because GEM has to compute the gradients according to the memory, i.e., the input features of the final layer, at each iteration. Instead, the proposed DCL method uses a subtraction operation (i.e., Eq. 6) to compute the accumulated gradient. Thus, it is faster than GEM. Moreover, we discuss the effects of β w and N r on the computational cost here. The computational cost w.r.t. β w and N r with ResNet on Tiny ImageNet is reported in <ref type="table" target="#tab_2">Table 12</ref>. As β w indicates the effective window, it is implemented by a subtraction operation according to Eq. (6) and updating the reference point is a copying operation in RAM which is fast. Therefore, β w would not affect computational cost. On the other hand, the time difference between various N r is small because we only apply the proposed DCL method to the downstream layer, i.e., the final layer, where the parameters are much fewer than the ones used by the whole network. For example, there are only 2304 parameters in the final convolutional layer for saliency prediction. Any quadratic programming solver like quadprop can efficiently handle the corresponding dual problem (8) in a small scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Discussion of Generalization</head><p>Incongruency is ubiquitous in the learning process. It results from the diversity of the input data, e.g., real-world images, and rich task-specific semantics. The proposed DCL method can effectively alleviate the incongruency problem in saliency prediction, continual learning, and classification. Specifically, saliency prediction can be seen as a typical regression problem while continual learning and classification can be seen as a typical learning problem that aims to predict a discrete label. In this sense, the input-output mapping and the learning settings of the three tasks are fundamental to other vision tasks.</p><p>From the point of view of task-dependent incongruency, here we consider general vision tasks to be cast into three groups according to the form of input and output. The first group consists of visual tasks that take images as input for classification or regression, e.g., object detection <ref type="bibr" target="#b40">[41]</ref> and visual sentiment analysis <ref type="bibr" target="#b60">[61]</ref>. In object detection, visual appearance of a region of interest could be diverse in terms of its label and location, while an arbitrary sentiment class can have a number of visual representations in visual sentiment analysis. Since tasks in this group has similar incongruency as that in image classification, i.e., the diversity of raw image features w.r.t. a certain label, the proposed DCL method is expected to boost this type of vision tasks. The second group consists of visual tasks that have complex outputs of regression or classification, e.g., visual relationship detection <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref> and human object interaction <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b56">[57]</ref> whose output can involve multiple possible relationships among two or more objects that belong to various visual concepts. The incongruency of tasks in this group lies in the diversity of raw image features w.r.t. a higher dimensional variable, e.g., a relationship which involves multiple objects and corresponding predicates. Last but not least,  the third group consists of visual tasks that take a series of images, e.g., action recognition <ref type="bibr" target="#b53">[54]</ref>. Usually, it takes a clip of videos as input and incorporates temporal information. The incongruency of tasks in this group lies in the diversity of temporal raw image features w.r.t. a certain label, and the feature space with clips is often more complicated than that in static images. Therefore, the incongruency of tasks in the second and third groups could be more remarkable than that of tasks in the first group. Note that the proposed DCL method is gradient-based and not restricted to specific forms of input or output. Therefore, it could naturally generalize or be used as a starting point to alleviate incongruency for tasks with different forms of input and output in the three groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we define congruency as the agreement between new information and the learned knowledge in a learning process. We propose a Direction Concentration Learning (DCL) method to take into account the congruency in a learning process to search for a local minimum. We study the congruency in the three tasks, i.e., saliency prediction, continual learning, and classification. The proposed DCL method generally improves the performances of the three tasks. More importantly, our analysis shows that the proposed DCL method improves catastrophic forgetting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>demonstrates the effect of constraints in optimization. The dashed line in the same color indicates the border of feasible</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of DCL constraints with two reference points r 0 = w 0 , r 1 = w 1 .ĝ r0 is the pink arrow whileĝ r1 is the green one. The colored dashed line indicates the border of feasible region with regards to −ĝ ri , i ∈ {0, 1}, since Constraint (6) forces −ηg k to have an angle which is smaller than or equal to 90 • w.r.t.ĝ r0 andĝ r1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 4 . 1 (</head><label>41</label><figDesc>Duality). if z = z * is a solution to Problem (7) then a solution (u, v) = (u * , v * ) exists to Problem<ref type="bibr" target="#b7">(8)</ref>. Conversely, if a solution (u, v) = (u * , v * ) to Problem (8) exists then a solution which satisfies Cz = Cu * to Problem (7) also exists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 4 . 2 (</head><label>42</label><figDesc>Quadratic upper bound</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Corollary 4 . 4 .</head><label>44</label><figDesc>Let x k be the k-th iteration result of gradient descent and η k ≥ 0 the k-th step size. If ∇f is L-Lipschitz continuous, then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Convergence with GD at iteration 50 (b) Convergence with RMSProp at iteration 150 (c) Convergence with Adam at iteration 200 (d) Curves of z v.s. iteration with GD (e) Curves of z v.s. iteration with RMSProp (f) Curves of z v.s. iteration with Adam</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a)-(c), while the corresponding z v.s. iteration curves are plotted in Figure 5(d)-(f). 3. https://github.com/Jaewan-Yun/optimizer-visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The congruencies (Cong.) generated by the given references (Ref.) and samples with the baseline ResNet-50 RMSP in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>The congruencies (Cong.) generated by the given references (Ref.) and samples with the baseline ResNet-101 SGD in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) sAUC vs. βw (b) sAUC vs. Nr (c) Top 1 error vs. βw (d) Top 1 error vs. Nr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Ablation study w.r.t. effective window size βw and references number Nr. (a) and (b) are the experimental results on the SALICON validation set, while (c) and (d) are with the Tiny ImageNet validation set. βw = ∞ in (b) and βw = 50 in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>The average congruencies over epochs in training on the three datasets for continual learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(a) Avg. cong. on CIFAR-10.(b) dw 1 on CIFAR-10. (c) dw i−1 on CIFAR-10. (d) Avg. cong. on CIFAR-100. (e) dw 1 on CIFAR-100. (f) dw i−1 on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>and 13b, respectively. The results show that the proposed DCL method yields lower training errors from epoch 1 to epoch 16. From epoch 15 onwards, the proposed DCL method is little different from the baseline in terms of the mean because they are both around 0.1. Therefore, we plot the representative curve at epoch 1, 5, 10, and 15 in Figure 13c-13f.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>15 Figure 13 :</head><label>1513</label><figDesc>(a) Mean of errors over epochs (b) Std of errors over epochs (c) Training error vs. iter. at epoch 1 (d) Training error vs. iter. at epoch 5 (e) Training error vs. iter. at epoch 10 (f) Training error vs. iter. at epoch Training error vs. iteration on Tiny ImageNet with ResNet-101. (a) and (b) plot the mean and standard deviation of training errors at each epoch, respectively. Specifically, we show four representative curves of training error vs. iteration at epoch 1, 5, 10, and 15 in (c) -(f), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>( a )</head><label>a</label><figDesc>Saliency prediction on SALICON. (b) Continual learning on iCIFAR-100. (c) Classification on Tiny ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 :</head><label>14</label><figDesc>Validation loss vs. epoch/task. In (c), the dashed blue curve indicates that the classification losses of GEM on Tiny ImageNet are all above 1.0 so they are not shown in the figure for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Saliency prediction performance of the models which are trained on OSIE and tested on MIT1003. Each experiment is repeated for 3 times and the mean and std of the scores are reported.</figDesc><table><row><cell></cell><cell>NSS</cell><cell>sAUC</cell><cell>AUC</cell><cell>CC</cell></row><row><cell>ResNet-50 RMSP</cell><cell>2.4047±0.0055</cell><cell>0.7612±0.0019</cell><cell>0.8455±0.0028</cell><cell>0.7595±0.0002</cell></row><row><cell>ResNet-50 RMSP GEM</cell><cell>2.3960±0.0057</cell><cell>0.7566±0.0045</cell><cell>0.8412±0.0055</cell><cell>0.7500±0.0037</cell></row><row><cell>ResNet-50 RMSP DCL-∞-1</cell><cell>2.4252±0.0053</cell><cell>0.7620±0.0018</cell><cell>0.8469±0.0027</cell><cell>0.7658±0.0016</cell></row><row><cell>ResNet-50 Adam</cell><cell>2.4064±0.0015</cell><cell>0.7597±0.0012</cell><cell>0.8429±0.0021</cell><cell>0.7618±0.0005</cell></row><row><cell>ResNet-50 Adam GEM</cell><cell>2.3685±0.0065</cell><cell>0.7594±0.0007</cell><cell>0.8427±0.0017</cell><cell>0.7524±0.0011</cell></row><row><cell>ResNet-50 Adam DCL-∞-1</cell><cell>2.4108±0.0063</cell><cell>0.7613±0.0007</cell><cell>0.8442±0.0008</cell><cell>0.7617±0.0007</cell></row><row><cell>DINet Adam</cell><cell>2.4406±0.0058</cell><cell>0.7570±0.0005</cell><cell>0.8442±0.0016</cell><cell>0.7534±0.0005</cell></row><row><cell>DINet Adam GEM</cell><cell>2.4456±0.0037</cell><cell>0.7571±0.0005</cell><cell>0.8432±0.0003</cell><cell>0.7540±0.0006</cell></row><row><cell>DINet Adam DCL-120-1</cell><cell>2.4566±0.0007</cell><cell>0.7611±0.0011</cell><cell>0.8476±0.0008</cell><cell>0.7597±0.0008</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Performances on MNIST-R in continual learning setting using SGD<ref type="bibr" target="#b41">[42]</ref> as the optimizer. The reported accuracy is in percentage. MEM indicates that the constraints of GEM<ref type="bibr" target="#b32">[33]</ref> are concatenated to use as Eq.(19)describes.</figDesc><table><row><cell></cell><cell>Accuracy</cell><cell>BWT</cell><cell>FWT</cell></row><row><cell>EWC</cell><cell>54.61</cell><cell>-0.2087</cell><cell>0.5574</cell></row><row><cell>GEM</cell><cell>83.35</cell><cell>-0.0047</cell><cell>0.6521</cell></row><row><cell>MLP DCL-30-1 MEM</cell><cell>84.08</cell><cell>0.0094</cell><cell>0.6423</cell></row><row><cell>MLP DCL-40-1 MEM</cell><cell>84.02</cell><cell>0.0127</cell><cell>0.6351</cell></row><row><cell>MLP DCL-50-1 MEM</cell><cell>82.77</cell><cell>0.0238</cell><cell>0.6111</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Performances on MNIST-P in continual learning setting using SGD as the optimizer.</figDesc><table><row><cell></cell><cell>Accuracy</cell><cell>BWT</cell><cell>FWT</cell></row><row><cell>EWC</cell><cell>59.31</cell><cell>-0.1960</cell><cell>-0.0075</cell></row><row><cell>GEM</cell><cell>82.44</cell><cell>0.0224</cell><cell>-0.0095</cell></row><row><cell>MLP DCL-3-1 MEM</cell><cell>82.30</cell><cell>0.0248</cell><cell>-0.0038</cell></row><row><cell>MLP DCL-4-1 MEM</cell><cell>82.58</cell><cell>0.0402</cell><cell>-0.0092</cell></row><row><cell>MLP DCL-5-1 MEM</cell><cell>82.10</cell><cell>0.0464</cell><cell>-0.0095</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>Accuracy</cell><cell>BWT</cell><cell>FWT</cell></row><row><cell>EWC</cell><cell>48.33</cell><cell>-0.1050</cell><cell>0.0216</cell></row><row><cell>iCARL</cell><cell>51.56</cell><cell>-0.0848</cell><cell>0.0000</cell></row><row><cell>ResNet GEM</cell><cell>66.67</cell><cell>0.0001</cell><cell>0.0108</cell></row><row><cell>ResNet DCL-4-1 MEM</cell><cell>67.92</cell><cell>0.0063</cell><cell>0.0102</cell></row><row><cell>ResNet DCL-8-1 MEM</cell><cell>67.27</cell><cell>0.0104</cell><cell>0.0190</cell></row><row><cell>ResNet DCL-12-1 MEM</cell><cell>66.58</cell><cell>0.0089</cell><cell>0.0139</cell></row><row><cell>ResNet DCL-20-1 MEM</cell><cell>66.56</cell><cell>0.0030</cell><cell>0.0102</cell></row><row><cell>ResNet DCL-24-1 MEM</cell><cell>64.97</cell><cell>0.0082</cell><cell>0.0238</cell></row><row><cell>ResNet DCL-32-1 MEM</cell><cell>66.10</cell><cell>0.0305</cell><cell>0.0176</cell></row><row><cell>ResNet DCL-50-1 MEM</cell><cell>64.86</cell><cell>0.0244</cell><cell>0.0125</cell></row><row><cell>EffNet GEM</cell><cell>80.80</cell><cell>0.0318</cell><cell>-0.0050</cell></row><row><cell>EffNet DCL-4-1 MEM</cell><cell>81.55</cell><cell>0.0383</cell><cell>-0.0048</cell></row><row><cell>EffNet DCL-8-1 MEM</cell><cell>80.84</cell><cell>0.0367</cell><cell>0.0068</cell></row><row><cell>EffNet DCL-12-1 MEM</cell><cell>79.45</cell><cell>0.0322</cell><cell>0.0011</cell></row><row><cell>EffNet DCL-20-1 MEM</cell><cell>79.33</cell><cell>0.0316</cell><cell>-0.0095</cell></row><row><cell>EffNet DCL-24-1 MEM</cell><cell>79.05</cell><cell>0.0375</cell><cell>-0.0006</cell></row><row><cell>EffNet DCL-32-1 MEM</cell><cell>79.97</cell><cell>0.0452</cell><cell>-0.0145</cell></row><row><cell>EffNet DCL-50-1 MEM</cell><cell>77.87</cell><cell>0.0602</cell><cell>-0.0101</cell></row><row><cell cols="4">learning. Specifically, following Eq. (19), we concatenate the</cell></row><row><cell cols="3">DCL constraints with the GEM constraints</cell><cell></cell></row></table><note>Performances on iCIFAR-100 in continual learning setting using SGD as the optimizer. EffNet stands for EfficientNet [47].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Top 1 error rate (in %) on CIFAR with various models.</figDesc><table><row><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 reports</head><label>8</label><figDesc></figDesc><table><row><cell>Ref.</cell><cell>Ref.</cell><cell>Ref.</cell><cell>Ref.</cell></row><row><cell>Cong. Sim.</cell><cell>Cong. Sim.</cell><cell>Cong. Sim.</cell><cell>Cong. Sim.</cell></row><row><cell>0.4155 -0.0066</cell><cell>0.6377 0.1364</cell><cell>0.6696 -0.1189</cell><cell>0.1667 -0.1788</cell></row><row><cell>0.3699 0.1908</cell><cell>0.2731 -0.1109</cell><cell>0.5075 -0.0341</cell><cell>-0.1965 -0.5603</cell></row></table><note>the mean and std of 1-crop validation error of ResNet-50 on ImageNet. Comparing to Tiny ImageNet and CIFAR, ImageNet has more categories and more high resolution</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>Top 1 and top 5 error rate (in %) on the validation set of Tiny ImageNet with various models.</figDesc><table><row><cell></cell><cell>Top 1 error</cell><cell>Top 5 error</cell></row><row><cell>ResNet-101 SGD</cell><cell>17.34</cell><cell>4.82</cell></row><row><cell>ResNet-101 SGD GEM</cell><cell>21.78</cell><cell>7.21</cell></row><row><cell>ResNet-101 SGD DCL-60-1</cell><cell>16.89</cell><cell>4.50</cell></row><row><cell>DenseNet-169-32 SGD</cell><cell>20.24</cell><cell>6.11</cell></row><row><cell>DenseNet-169-32 SGD GEM</cell><cell>26.81</cell><cell>9.43</cell></row><row><cell>DenseNet-169-32 SGD DCL-50-1</cell><cell>19.55</cell><cell>6.09</cell></row><row><cell>EfficientNet-B1 SGD</cell><cell>15.73</cell><cell>3.90</cell></row><row><cell>EfficientNet-B1 SGD GEM</cell><cell>28.74</cell><cell>11.31</cell></row><row><cell>EfficientNet-B1 SGD DCL-8-1</cell><cell>15.61</cell><cell>3.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Top 1 and top 5 1-crop validation error (in %) on ImageNet with SGD optimizer. β w = 5 and N r = 1 are used for ResNet-50 DCL. Within the same experimental settings, ResNet-50 GEM does not converge in this experiment. The mean and std of errors are computed over three runs.</figDesc><table><row><cell>Top 1 error</cell><cell>Top 5 error</cell></row></table><note>samples w.r.t. food, and 20 training samples w.r.t. various scenes and categories were sampled from SALICON.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>, i.e., ResNeXt-29 SGD</cell></row><row><cell>(baseline), ResNeXt-29 SGD GEM (GEM), and ResNeXt-29 SGD</cell></row><row><cell>DCL-∞-1 (DCL), in term of the resulting congruency of each</cell></row><row><cell>epoch in the learning process on CIFAR. Similarly, ResNet-101</cell></row><row><cell>SGD, ResNet-101 SGD GEM, and ResNet-101 SGD DCL-50-1</cell></row><row><cell>in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9 :</head><label>9</label><figDesc>Top 1 and top 5 error rate (in %) on the validation set of Tiny ImageNet. We compare of (1) training the models from scratch (TFS) on Tiny ImageNet, and (2) fine-tuning (FT) the pretrained ImageNet models on Tiny ImageNet. ResNet-101 SGD DCL is with β w = 60 and N r = 1. The validation errors of FT are fromTable 7.</figDesc><table><row><cell></cell><cell cols="2">Top 1 error</cell><cell cols="2">Top 5 error</cell></row><row><cell></cell><cell>TFS</cell><cell>FT</cell><cell>TFS</cell><cell>FT</cell></row><row><cell>ResNet-101 SGD</cell><cell>68.21</cell><cell>17.34</cell><cell>42.72</cell><cell>4.82</cell></row><row><cell>ResNet-101 SGD GEM</cell><cell>77.20</cell><cell>21.78</cell><cell>53.18</cell><cell>7.21</cell></row><row><cell>ResNet-101 SGD DCL</cell><cell>67.56</cell><cell>16.89</cell><cell>40.74</cell><cell>4.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 10 :</head><label>10</label><figDesc>Computational cost of training models on Tiny Ima-geNet. The processing time (proc time) per image is calculated by (batch time − data time)/batch size.</figDesc><table><row><cell></cell><cell># params</cell><cell>proc time</cell></row><row><cell>ResNet-101</cell><cell>42.50M</cell><cell>47 ms</cell></row><row><cell>ResNet-101 GEM</cell><cell>42.91M</cell><cell>78 ms</cell></row><row><cell>ResNet-101 DCL</cell><cell>42.91M</cell><cell>49 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 11 :</head><label>11</label><figDesc>Computational cost of training models on SALICON and ImageNet.</figDesc><table><row><cell></cell><cell cols="2">SALICON</cell><cell cols="2">ImageNet</cell></row><row><cell></cell><cell># params</cell><cell>proc time</cell><cell># params</cell><cell>proc time</cell></row><row><cell>ResNet-50</cell><cell>23.51M</cell><cell>64 ms</cell><cell>23.50M</cell><cell>6 ms</cell></row><row><cell>ResNet-50 GEM</cell><cell>23.51M</cell><cell>102 ms</cell><cell>25.55M</cell><cell>10 ms</cell></row><row><cell>ResNet-50 DCL</cell><cell>23.51M</cell><cell>65 ms</cell><cell>25.55M</cell><cell>8 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 12 :</head><label>12</label><figDesc>The effect of β w and N r on computational cost (i.e., proc time) with ResNet on Tiny ImageNet. Note that β w would not affect computational cost because β w indicates the effective window and resetting the references is implemented as a subtraction operation according to Eq. (6).</figDesc><table><row><cell cols="2">βw (Nr = 1) proc time</cell><cell cols="2">Nr (βw = 50) proc time</cell></row><row><cell>10</cell><cell>49 ms</cell><cell>1</cell><cell>49 ms</cell></row><row><cell>20</cell><cell>49 ms</cell><cell>5</cell><cell>51 ms</cell></row><row><cell>30</cell><cell>49 ms</cell><cell>10</cell><cell>53 ms</cell></row><row><cell>40</cell><cell>49 ms</cell><cell>15</cell><cell>54 ms</cell></row><row><cell>50</cell><cell>49 ms</cell><cell>20</cell><cell>54 ms</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention based on information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="950" to="950" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The what-and-where filter: a spatial mapping neural network for object recognition and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Lesher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic pooling for complex event analysis in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1617" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting human eye fixations via an lstm-based saliency attentive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5142" to="5154" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Duality in quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Dorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lecture notes: optimization algorithms, February 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<ptr target="https://math.nyu.edu/∼cfgranda/pages/OBDAspring16/material/optimizationalgorithms.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentional allocation during the perception of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">760</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning: Lecture 6a -Overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/∼tijmen/csc321/slides/lectureslideslec6.pdf" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LIBOL: a library for online learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="495" to="499" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Realistic Avatar Eye and Head Animation Using a Neurobiological Model of Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhavale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE 48th Annual International Symposium on Optical Science and Technology</title>
		<meeting>SPIE 48th Annual International Symposium on Optical Science and Technology</meeting>
		<imprint>
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>MIT-CSAIL-TR-2012-001</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Natural gradient deep q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lerner</surname></persName>
		</author>
		<idno>abs/1803.07482</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>4</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding low-and high-level contributions to fixation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kummerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S A</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4789" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual-glance model for deciphering social relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2669" to="2678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3585" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6470" to="6479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of learning and motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Introductory Lectures on Convex Optimization: A Basic Course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated, 1st edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o (1/kˆ2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Empirical validation of the saliency-based model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Von</forename><surname>Wartburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hugli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Müri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Letters on Computer Vision and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Connectionist models of recognition memory: constraints imposed by learning and forgetting functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">285</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">iCaRL: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention links sensing to recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Rothenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="15" to="15" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geometric mean for subspace selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="274" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Lecture notes: optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<ptr target="http://www.stat.cmu.edu/∼ryantibs/convexopt-F13/scribes/lec6.pdf" />
		<imprint>
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual saliency and semantic incongruency influence eye movements when inspecting pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Foulsham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1931" to="1949" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Congruency, saliency and gist in the inspection of objects in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eye Movements</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">563</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="988" to="999" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Five factors that guide attention in visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2019" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Predicting human gaze beyond pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="28" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A dilated inception network for visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Feature selection for multimedia analysis by sharing information among multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="661" to="669" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visual sentiment analysis by attending on local image regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="237" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
