<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predictive Business Process Monitoring with LSTM Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niek</forename><surname>Tax</surname></persName>
							<email>n.tax@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Verenich</surname></persName>
							<email>ilya.verenich@qut.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Tartu</orgName>
								<address>
									<country key="EE">Estonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><forename type="middle">La</forename><surname>Rosa</surname></persName>
							<email>m.larosa@qut.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlon</forename><surname>Dumas</surname></persName>
							<email>marlon.dumas@ut.ee</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Tartu</orgName>
								<address>
									<country key="EE">Estonia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predictive Business Process Monitoring with LSTM Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predictive business process monitoring methods exploit logs of completed cases of a process in order to make predictions about running cases thereof. Existing methods in this space are tailor-made for specific prediction tasks. Moreover, their relative accuracy is highly sensitive to the dataset at hand, thus requiring users to engage in trial-anderror and tuning when applying them in a specific setting. This paper investigates Long Short-Term Memory (LSTM) neural networks as an approach to build consistently accurate models for a wide range of predictive process monitoring tasks. First, we show that LSTMs outperform existing techniques to predict the next event of a running case and its timestamp. Next, we show how to use models for predicting the next task in order to predict the full continuation of a running case. Finally, we apply the same approach to predict the remaining time, and show that this approach outperforms existing tailor-made methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Predictive business process monitoring techniques are concerned with predicting the evolution of running cases of a business process based on models extracted from historical event logs. A range of such techniques have been proposed for a variety of prediction tasks: predicting the next activity <ref type="bibr" target="#b1">[2]</ref>, predicting the future path (continuation) of a running case <ref type="bibr" target="#b24">[25]</ref>, predicting the remaining cycle time <ref type="bibr" target="#b26">[27]</ref>, predicting deadline violations <ref type="bibr" target="#b21">[22]</ref> and predicting the fulfillment of a property upon completion <ref type="bibr" target="#b19">[20]</ref>. The predictions generated by these techniques have a range of applications. For example, predicting the next activity (and its timestamp) or predicting the sequence of future activities in a case provide valuable input for planning and resource allocation. Meanwhile, predictions of the remaining execution time can be used to prioritize process instances in order to fulfill service-level objectives (e.g. to minimize deadline violations).</p><p>Existing predictive process monitoring approaches are tailor-made for specific prediction tasks and not readily generalizable. Moreover, their relative accuracy varies significantly depending on the input dataset and the point in time when arXiv:1612.02130v2 [stat.AP] <ref type="bibr" target="#b15">16</ref> May 2017 the prediction is made. A technique may outperform another one for one log and a given prediction point (e.g. making prediction at the mid-point of each trace), but under-perform it for another log at the same prediction point, or for the same log at an earlier prediction point <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>. In some cases, multiple techniques need to be combined <ref type="bibr" target="#b21">[22]</ref> or considerable tuning is required (e.g. using hyperparameter optimization) <ref type="bibr" target="#b10">[11]</ref> in order to achieve more consistent accuracy.</p><p>Recurrent neural networks with Long Short-Term Memory (LSTM) architectures <ref type="bibr" target="#b13">[14]</ref> have been shown to deliver consistently high accuracy in several sequence modeling application domains, e.g. natural language processing <ref type="bibr" target="#b22">[23]</ref> and speech recognition <ref type="bibr" target="#b12">[13]</ref>. Recently, Evermann et al. <ref type="bibr" target="#b8">[9]</ref> applied LSTMs to predictive process monitoring, specifically to predict the next activity in a case.</p><p>Inspired by these results, this paper investigates the following questions: (i) can LSTMs be applied to a broad range of predictive process monitoring problems, and how? and (ii) do LSTMs achieve consistently high accuracy across a range of prediction tasks, event logs and prediction points? To address these questions, the paper puts forward LSTM architectures for predicting: (i) the next activity in a running case and its timestamp; (ii) the continuation of a case up to completion; and (iii) the remaining cycle time. The outlined LSTM architectures are empirically compared against tailor-made approaches with respect to their accuracy at different prediction points, using four real-life event logs.</p><p>The paper is structured as follows. Section 2 discusses related work. Section 3 introduces foundational concepts and notation. Section 4 describes a technique to predict the next activity in a case and its timestamp, and compares it against tailor-made baselines. Section 5 extends the previous technique to predict the continuation of a running case. Section 6 shows how this latter method can be used to predict the remaining time of a case, and compares it against tailor-made approaches. Section 7 concludes the paper and outlines future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section discusses existing approaches to predictive process monitoring for three prediction tasks: time-related predictions, predictions of the outcome of a case and predictions of the continuation of a case and/or characteristics thereof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prediction of time-related properties</head><p>A range of research proposals have addressed the problem of predicting delays and deadline violations in business processes. Pika et al. <ref type="bibr" target="#b23">[24]</ref> propose a technique for predicting deadline violations. Metzger et al. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> present techniques for predicting "late show" events (i.e. delays between the expected and the actual time of arrival) in a freight transportation process. Senderovich et al. <ref type="bibr" target="#b27">[28]</ref> apply queue mining techniques to predict delays in case executions.</p><p>Another body of work focuses on predicting the remaining cycle time of running cases. Van Dongen et al. predict the remaining time by using nonparametric regression models based on case variables <ref type="bibr" target="#b7">[8]</ref>. Van der Aalst et al.</p><p>[1] propose a remaining time prediction method by constructing a transition system from the event log using set, bag, or sequence abstractions. Rogge-Solti &amp; Weske <ref type="bibr" target="#b26">[27]</ref> use stochastic Petri nets to predict the remaining time of a process, taking into account elapsed time since the last observed event. Folino et al. <ref type="bibr" target="#b9">[10]</ref> develop an ad-hoc clustering approach to predict remaining time and overtime faults. In this paper, we show that prediction of the remaining cycle time can be approached as a special case of prediction of a process continuation. Specifically, our approach is proven to generally provide better accuracy than <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prediction of case outcome</head><p>The goal of approaches in this category is to predict cases that will end up in an undesirable state. <ref type="bibr">Maggi et al. [20]</ref>, propose a framework to predict the outcome of a case (normal vs. deviant) based on the sequence of activities executed in a given case and the values of data attributes of the last executed activity in a case. This latter framework constructs a classifier on-the-fly (e.g. a decision tree or random forest) based on historical cases that are similar to the (incomplete) trace of a running case. Other approaches construct a collection of classifiers offline. For example, <ref type="bibr" target="#b18">[19]</ref> construct one classifier for every possible prediction point (e.g. predicting the outcome after the first event, the second one and so on). Meanwhile, <ref type="bibr" target="#b11">[12]</ref> apply clustering techniques to group together similar prefixes of historical traces and then construct one classifier per cluster.</p><p>The above approaches require one to extract a feature vector from a prefix of an ongoing trace. De Leoni et al. <ref type="bibr" target="#b17">[18]</ref> propose a framework that classifies possible approaches to extract such feature vectors.</p><p>In this paper, we do not address the problem of case outcome prediction, although the proposed architectures could be extended in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prediction of future event(s)</head><p>Breuker et al. <ref type="bibr" target="#b2">[3]</ref> use probabilistic finite automaton to tackle the next-activity prediction problem, while Evermann et al. <ref type="bibr" target="#b8">[9]</ref> use LSTMs. Using the latter approach as a baseline, we propose an LSTM architecture that solves the nextactivity prediction problem with higher accuracy than <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b2">[3]</ref>, and that can be generalized to other prediction problems.</p><p>Pravilovic et al. <ref type="bibr" target="#b25">[26]</ref> propose an approach that predicts both the next activity and its attributes (e.g. the involved resource). In this paper we use LSTMs to tackle a similar problem: predicting the next activity and its timestamp.</p><p>Lakshmanan et al. <ref type="bibr" target="#b15">[16]</ref> use Markov chains to estimate the probability of future execution of a given task in a running case. Meanwhile, Van der Spoel et al <ref type="bibr" target="#b28">[29]</ref> address the more ambitious problem of predicting the entire continuation of a case using a shortest path algorithm over a causality graph. Polato et al. <ref type="bibr" target="#b24">[25]</ref> refine this approach by mining an annotated transition system from an event log and annotating its edges with transition probabilities. In this paper, we take this latter approach as a baseline and show how LSTMs can improve over it while providing higher generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section we introduce concepts used in later sections of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Event logs, traces and sequences</head><p>For a given set A, A * denotes the set of all sequences over A and σ = a 1 , a 2 , . . . , a n a sequence of length n;</p><p>is the empty sequence and σ 1 · σ 2 is the concatenation of sequences σ 1 and σ 2 . hd k (σ) = a 1 , a 2 , . . . , a k is the prefix of length k (0 &lt; k &lt; n) of sequence σ and tl k (σ) = a k+1 , . . . , a n is its suffix. For example, for a sequence σ 1 = a, b, c, d, e , hd 2 (σ 1 ) = a, b and tl 2 (σ 1 ) = c, d, e .</p><p>Let E be the event universe, i.e., the set of all possible event identifiers, and T the time domain. We assume that events are characterized by various properties, e.g., an event has a timestamp, corresponds to an activity, is performed by a particular resource, etc. We do not impose a specific set of properties, however, given the focus of this paper we assume that two of these properties are the timestamp and the activity of an event, i.e., there is a function π T ∈ E → T that assigns timestamps to events, and a function π A ∈ E → A that assigns to each event an activity from a finite set of process activities A.</p><p>An event log is a set of events, each linked to one trace and globally unique, i.e., the same event cannot occur twice in a log. A trace in a log represents the execution of one case.</p><p>Definition 1 (Trace, Event Log). A trace is a finite non-empty sequence of events σ ∈ E * such that each event appears only once and time is non-decreasing, i.e., for 1 ≤ i &lt; j ≤ |σ| : σ(i) = σ(j) and π T (σ(i)) ≤ π T (σ(j)). C is the set of all possible traces. An event log is a set of traces L ⊆ C such that each event appears at most once in the entire log.</p><p>Given a trace and a property, we often need to compute a sequence consisting of the value of this property for each event in the trace. To this end, we lift the function f p that maps an event to the value of its property p, in such a way that we can apply it to sequences of events (traces).</p><p>Definition 2 (Applying Functions to Sequences). A function f ∈ X → Y can be lifted to sequences over X using the following recursive definition: (1) f ( ) = ; (2) for any σ ∈ X * and x ∈ X:</p><formula xml:id="formula_0">f (σ · x ) = f (σ) · f (x) .</formula><p>Finally, π A (σ) transforms a trace σ to a sequence of its activities. For example, for trace σ = e 1 , e 2 , with π A (e 1 ) = a and π A (e 2 ) = b, π A (σ) = a, b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Networks &amp; Recurrent Neural Networks</head><p>A neural network consists of one layer of inputs units, one layer of outputs units, and multiple layers in-between which are referred to as hidden units. The outputs of the input units form the inputs of the units of the first hidden layer (i.e., the first layer of hidden units), and the outputs of the units of each hidden layer form the input for each subsequent hidden layer. The outputs of the last hidden layer form the input for the output layer. The output of each unit is a function over the weighted sum of its inputs. The weights of this weighted sum performed in each unit are learned through gradient-based optimization from training data that consists of example inputs and desired outputs for those example inputs. Recurrent Neural Networks (RNNs) are a special type of neural networks where the connections between neurons form a directed cycle.</p><p>RNNs can be unfolded, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Each step in the unfolding is referred to as a time step, where x t is the input at time step t. RNNs can take an arbitrary length sequence as input, by providing the RNN a feature representation of one element of the sequence at each time step. s t is the hidden state at time step t and contains information extracted from all time steps up to t. The hidden state s is updated with information of the new input x t after each time step:</p><formula xml:id="formula_1">s t = f (U x t + W s t−1 )</formula><p>, where U and W are vectors of weights over the new inputs and the hidden state respectively. Function f , known as the activation function, is usually either the hyperbolic tangent or the logistic function, often referred to as the sigmoid function: sigmoid (x) = 1 1+exp(−x) . In neural network literature the sigmoid function is often represented with the letter σ, but we will fully write sigmoid to avoid confusion with traces. o t is the output at step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Long Short-Term Memory for Sequence Modeling</head><p>A Long Short-Term Memory model (LSTM) <ref type="bibr" target="#b13">[14]</ref> is a special Recurrent Neural Network architecture that has powerful modeling capabilities for long-term dependencies. The main distinction between a regular RNN and a LSTM is that the latter has a more complex memory cell C t replacing s t . Where the value of state s t in a RNN is the result of a function over the weighted average over s t−1 and x t , the LSTM state C t is accessed, written, and cleared through controlling gates, respectively o t , i t , and f t . Information on a new input will be accumulated to the memory cell if i t is activated. Additionally, the past memory cell status C t−1 can be "forgotten" if f t is activated. The information of C t will be propagated to the output h t based on the activation of output gate o t . Combined, the LSTM model can be described with the following formulas:</p><formula xml:id="formula_2">f t = sigmoid (W f · [h t−1 , x t ] + b f ) i t = sigmoid (W i · [h t−1 , x t ] + b i ) C t = tanh(W c · [h t−1 , x t ] + b C ) C t = f t * C t−1 + i i * C t o t = sigmoid (W o [h t−1 , x t ] + b o ) h t = o t * tanh(C t )</formula><p>In these formulas all W variables are weights and b variables are biases and both are learned during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Next Activity and Timestamp Prediction</head><p>In this section we present and evaluate multiple architectures for next event and timestamp prediction using LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approach</head><p>We start by predicting the next activity in a case and its timestamp, by learning an activity prediction function f 1 a and a time prediction function f 1 t . We aim at functions f 1 a and f 1 t such that f 1 a (hd k (σ)) = hd 1 (tl k (π A (σ))) and f 1 t (hd k (σ)) = hd 1 (tl k (π T (σ))) for any prefix length k. We transform each event e ∈ hd k (σ) into a feature vector and use these vectors as LSTM inputs x 1 , . . . , x k . We build the feature vector as follows. We start with |A| features that represent the type of activity of event e in a so called one-hot encoding. We take an arbitrary but consistent ordering over the set of activities A, and use index ∈ A → {1, . . . , |A|} to indicate the position of an activity in it. The one-hot encoding assigns the value 1 to feature number index (π A (e)) and a value of 0 to the other features. We add three time-based features to the one-hot encoding feature vector. The first time-based feature of event e = σ(i) is the time between the previous event in the trace and the current event, i.e., fv t1 (e) = 0 if i = 1, π T (e) − π T (σ(i − 1)) otherwise. .</p><p>This feature allows the LSTM to learn dependencies between the time differences at different points (indexes) in the process. Many activities can only be performed during office hours, therefore we add a time feature fv t2 that contains the time within the day (since midnight) and fv t3 that contains the time within the week (since midnight on Sunday). fv t2 and fv t3 are added to learn the LSTM such that if the last event observed occurred at the end of the working day or at the end of the working week, the time until the next event is expected to be longer. At learning time, we set the target output o k a of time step k to the one-hot encoding of the activity of the event one time step later. However, it can be the case that the case ends at time k, in which case there is no new event to predict. Therefore we add an extra element to the output one-hot-encoding vector, which has value 1 when the case ends after k. We set a second target output o k t equal to the fv t1 feature of the next time step, i.e. the target is the time difference between the next and the current event. However, knowing the timestamp of the current event, we can calculate the timestamp of the following event. We optimize the weights of the neural network with the Adam learning algorithm [15] such that the cross entropy between the ground truth one-hot encoding of the next event and the predicted one-hot encoding of the next event as well as the mean absolute error (MAE) between the ground truth time until the next event and the predicted time until the next event are minimized.</p><p>Modeling the next activity prediction function f 1 a and time prediction function f 1 t with LSTMs can be done using several architectures. Firstly, we can train two separate models, one for f 1 a and one for f 1 t , both using the same input features at each time step, as represented in <ref type="figure" target="#fig_1">Figure 2 (a)</ref>. Secondly, f 1 a and f 1 t can be learned jointly in a single LSTM model that generates two outputs, in a multi-task learning setting <ref type="bibr" target="#b3">[4]</ref>  <ref type="figure" target="#fig_1">(Figure 2 (b)</ref>). The usage of LSTMs in a multi-task learning setting has shown to improve performance on all individual tasks when jointly learning multiple natural language processing tasks, including part-of-speech tagging, named entity recognition, and sentence classification <ref type="bibr" target="#b5">[6]</ref>. A hybrid option between the architecture of Figures 2 (a) and (b) is an architecture of a number of shared LSTM layers for both tasks, followed by a number of layers that specialize in either prediction of the next activity or prediction of the time until the next event, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c).</p><p>It should be noted that activity prediction function f 1 a outputs the probability distribution of various possible continuations of the partial trace. For evaluation purposes, we will only use the most likely continuation.</p><p>We implemented the technique as a set of Python scripts using the recurrent neural network library Keras <ref type="bibr" target="#b4">[5]</ref>. The experiments were performed on a single NVidia Tesla k80 GPU, on which the experiments took between 15 and 90 seconds per training iteration depending on the neural network architecture. The execution time to make a prediction is in the order of milliseconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>In this section we describe and motivate the metrics, datasets, and baseline methods used for evaluation of the predictions of the next activities and of the timestamps of the next events. To the best of our knowledge, there is no existing technique to predict both the next activity and its timestamp. Therefore, we utilize one baseline method for activity prediction and a different one for timestamp prediction.</p><p>Well-known error metrics for regression tasks are Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Time differences between events tend to be highly varying, with values at different orders of magnitude. We evaluate the predictions using MAE, as RMSE would be very sensitive to errors on outlier data points, where the time between two events in the log is very large.</p><p>The remaining cycle time prediction method proposed by van der Aalst et al. <ref type="bibr" target="#b0">[1]</ref> can be naturally adjusted to predict the time until the next event. To do so we build a transition system from the event log using either set, bag, or sequence abstraction, as in <ref type="bibr" target="#b0">[1]</ref>, but instead we annotate the transition system states with the average time until the next event. We will use this approach as a baseline to predict the timestamp of next event.</p><p>We evaluate the performance of predicting the next activity and its timestamp on two datasets. We use the chronologically ordered first 2/3 of the traces as training data, and evaluate the activity and time predictions on the remaining 1/3 of the traces. We evaluate the next activity and the timestamp prediction on all prefixes hd k (σ) of all trace σ in the set of test traces for 2 ≤ k &lt; |σ|. We do not make any predictions for the trace prefix of size one, since for those prefixes there is insufficient data available to base the prediction upon.</p><p>Helpdesk dataset This log contains events from a ticketing management process of the help desk of an Italian software company 1 . The process consists of 9 activities, and all cases start with the insertion of a new ticket into the ticketing management system. Each case ends when the issue is resolved and the ticket is closed. This log contains around 3,804 cases and 13,710 events.</p><p>BPI'12 subprocess W dataset This event log originates from the Business Process Intelligence Challenge (BPI'12) 2 and contains data from the application procedure for financial products at a large financial institution. This process consists of three subprocesses: one that tracks the state of the application, one that tracks the states of work items associated with the application, and a third one that tracks the state of the offer. In the context of predicting the coming events and their timestamps we are not interested in events that are performed automatically. Thus, we narrow down our evaluation to the work items subprocess, which contains events that are manually executed. Further, we filter the log to retain only events of type complete. Two existing techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> for the next activity prediction, described in Section 2, have been evaluated on this event log with identical preprocessing, enabling comparison. -  <ref type="table" target="#tab_1">Table 1</ref> shows the performance of various LSTM architectures on the helpdesk and the BPI'12 W subprocess logs in terms of MAE on predicted time, and accuracy of predicting the next event. The specific prefix sizes are chosen such that they represent short, medium, and long traces for each log. Thus, as the BPI'12 W log contains longer traces, the prefix sizes evaluated are higher for this log. In the table, all reports the average performance on all prefixes, not just the three prefix sizes reported in the three preceding columns. The number of shared layers represents the number of layers that contribute to both time and activity prediction. Rows where the numbers of shared layers are 0 correspond to the architecture of <ref type="figure" target="#fig_1">Figure 2 (a)</ref>, where the prediction of time and activities is performed with separate models. When the number of shared layers is equal to the number of layers, the neural network contains no specialized layers, corresponding to the architecture of <ref type="figure" target="#fig_1">Figure 2 (b)</ref>. <ref type="table" target="#tab_1">Table 1</ref> also shows the results of predicting the time until the end of the next event using the adjusted method from van der Aalst et al. <ref type="bibr" target="#b0">[1]</ref> for comparison. All LSTM architectures outperform the baseline approach on all prefixes as well as averaged over all prefixes on both datasets. Further, it can be observed that the performance gain between the best LSTM model and the best baseline model is much larger for the short prefix than for the long prefix. The best performance obtained on next activity prediction over all prefixes was a classification accuracy of 71% on the helpdesk log. On the BPI'12 W log the best accuracy is 76%, which is higher than the 71.9% accuracy on this log reported by Breuker et al. <ref type="bibr" target="#b2">[3]</ref> and the 62.3% accuracy reported by Evermann et al. <ref type="bibr" target="#b8">[9]</ref>. In fact, the results obtained with LSTM are consistently higher than both approaches. Even though Evermann et al. <ref type="bibr" target="#b8">[9]</ref> also rely on LSTM in their approach, there are several differences which are likely to cause the performance gap. First of all, <ref type="bibr" target="#b8">[9]</ref> uses a technique called embedding <ref type="bibr" target="#b22">[23]</ref> to create feature descriptions of events instead of the features described above. Embeddings automatically transform each activity into a "useful" large dimensional continuous feature vector. This approach has shown to work really well in the field of natural language processing, where the number of distinct words that can be predicted is very large, but for process mining event logs, where the number of distinct activities in an event log is often in the order of hundreds or much less, no useful feature vector can be learned automatically. Second, <ref type="bibr" target="#b8">[9]</ref> uses a two-layer architecture with 500 neurons per layer, and does not explore other variants. We found performance to decrease when increasing the number of neurons from 100 to 150, which makes it likely that the performance of a 500 neuron model will decrease due to overfitting. A third and last explanation for the performance difference is the use of multi-task learning, which as we showed, slightly improves prediction performance on the next activity. Even though the performance differences between our three LSTM architectures are small for both logs, we observe that most best performances (indicated in bold) of the LSTM model in terms of time prediction and next activity prediction are either obtained with the completely shared architecture of <ref type="figure" target="#fig_1">Figure  2</ref> (b) or with the hybrid architecture of <ref type="figure" target="#fig_1">Figure 2</ref> (c). We experimented with decreasing the number of neurons per layer to 75 and increasing it to 150 for architectures with one shared layer, but found that this results in decreasing performance in both tasks. It is likely that 75 neurons resulted in underfitting models, while 150 neurons resulted in overfitting models. We also experimented with traditional RNNs on one layer architectures, and found that they perform significantly worse than LSTMs on both time and activity prediction.</p><formula xml:id="formula_3">- - - - - - - - 0.623 Breuker et al. [3] - - - - - - - - - 0.719</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Suffix Prediction</head><p>Using functions f 1 a and f 1 t repeatedly allows us to make longer-term predictions that predict further ahead than a single time step. We use f ⊥ a and f ⊥ t to refer to activity and time until next event prediction functions that predict the whole continuation of a running case, and aim at those functions to be such that f ⊥ a (hd k (σ)) = tl k (π A (σ)) and f ⊥ t (hd k (σ)) = tl k (π T (σ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Approach</head><p>The suffix can be predicted by iteratively predicting the next activity and the time until the next event, until the next activity prediction function f 1 a predicts the end of case, which we represent with ⊥. More formally, we calculate the complete suffix of activities as follows:</p><formula xml:id="formula_4">f ⊥ a (σ) =      σ if f 1 a (σ) = ⊥ f ⊥ a (σ · e)</formula><p>, with e ∈ E, π A (e) = f 1 a (σ)∧ π T (e) = (f 1 t (σ) + π T (σ(|σ|))) otherwise and we calculate the suffix of times until the next events as follows:</p><formula xml:id="formula_5">f ⊥ t (σ) =      σ, if f 1 t (σ) = ⊥ f ⊥ t (σ · e)</formula><p>, with e ∈ E, π A (e) = f 1 a (σ)∧ π T (e) = (f 1 t (σ) + π T (σ(|σ|))) otherwise</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>For a given trace prefix hd k (σ) we evaluate the performance of f ⊥ a by calculating the distance between the predicted continuation f ⊥ a (hd k (σ)) and the actual continuation π A (tl k (σ)). Many sequence distance metrics exist, with Levenshtein distance being one of the most well-known ones. Levenshtein distance is defined as the minimum number of insertion, deletion, and substitution operations needed to transform one sequence into the other.</p><p>Levenshtein distance is not suitable when the business process includes parallel branches. Indeed, when a, b are the next predicted events, and b, a are the actual next events, we consider this to be only a minor error, since it is often not relevant in which order two parallel activities are executed. However, Levenshtein distance would assign a cost of 2 to this prediction, as transforming the predicted sequence into the ground truth sequence would require one deletion and one insertion operation. An evaluation measure that better reflects the prediction quality of is the Damerau-Levenstein distance <ref type="bibr" target="#b6">[7]</ref>, which adds a swapping operation to the set of operations used by Levenshtein distance. Damerau-Levenshtein distance would assign a cost of 1 to transform a, b into b, a . To obtain comparable results for traces of variable length, we normalize the Damerau-Levenshtein distance by the maximum of the length of the ground truth suffix and the length of the predicted suffix and subtract the normalized Damerau-Levenshtein distance from 1 to obtain Damerau-Levenshtein Similarity (DLS).</p><p>To the best of our knowledge, the most recent method to predict an arbitrary number of events ahead is the one by Polato et al. <ref type="bibr" target="#b24">[25]</ref>. The authors first extract a transition system from the log and then learn a machine learning model for each transition system state to predict the next activity. They evaluate on predictions of a fixed number of events ahead, while we are interested in the continuation of the case until its end. We redid the experiments with their ProM plugin to obtain the performance on the predicted full case continuation.</p><p>For the LSTM experiments, we use a two-layer architecture with one shared layer and 100 neurons per layer, which showed good performance in terms of next activity prediction and predicting the time until the next event in the previous experiment <ref type="table" target="#tab_1">(Table 1</ref>). In addition to the two previously introduced logs, we evaluate prediction of the suffix on an additional dataset, described below, which becomes feasible now that we have fixed the LSTM architecture.</p><p>Environmental permit dataset This is a log of an environmental permitting process at a Dutch municipality. <ref type="bibr" target="#b0">1</ref> Each case refers to one permit application. The log contains 937 cases and 38,944 events of 381 event types. Almost every case follows a unique path, making the suffix prediction more challenging. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results of suffix prediction for each log. As can be seen, the LSTM outperforms the baseline <ref type="bibr" target="#b24">[25]</ref> on all logs. Even though it improves over the baseline, the performance on the BPI'12 W log is low given that the log only contains 6 activities. After inspection we found that this log contains many sequences of two or more events in a row of the same activity, where occurrences of 8 or more identical events in a row are not uncommon. We found that LSTMs have problems dealing with this log characteristic, causing it to predict overly long sequences of the same activity, resulting in predicted suffixes that are much longer than the ground truth suffixes. Hence, we also evaluated suffix prediction on a modified version of the BPI'12 W log where we removed repeated occurrences of the same event, keeping only the first occurrence. However, we can only notice a mild improvement over the unmodified log. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Remaining Cycle Time Prediction</head><p>Time prediction function f ⊥ t predicts the timestamps of all events in a running case that are still to come. Since the last predicted timestamp in a prediction generated by f ⊥ t is the timestamp of the end of the case, it is easy to see that f ⊥ t can be used for predicting the remaining cycle time of the running case. For a given unfinished case σ,σ t = f ⊥ t (σ) contains the predicted timestamps of the next events, andσ t (|σ t |) contains the predicted end time of σ, therefore the estimated remaining cycle time can be obtained throughσ t (|σ t |) − π(σ(|σ|)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We use the same architecture as for the suffix prediction experiments. We predict and evaluate the remaining time after each passed event, starting from prefix size 2. We use the remaining cycle time prediction methods of van der Aalst et al. <ref type="bibr" target="#b0">[1]</ref> and van Dongen et al. <ref type="bibr" target="#b7">[8]</ref> as baseline methods. <ref type="figure" target="#fig_2">Figure 3</ref> shows the mean absolute error for each prefix size, for the four logs (Helpdesk, BPI'12 W, BPI'12 W with no duplicates and Environmental Permit). It can be seen that LSTM consistently outperforms the baselines for the Helpdesk log. An exception is the BPI'12 W log, where LSTM performs worse than the baselines on short prefixes. This is caused by the problem that LSTMs have in predicting the next event when the log has many repeated events, as described in Section 5. This problem causes the LSTM to predict suffixes that are too long compared to the ground truth, and, thereby, also overestimating the remaining cycle time. We see that the LSTM does outperform the baseline on the modified version of the BPI'12 W log where we only kept the first occurrence of each repeated event in a sequence. Note that we do not remove the last event of the case, even if it is a repeated event, as that would change the ground truth remaining cycle time for the prefix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion &amp; Future Work</head><p>The foremost contribution of this paper is a technique to predict the next activity of a running case and its timestamp using LSTM neural networks. We showed that this technique outperforms existing baselines on real-life data sets. Additionally, we found that predicting the next activity and its timestamp via a single model (multi-task learning) yields a higher accuracy than predicting them using separate models. We then showed that this basic technique can be generalized to address two other predictive process monitoring problems: predicting the entire continuation of a running case and predicting the remaining cycle time. We empirically showed that the generalized LSTM-based technique outperforms tailor-made approaches to these problems. We also identified a limitation of LSTM models when dealing with traces with multiple occurrences of the same activity, in which case the model predicts overly long sequences of the same event. Addressing this latter limitation is a direction for future work. The proposed technique can be extended to other prediction tasks, such as prediction of aggregate performance indicators and case outcomes. The latter task can be approached as a classification problem, wherein each neuron of the output layer predicts the probability of the corresponding outcome. Another avenue for future work is to extend feature vectors with additional case and event attributes (e.g. resources). Finally, we plan to extend the multi-task learning approach to predict other attributes of the next activity besides its timestamp.</p><p>Reproducibility. The source code and supplementary material required to reproduce the experiments reported in this paper can be found at http:// verenich.github.io/ProcessSequencePrediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A simple recurrent neural network (taken from<ref type="bibr" target="#b16">[17]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Neural Network architectures with single-task layers (a), with shared multitasks layer (b), and with n + m layers of which n are shared (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>MAE values using prefixes of different lengths for helpdesk (a), BPI'12 W (b), BPI'12 W (no duplicates) (c) and environmental permit (d) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experimental results for the Helpdesk and BPI'12 W logs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Suffix prediction results in terms of Damerau-Levenshtein Similarity.</figDesc><table><row><cell>Method</cell><cell cols="4">Helpdesk BPI'12 W BPI'12 W (no duplicates) Environmental permit</cell></row><row><cell cols="2">Polato [25] 0.2516</cell><cell>0.0458</cell><cell>0.0336</cell><cell>0.0260</cell></row><row><cell>LSTM</cell><cell>0.7669</cell><cell>0.3533</cell><cell>0.3937</cell><cell>0.1522</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">doi:10.17632/39bp3vv62t.1 2 doi:10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">doi:10.4121/uuid:26aba40d-8b2d-435b-b5af-6d4bfbd7a270</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research is funded by the Australian Research Council (grant DP150103356), the Estonian Research Council (grant IUT20-55) and the RISE BPM project (H2020 Marie Curie Program, grant 645751).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time prediction based on process mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M P</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Schonenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="475" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing and implementing a framework for event-based predictive modelling of business processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Breuker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Delfmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedigns of the 6th International Workshop on Enterprise Modelling and Information Systems Architectures</title>
		<meeting>eedigns of the 6th International Workshop on Enterprise Modelling and Information Systems Architectures</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comprehensible predictive models for business processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Breuker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Delfmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1009" to="1034" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A technique for computer detection and correction of spelling errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cycle time prediction: when will this case finally be finished? In: CoopIS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Van Dongen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Crooy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M P</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="319" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep learning approach for predicting process behaviour at runtime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Rehse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fettke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Runtime Analysis of Process-Aware Information Systems</title>
		<meeting>the 1st International Workshop on Runtime Analysis of Process-Aware Information Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discovering context-aware models for predicting business process performances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Folino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guarascio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pontieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoopIS. pp</title>
		<imprint>
			<biblScope unit="page" from="287" to="304" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predictive business process monitoring framework with hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Francescomarino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ghidini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Maggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAiSE</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Clustering-based predictive process monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Francescomarino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Maggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Teinemaa</surname></persName>
		</author>
		<idno>abs/1506.01428</idno>
		<ptr target="http://arxiv.org/abs/1506.01428" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to appear in Transactions on Services Computing</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference for Learning Representations</title>
		<meeting>the 3rd International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A markov prediction model for data-driven semi-structured business processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Doganata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unuvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khalaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="126" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A general process mining framework for correlating, predicting and clustering dynamic behavior based on event logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Leoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M P</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="235" to="257" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Complex symbolic sequence encodings for predictive monitoring of business processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leontjeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Conforti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Di Francescomarino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Maggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predictive monitoring of business processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Maggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Di Francescomarino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ghidini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAiSE</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predictive monitoring of heterogeneous service-oriented business networks: The transport and logistics case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Annual SRII Global Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparing and combining predictive business process monitoring techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmieders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dustdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="276" to="290" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Predicting deadline transgressions using event logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M P</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fidge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H M</forename><surname>Ter Hofstede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Wynn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Time and activity sequence prediction of business process instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Polato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burattin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Leoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07566</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Process mining to forecast the future of running cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pravilovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Appice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malerba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on New Frontiers in Mining Complex Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Prediction of remaining service execution time using stochastic Petri nets with arbitrary firing delays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogge-Solti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weske</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="389" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Queue mining -predicting delays in service processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senderovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandelbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAiSE. pp</title>
		<imprint>
			<biblScope unit="page" from="42" to="57" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Process prediction in noisy data sets: a case study in a dutch hospital</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Spoel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Keulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amrit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Data-Driven Process Discovery and Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="60" to="83" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
