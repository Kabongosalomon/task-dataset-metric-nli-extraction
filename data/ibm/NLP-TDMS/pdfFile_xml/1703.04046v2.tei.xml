<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THIS ARTICLE HAS BEEN PUBLISHED IN IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING. DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw Single-Channel EEG</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akara</forename><surname>Supratak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yike</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">THIS ARTICLE HAS BEEN PUBLISHED IN IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING. DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw Single-Channel EEG</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TNSRE.2017.2721116</idno>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Sleep Stage Scoring</term>
					<term>Deep Learning</term>
					<term>Single- channel EEG</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The present study proposes a deep learning model, named DeepSleepNet, for automatic sleep stage scoring based on raw single-channel EEG. Most of the existing methods rely on hand-engineered features which require prior knowledge of sleep analysis. Only a few of them encode the temporal information such as transition rules, which is important for identifying the next sleep stages, into the extracted features. In the proposed model, we utilize Convolutional Neural Networks to extract timeinvariant features, and bidirectional-Long Short-Term Memory to learn transition rules among sleep stages automatically from EEG epochs. We implement a two-step training algorithm to train our model efficiently. We evaluated our model using different single-channel EEGs (F4-EOG(Left), Fpz-Cz and Pz-Oz) from two public sleep datasets, that have different properties (e.g., sampling rate) and scoring standards (AASM and R&amp;K). The results showed that our model achieved similar overall accuracy and macro F1-score (MASS: 86.2%-81.7, Sleep-EDF: 82.0%-76.9) compared to the state-of-the-art methods (MASS: 85.9%-80.5, Sleep-EDF: 78.9%-73.7) on both datasets. This demonstrated that, without changing the model architecture and the training algorithm, our model could automatically learn features for sleep stage scoring from different raw single-channel EEGs from different datasets without utilizing any hand-engineered features. Our code is publicly available at https://github.com/akaraspt/ deepsleepnet. The final version of this paper can be found in</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S LEEP plays an important role in human health. Being able to monitor how well people sleep has a significant impact on medical research and practice <ref type="bibr" target="#b0">[1]</ref>.</p><p>Typically, sleep experts determine the quality of sleep using electrical activity recorded from sensors attached to different parts of the body. A set of signals from these sensors is called a polysomnogram (PSG), consisting of an electroencephalogram (EEG), an electrooculogram (EOG), an electromyogram (EMG), and an electrocardiogram (ECG). This PSG is segmented into 30-s epochs, which are then be classified into different sleep stages by the experts according to sleep manuals such as the Rechtschaffen and Kales (R&amp;K) <ref type="bibr" target="#b1">[2]</ref> and the American Academy of Sleep Medicine (AASM) <ref type="bibr" target="#b2">[3]</ref>. This process is called sleep stage scoring or sleep stage classification. This manual approach is, however, labor-intensive A. Supratak, H. <ref type="bibr">Dong</ref> and time-consuming due to the need for PSG recordings from several sensors attached to subjects over several nights. There have been a number of studies trying to develop a method to automate sleep stage scoring based on multiple signals such as EEG, EOG and EMG <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, or single-channel EEG <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. These methods firstly extract time-domain, frequency-domain and time-frequency-domain features from each recording epoch. In the case of multiple signals, the features from all signals in one epoch were concatenated into one feature vector. The features are then used to train classifiers to identify the sleep stage of the epoch. However, we believe that these methods may well not generalize to a larger population due to the heterogeneity among subjects and recording hardware. This is because these features were handengineered based on the characteristics of the available dataset.</p><p>Recently, deep learning, a branch of machine learning that utilizes multiple layers of linear and non-linear processing units to learn hierarchical representations or features from input data, has been employed in sleep stage scoring. For instance, the authors in <ref type="bibr" target="#b9">[10]</ref> have investigated a capability of Deep Belief Nets (DBNs) to learn probabilistic representations from preprocessed raw PSG. Convolutional Neural Networks (CNNs) have also been applied to learn multiple filters that are used to convolve with small portions of input data (i.e., convolution) to extract time-invariant features from raw Fpz-Cz EEG channel <ref type="bibr" target="#b10">[11]</ref>. However, the results from the literature showed that applying deep learning on hand-engineered features performed better than on raw signals <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. This might well be because the authors did not consider temporal information that sleep experts use when they determine the sleep stage of each epoch.</p><p>Only a few number of literature have explored Recurrent Neural Networks (RNNs) in sleep stage scoring. RNNs are capable of conditioning outputs on all previous inputs, as they maintain internal memory and utilizes feedback (or loop) connections to learn temporal information from sequences of inputs. The main advantage of RNNs is that they can be trained to learn long-term dependencies such as transition rules <ref type="bibr" target="#b2">[3]</ref> that sleep experts use to identify the next possible sleep stages from a sequence of PSG epochs. Elman RNNs have been applied on energy features from the Fpz-Cz EEG channel <ref type="bibr" target="#b11">[12]</ref>. In our previous work <ref type="bibr" target="#b12">[13]</ref>, we also applied Long Short-Term Memory (LSTM) on time-frequency-domain features from the F4-EOG and Fp2-EOG channels separately. Even though the reported results were promising, these methods still rely on hand-engineered features. This paper introduces DeepSleepNet, a model for automatic sleep stage scoring based on raw single-channel EEG, which is different from the existing works that develop algorithms to extract features from EEG. We aim to automate the process of hand-engineering features by utilizing the feature extraction capabilities of deep learning. The main contributions of this work are as follows:</p><p>• We develop a new model architecture that utilizes two CNNs with different filter sizes at the first layers and bidirectional-LSTMs. The CNNs can be trained to learn filters to extract time-invariant features from raw singlechannel EEG, while the bidirectional-LSTMs can be trained to encode temporal information such as sleep stage transition rules into the model. <ref type="bibr">•</ref> We implement a two-step training algorithm that can effectively train our model end-to-end via backpropagation, while preventing the model from suffering class imbalance problem (i.e., learning to classify only the majority of sleep stages) presented in a large sleep dataset. • We show that, without changing the model architecture and the training algorithm, our model could automatically learn features for sleep stage scoring from different raw single-channel EEGs from two datasets, that have different properties (e.g., sampling rate) and scoring standards (AASM and R&amp;K), without utilizing any handengineered features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEPSLEEPNET</head><p>The architecture of DeepSleepNet consists of two main parts as shown in <ref type="figure">Fig. 1</ref>. The first part is representation learning, which can be trained to learn filters to extract time-invariant features from each of raw single-channel EEG epochs. The second part is sequence residual learning, which can be trained to encode the temporal information such as stage transition rules <ref type="bibr" target="#b2">[3]</ref> from a sequence of EEG epochs in the extracted features. This architecture is designed for scoring 30-s EEG epochs following the standard of AASM and R&amp;K manuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Representation Learning</head><p>We employ two CNNs with small and large filter sizes at the first layers to extract time-invariant features from raw singlechannel 30-s EEG epochs. This architecture is inspired by the way signal processing experts control the trade-off between temporal and frequency precision in their feature extraction algorithms <ref type="bibr" target="#b13">[14]</ref>. The small filter is better to capture temporal information (i.e., when certain of EEG patterns appear), while the larger filter is better to capture frequency information (i.e., frequency components).</p><p>In our model, each CNN consists of four convolutional layers and two max-pooling layers. Each convolutional layer performs three operations sequentially: 1D-convolution with its filters, batch normalization <ref type="bibr" target="#b14">[15]</ref>, and applying the rectified linear unit (ReLU) activation (i.e., relu(x) = max(0, x)). Each pooling layer downsamples inputs using max operation. The specifications of the filter sizes, the number of filters, stride sizes and pooling sizes can be found in <ref type="figure">Fig. 1</ref>. Each conv block shows a filter size, the number of filters, and a stride size. Each max-pool block shows a pooling size and a stride size. We will explain dropout blocks later in Section III-C. Formally, suppose there are N 30-s EEG epochs {x 1 , ..., x N } from single-channel EEG. We use the two CNNs to extract the i-th feature a i from the i-th EEG epoch x i as follows:</p><formula xml:id="formula_0">h s i = CN N θs (x i ) (1) h l i = CN N θ l (x i ) (2) a i = h s i ||h l i (3)</formula><p>where CN N (x i ) is a function that transform a 30-s EEG epoch x i into a feature vector h i using a CNN, θ s and θ l are parameters of the CNNs with small and large filter sizes in the first layer respectively, and || is a concatenate operation that combines the outputs from two CNNs together. These concatenated or linked features {a 1 , ..., a N } are then forwarded to the sequence residual learning part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sequence Residual Learning</head><p>We apply the residual learning framework <ref type="bibr" target="#b15">[16]</ref> to design our sequence residual learning part. This part consists of two main components: bidirectional-LSTMs <ref type="bibr" target="#b16">[17]</ref> and a shortcut connection (see <ref type="figure">Fig. 1</ref>).</p><p>We employ two layers of bidirectional-LSTMs to learn temporal information such as stage transition rules <ref type="bibr" target="#b2">[3]</ref> which sleep experts use to determine the next possible sleep stages based on the previous stages. For instance, the AASM manual suggests that if a subject is in sleep stage N2, continue to score epochs with low amplitude and mixed frequency EEG activity as N2 even though K complexes or sleep spindles are not present. In this case, the bidirectional-LSTMs can learn to remember that it has seen the stage N2, and continue to score successive epochs as N2 if they still detect the low amplitude and mixed frequency EEG activity. Bidirectional-LSTMs extends the LSTM <ref type="bibr" target="#b17">[18]</ref> by having two LSTMs process forward and backward input sequences independently <ref type="bibr" target="#b16">[17]</ref>. In other words, the outputs from forward and backward LSTMs are not connected to each other. The model is therefore able to exploit information both from the past and the future. We also use peephole connections <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> in our LSTMs which allow their gating mechanism to inspect their current memory cell before the modification.</p><p>We use a shortcut connect to reformulate the computation of this part into a residual function. This enables our model to be able to add temporal information it learns from the previous input sequences into the feature extracted from the CNNs. We also use a fully-connected layer in the shortcut connection to transform the features from the CNNs into a vector that can be added to the output from the LSTMs. This layer performs matrix multiplication with its weight parameters, batch normalization, and applying the ReLU activation sequentially.</p><p>Formally, suppose there are N features from the CNNs {a 1 , ..., a N } arranged sequentially and t = 1...N denotes the time index of 30-s EEG epochs, our sequence residual learning is defined as follows:</p><formula xml:id="formula_1">h f t , c f t = LST M θ f (h f t−1 , c f t−1 , a t )<label>(4)</label></formula><formula xml:id="formula_2">h b t , c b t = LST M θ b (h b t+1 , c b t+1 , a t )<label>(5)</label></formula><formula xml:id="formula_3">o t = h f t ||h b t + F C θ (a t )<label>(6)</label></formula><p>where LST M represents a function that processes sequences of features a t using the two-layers LSTM parameterized by θ f and θ b for forward and backward directions; h and c are vectors of hidden and cell states of the LSTMs;</p><formula xml:id="formula_4">h f 0 , c f 0 , h b N +1</formula><p>and c b N +1 of forward and backward LSTMs are set to zero vectors; F C represents a function that transform features a t into a vector that can be added (element-wise) with the concatenated output vector h f t ||h b t from the bidirectional-LSTMs. The specifications of the hidden size of forward and backward LSTMs, and the fully-connected layers can be found in <ref type="figure">Fig. 1</ref>.</p><p>Each bidirect-lstm block shows hidden sizes of forward and backward LSTMs. Each fc block shows a hidden size.</p><p>It should be noted that the hidden and cell states <ref type="formula" target="#formula_1">(4)</ref> and <ref type="formula" target="#formula_2">(5)</ref> will be re-initialized to zeros at the beginning of each patient data during the training and testing. This is to make sure that the model uses only temporal information from the current subject data for both training and testing.</p><formula xml:id="formula_5">h f t , h b t , c f t and c b t in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Specification</head><p>For the representation learning part, the parameters of the CNN-1 and CNN-2 were selected with the aim to capture temporal and frequency information from the EEG according to the guideline provided by <ref type="bibr" target="#b13">[14]</ref>. For instance, in <ref type="figure">Fig. 1</ref>, the filter size of the conv1 layers of the CNN-1 was set to Fs/2 (i.e., half of the sampling rate (Fs)), and its stride size was set to Fs/16 to detect when certain of EEG patterns appear. On the other hand, the filter size of the conv1 layer of the CNN-2 was set to Fs×4 to better capture the frequency components from the EEG. Its stride size was also set to Fs/2, which is higher than the conv1 layer of the CNN-1, as it is not necessary to perform a fine-grained convolution to extract frequency components. The filter and stride sizes of the subsequent convolutional layers conv2 <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> were chosen to be small fix sizes. It is believed that the use of multiple convolutional layers with a small filter size instead of a single convolutional layer with a large filter can reduce the number of parameters and the computational cost, and can still achieve the similar level of model expressiveness <ref type="bibr" target="#b20">[21]</ref>.</p><p>For the sequence residual learning part, the parameters of the bidirect-lstm and fc layers were set to be smaller than the output of the representation learning part, which is 1024 in <ref type="figure">Fig. 1</ref>. This is to restrict our model to select and combine only the important features to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TWO-STEP TRAINING ALGORITHM</head><p>The two-step training algorithm (see Algorithm 1) is a technique we develop to effectively train our model end-to-end via backpropagation, while preventing the model from suffering class imbalance problem (i.e., learning to classify only the majority of sleep stages) present in a large sleep dataset. The algorithm first pre-trains the representation learning part of the model and then fine-tunes the whole model using two different learning rates. We use the cross-entropy loss to quantify the agreement between the predicted and the target sleep stages in both of these training steps. The combination of the softmax function (i.e., the last layer in <ref type="figure">Fig. 1</ref>) and the cross-entropy loss are used to train our model to output probabilities for mutually exclusive classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pre-training</head><p>The first step is to perform a supervised pre-training on the representation learning part of the model with a classbalance training set so that the model does not overfit to the majority of sleep stages. This can be seen in Algorithm 1, lines 1-8. Specifically, the two CNNs are extracted from the model and then stacked with a softmax layer, sof tmax. It is important to note that this sof tmax is different from the last layer in the model (see <ref type="figure">Fig. 1</ref>). This stacked softmax layer is only used in this step to pre-train the two CNNs, in which its parameters are discarded at the end of the pre-training. We denote these two CNNs stacked with sof tmax as pre model. Then the pre model is trained with a class-balance training set using a mini-batch gradient-based optimizer called Adam <ref type="bibr" target="#b21">[22]</ref> with a learning rate, lr. At the end of the pre-training, the softmax layer is discarded. The class-balance training set is obtained from duplicating the minority sleep stages in the original training set such that all sleep stage have the same number of samples (i.e., oversampling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fine-tuning</head><p>The second step is to perform a supervised fine-tuning on the whole model with a sequential training set. This can be seen in Algorithm 1, lines 9-19. This step is to encode the stage transition rules into the model as well as to perform necessary adjustments on the pre-trained CNNs. Specifically, the parameters θ s and θ l of the two CNNs of init model are replaced with the ones from the pre model, resulting in model. Then the model is trained with the sequence training set using a mini-batch Adam optimizer with two different learning rates, lr 1 and lr 2 . As the CNNs part has already been pre-trained, we, therefore, use a lower learning rate lr 1 for the CNNs part and a higher learning rate lr 2 for the sequence residual learning part, and a softmax layer. We found that when we used the same learning rate to fine-tune the whole network, the pre-trained CNN parameters were excessively adjusted to the sequential data, which were not class-balanced. As a consequence, the model started to overfit to the majority of the sleep stages toward the end of the fine-tuning. Therefore, two different learning rates are used during fine-tuning. Also, we use a heuristic gradient clipping technique to prevent the exploding gradients, which is a well-known problem when training RNNs such as LSTMs <ref type="bibr" target="#b22">[23]</ref>. This technique rescales the gradients to smaller values using their global norm whenever they exceed a pre-defined threshold. The sequential training set is obtained by arranging the original training set sequentially according to time across all subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Regularization</head><p>We employed two regularization techniques to help prevent overfitting problems. The first technique was dropout <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> that randomly sets the input values to 0 (i.e., dropping units along with their connection) with the specified probability during training. Dropout layers with the probability of 0.5 were used throughout the model as shown in <ref type="figure">Fig. 1</ref>. It is important to note that these dropout layers were used for training only, Algorithm 1 Two-step Training Input: init model, data Output: model Initialization:</p><formula xml:id="formula_6">1: init CN N θs,θ l ← extract cnns(init model) 2: pre model ← stack(init CN N θs,θ l , sof tmax) 3: data over ← oversample(data)</formula><p>Pre-training Step: 4: for i = 1 to n pretrain epochs do <ref type="bibr">5:</ref> for each batch in shuf f le(data over ) do <ref type="bibr">6:</ref> pre model ← adam lr (pre model, batch) <ref type="bibr">7:</ref> end for 8: end for Fine-tuning Step: <ref type="bibr">9:</ref> pre CN N θs,θ l ← extract cnns(pre model) 10: model ← replace cnns(init model, pre CN N θs,θ l ) 11: for i = 1 to n f inetune epochs do <ref type="bibr">12:</ref> for each subject in data do <ref type="bibr">13:</ref> model ← reset lstm cell state(model) <ref type="bibr">14:</ref> subject data seq ← arrange sequence(subject) <ref type="bibr">15:</ref> for each batch in subject data seq do <ref type="bibr">16:</ref> model ← adam lr1,lr2 (model, batch) <ref type="bibr">17:</ref> end for <ref type="bibr">18:</ref> end for 19: end for 20: return model and were removed from the model during testing to provide deterministic outputs.</p><p>The second technique was L2 weight decay, which adds a penalty term into a loss function to prevent large values of the parameters in the model (i.e., exploding gradients). We only applied the weight decay on the first layers of the two CNNs because of the two main reasons. Firstly, it is pointed out in [23] that L2 weight decay can limit the model capabilities of learning long-term dependencies. Secondly, we found that, without weight decay, the filters of the first layers of the CNNs overfitted to noises or artifacts in EEG data. This weight decay helped the model learn smoother filters (i.e., containing less high-frequency elements) which resulted in slightly performance gains. The weight decay parameter that defines the degree of penalty, lambda, was set to 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>We evaluated our model using different EEG channels from two public datasets: Montreal Archive of Sleep Studies (MASS) <ref type="bibr" target="#b25">[26]</ref> and Sleep-EDF <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p><p>MASS. In MASS cohort 1, there were five subsets of recordings, SS1-SS5, which were organized according to their research and acquisition protocols. We used data from SS3, which contained PSG recordings from 62 healthy subjects (age 42.5±18.9). Each recording contained 20 scalp-EEG, 2 EOG (left and right), 3 EMG and 1 ECG channels. The EEG electrodes were positioned according to the international 10-20 system, and the EOG electrodes were positioned diagonally on the outer edges of the eyes. EEG and EOG recordings were pre-processed with a notch filter of 60 Hz, and band-pass filters of 0.30-100 Hz (EEG) and 0.10-100 Hz (EOG). All EEG and EOG recordings had the same sampling rate of 256 Hz. These recordings were manually classified into one of the five sleep stages (W, N1, N2, N3 and REM) by a sleep expert according to the AASM standard <ref type="bibr" target="#b2">[3]</ref>. There were also movement artifacts at the beginning and the end of each subject's recordings that were labeled as UNKNOWN. We evaluated our model using the F4-EOG (Left) channel, which was obtained via montage reformatting <ref type="bibr" target="#b28">[29]</ref> without any further pre-processing. Sleep-EDF. There were two sets of subjects from two studies: age effect in healthy subjects (SC) and Temazepam effects on sleep (ST). We used 20 subjects (age 28.7±2.9) from SC. Each PSG recording contained 2 scalp-EEG signals from Fpz-Cz and Pz-Cz channels, 1 EOG (horizontal), 1 EMG, and 1 oro-nasal respiration signal. All EEG and EOG had the same sampling rate of 100 Hz. These recordings were manually classified into one of the eight classes (W, N1, N2, N3, N4, REM, MOVEMENT, UNKNOWN) by sleep experts according to the R&amp;K standard <ref type="bibr" target="#b1">[2]</ref>. We evaluated our model using the Fpz-Cz and Pz-Cz channels without any further preprocessing. We also merged the N3 and N4 stages into a single stage N3 to use the same AASM standard as the MASS dataset. There were long periods of awake or stage W at the start and the end of each recording, in which a subject was not sleeping. We only included 30 minutes of such periods just before and after the sleep periods, as we were interested in sleep periods.</p><p>We excluded MOVEMENT and UNKNOWN (which were at the start or the end of the each recording) stages, as they did not belong to the five sleep stages <ref type="bibr" target="#b2">[3]</ref>. <ref type="table" target="#tab_2">Table I</ref> summarizes the number of 30-s epochs for each sleep stage from these two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Design</head><p>We evaluated our model using a k-fold cross-validation scheme, where k was set to 31 and 20 for the MASS and Sleep-EDF datasets respectively. Specifically, in each fold, we used recordings from N s −(N s /k) to train the model, and from the remaining N s /k subjects to test the trained model, where N s is the number of subjects in the dataset. This process was repeated k times so that all of the recordings were tested. Then we combined the predicted sleep stages from all folds and computed the performance metrics, which will be discussed in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Metrics</head><p>We evaluated the performance of our model using per-class precision (PR), per-class recall (RE), per-class F1-score (F1), macro-averaging F1-score (MF1), overall accuracy (ACC), and Cohen's Kappa coefficient (κ) <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The per-class metrics are computed by considering a single class as a positive class, and all other classes combined as a negative class. The MF1 and ACC are calculated as follows:</p><formula xml:id="formula_7">ACC = C c=1 TP c N (7) MF1 = C c=1 F1 c C<label>(8)</label></formula><p>where TP c is the true positives of class c, F1 c is per-class F1-score of class c, C is the number of sleep stages, and N is the total number of test epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Parameters</head><p>The representation learning part was pre-trained using the oversampled training set with the mini-batch size of 100. The Adam optimizer's parameters lr, beta1, and beta2 were set to 10 −4 , 0.9 and 0.999 respectively. Then the whole model was fine-tuned using the sequential training set. Specifically, we equally split the sequences of 30-s EEG epochs from each subject data into 10 sub-sequences (i.e., batch size was 10). Then we fed 25 epochs (i.e., sequence length was 25) from each sub-sequence yielding 250 epochs per one step training. The Adam optimizer's parameters were similar to the pretraining step except that the learning rate of each part of the model, lr1 and lr2, were set to 10 −6 and 10 −4 respectively. The threshold of the gradient clipping was set to 10. The numbers of epochs for the pre-training and the fine-tuning steps were set to 100 and 200 respectively. There was no early stopping as there was no validation set in our evaluation scheme.</p><p>For the batch normalization in conv and fc blocks, the constant of 10 −5 was added to the mini-batch variance for numerical stability. The mean and variance of the training set, which were used as fixed parameters during testing, were estimated by computing the moving average of with a decay rate of 0.999 from the sampling mean and variance of each mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation</head><p>We implemented our model using TensorLayer (https:// github.com/zsdonghao/tensorlayer), which is a deep learning library extended from Google Tensorflow <ref type="bibr" target="#b31">[32]</ref>. This library allows us to deploy numerical computation such as the training and validation tasks to multiple CPUs and GPUs. We ran the k-fold cross-validation using the eTRIKS Analytical Environment (eAE) (https://eae.doc.ic.ac.uk/), which provides a cluster of high-performance computing nodes. Each node was equipped with an NVIDIA GeForce GTX 980. The training time for each validation fold was approximately 3 hours on each node. The testing or prediction time for each batch of 25 EEG epochs (according to the sequence length specified during training in Section IV-D) was approximately 50 milliseconds on each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Initial Experiments</head><p>We initially conducted experiments with the first fold of the 31-fold cross-validation with the MASS dataset. This was to design the architecture and the parameters for DeepSleepNet. For model architecture, we tried several configurations such as increasing/decreasing convolutional layers, changing the number of filters and the stride sizes, and changing the number of hidden sizes in the bidirectional-LSTMs and the fullyconnected layer. The architecture in <ref type="figure">Fig. 1</ref> gave us the best performance. For regularization parameters, we tried several values for the weight decay parameters ranging from 10 −1 to 10 −5 . The value of 10 −3 gave us the best performance. For training parameters, we tried several values of learning rates ranging from 10 −3 to 10 −8 . We also experimented with the mini-batch size (from 50 to 200) during the pretraining, the batch size (from 5 to 40) and sequence length (from 5 to 40) during fine-tuning. Other parameters such as beta1, beta2 and the threshold of the gradient clipping were chosen from the default values reported in the literature. The training parameters mentioned in Section IV-D gave us the best performance. With these settings, the pre-training and fine-tuning steps started to converge after 100 and 200 epochs respectively. <ref type="table" target="#tab_2">Table II</ref> and III show confusion matrices obtained from the 31-fold and the 20-fold cross-validation on the F4-EOG (Left) and the Fpz-Cz channels from the MASS and Sleep-EDF datasets respectively. We did not include the confusion matrix obtained from the Pz-Oz channel from the Sleep-EDF dataset as the Fpz-Cz channel gave a better performance. Each row and column represent the number of 30-s EEG epochs of each sleep stage classified by the sleep expert and our model respectively. The numbers in bold indicate the number of epochs that were correctly classified by our model. The last three columns in each row indicate per-class performance metrics computed from the confusion matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Sleep Stage Scoring Performance</head><p>It can be seen that the poorest performance was noted for the stage N1, with the F1 less than 60, while the F1 for other stages were significantly better, with the range between 81.5 and 90.3. Most of the misclassified stages were between N2 and N3. It can also be seen that the confusion matrix is almost symmetric via the diagonal line (except the pair of N2-N3). This indicates that the misclassifications were less likely to be due to the imbalance-class problem. <ref type="figure" target="#fig_0">Fig. 2</ref> demonstrate examples of hypnograms that were manually scored by a sleep expert, and automatically scored by our DeepSleepNet for Subject-1 from the MASS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Comparison with State-of-the-Art Approaches</head><p>Table IV shows a comparison between our method and other sleep stage scoring methods across ACC, MF1, κ and F1. These methods include the ones that utilize handengineered features <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, CNNs only <ref type="bibr" target="#b10">[11]</ref> or LSTMs only (which is our previous work) <ref type="bibr" target="#b12">[13]</ref>. The other methods' metrics were computed using the confusion matrices reported in their papers. We classified the methods into two groups: non-independent and independent training and test sets. The non-independent ones were the methods that included parts of the test subjects' epochs in the training data, while the independent ones were the methods that excluded all epochs of the test subjects from the training data. We believe that the practical evaluation scheme should not include any epochs from the test subjects. Also, it has been shown that the non-independent scheme resulted in an improvement of the performance <ref type="bibr" target="#b6">[7]</ref>. Thus we did not compare the performance of our method with the non-independent group. The numbers in bold indicate the highest performance metrics of all methods in each dataset of each group. Among the methods in the independent group, it can be seen that our method achieved a similar performance compared to the state-of-the-art methods that used the same EEG channel and dataset. It should also be emphasized that our model achieved the similar performance without sacrificing the performance on the stage N1, which is the most difficult sleep stage to classify. This indicates that our method was not biased by favoring the majority of the sleep stages than the minority ones. According to the κ coefficient, it showed that the agreement between the sleep experts and our model were substantial (between 0.61 and 0.80) <ref type="bibr" target="#b8">[9]</ref>. It should also be noted that our model performed better when applied on the Fpz-Cz channel compared to the Pz-Oz, which is similar to <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Sequence Residual Learning</head><p>We performed additional experiments to verify the important of the sequence residual learning part with the MASS dataset. <ref type="table" target="#tab_5">Table V</ref> shows a confusion matrix obtained from 31-fold cross-validation on the F4-EOG (Left) channel using DeepSleepNet without the sequence residual learning part (i.e., using the pre model in Algorithm 1). It can be seen that the  F1 of all sleep stages, except the stage N3, were lower than the ones in <ref type="table" target="#tab_2">Table II</ref>. This was because of an increase in the misclassifications between the pairs of N1-N2, N2-N3 and N1-REM. This may well be due to the effects of oversampling the training set to have balanced-class samples. As a consequence the model tended to predict more of stages N1 and N3. These results indicated that the process to stack the pretrained representation learning part with the sequence residual learning part, and then fine-tune the both parts with sequential training set helped improve the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Model Analysis</head><p>To better understanding how our model classified a sequence of 30-s EEG epochs, we analyzed and compared: 1) the learned filters at the first convolutional layers of the two CNNs in the representation learning part; and 2) the memory cells inside the bidirectional-LSTMs in the sequence residual learning part. This analysis was carried out with the MASS dataset across 31 cross-validation folds.</p><p>Firstly, we analyzed how our model utilized the learned filter at the first convolutional layers of the two CNNs to classify different sleep stages. Specifically, we determined which filters were mostly active for each sleep stage by computing the average of the sum of the activations of all filters across samples of each sleep stage. Formally, suppose there were N 30-s EEG epochs from each validation fold {x 1 , ..., x N }. We fed these epochs to our model to obtain activations z from the first convolutional layer of each CNN: {z 1 , ..., z N }, where z i ∈ R p×q , and p and q are the activation output size and the number of filters of the first convolutional layer. The average of the sum of the activations of the filter k for the sleep stage c is computed as follows:</p><formula xml:id="formula_8">u c,k = Ny pred =c i=1 q j=1 z i,j,k N y pred =c<label>(9)</label></formula><p>where u c,k is the average of the sum of the activation of the filter k for sleep stage c, z i,j,k is the j-th index of the activation vector z i of the filter k, and N y pred =c is the number of EEG epochs that our model predicted as stage c. After we computed the u c,k of all filters for sleep stage c, we rescaled them into a range of 0 and 1. We denote this scaled k-dimension vector u c as filter activations for stage c. This process was repeated  <ref type="formula" target="#formula_8">(9)</ref>, where 1 (i.e., active) is white and 0 (i.e., inactive) is black. Each row corresponds to the 64-dimension vector (i.e., k is 64) for each sleep stage c. The first row is from stage W and the last row is from stage REM. Each image also has labels indicating which filters are mostly active for which sleep stages. for all sleep stages. Once we got the filter activations from all sleep stages, we stacked them together, and rearranged the order of the filters such that the filters that were mostly active for each sleep stage were grouped together. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates an example of the filter activations from the small (a) and large (b) filters obtained by feeding our model with data from 3 subjects. Each image has 5 rows and 64 columns, corresponding to 5 sleep stages and 64 filters respectively. Each pixel represents the value of u c,k from (9) scaled into a range of 0 and 1, where 1 (i.e., active) is white and 0 (i.e., inactive) is black. Each row corresponds to the 64-dimension vector (i.e., k is 64) for each sleep stage c. The first row is from stage W and the last row is from stage REM. Each image also has labels indicating which filters are mostly active for which sleep stages. We found that there were two types of filters: ones that were mostly active for each sleep stage, and the other ones that were mostly active for multiple sleep stages. For instance, some of the small and large filters were mostly active for both stage N2 and N3. After we had analyzed all of the filter activations from different cross-validation folds, we found that the number of active filters for different sleep stages varied across subjects, and most of the small filters were mostly active for stage N2 and N3. We also found that, for a few subjects, no small filter was active for stage N1. This might well be because there were only a few stage N1 in the dataset. Secondly, we analyzed how our model utilized the bidirectional-LSTMs to learn the temporal information from a sequence of EEG epochs. Specifically, we investigated how the bidirectional-LSTMs managed their memory cells (i.e., c in (4) and (5)) using the visualization technique from <ref type="bibr" target="#b32">[33]</ref>. We found several memory cells of the forward LSTMs that were interpretable. For instance, several cells were keeping track of the wakefulness or the sleep onset, which reset their values to positive numbers (i.e., active) when a subject was in the stage W or N1 respectively. The cell values then decreased to negative values (i.e., becoming inactive) during stages N2, N3 and REM (or R in short). <ref type="figure">Fig. 4</ref> illustrates the changes of this cell value according to a sequence of sleep stages predicted by our model. The sleep stages are arranged according through time from left-to-right, and top-to-bottom. Each sleep stage color corresponds to tanh(c), where +1 (i.e., active) is blue and -1 is red (i.e., inactive). There were also other interpretable cells such as the ones that started with a high value at the beginning of each subject data and then slowly decreased with each sleep stage until the end of the subject data, or the ones that turned on when they found a continuous sequence of stages N3 and REM. The existence of these cells showed that the LSTMs inside the sequence residual learning part learned to keep track of the current status of each subject, which is important to correctly identify the next sleep stages according to the stage transition rules <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>We propose the DeepSleepNet model that utilizes CNNs and bidirectional-LSTMs to automatically learn features for sleep stage scoring from raw single-channel EEGs without using any hand-engineered features. The results showed that, without changing the model architecture and the training algorithm, the model could be applied on different EEG channels (F4-EOG (Left), Fpz-Cz and Pz-Oz). It achieved similar overall accuracy and macro F1-score compared to the state-of-the-art hand-engineering methods on both the MASS <ref type="figure">Fig. 4</ref>. An example of the LSTM cell that is active at the beginning of wakefulness (i.e., stage W) or the sleep onset (i.e., stage N1). The sequences of sleep stages are the predictions from DeepSleepNet on one subject data, arranged through time from left-to-right and top-to-bottom. The background color of each stage corresponds to tanh(c), where +1 is blue and -1 is red. and Sleep-EDF datasets, which have different properties such as sampling rate and scoring standards (AASM and R&amp;K). The results also showed that the temporal information learned from the sequence residual learning part helped improve the classification performance. These demonstrated that our model could automatically learn features for sleep stage scoring from different raw single-channel EEGs.</p><p>There are two main reasons that we evaluated our model with the F4-EOG (Left) channel from the MASS dataset, which is different from most of the existing methods reported in the literature that rely on the electrodes at the central lobe such as Cz, C4 and C3. The first reason is to compare the scoring performance with our previous hand-engineering approach. The second reason is that it is much easier and more comfortable to collect data either at sleep clinics or from home environment compared to the existing methods. This is because both of the electrodes do not have problems of reading the electrical activity from the hairy scalp. Even though the F4-EOG (Left) channel does not have information from the central and occipital lobes as recommended in the AASM manual <ref type="bibr" target="#b2">[3]</ref>, our results showed that our model was still able to achieve a similar performance compared to the state-of-the-art methods.</p><p>Based on the results of our simple model analysis, we found that our model learned several interesting features that were consistent with the AASM manual (which is the same manual the experts followed to score the MASS dataset). In the representation learning part, some of the learned filters at the first convolutional layers of the two CNNs were mostly active for stage N2-N3 and W-N1-REM (see <ref type="figure" target="#fig_1">Fig 3)</ref>. This implies that our model recognized some patterns that are similar among such stages. Our model might learn the filters to detect sleep spindles that can appear in both N2 and N3 stages, and to detect different features of the eye movements from the EOG (Left) that can be used to distinguish among W, N1 and REM stages. Also, in the sequence residual learning part, we found some interpretable memory cells in the bidirectional-LSTMs such as the cells that were keeping track of the wakefulness or the sleep onset, the cells that increased or decreased its value over time, and the cells that detected a train of stage N3 and REM. Our model utilized a combination of these cells to understand the current status of each subject, and to formulate transition rules. For instance, our model might remember that the subject was now awake or in the stage W. The next possible stage was very likely to be either stage W or N1. It should be emphasized that our model can learn these features from raw single-channel EEG without utilizing any hand-engineered features. Moreover, we observed that the features that our model learned were consistent across different folds. Therefore, we believe that DeepSleepNet is a better approach to implement automatic sleep stage scoring system compared to the hand-engineering ones that require prior knowledge to design feature extraction algorithms.</p><p>Even though our results are encouraging, our model is still subject to several limitations. Firstly, our model requires being trained with a sufficient amount of sleep dataset. This is due to the nature of the deep learning techniques that require a significant amount of training data to learn useful representations from the data. We performed additional experiments with the MASS dataset to estimate the number of epochs required to train our DeepSleepNet. We tried different numbers of folds (i.e., k) for the k-fold cross-validation from 2 to 31. We found that the scoring performance started to drop when k was less than 12, or when the number of training epochs was approximately less than 54000. Secondly, as our model learns features from the training data, it might not perform well when the trained model is applied to the data that have properties different from the training data such as data from different EEG channels. The model might have to be re-trained or fine-tuned before it can be applied to the data with different properties. Lastly, as our model utilizes bidirectional-LSTMs, the model has to wait until it has collected enough 30-s EEG epochs (depending on the sequence length of the EEG epochs used during the training process) before it can score these epochs. For instance, when the sequence length is set to 25, the model has to wait for 30×25 seconds (or 12.5 minutes) before it can identify sleep stages for these 25 EEG epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>We propose a deep learning model, named DeepSleepNet, for automatic sleep stage scoring based on raw single-channel EEG without utilizing any hand-engineered features. Our model utilizes CNNs to extract time-invariant features, and bidirectional-LSTMs to learn stage transition rules among sleep stages from EEG epochs. We also implement the twostep training algorithm that pre-trains our model with the oversampled dataset to alleviate class-imbalance problems, and fine-tunes the model with the sequences of EEG epochs to encode the temporal information into the model. Our results showed that, without changing the model architecture and the training algorithm, our model was able to automatically learn features for sleep stage scoring from different raw single-channel EEGs from two datasets that have different properties and scoring standards. Our model analysis results also demonstrated that our model learned several features that are consistent with the AASM manual. As our model automatically learn features from raw EEG, we believe that DeepSleepNet is a better approach to realize a remote sleep monitoring compared to the hand-engineering ones.</p><p>In the future, we plan to improve our DeepSleepNet to be able to apply on the single-channel EEG such as the F4-EOG (Left) and Fp2-EOG (Left) collected from wearable devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of the hypnogram manually scored by a sleep expert (top) and the hypnogram automatically scored by DeepSleepNet (bottom) for Subject-1 from the MASS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of the filter activations from the first convolutional layers of the two CNNs obtained by feeding our model with data from 3 subjects. The filter activations from the small filters are on the left (a), and the larger filters are on the right (b). Each image has 5 rows and 64 columns, corresponding to 5 sleep stages and 64 filters respectively. Each pixel represents the scaled value of u c,k from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, C. Wu and Y. Guo are with the Department of Computing, Imperial College London, London, SW7 2AZ, UK (e-mail: {as12212, hao.dong11, chao.wu, y.guo}@ic.ac.uk)</figDesc><table /><note>* Corresponding author DOI: 10.1109/TNSRE.2017.2721116 Copyright has been transferred to IEEE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>EEG epoch Sleep Stage</head><label></label><figDesc></figDesc><table><row><cell cols="2">[Fs/2] conv, 64, /[Fs/16] 8 conv, 128 8 conv, 128 8 conv, 128 0.5 dropout 8 max-pool, /8 30-s Representation Learning</cell><cell>[Fs×4] conv, 64, /[Fs/2] 6 conv, 128 6 conv, 128 6 conv, 128 0.5 dropout 4 max-pool, /4</cell></row><row><cell></cell><cell>4 max-pool, /4</cell><cell>2 max-pool, /2</cell></row><row><cell></cell><cell cols="2">0.5 dropout</cell></row><row><cell></cell><cell cols="2">512/512 bidirect-lstm</cell><cell>1024 fc</cell></row><row><cell>Learning</cell><cell cols="2">0.5 dropout</cell></row><row><cell>Residual</cell><cell cols="2">512/512 bidirect-lstm</cell></row><row><cell>Sequence</cell><cell cols="2">0.5 dropout +</cell></row><row><cell></cell><cell cols="2">0.5 dropout</cell></row><row><cell></cell><cell cols="2">5 softmax</cell></row><row><cell></cell><cell>Trainable Layer</cell><cell>Element-wise Operation</cell></row><row><cell></cell><cell>Non-trainable Layer</cell><cell>Concatenate</cell></row><row><cell></cell><cell>Non-trainable Layer (included only training)</cell><cell>Copy</cell></row><row><cell cols="3">Fig. 1. An overview architecture of DeepSleepNet consisting of two main</cell></row><row><cell cols="3">parts: representation learning and sequence residual learning. Each trainable</cell></row><row><cell cols="3">layer is a layer containing parameters to be optimized during a training</cell></row><row><cell cols="3">process. The specifications of the first convolutional layers of the two CNNs</cell></row><row><cell cols="3">depends on the sampling rate (Fs) of the EEG data (see Section II-C).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I NUMBER</head><label>I</label><figDesc>OF 30-S EPOCHS FOR EACH SLEEP STAGE FROM TWO DATASETS</figDesc><table><row><cell>Dataset</cell><cell>W</cell><cell>N1</cell><cell>N2</cell><cell>N3 (N4)</cell><cell>REM</cell><cell>Total</cell></row><row><cell>MASS</cell><cell cols="3">6227 4724 29534</cell><cell>7651</cell><cell cols="2">10464 58600</cell></row><row><cell>Sleep-EDF</cell><cell cols="3">7927 2804 17799</cell><cell>5703</cell><cell>7717</cell><cell>41950</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II CONFUSION</head><label>II</label><figDesc>MATRIX OBTAINED FROM 31-FOLD CROSS-VALIDATION ON F4-EOG (LEFT) CHANNEL FROM THE MASS DATASET</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Predicted</cell><cell></cell><cell></cell><cell cols="3">Per-class Metrics</cell></row><row><cell></cell><cell>W</cell><cell>N1</cell><cell>N2</cell><cell>N3</cell><cell>REM</cell><cell>PR</cell><cell>RE</cell><cell>F1</cell></row><row><cell>W</cell><cell>5433</cell><cell>572</cell><cell>107</cell><cell>13</cell><cell>102</cell><cell cols="3">87.3 87.2 87.3</cell></row><row><cell>N1</cell><cell>452</cell><cell>2802</cell><cell>827</cell><cell>4</cell><cell>639</cell><cell cols="3">60.4 59.3 59.8</cell></row><row><cell>N2</cell><cell>185</cell><cell>906</cell><cell>26786</cell><cell>1158</cell><cell>499</cell><cell cols="3">89.9 90.7 90.3</cell></row><row><cell>N3</cell><cell>18</cell><cell>4</cell><cell>1552</cell><cell>6077</cell><cell>0</cell><cell cols="3">83.8 79.4 81.5</cell></row><row><cell>REM</cell><cell>132</cell><cell>356</cell><cell>533</cell><cell>1</cell><cell>9442</cell><cell cols="3">88.4 90.2 89.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">CONFUSION MATRIX OBTAINED FROM 20-FOLD CROSS-VALIDATION ON</cell></row><row><cell></cell><cell cols="7">FPZ-CZ CHANNEL FROM THE SLEEP-EDF DATASET</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Predicted</cell><cell></cell><cell></cell><cell cols="3">Per-class Metrics</cell></row><row><cell></cell><cell>W</cell><cell>N1</cell><cell>N2</cell><cell>N3</cell><cell>REM</cell><cell>PR</cell><cell>RE</cell><cell>F1</cell></row><row><cell>W</cell><cell>6614</cell><cell>745</cell><cell>181</cell><cell>81</cell><cell>306</cell><cell cols="3">86.0 83.4 84.7</cell></row><row><cell>N1</cell><cell>295</cell><cell>1406</cell><cell>631</cell><cell>30</cell><cell>442</cell><cell cols="3">43.5 50.1 46.6</cell></row><row><cell>N2</cell><cell>391</cell><cell>618</cell><cell>14542</cell><cell>1473</cell><cell>775</cell><cell cols="3">90.5 81.7 85.9</cell></row><row><cell>N3</cell><cell>29</cell><cell>9</cell><cell>291</cell><cell>5370</cell><cell>4</cell><cell cols="3">77.1 94.2 84.8</cell></row><row><cell>REM</cell><cell>360</cell><cell>457</cell><cell>419</cell><cell>7</cell><cell>6474</cell><cell cols="3">80.9 83.9 82.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="12">COMPARISON BETWEEN DEEPSLEEPNET AND OTHER SLEEP STAGE SCORING METHODS THAT UTILIZES HAND-ENGINEERING FEATURES ACROSS</cell></row><row><cell cols="11">OVERALL ACCURACY (ACC), MACRO-F1 SCORE (MF1), COHEN'S KAPPA (κ), AND PER-CLASS F1-SCORE (F1)</cell><cell></cell></row><row><cell>Methods</cell><cell>Dataset</cell><cell>EEG Channel</cell><cell>Test Epochs</cell><cell cols="3">Overall Metrics ACC MF1 κ</cell><cell>W</cell><cell cols="3">Per-class F1-Score (F1) N1 N2 N3</cell><cell>REM</cell></row><row><cell cols="3">Non-independent Training and Test Sets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ref. [12]</cell><cell>Sleep-EDF</cell><cell>Fpz-Cz</cell><cell>960</cell><cell>90.3</cell><cell>76.5</cell><cell>-</cell><cell cols="3">77.3 46.5 94.9</cell><cell>72.2</cell><cell>91.8</cell></row><row><cell>Ref. [8]</cell><cell>Sleep-EDF</cell><cell>Pz-Oz</cell><cell>15136</cell><cell>91.3</cell><cell>77</cell><cell>0.86</cell><cell>97.8</cell><cell>30.4</cell><cell>89</cell><cell>85.5</cell><cell>82.5</cell></row><row><cell>Ref. [9]</cell><cell>Sleep-EDF</cell><cell>Pz-Oz</cell><cell>7596</cell><cell>90.8</cell><cell>80</cell><cell>0.85</cell><cell>96.9</cell><cell>49.1</cell><cell>89</cell><cell>84.2</cell><cell>81.2</cell></row><row><cell cols="3">Independent Training and Test Sets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ref. [7]</cell><cell>Sleep-EDF</cell><cell>Fpz-Cz</cell><cell>37022</cell><cell>78.9</cell><cell>73.7</cell><cell>-</cell><cell>71.6</cell><cell cols="3">47.0 84.6 84.0</cell><cell>81.4</cell></row><row><cell>Ref. [11]</cell><cell>Sleep-EDF</cell><cell>Fpz-Cz</cell><cell>37022</cell><cell>74.8</cell><cell>69.8</cell><cell>-</cell><cell cols="4">65.4 43.7 80.6 84.9</cell><cell>74.5</cell></row><row><cell>DeepSleepNet</cell><cell>Sleep-EDF</cell><cell>Fpz-Cz</cell><cell>41950</cell><cell>82.0</cell><cell>76.9</cell><cell>0.76</cell><cell cols="3">84.7 46.6 85.9</cell><cell>84.8</cell><cell>82.4</cell></row><row><cell>DeepSleepNet</cell><cell>Sleep-EDF</cell><cell>Pz-Oz</cell><cell>41950</cell><cell>79.8</cell><cell>73.1</cell><cell>0.72</cell><cell>88.1</cell><cell>37</cell><cell cols="2">82.7 77.3</cell><cell>80.3</cell></row><row><cell>Ref. [13]</cell><cell>MASS</cell><cell>F4-EOG (Left)</cell><cell>59066</cell><cell>85.9</cell><cell>80.5</cell><cell>-</cell><cell cols="4">84.6 56.3 90.7 84.8</cell><cell>86.1</cell></row><row><cell>DeepSleepNet</cell><cell>MASS</cell><cell>F4-EOG (Left)</cell><cell>58600</cell><cell>86.2</cell><cell>81.7</cell><cell>0.80</cell><cell cols="4">87.3 59.8 90.3 81.5</cell><cell>89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="9">CONFUSION MATRIX OBTAINED FROM 31-FOLD CROSS-VALIDATION ON</cell></row><row><cell cols="9">THE F4-EOG (LEFT) CHANNEL FROM THE MASS DATASET USING</cell></row><row><cell cols="8">DEEPSLEEPNET WITHOUT SEQUENCE RESIDUAL LEARNING</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Predicted</cell><cell></cell><cell></cell><cell cols="3">Per-class Metrics</cell></row><row><cell></cell><cell>W</cell><cell>N1</cell><cell>N2</cell><cell>N3</cell><cell>REM</cell><cell>PR</cell><cell>RE</cell><cell>F1</cell></row><row><cell>W</cell><cell>5215</cell><cell>709</cell><cell>94</cell><cell>19</cell><cell>190</cell><cell cols="3">84.5 83.7 84.1</cell></row><row><cell>N1</cell><cell>468</cell><cell>2582</cell><cell>747</cell><cell>11</cell><cell>916</cell><cell cols="3">40.8 54.7 46.8</cell></row><row><cell>N2</cell><cell>241</cell><cell cols="2">1846 24140</cell><cell>2435</cell><cell>872</cell><cell cols="3">93.4 81.7 87.2</cell></row><row><cell>N3</cell><cell>19</cell><cell>3</cell><cell>472</cell><cell>7156</cell><cell>1</cell><cell cols="3">74.3 93.5 82.8</cell></row><row><cell>REM</cell><cell>227</cell><cell>1181</cell><cell>383</cell><cell>5</cell><cell>8668</cell><cell cols="3">81.4 82.8 82.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Douglas McIlwraith and Axel Oehmichen from Imperial College London who reviewed the contents of the paper, and provided support for running the k-fold cross-validation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sleep and circadian rhythm disruption in psychiatric and neurodegenerative disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wulff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="589" to="599" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A manual of standardized terminology, techniques and scoring system for sleep stages of human subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electroencephalography and Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">644</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The AASM manual for the scoring of sleep and associated events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Iber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning machines and sleeping brains: Automatic sleep stage classification using decision-tree multi-class support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lajnef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page" from="94" to="105" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-based identification of sleep stages based on two forehead electroencephalogram channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">263</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient sleep stage recognition system based on EEG signal using k-means clustering based feature weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Güne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yosunkaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. with Applicat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7922" to="7928" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1587" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic sleep stages classification based on iterative filtering of electroencephalogram signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Pachori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Upadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applicat</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A decision support system for automated identification of sleep stages from single-channel EEG signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Syst</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Längkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sleep Stage Classification Using Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01683</idno>
		<title level="m">Automatic Sleep Stage Scoring with Single-Channel EEG Using Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic sleep stage recurrent neural classifier using energy features of EEG signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06421</idno>
		<title level="m">Mixed Neural Network Approach for Temporal Sleep Stage Classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Analyzing Neural Time Series Data: Theory and Practice</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cvpr</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN&apos;2000, Int. Joint Conf. on Neural Networks</title>
		<meeting>IJCNN&apos;2000, Int. Joint Conf. on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.1128</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On the difficulty of training Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout : A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recurrent Neural Network Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Montreal Archive of Sleep Studies: an open-access resource for instrument benchmarking and exploratory research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Sleep Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="page" from="591" to="596" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analysis of a sleep-dependent neuronal feedback loop: The slow-wave microcontinuity of the EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Manipulating the magic of digital EEG: Montage reformatting and filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Lagerlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. of Electroneurodiagnostic Tech</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="136" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Coefficient of Agreement for Nominal Scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A systematic analysis of performance measures for classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Process. and Manage</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and Understanding Recurrent Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
