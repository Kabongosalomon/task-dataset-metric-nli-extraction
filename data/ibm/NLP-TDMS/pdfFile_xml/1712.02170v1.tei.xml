<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Curve Text in the Wild: New Dataset and New Solution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
							<email>liu.yuliang@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Information Engineering South</orgName>
								<orgName type="institution">China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
							<email>*lianwen.jin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Information Engineering South</orgName>
								<orgName type="institution">China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaitao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Information Engineering South</orgName>
								<orgName type="institution">China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Information Engineering South</orgName>
								<orgName type="institution">China University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Curve Text in the Wild: New Dataset and New Solution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text detection has been made great progress in recent years. The detection manners are evolving from axisaligned rectangle to rotated rectangle and further to quadrangle. However, current datasets contain very little curve text, which can be widely observed in scene images such as signboard, product name and so on. To raise the concerns of reading curve text in the wild, in this paper, we construct a curve text dataset named CTW1500, which includes over 10k text annotations in 1,500 images (1000 for training and 500 for testing). Based on this dataset, we pioneering propose a polygon based curve text detector (CTD) which can directly detect curve text without empirical combination. Moreover, by seamlessly integrating the recurrent transverse and longitudinal offset connection (TLOC), the proposed method can be end-to-end trainable to learn the inherent connection among the position offsets. This allows the CTD to explore context information instead of predicting points independently, resulting in more smooth and accurate detection. We also propose two simple but effective post-processing methods named nonpolygon suppress (NPS) and polygonal non-maximum suppression (PNMS) to further improve the detection accuracy. Furthermore, the proposed approach in this paper is designed in an universal manner, which can also be trained with rectangular or quadrilateral bounding boxes without extra efforts. Experimental results on CTW-1500 demonstrate our method with only a light backbone can outperform state-of-the-art methods with a large margin. By evaluating only in the curve or non-curve subset, the CTD + TLOC can still achieve the best results. Code is available at https://github.com/Yuliang-Liu/Curve-Text-Detector.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text in the wild conveys valuable information, which can be used for real-time multi-lingual translation, behavior analysis, product identification, automotive assistance, etc. Recently, the emergence of many text datasets, which are  constructed on specific tasks and scene, have contributed to great progresses of text detection and recognition methods. Interestingly, it is observed that labels of text bounding boxes from emerging datasets also develop from rectangle to flexible quadrangle. For example, horizontal rectangular labels in ICDAR 2013 "Focus Scene Text" <ref type="bibr" target="#b16">[17]</ref> and SVT <ref type="bibr" target="#b33">[34]</ref>; rotated rectangular labels in MSRA-TD500 <ref type="bibr" target="#b34">[35]</ref> and USTB-SV1K <ref type="bibr" target="#b36">[37]</ref>; four points labels in ICDAR 2015 "Incidental Text" <ref type="bibr" target="#b15">[16]</ref>, RCTW-17 <ref type="bibr" target="#b29">[30]</ref> and recent MLT competition dataset <ref type="bibr" target="#b12">[13]</ref>. Similarly, the advancements of scene text detection methods also change from axis-aligned rectangle based to rotated rectangle based and to quadrangle based. It was indicated in <ref type="bibr" target="#b20">[21]</ref> that once the bounding box becomes tighter and flexible, it can improve the detecting confidence, reduce the risk of being suppressed by postprocessing and be beneficial to subsequent text recognition.</p><p>To recognize the scene text, it is a strong requirement that the text can be tightly and robustly localized in advance. However, current datasets have very little curve text, and it is defective to label such text with quadrangle let alone rectangle. For example, as showed in <ref type="figure" target="#fig_1">Figure 1</ref>, using curve bounding box has three remarkable advantages:</p><p>• Avoid needless overlap. Because the text may appear in many ways, the traditional four points localization may not handle such elusive peculiarity well. As shown in <ref type="figure" target="#fig_1">Figure 1</ref> (a), quadrilateral bounding box can not avoid a mass of tanglesome overlap while curve bounding box can.</p><p>• Less background noise. As shown in <ref type="figure" target="#fig_1">Figure 1 (b)</ref>, if text appear in the curve form, quadrilateral bounding box suffers from background noise.</p><p>• Avoid multiple text lines. The recent popular recognition methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref> all require single row text line in each bounding box. However, in some cases like <ref type="figure" target="#fig_1">Figure 1</ref> (c), quadrilateral bounding box can not avoid multiple text lines from disturbing with each others while curve bounding box can exquisitely solve this problem.</p><p>Actually, curve text are also very common in our realworld. For examples, text in most kinds of columnar objects (bottles, stone piles, etc.), spherical objects, plicated plane (clothes, streamer, etc.), coins, logos, signboard and so on. However, to our best knowledge, current methods can not directly detect the curve text. The linking methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12]</ref> can detect components of the text and then grouping them together to match the curve bounding box. But if there are many texts stack up together, in many cases like <ref type="figure" target="#fig_1">Figure 1</ref> (a), empirical connection rules are almost impossible to group the tiny components properly, and somehow these methods always bring more false positives than direct detection manners in practice. Therefore, in this paper, we collect text from various natural scenes, websites or image library like google open-image <ref type="bibr" target="#b17">[18]</ref>, and constructing a new dataset named CTW1500. This dataset contains 1500 images with over 10k text annotation, and each image contains at least one curve text. For evaluation and comparison, we split 1000 images as training set and 500 for testing. Based on our observation, for all kinds of the curve text regions, a 14 points polygon can be sufficient to localize them as shown in Figure 1 and <ref type="figure" target="#fig_2">Figure 2</ref>. By using the referenced equant lines, it doesn't require much manpower to label as introduced in Sec. 3.</p><p>Based on the proposed dataset, we propose a simple but effective polygon based curve text detector (CTD), which may be a pioneering method that can directly detect the curve text. Unlike traditional detecting methods, the CTD separates the branches for width/height offsets predictions, which can be run below 4GB video memory with the speed of 13 FPS. In addition, the network architecture can be seamlessly integrated with a ingenious method we proposed, namely transverse and longitudinal offset connection (TLOC), which uses RNN to learn the inherent connection between locating points, making the detection more accurate and smooth. The CTD is also designed as an universal method, which can be trained with rectangular and quadrilateral bounding boxes without extra manual labels. Two simple but effective post-processing methods named nonpolygon suppression (NPS) and polygonal non-maximum suppression (PNMS) are proposed to further intensify the generalization ability of CTD.</p><p>On the proposed dataset, the results demonstrate the CTD with a light reduced resnet-50 <ref type="bibr" target="#b8">[9]</ref> can effectively detect the curve text and outperform state-of-the-art methods with a large margin. By using TLOC, our method can remarkably improve the performance. Furthermore, we also evaluate our method on mere curve or non-curve test subset (by making the other text as not care regions), and the CTD+TLOC can still achieve the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the past dozen years, scene text detection methods have achieved great improvement. One of the main reasons of such unceasing progress is the evolution of the benchmark datasets -the data become harder, the amounts become larger and the labels become tighter. From 2003 on, rectangular labeled datasets such as ICDAR'03 <ref type="bibr" target="#b21">[22]</ref>, IC-DAR'11 <ref type="bibr" target="#b25">[26]</ref>, ICDAR'13 <ref type="bibr" target="#b16">[17]</ref> and COCO-Text <ref type="bibr" target="#b32">[33]</ref> have attracted a great number of research efforts. After 2010, the multi-oriented datasets with rotated rectangular labels (NEOCR <ref type="bibr" target="#b22">[23]</ref>, OSTD <ref type="bibr" target="#b35">[36]</ref>, MSRA-TD500 <ref type="bibr" target="#b34">[35]</ref> and USTB-SV1K <ref type="bibr" target="#b36">[37]</ref>) come out, which stimulate many influential multi-oriented detecting methods in the literatures. And in 2015, the first quadrilateral labeled dataset ICDAR 2015 "Incidental Scene Text" <ref type="bibr" target="#b15">[16]</ref> appears, which unprecedentedly attracted lots of attention according to its evaluating website <ref type="bibr" target="#b15">[16]</ref> and many recent progress. Since then, quite a few larger and more challenge quadrilateral labeled dataset in ICDAR 2017 competition like RCTW-17 <ref type="bibr" target="#b29">[30]</ref> (dataset for Chinese and English text), DOST <ref type="bibr" target="#b13">[14]</ref> (scene texts ob- served by video in the real environment) and MLT <ref type="bibr" target="#b12">[13]</ref> (dataset for multi-lingual text) seem to become the next mainstream datasets.</p><p>Interestingly, the development of detecting manners also show a similar evolution tendency with datasets. Although the rectangular methods still receive interests, they seem becoming less mainstream. Since 2011, almost every year there are methods with rotated rectangular bounding box proposed. And in 2017, plentiful quadrilateral based detection methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4]</ref> emerged. Basically, the quadrilateral based detection methods can also achieve best performance in rotated datasets or horizontal datasets (by evaluating the circumscribed rectangle), and they all show a fact that they can beat horizontal manners in multioriented datasets (by using the circumscribed rectangle to train and test) especially in terms of recall rate. This is mainly because the stronger supervision for quadrilateral labeled methods can avoid much background noise, unreasonable suppression and information loss. The viewpoint that stronger supervision helps detection can also be found on Mask-RCNN <ref type="bibr" target="#b7">[8]</ref> which improves detecting results by jointly training with a branch of segmentation, and Ren et at. <ref type="bibr" target="#b19">[20]</ref> also shows training with recognition can be conducive to text detection.</p><p>However, current text detection methods, even quadrangle based methods all show disappointing performance in curve texts, which are commonly appeared in our real world as introduced in section 1. The linking method like <ref type="bibr" target="#b26">[27]</ref> can not detect the strong bending text (the last image in second row in its <ref type="figure" target="#fig_3">Fig. 3</ref>) as well. One reason is that all current datasets contain very little curve text and many of the curve text are labeled with unsatisfactory rectangles. The other reason is that current four points based detection methods can only loosely detect the curve text, which may cause severely mutual interference like <ref type="figure" target="#fig_1">Figure 1</ref> (c). Therefore, in order to address the challenging problem of detecting curve text in the wild, we construct a new curve text based dataset named CTW1500, and then propose a novel method that can directly detect the curve text effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CTW1500 Dataset and Annotation</head><p>Data description. The CTW1500 dataset contains 1500 images, with 10,751 bounding boxes (3,530 are curve bounding boxes) and at least one curve text per image. The images are manually harvested from internet, image library like google Open-Image <ref type="bibr" target="#b17">[18]</ref> and our own data collected by phone cameras, which also contain lots of horizontal and multi-oriented text. The distribution of the images is various, containing indoor, outdoor, born digital, blurred, perspective distortion texts and so on. In addition, our dataset is multi-lingual with mainly Chinese and English text.</p><p>Annotation. The text is manually labeled by ourself with our labeling tool. For labeling text with horizontal or  quadrilateral shape, simply two or four clicks are required.</p><p>To surround the curve text, we create ten equidistant referenced lines to help label the extra 10 points (we practically find extra 10 points are sufficient to label all kinds of curve text as shown in <ref type="figure" target="#fig_2">Figure 2</ref>). The reason we use the equidistant lines is to ease the labeling effort, and reduce subjective interference. To evaluate the localization performance, we simply follow the PASCAL VOC protocol <ref type="bibr" target="#b6">[7]</ref>, which uses 0.5 IoU threshold to decide true or false positive. The only difference is we calculate the exact intersection-over-union (IoU) between the polygons instead of axis-aligned rectangles.</p><p>The labeling procedure is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Firstly, we click the four vertexes marked as 1, 2, 3, 4, and the referenced dashed line (blue) will be automatically created. Moving one of the mouse's referenced line (horizontal and vertical black dashed lines) to the appropriate position (intersection of two referenced lines) and then click to determine the next point, and so on for the remanent points. We roughly calculate the labeling time of three shapes of text in <ref type="table" target="#tab_0">Table 1</ref>, which shows labeling one curve text consumes approximately triple time than labeling with quadrangle. The CTW1500 dataset can be download at https://github.com/Yuliang-Liu/Curve-Text-Detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>This section presents details of our curve text detector (CTD). We will first illustrate the architecture of the CTD and how we make use of the polygonal labels. After that, we will describe how a recurrent neural network (RNN) component is seamlessly connected to the CTD and subsequently introducing the universality of this method. Finally, we will present our two simple but effective post-processing methods that can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head><p>The overall architecture of our CTD is shown in <ref type="figure" target="#fig_4">Figure 4</ref>, which can be divided into three parts: backbone, RPN and regression module. Backbone usually adopts the popular models pre-trained by ImageNet <ref type="bibr" target="#b4">[5]</ref> and then uses the corresponding model to finetune, such as VGG-16 <ref type="bibr" target="#b30">[31]</ref>, ResNet <ref type="bibr" target="#b8">[9]</ref> and so on. Region proposal network (RPN) and regression module are respectively connected to the backbone; while the former generates proposals for roughly recalling text and the latter finely adjusts the proposals to make it tighter.</p><p>In this paper, we use a reduced ResNet-50 (simply remove the last residual block) as our backbone, which requires less memory and can be faster. In the RPN stage, we use the default rectangular anchors to roughly recall the text but we set a very loose RPN-NMS threshold to avoid premature suppression. To detect the curve text with polygon, the CTD only need to modify the regression module by adding the curve locating points, which is inspired by DMPNet <ref type="bibr" target="#b20">[21]</ref> and East <ref type="bibr" target="#b37">[38]</ref> that adopting quadrilateral regressing branch separated with circumscribed rectangle regression. The rectangular branch can be easily learned by the network and let it converses fast, which can also roughly detect the text region in advanced and alleviate the following regression. Contrastively, the quadrilateral branch offers stronger supervision to guide the network being more accurate.</p><p>Similar to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref>, we also regress the relative positions for each points. Unlike <ref type="bibr" target="#b20">[21]</ref>, we use the minimum x and minimum y of the circumscribed rectangle as the datum point. Therefore, the relative length w i and h i (i ∈ 1, 2, ..., 14) of every point is greater than zero, which is somehow easier to train in practice. In addition, we separately predict the offsets w and h, which can not only reduce the parameters but more reasonable for sequential learning as introduced in the following subsection. The total number of the regressing items is 32; while 28 are the offset of the 14 points, and 4 are the x,y minimum and maximum of the circumscribed rectangle. The parameterizations of the 14 offsets (d wi and d hi ) are listed below:</p><formula xml:id="formula_0">d wi = p * w i −pw i w chr , d hi = p * h i −p h i h chr , (i ∈ (1, 2, ..., 14))<label>(1)</label></formula><p>Where, p * and p are ground truth and predicted offsets respectively. Besides, w chr and h chr are the width and height of the circumscribed rectangle. For boundary regression, we follow the same as Faster R-CNN <ref type="bibr" target="#b24">[25]</ref>. It is worth noticing that 28 values are enough to determine the position of 14 points, but in relative regressing mode, 32 values can be easier to retrieve the rest of 14 points and offer stronger supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text?</head><p>Score: 1.000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVEPooling (1x256)</head><p>Each bin is the predicted offset of . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLOC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Recurrent Transverse and Longitudinal Offset Connection (TLOC)</head><p>Using recurrent connection in text detection task was proved robust and effective by CTPN <ref type="bibr" target="#b31">[32]</ref>, which learns the latent sequence of tiny proposals and produces better results. However, CTPN is a linking based method, which requires empirical connection. Also, its connectionist proposal requires fixed images size for ensuring the fixed number of input time sequences of RNN. Unlike CTPN, our method can directly localize the curve region without exterior connection, and the number of the time sequences of RNN is not constrained by input image size. This is because the RNN is connected to the output of the positionsensitive RoI Pooling (PSROIPooling) <ref type="bibr" target="#b2">[3]</ref>, and the number of the output targets are fixed (14 width offsets and 14 height offsets). Basically, PSROIPooling is used to predict and vote the class probabilities and localization offsets, which evenly partitions each RoI into p × p bins to estimate position information. The dimension of the input convolutional layer should be (class + 1)p 2 , and thus the PSROIPooling can produce a p 2 score map for each category. For transverse and longitudinal offset prediction, we remove the background class localization score map and use a 7×7 bin, so the input convolutional dimension is 14×7×7 for height and width separately. Each value of (i; j)-th bin (0 &lt; i; j &lt; p − 1) is computed from the corresponding po-sition in the (i; j)-th score map by using a average pooling:</p><formula xml:id="formula_1">r c (i, j|Θ)| = (x,y)∈bin(i,j) s i,j,c (x + x min , y + y min |Θ) n ,<label>(2)</label></formula><p>where, r c (i, j|Θ) is the pooled value in the (i, j)-th bin for category c, s i,j,c represents a score map from the corresponding dimension. (x min , y min ) denotes the left-top coordinate of a RoI, n denotes the amount of pixels in the bin, and Θ stands for all network parameters. After the PSROIPooling procedure, CTD will get the scores or the estimated offsets of each RoI via globally pooling on the p 2 position-sensitive score maps: r c (Θ) = rc(i,j|Θ) p 2 , which produces a (C + 1)-dimensional vector. The voting class score is then computed by the softmax operation across all categories and output the final confidence:</p><formula xml:id="formula_2">s c (Θ) = e rc(Θ) C c =0 e r c (Θ) .<label>(3)</label></formula><p>The localization offsets would be fed into the localization loss function. During training phase, we choose the similar multi-task loss functions for score and offsets prediction as follow:</p><formula xml:id="formula_3">L(c, c * , b, b * , w, w * , h, h * ) = 1 N (λ × L cls (c, c * )+ L loc (b, b * )) + µ N p (L loc (h, h * ) + L loc (w, w * ))<label>(4)</label></formula><p>where N is the amount of both positive and negative proposals that match specific overlapping range and N p is the number of positive proposals because it is not necessary to refine the negative proposals. Besides, λ and µ are balance factors which weighs the importance among the classification and detection losses (L cls represents classification loss function; L loc is localization loss function which can be smooth-L1 loss or smooth-Ln <ref type="bibr" target="#b20">[21]</ref> loss). Practically, we set λ to 3 or even more to balance localization loss which has much more targets. <ref type="figure">Moreover, (c, b, w, h)</ref> represent the predicted class, estimated bounding-box, width and height offset respectively, and (c * , b * , w * , h * ) denote the corresponding ground-truth. To improve detection performance, we separate transverse and longitudinal branches to predict the offsets for localizing the text region. Intuitively, each point is restricted by the last and next points and the textual region. For example, in the case of <ref type="figure" target="#fig_3">Figure 3</ref>, the offset width of the sixth labeling point should larger than the fifth point and less than the seventh point. Independently predicting each offset may lead to unsmooth text region, and somehow it may bring more false detection. Therefore, we assume the width/height of each point has associated context information, and using RNN to learn their latent characteristics. We name this method as recurrent transverse and longitudinal offset connection (TLOC). The TLOC structure is shown with purple patterns in <ref type="figure" target="#fig_4">Figure 4</ref> and we also list some examples in <ref type="figure" target="#fig_5">Figure 5</ref> to show the difference of whether adding TLOC or not. To adopt TLOC, we find the output of PSROIPooling is suitable to encode the offsets context information. Take the width offset branch as an example; firstly, PSROIPooling outputs 14 p 2 score map for voting w 1 , ..., w 14 of each proposal, and p 2 bins of the i-th score map have p 2 voting values from respective position, which can be encoded as the feature of w i ; The RNN than takes width offsets feature of each point as sequential inputs, and recurrently updates the inherent state inside the hidden layers, L t , i.e.</p><formula xml:id="formula_4">L t = ϕ(L t−1 , O t ), t = 1, 2, ..., 14<label>(5)</label></formula><p>where O t ∈ P 2 is the t-th predicting offset from the corresponding psroi-pooling output channel. L t is a recurrent internal state computed from both current input (O t ) and the last state encoded in L t−1 . The recurrence is computed by using a non-linear function ϕ, where we adopt a bidirectional long short-term memory (BLSTM) architecture <ref type="bibr" target="#b10">[11]</ref> as our RNN. The internal state inside the RNN hidden layer associates the sequential context information by all previous estimated offsets through the recurrent connection, and we empirically use a 256D BLSTM hidden layer, thus L t ∈ 256 . Finally, the output of the BLSTM is a 14 dimensional 1 × 256 vector, which is globally pooled by a (1 × 256) kernel to output the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Long Side Interpolation</head><p>As introduced in Sec. 4.1, for each bounding box, the CTD outputs offsets for 14 points. However, almost all current benchmark datasets only have labels of two or four vertexes information. To train in these datasets, we can easily interpolate the equal division points in the largest side and its opposite side as shown in 6. Promisingly, by simply interpolating the points in the bounding box, our CTD can be effectively trained with all text region and it can achieve better results as demonstrated in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Polygonal Post Processing</head><p>Non-polygon suppression (NPS). False positive detecting results are one of the important reasons that restricting the performance of text detection. However, in CTD, some diffident false positives will appear with invalid shape (for a valid polygon, there is not any intersecting side). In addition, there is hardly any scene text come out with intersecting side, and these invalid polygons are nearly impossible to recognize. Therefore, we simply suppress all these invalid polygons and we named it a non-polygon suppression (NPS), which can slightly improve the accuracy without influencing the recall rate.</p><p>Polygonal non-maximum suppression (PNMS). Nonmaximum suppression <ref type="bibr" target="#b23">[24]</ref> is proved very effective for object detection task. Because of the particularity of the curve scene text, rectangular NMS is limited to handle dense multi-oriented text as shown in <ref type="figure" target="#fig_1">Figure 1 (a)</ref> and (c). To solve this problem, <ref type="bibr" target="#b37">[38]</ref> propose a locality-aware NMS and <ref type="bibr" target="#b3">[4]</ref> devise a Mask-NMS to suppress the final output results. In this paper, we also improve the NMS by computing the overlapping area between polygons, named polygon nonmaximum suppression (PNMS), which is proved effective in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we carry experiments on the proposed CTW1500 dataset to test our method. The testing environment is Ubuntu 16.04 64bit with single Nvidia 1080 GPU. All the text detection results are evaluated with the protocol introduced in section 3, which exactly calculates the IoU between the polygons instead of axis-align rectangles. For fair comparison, all the experiments only use the provided training data without any data augmentation.</p><p>Effectiveness of TLOC and PNMS. We first evaluate the availability of proposed TLOC and PNMS, and the results are given in <ref type="table">Table 3</ref>. In this table, we can find whatever using CTD or CTD + TLOC, the PNMS can always slightly outperform the classic NMS. On the other hand, the results also demonstrate that by simply adding TLOC, the proposed CTD can be improved about 4 percents in terms of Hmean. Note that we don't compare NPS here because it is a prerequisite post-processing method for evaluating polygonal overlapping area, which can slightly improve the accuracy.</p><p>Comparison with state-of-the-arts methods. For comprehensively evaluating our method, we compare the proposed CTD and CTD + TLOC with several state-of-the-art and common text detection methods. Note that for East <ref type="bibr" target="#b37">[38]</ref> and Seglink <ref type="bibr" target="#b26">[27]</ref>, we re-implement these methods based on the unofficial source codes from github, and for fair comparison, we do not use huge synthetic data to pre-train seglink which may reduce its performance. For DMPNet <ref type="bibr" target="#b20">[21]</ref> and CTPN <ref type="bibr" target="#b31">[32]</ref>, we both use Caffe <ref type="bibr" target="#b14">[15]</ref> framework to re-implement these methods. Besides, because none of these methods can be trained with curve text region, we follow the traditional circumscribed rectangular bounding box to make the labels trainable for them. <ref type="table">Table 2</ref> lists the experimental results. The results of the whole CTW-1500 test set show that the proposed CTD + TLOC can outperform state-of-the-art methods with more than 10 percents in terms of Hmean. To further evaluating the effectiveness, we also split the curve and non-curve text from the whole test set by simply regarding the other kind of text as difficult (not care) and making comparison. Promisingly, in the curve subset, proposed CTD + TLOC can outperform stateof-the-art methods with at least 28 percents of Hmean, and meanwhile, this method can also achieve the best results in detecting only non-curve texts, demonstrating its robustness and universality. Note that for our method, the results of the subset are both less than the whole set are because the way treating the other text as difficult will reduce the precision.</p><p>Besides, we compare the detection speed in the last column of the <ref type="table">Table 2</ref> and the results (13.3 or 15.2 FPS) show that our method is the second fastest, which also proves the effectiveness of our method. Examples of the detection results are visualized in <ref type="figure" target="#fig_7">Figure  7</ref>. In the last column of this figure, we also use our CTD to detect the curve texts from other dataset, which qualitatively demonstrates its powerful ability for detecting curve texts, as well as its generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>Curve text is common in our real world but currently few datasets or methods are aiming at curve text detection. In order to facilitate this new challenging research of reading curve text in the wild, in this paper, we propose a new dataset named CTW1500, which is a novel dataset mainly constructed by curve text. The curve text in this dataset are approximated labeled by polygon which do not require too much manpower. In addition, we propose a new CTD approach which may be the first attempt to directly detect the curve texts. By devising a transverse and longitudinal offset connection (TLOC) method, the CTD can be seamlessly connected with RNN, which significantly improves the detecting performance. We also propose a simple but effective long side interpolation technique, which let CTD become an universal method that can also be trained with rectangular or quadrilateral bounding boxes without additional manual efforts. Lastly, we design two post-processing methods that are also demonstrated effective.</p><p>In future, the propose dataset could be enlarged as a curve text based recognition dataset, because the labeling manner seems good for recognition. Besides, although the flexible detecting manner like our CTD may be slightly slower than an rigid rectangle based detector, the former can solve more complicated problems like detecting curve text and achieving better results, which should worth further exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Curve bounding box can avoid needless overlap. (b) Curve bounding box can reduce redundant background noise. (c) Curve bounding box is easier to recognize text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of quadrilateral bounding box and rectangular bounding box for localizing texts. Left: using quadrilateral label. Right: using polygonal label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Examples curve text annotations of CTW1500 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of labeling curve text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Overall structure of our Curve Text Detector (CTD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Top: Detection results without TLOC. Bottom: Detection results with TLOC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the interpolation for 4 points bounding boxes. The 10 equal division points will be respectively interpolated in two Red sides of each bounding box. Green means straight line without interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Detection results visualization. The fourth column lists some inferior results and the images from last column are from other datasets for further testing the generalization ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Time cost of labeling different shapes of text.</figDesc><table><row><cell>Bounding Box</cell><cell cols="3">Horizontal Quadrilateral Curve</cell></row><row><cell>Labeling Time (s)</cell><cell>2.5</cell><cell>4</cell><cell>13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Experiments on the proposed CTW1500 test set, curve subset and non-curve subset. R: Recall. P: Precision. H: Hmean. S: Speed. Speed column evaluates the time of the forward procedure without post-processing. Experiments to evaluate TLOC and PNMS. Here we can use NMS is because CTD remains the circumscribed rectangle branch, and the float number is the threshold. R: Recall rate. P: Precision. H: Hmean.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Whole set R (%) P (%) H (%)</cell><cell cols="3">Non-curve subset R (%) P (%) H (%)</cell><cell cols="3">Curve subset R (%) P (%) H (%)</cell><cell>S (FPS)</cell></row><row><cell>Seglink [27]</cell><cell>40.0</cell><cell>42.3</cell><cell>40.8</cell><cell>48.4</cell><cell>38.3</cell><cell>42.8</cell><cell>19.4</cell><cell>9.9</cell><cell>13.2</cell><cell>10.7</cell></row><row><cell>SWT [6]</cell><cell>9.0</cell><cell>20.7</cell><cell>12.5</cell><cell>5.8</cell><cell>13.4</cell><cell>8.1</cell><cell>6.4</cell><cell>7.0</cell><cell>6.7</cell><cell>-</cell></row><row><cell>CTPN [32]</cell><cell>53.8</cell><cell>60.4</cell><cell>56.9</cell><cell>59.4</cell><cell>54.3</cell><cell>56.7</cell><cell>37.7</cell><cell>34.1</cell><cell>35.8</cell><cell>7.14</cell></row><row><cell>EAST [38]</cell><cell>49.1</cell><cell>78.7</cell><cell>60.4</cell><cell>57.5</cell><cell>71.0</cell><cell>63.6</cell><cell>29.9</cell><cell>40.9</cell><cell>34.6</cell><cell>21.2</cell></row><row><cell>DMPNet [21]</cell><cell>56.0</cell><cell>69.9</cell><cell>62.2</cell><cell>61.7</cell><cell>63.9</cell><cell>62.7</cell><cell>39.3</cell><cell>35.5</cell><cell>37.3</cell><cell>12.3</cell></row><row><cell>AdaBoost [2]</cell><cell>4.4</cell><cell>6.7</cell><cell>5.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CTD (ours)</cell><cell>65.2</cell><cell>74.3</cell><cell>69.5</cell><cell>60.3</cell><cell>67.3</cell><cell>63.5</cell><cell>73.9</cell><cell>52.9</cell><cell>61.6</cell><cell>15.2</cell></row><row><cell>CTD + TLOC (ours)</cell><cell>69.8</cell><cell>77.4</cell><cell>73.4</cell><cell>62.3</cell><cell>70.8</cell><cell>66.3</cell><cell>77.1</cell><cell>57.1</cell><cell>65.6</cell><cell>13.3</cell></row><row><cell>Algorithm</cell><cell></cell><cell cols="3">R (%) P (%) H (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTD + NMS0.3</cell><cell></cell><cell>64.4</cell><cell>74.9</cell><cell>69.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTD + PNMS0.1</cell><cell></cell><cell>65.2</cell><cell>74.3</cell><cell>69.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + PNMS0.1</cell><cell>69.8</cell><cell>77.4</cell><cell>73.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + PNMS0.2</cell><cell>70.1</cell><cell>75.0</cell><cell>72.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + PNMS0.3</cell><cell>70.8</cell><cell>71.6</cell><cell>71.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + PNMS0.4</cell><cell>71.7</cell><cell>65.3</cell><cell>68.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + NMS0.1</cell><cell>63.7</cell><cell>78.4</cell><cell>70.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + NMS0.2</cell><cell>68.6</cell><cell>78.1</cell><cell>73.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + NMS0.3</cell><cell>69.7</cell><cell>77.1</cell><cell>73.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTD + TLOC + NMS0.4</cell><cell>70.8</cell><cell>74.7</cell><cell>72.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriched deep recurrent visual attention model for multiple object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ablavatski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<meeting>the 2004 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fused text segmentation networks for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03272</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">icdar2017 robust reading competition challenge on multi-lingual scene text detection and script identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bizid</forename></persName>
		</author>
		<ptr target="http://rrc.cvc.uab.es/?ch=8" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T M N S H I Y K</forename></persName>
		</author>
		<title level="m">2017 robust reading challenge on omnidirectional video</title>
		<imprint/>
	</monogr>
	<note>online</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G I</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P D L</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multiclass image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards end-to-end text spotting with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Seventh International Conference on</title>
		<meeting>Seventh International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
	<note>Document Analysis and Recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neocr: A configurable dataset for natural image text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Meyer-Wegener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Camera-Based Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="150" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Icdar 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2011 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09585</idno>
		<title level="m">Serge Belongie, and B. X. Icdar2017 competition on reading chinese text in the wild (rctw-17)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Matera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<title level="m">Tomas. Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Text string detection from natural scenes by structure-based partition and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2594" to="2605" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiorientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1930</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
