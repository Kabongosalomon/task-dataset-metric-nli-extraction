<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Webly Supervised Image Classification with Self-Contained Confidence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
							<email>yangjingkang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
							<email>fenglitong@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirong</forename><surname>Chen</surname></persName>
							<email>chenweirong@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Chinese</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
							<email>yanxiaopeng@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Zheng</surname></persName>
							<email>zhenghuabin@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<email>wayne.zhang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Webly Supervised Image Classification with Self-Contained Confidence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Webly supervised learning</term>
					<term>noisy labels</term>
					<term>model uncertainty</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on webly supervised learning (WSL), where datasets are built by crawling samples from the Internet and directly using search queries as web labels. Although WSL benefits from fast and low-cost data collection, noises in web labels hinder better performance of the image classification model. To alleviate this problem, in recent works, self-label supervised loss Ls is utilized together with webly supervised loss Lw. Ls relies on pseudo labels predicted by the model itself. Since the correctness of the web label or pseudo label is usually on a case-by-case basis for each web sample, it is desirable to adjust the balance between Ls and Lw on sample level. Inspired by the ability of Deep Neural Networks (DNNs) in confidence prediction, we introduce Self-Contained Confidence (SCC) by adapting model uncertainty for WSL setting, and use it to sample-wisely balance Ls and Lw. Therefore, a simple yet effective WSL framework is proposed. A series of SCC-friendly regularization approaches are investigated, among which the proposed graph-enhanced mixup is the most effective method to provide high-quality confidence to enhance our framework. The proposed WSL framework has achieved the state-of-the-art results on two largescale WSL datasets, WebVision-1000 and Food101-N. Code is available at https://github.com/bigvideoresearch/SCC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale human-labeled data plays a vital role in deep learning-based applications such as image classification <ref type="bibr" target="#b7">[3]</ref>, scene recognition <ref type="bibr" target="#b45">[41]</ref>, face recognition <ref type="bibr" target="#b34">[30]</ref>, etc. However, high-quality human annotations require significant cost in labor and time. Webly supervised learning (WSL), therefore, has attracted more attention recently as a cost-effective approach for developing learning systems from  abundant web data. Generally, search queries fed into image crawlers are directly used as web labels for crawled images, which also introduce label noise due to semantic ambiguity and search engine bias. How to deal with these unreliable and noisy web labels becomes a key task in WSL.</p><p>A straight-forward approach of WSL is to treat web labels as ground truth and all web samples are directly used to train DNNs <ref type="bibr" target="#b24">[20,</ref><ref type="bibr" target="#b33">29]</ref>. Some previous methods <ref type="bibr" target="#b18">[14,</ref><ref type="bibr" target="#b21">17]</ref> require additional clean subsets to learn a guidance model to judge the correctness of web labels and adopt a sample reweighting strategy for robust training of DNNs. CurriculumNet <ref type="bibr" target="#b12">[8]</ref> avoids extra clean set by leveraging density assumption that samples from the high-density region are more reliable, and trains the model in a curriculum learning manner. As all the above works only use webly supervised loss L w , recent works attempt to combine self-label supervised loss L s with L w <ref type="bibr" target="#b13">[9,</ref><ref type="bibr" target="#b36">32]</ref>. L s comes from the predictions of the model itself in a fashion of self-distillation <ref type="bibr" target="#b17">[13]</ref> or prototype-based rectification <ref type="bibr" target="#b32">[28]</ref>.</p><p>Although it is promising to utilize L s together with L w , we argue that the ratio balancing L w and L s should not be a constant across the entire dataset as in previous works <ref type="bibr" target="#b13">[9,</ref><ref type="bibr" target="#b36">32]</ref>. The correctness of web labels varies on a case-bycase basis, due to various causes of real-world label noise. Motivated by this observation, we design a framework that adaptively balances L w and L s on sample level.</p><p>Inspired by the uncertainty prediction ability of DNNs <ref type="bibr" target="#b9">[5]</ref>, we use DNN's prediction confidence, termed as self-contained confidence (SCC), to achieve a sample-wise balance between L w and L s . Model uncertainty shows how unsure the model considers its correctness on its own prediction, which is revealed by DNN's soft label output. When the model is trained with binary cross entropy (BCE) loss, the model uncertainty can be estimated independently across all categories. Here, we regard model uncertainty corresponding to the cate-gory of the sample's web label as SCC, reflecting the likelihood of web label correctness from the model's scope <ref type="bibr" target="#b9">[5]</ref>. <ref type="figure" target="#fig_1">Fig. 1a</ref> vividly shows a strong positive correlation between SCC and the correctness of web labels. This association is further confirmed by Expected Calibrated Error (ECE) plot <ref type="bibr" target="#b11">[7]</ref>, who groups samples with SCC scores within an interval and calculates their average web label correctness rate using a human-annotated verification set from Food-101N <ref type="bibr" target="#b21">[17]</ref>. According to <ref type="figure" target="#fig_1">Fig. 1b</ref>, samples who lie in higher SCC intervals generally have larger probabilities of correct web labels.</p><p>With SCC as an effective indicator of web label correctness, a generic SCCbased WSL framework is proposed. Intuitively, with the help of SCC, our framework enforces a webly supervised loss L w if a web label is considered reliable, and a self-label supervised loss L s otherwise. The self-label supervised loss utilizes the soft label predicted by a model pretrained on the WSL dataset as a self-supervised target. SCC, which is also extracted from the pretrained model, balances the ratio between L w and L s for each web sample. Our SCC is emphasized as 'self-contained', as no extra guidance model or labeled clean dataset is needed. Following the uncertainty calibration approaches <ref type="bibr" target="#b11">[7,</ref><ref type="bibr" target="#b37">33]</ref>, we also investigate the relationship between statistical metrics (e.g. ECE metric) and image classification accuracy.</p><p>Our contributions are summarized as follows:</p><p>-A generic noise-robust WSL framework that does not require a humanverified clean dataset is proposed, novelly featured by sample-level confidence from the perspective of model uncertainty. -Based on our framework, we further design a graph-enhanced mixup method that stands out among a series of SCC-friendly regularization methods to achieve better classification performance. -We empirically conclude that under our framework, the statistical metrics of SCC are positively correlated with final classification accuracy, and self-label supervision is superior to consistency regularization for WSL tasks. -The proposed framework achieves state-of-the-art results on two large-scale realistic WSL datasets, WebVision-1000 and Food-101N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Webly Supervised Learning</head><p>Learning with noisy labels can be divided into two categories of problems according to sources of label noise, i.e., synthetic or realistic. For synthetic label noise, some works estimate a noisy channel (e.g., a transition matrix) to model the label noise <ref type="bibr" target="#b27">[23,</ref><ref type="bibr" target="#b39">35,</ref><ref type="bibr" target="#b42">38]</ref>. However, the designed or estimated channels might not stay effective in the real-world scenario. WSL lies in the realistic noisy label problem. Seminal WSL works attempted to leverage a subset of human-verified samples, referred as 'clean set'. MentorNet <ref type="bibr" target="#b18">[14]</ref> learns a dynamic curriculum from the clean set for the sample-reweighting scheme, making the StudentNet only focus on probably correct samples. CleanNet <ref type="bibr" target="#b21">[17]</ref> transfers knowledge of label noise learned from a clean set with partial categories towards all categories, and adjust sample weights accordingly to alleviate the impact of noisy labels. In contrast to 'clean set' prior, CurriculumNet <ref type="bibr" target="#b12">[8]</ref> assumes that samples with correct labels usually locate at high-density regions in visual feature space and designs a three-stage training strategy to train the model with data stratified by cleanness-levels. Self labeling is another solution to purify noisy labels by replacing unreliable web labels with predictions by a model. Joint Optimization <ref type="bibr" target="#b36">[32]</ref> uses DNN's predictions as self labels, and Self-Learning <ref type="bibr" target="#b13">[9]</ref> generates self labels by prototype voting and combines web labels and pseudo-labels using constant ratio. Compared to them, we balance self labels and web labels using sample-wise confidence, which relies on our observation that DNNs are capable of perceiving noisy labels with self-contained confidences. Self labels and confidences are unified in a single pretrained model in our approach. <ref type="table" target="#tab_0">Table 1</ref> clarifies the differences between other WSL solutions and ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-Supervised Learning</head><p>Semi-supervised learning (SSL) utilizes a small fraction of labeled data and a large unlabeled data set altogether <ref type="bibr" target="#b46">[42]</ref>. Solutions to SSL are basically within two main categories. One uses consistency regularization to ensure the model robustness by forcing networks producing identical predictions upon inputs with different augmentations, which is used in MixMatch [1] and UDA <ref type="bibr" target="#b40">[36]</ref>. Another uses pseudo-labeling in the representative methods of Billion Scale <ref type="bibr" target="#b41">[37]</ref> and data distillation <ref type="bibr" target="#b29">[25]</ref>, which firstly trains models on the clean labeled set and then provides pseudo-labels for unlabeled data.</p><p>The differences between WSL and SSL settings lead to key differences between our method and SSL methods. First, the self-label supervision in our method has a close connection with pseudo-labeling. However, our method utilizes all samples with both web labels and self labels, and SSL methods utilize a subset of unlabeled data with pseudo-labels only. The model for self-labeling in our method is learned from the entire noisy dataset, and the model for pseudolabeling in SSL methods is trained on a small 'clean' labeled set. Second, our self-label supervised loss has a similar form to consistency regularization. However, consistency regularization may be less powerful to correct the bias caused by label noise than cleaning the labels with self-labeling explicitly. Details are discussed in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Uncertainty</head><p>Model uncertainty refers to the level of distrust that the model considers its own prediction, which is vital for real-world applications. For classification tasks, the calculation is as simple as leveraging the highest score of the softmax output. To quantify the quality of model uncertainty, expected calibration error (ECE) is one widely used metric that claims an accurate uncertainty should align model predictions with classification accuracy <ref type="bibr" target="#b11">[7,</ref><ref type="bibr" target="#b26">22]</ref>. For instance, if a network predicts a group of samples with a probability of 0.6, we expect exactly 60% samples of this group are classified correctly.</p><p>Following this path, several methods were proposed to improve the quality of uncertainty. Post-hoc calibration such as temperature scaling is one family of methods, which optimizes the mapping of produced uncertainty on the verification set <ref type="bibr" target="#b11">[7]</ref>. However, such data-dependent rescaling methods cannot improve confidence quality fundamentally. Some other works explored within-training strategies that can provide high-quality model uncertainty, such as label smoothing <ref type="bibr" target="#b25">[21]</ref>, dropout <ref type="bibr" target="#b10">[6]</ref>, mixup <ref type="bibr" target="#b37">[33]</ref>, Bayesian models <ref type="bibr" target="#b20">[16]</ref>, etc. AugMix <ref type="bibr" target="#b16">[12]</ref> is directly designed to improve uncertainty estimates through a data augmentation approach. However, few research works utilized the model confidence to architect model training.</p><p>In our work, model uncertainty is adapted for web label confidence estimation. Instead of using the maximum of the model's output probabilities, we pick the value on the exact web label from the probability distribution, which estimates the correctness of the sample's web label. <ref type="bibr" target="#b9">5</ref> Metrics such as ECE can also be adapted, i.e., web label confidence is considered well-calibrated if a model predicts all samples in a group with web label confidences of 0.6, 60% samples in this group have correct web labels. Being aware that the extraction of web label confidence requires the probability of each class to be calculated independently, binary cross-entropy (BCE) rather than softmax cross-entropy loss is used for training the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, after a formal description of WSL task, we introduce two loss functions and the proposed framework with highlighted SCC. Our framework is </p><formula xml:id="formula_0">Dataset ! ! ! ( | ! , ) ( | , " ) = ( ($) | , " ) 5 Augmentation Self-Label Supervised Loss ℒ &amp; Webly Supervised Loss ℒ ' × × 1 − Web label</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Webly Supervised Learning: Problem Statement and Notations</head><p>Webly Supervised Learning (WSL) aims at training an optimal deep neural network M θ from a dataset D = {(x 1 , y * 1 ), . . . , (x N , y * N )} collected from the Internet. x i denotes the i-th sample in the dataset, and the one-hot web label y * i is the one-hot encoding of the web label ω i (referring to ω i -th category). The web label ω i is obtained from the search query of crawling the image x i . Consider the massive noise in retrieved images from a search engine, ω i or y * i might not reflect the correct category that x i belongs to. Therefore, suppressing the noise in unreliable web labels becomes the main challenge in WSL.</p><p>For convenience, we use symbols x, y * , ω directly to represent an arbitrary sample, its one-hot web label and its web label, respectively. For the multi-label problem, y (j) denotes sample's label on j-th class. p(y|x, θ) denotes the label prediction of sample x by the model M θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Webly Supervised Loss and Self-Label Supervised Loss</head><p>Webly supervised loss and self-label supervised loss are two widely adopted loss functions in WSL <ref type="bibr" target="#b12">[8,</ref><ref type="bibr" target="#b13">9,</ref><ref type="bibr" target="#b30">26,</ref><ref type="bibr" target="#b36">32]</ref>. Webly supervised loss utilizes web labels as supervision information, and self-label supervised loss <ref type="bibr" target="#b13">[9,</ref><ref type="bibr" target="#b36">32]</ref> utilizes predictions of a pretrained model instead. Formally, we define them as follows.</p><p>For webly supervised loss, given x augmented from x with web label ω, the loss function can be expressed as</p><formula xml:id="formula_1">L w = − log p(y (ω) |x , θ) + j∈S\ω log 1 − p(y (j) |x , θ) .</formula><p>(1)</p><p>Notice that webly supervised loss is in the form of binary cross-entropy (BCE) loss, because a webly-crawled image probably has multi-label semantics.</p><p>For self-label supervised loss, we use the prediction of the pretrained model M θ0 , which is trained directly on the original web label dataset. As the predictions on samples will be used for finetuning M θ0 itself, We call them self labels. Therefore, with self label p(y|x, θ 0 ) from model M θ0 , the self-label supervised loss is</p><formula xml:id="formula_2">L s = − j∈S p(y (j) |x, θ 0 ) log p(y (j) |x , θ) + 1 − p(y (j) |x, θ 0 ) log 1 − p(y (j) |x , θ) ,<label>(2)</label></formula><p>where y (j) represents the prediction for j-th class in label set S for multi-class classification problem.</p><p>A similar loss to self-label supervised loss is consistency loss [1, <ref type="bibr" target="#b40">36]</ref>, which provides an auxiliary regularization by enforcing a model to output similar predictions on different augmented counterparts of the same image. Consistency loss is proven to be effective on a large number of unlabeled images for semi-supervised learning. In WSL, however, as the quality of self labels can be guaranteed by feeding a pretrained model with weak augmented images, we found that the high-quality self-supervised loss is more effective than auxiliary consistency loss. An experimental comparison will be shown in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Contained Confidence</head><p>It is desirable to adaptively balance webly supervised loss and self-label supervised loss on sample level. Intuitively, we should trust webly supervised loss more on samples with reliable web labels, while self-label supervised loss would dominate the total loss confronting incorrect web labels.</p><p>In our method, model M θ0 provides only self labels, but also the reliability of web labels. Notice that with BCE loss, model M θ0 predicts the probability that x belongs to class i as p(y (i) |x, θ 0 ). Specially, we focus on the model prediction on the one-hot web label y * whose category index is ω, denoted as p(y (ω) |x, θ 0 ). Therefore, the only trainable parameter θ would be updated by minimizing the final loss</p><formula xml:id="formula_3">L = c × L w + (1 − c) × L s , where c = p(y (ω) |x, θ 0 ).<label>(3)</label></formula><p>The confidence c is named as self-contained confidence (SCC), as it is self contained in the pretrained model and requires no extra data or knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph-Based Aggregation</head><p>A key component in the proposed method is the pretrained model M θ0 for estimating both SCC and self labels. As the model is trained on noisy web labels, we employ mixup <ref type="bibr" target="#b43">[39]</ref>, which is known as an effective regularization to make DNNs less prone to over-confident predictions and predicted scores of DNNs better calibrated to the actual confidence of a correct prediction <ref type="bibr" target="#b37">[33]</ref>.</p><p>In addition, we propose a graph-based aggregation (GBA) method to further boost the confidence quality and classification performance. GBA does a smoothing operation on a visual similarity graph spanned by image features. By viewing every image as a node, a k-nearest-neighbor (k-NN) graph is firstly constructed based on features located before fc layer of pretrained model M θ0 . Cosine similarity of features is computed across every pair in the neighborhood as edge weight. Hereby, an undirected k-NN graph with weighted adjacent matrix A is obtained. Let P denote a matrix of self labels, and the corrected self labels after GBA are denoted aŝ</p><formula xml:id="formula_4">P = D − 1 2 (λI + A) D − 1 2 P,<label>(4)</label></formula><p>where D(i, i) = λ + N j=1 A(i, j). λ controls the portion of original self labels in the post-GBA self labels. SCC will also be extracted fromP. GBA is a postprocessing step with graph filtering <ref type="bibr" target="#b19">[15]</ref> and complementary to other methods such as mixup. We evaluate several potential methods and conclude mixup + GBA leads to the optimal performance in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we firstly introduce three public WSL datasets. Then, we investigate several SCC-friendly methods, among which GBA-enhanced mixup stands out as the best one in both statistical metrics and classification accuracy. More ablation studies demonstrate the effectiveness of both sample-wise adaptive loss and self-label supervision. Finally, we show that the proposed method reaches the state-of-the-art on the public WSL datasets. We leave the exploration of robustness of our framework and formal algorithm in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Configurations</head><p>WebVision-1000 [19] contains 2.4M noisy-labeled training images crawled from Flickr and Google, with keywords from 1000 class-labels in ILSVRC-2012 <ref type="bibr" target="#b7">[3]</ref>. The estimated web label accuracy is 48% <ref type="bibr" target="#b12">[8]</ref>. The ILSVRC-2012 validation set is also utilized along with WebVision-1000's own validation set.</p><p>WebVision-500 is a quarter-sized version of WebVision-1000 for evaluation and ablation study in low cost without losing generalization. We randomly sample one-half categories with one-half samples in the training set, and keep the full validation set of the selected 500 categories. This dataset is used for our ablation study in Sec. 4.2, 4.3, 4.4.</p><p>Food-101N <ref type="bibr" target="#b21">[17]</ref> is another web dataset with 310k images classified into 101 food categories. Images are crawled from Google, Yelp, etc. We evaluate our model on the test set of Food-101 <ref type="bibr" target="#b6">[2]</ref>, Food-101N's clean dataset counterpart. 60k human verification labels are provided, indicating the correctness of web labels. The estimated label accuracy is around 80%.</p><p>Configuration details. ResNet50 is selected as our CNN model in all experiments <ref type="bibr" target="#b14">[10]</ref>. For more efficient training on WebVision, a minor-revised ResNet50-D is utilized <ref type="bibr" target="#b15">[11]</ref>. Food101N uses standard ResNet50 for a fair comparison. We use the following settings that completely refer to <ref type="bibr" target="#b15">[11]</ref>. Batch size is set as 256 and mini-batch size as 32. We use the standard SGD with the momentum of 0.9 and weight decay of 10 −4 . A warm-start linearly reaches the initial learning rate (LR) in the first 10 epochs. The remained epochs are ruled by a cosine learning rate scheduler. A simple class reweighting is performed to deal with class imbalance. The initial LR is 0.1 with total L epochs for pretrained models. The main model has initial LR of 0.05 with identical epoch numbers. L=120 for WebVision-500 and Food101N, L=150 for WebVision-1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exploring Optimal Regularization Method</head><p>In this section, we experiment with seven different confidence-friendly regularization methods for M θ under our framework. We conclude that GBA-enhanced mixup (mixup+GBA) is the most efficient one for the best performance. However, as the main contribution of our work is the simple yet effective noise-robust pipeline with SCC, regularization is not a necessary part of our model.</p><p>Besides the standard setting with BCE loss, which is denoted as 'Vanilla', we introduce the following regularization methods for model M θ .</p><p>Label Smoothing prevents over-confidence problems by adding a small value of on the zero-values in one-hot encoding labels <ref type="bibr" target="#b35">[31]</ref>. We use = 0.1.</p><p>Entropy Regularizer discourages over-confident model prediction by adding a penalizing term to standard loss functions <ref type="bibr" target="#b28">[24]</ref>. Regularizer weight is set as 0.1.</p><p>MC Dropout is selected as the representation of Bayesian methods. It approximates Bayesian inference by randomness in dropout operation <ref type="bibr" target="#b10">[6]</ref>. Dropout rate p is set 0.5. When testing, we infer 50 times and average the predictions.</p><p>Mixup is a simple but effective pre-processing method that convexly combines every pair of two sampled images and labels <ref type="bibr" target="#b43">[39]</ref>. <ref type="bibr" target="#b37">[33]</ref> proves its strong uncertainty calibration capability beyond its label smoothing effects.</p><p>AugMix is another data augmentation method with consistency loss, which produces well-calibrated model uncertainty <ref type="bibr" target="#b16">[12]</ref>.</p><p>Ensemble utilizes several models with identical tasks to boost the ultimate performance <ref type="bibr" target="#b8">[4]</ref>. With E vanilla models with different random initializations, we average their predictions on every sample.</p><p>Graph-based Aggregation (GBA) is introduced in Sec. 3.4. We use k = 10 and λ = 0.5 as hyper-parameters.</p><p>Result Analysis. <ref type="table">Table 2</ref> reports the results. S1 is short for the pretraining stage for M θ0 , S2 for the finetuning stage using our framework. Generally, good performance in S1 favors S2. Mixup and Ensemble are the two most effective regularizers. As mentioned in Sec. 3.4, Mixup smooths discriminative spaces and ensemble averages models' biases. The advantages of these two methods are combined in GBA design, as a graph smoothing operator for neighbor predictions, which is proven effective empirically. Improvement from GBA is weaker on mixup compared to vanilla since mixup offers the same effect of smoothing space with GBA. However, mixup+GBA still reaches the optimal result besides the costly ensemble method. <ref type="table">Table 2</ref>. Performance of the pretrained model (S1) and finetuned model (S2) Method S1-WebVision S1-ImageNet S2-WebVision S2-ImageNet Top-1 Top- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Understanding Self-Contained Confidence</head><p>As SCC plays a critical role in our framework, we explore an interesting question: how great the SCC quality affects the final accuracy reported in the previous section? We also show the relationship between three statistical metrics adapted from uncertainty theories and our accuracy-based metric.</p><p>For statistical metrics, we manually create a verification set V = {v 1 , . . . , v n } for WebVision-500 by annotating whether the web label is correct on n = 12500 samples, with 50 randomly sampled cases from 250 random classes.</p><p>To evaluate the quality of SCC, The following metrics are utilized. Second-stage Accuracy on Vanilla (SAV). To empirically evaluate different SCCs, we use an identical vanilla pretrained model for self-labeling and finetuning under our framework. Therefore, the accuracy of second-stage finetuned model is only determined by the quality of SCC. Note that the models for producing SCC are different and with different regularization methods.</p><p>Mean Square Error (MSE). Verification set V can be considered as a set of ground-truth confidence since it values 1 with the correct web label and values 0 when incorrect. Thus, MSE estimates the squared difference between the given confidence and the ground-truth, which is defined as</p><formula xml:id="formula_5">MSE = 1 n n i=1 (v i − c i ) 2 .<label>(5)</label></formula><p>Expected Calibration Error (ECE). Calibration error is originally used to evaluate the model interpretability on their predictions <ref type="bibr" target="#b11">[7]</ref>, while we slightly adapt it for confidence quality evaluation. Formally, in the verification set V, for all samples whose confidences fall into ( m−1 M , m M ] form the m-th bin, where average confidence conf(B m ) = 1 |Bm| i∈Bm c i and the average web-label reliability  </p><formula xml:id="formula_6">ECE = M m=1 |B m | n rel(B m ) − conf(B m ) .<label>(6)</label></formula><p>Over-Confidence Error (OCE). Samples with high SCC but incorrect web labels are especially harmful to our framework, since introducing the wrong web label is much worse than using self labels. OCE evaluates the level of overconfidence by punishing more on higher-confident bins with low reliability, defined as</p><formula xml:id="formula_7">OCE = M m=1 |B m | n conf(B m ) × max conf(B m ) − rel(B m ), 0 .<label>(7)</label></formula><p>In this work, we calculate ECE and OCE with M = 100. For visualization in <ref type="figure" target="#fig_3">Fig. 3</ref>, we use M = 10. <ref type="figure" target="#fig_1">Fig.1b</ref> uses M = 100. Result Analysis. Best metric performance is reached by either mixup or Vanilla+GBA, while the ensemble also produces a good result. <ref type="figure" target="#fig_3">Fig. 3</ref> visualizes ECE diagrams, where GBA and mixup look more calibrated than any other model. A similar result is shown in Tabel 3 Column 2-4. Column 5-8 presents the metric of SAV which shows GBA provides good quality confidence that favors our proposed framework. According to our exploration of SCC, we conclude the following insights: (1) SCC can reflect the reliability of the web label according to <ref type="figure" target="#fig_3">Fig. 3;</ref> (2) SCC plays a key role in our pipeline through adaptively balancing two losses on the sample level since empirical metric SAV is generally proportional to the statistical metric ECE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>On Self-Contained Confidence. To show the necessity of sample-wise SCC, we follow the settings of <ref type="table" target="#tab_2">Table 3</ref> and replace SCC with constant confidence values. <ref type="figure" target="#fig_5">Fig. 4</ref> shows that any constant confidence is unable to surpass 77.17% WebVision Top-1 accuracy reached by Vanilla+GBA (marked as dashed line).</p><p>On Self-Label Supervised Loss. We demonstrate the superiority of selflabel supervised loss over consistency loss <ref type="bibr">[1,</ref><ref type="bibr" target="#b40">36]</ref>. Consistency loss is trained in an end-to-end fashion since it does not require a pretrained model M θ0 , whereas our self-label supervised loss expects a two-stage approach with static self labels and SCC. For fairness, we make comparisons using the same backbone with mixup regularization. <ref type="figure" target="#fig_5">Fig. 4b</ref> shows the model with our loss reaches better performance than consistency loss. An interesting observation is that a performance drop exists at the beginning of S2 in our method. Since S1 is trained with web labels, the model may memorize label noise and result in suboptimal performance. Thus, a large LR is required to destruct the noise-affected S1 model, causing a sudden performance drop with S2. Such a two-stage approach is adopted, because we find the end-to-end approach unsuitable for our method: in the early stage, inaccurate pseudo labels and SCC mislead the model, and in the late stage, the model finally obtains reliable SCC, however, small LR cannot correct the accumulated errors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Real-world Experiments</head><p>WebVision-1000. <ref type="table" target="#tab_3">Table 4</ref> reports experiments on WebVision-1000 using both vanilla and mixup models. With the vanilla model, using our pipeline, a 0.3% improvement is achieved for WebVision top-1 accuracy, and 0.7% increase on ImageNet top-1/5. GBA can further improve the performance of every metric. When enabling mixup operation (α = 0.2), although on top of a high-accuracy pretrained model, our method can still improve both ImageNet top-1 and top-5 accuracy by 1.9%. The WebVision top-5 accuracy is improved by 0.7%. The WebVision top-1 accuracy is improved a little. More improvements on ImageNet prove a good generalization ability of the proposed method. The larger improvement than vanilla may attribute to the higher SCC quality achieved by mixup. GBA advances an average 0.3% extra improvement on every metric. We also show the superiority of our method over state-of-the-art methods. Note that both MentorNet <ref type="bibr" target="#b18">[14]</ref> and CleanNet <ref type="bibr" target="#b21">[17]</ref> use extra human-verified datasets to train a guidance network first, and MentorNet <ref type="bibr" target="#b18">[14]</ref> chooses a backbone of InceptionResNetV2 stronger than ResNet50-D. Multimodal image classification <ref type="bibr" target="#b31">[27]</ref> uses ImageNet data for training visual embedding and a queryimage pairs dataset for training phrase generation. Stronger InceptionV3 is also selected as the backbone. Although with these disadvantages, our ResNet50-D still works the best among all.</p><p>Food-101N. According to <ref type="table" target="#tab_4">Table 5</ref>, we significantly advance the state-of-theart model without any usage of human annotations. For vanilla model, the second stage of our method pushes 0.8% higher accuracy than the first stage, and the usage of GBA even double the improvement. For the mixup model (α = 0.5), the second stage increases a higher 1.4% accuracy as mixup provides better SCC and self labels than vanilla, but the advance of GBA is deducted due to the overlapping effects of mixup and GBA. Rather than our normally used ResNet50-D, we use standard ResNet50 here for fair comparisons with others. While all the  CleanNet <ref type="bibr" target="#b21">[17]</ref> 83.95 Guidance Learning <ref type="bibr" target="#b22">[18]</ref> 84.20 MetaCleaner <ref type="bibr" target="#b44">[40]</ref> 85.05 Deep Self-Learning <ref type="bibr" target="#b13">[9]</ref> 85.11 SOMNet <ref type="bibr" target="#b38">[34]</ref> 87 other methods (except <ref type="bibr" target="#b44">[40]</ref>) train Food-101N from ImageNet pretrained model, we train our model from scratch and still reach optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a generic noise-robust framework featured by sample-level confidence balancing webly supervised loss and self-label supervised loss. Our framework is compatible with model regularization methods, among which our proposed mixup+GBA is the most effective.</p><p>Here we recall two main takeaway messages from our extensive experiments: (1) Reliability of the web label can be reflected by SCC (ref. <ref type="figure" target="#fig_3">Fig.3</ref>), and empirical metric SAV is generally proportional to the statistical metrics like ECE (ref. <ref type="table" target="#tab_2">Table 3</ref>). (2) Our framework is in favor of high-quality confidence provided by the pretrained model, and mixup and ensemble are the two most effective regularizers (ref. <ref type="figure" target="#fig_2">Fig.2&amp;3</ref>). Considering that mixup smooths discriminative spaces and ensemble averages models' biases, both advantages are combined in GBA design, as a graph smoothing operator for neighbor predictions.</p><p>We also leave a valuable discussion in the appendix for readers of interests, which basically shows: although the performance is largely dependent on the quality of SCC, the framework still works on Food101N even with a weak DNN backbone.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Work done during an internship at SenseTime EIG Research. arXiv:2008.11894v1 [cs.CV] 27 Aug 2020 (a) Images with 'hotdog' label in confidence intervals</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Exemplary images and ECE plot showing self-contained confidence (SCC) generally reflects web label correctness. A standard ResNet-50 is pretrained on the Food-101N training set for SCC extraction. (a) shows image samples grouped by low/medium/high confidences. The upper right tag on each image shows SCC value and the tag color indicates web label correctness (red: wrong, green: correct). (b) is the ECE plot using Food-101N human-verification set (M = 100)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Diagram of the proposed framework. Backward gradients pass through dashed arrows to update the only trainable parameter θ. Pretrained model M θ 0 learns from entire WSL dataset to provide self label and SCC. M θ is initialized by M θ 0 compatible with various regularization methods. Especially, we propose graphbased aggregation (GBA) to enhance SCC for network training. The diagram of our framework is shown in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>ECE diagrams of confidences from different SCC provider rel(B m ) = 1 |Bm| i∈Bm v i are calculated. Thus, ECE is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Ablation studies of sample-wise confidence and self-label supervised loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Highlighting the principal differences between other WSL methods and ours</figDesc><table><row><cell>Method</cell><cell>Clean Prior Set? Knowledge</cell><cell>How to suppress label noise?</cell></row><row><cell>MentorNet [14]</cell><cell>Clean Set</cell><cell>Low weight on Lw for noisy samples</cell></row><row><cell>CleanNet [17]</cell><cell>Clean Set</cell><cell>Low weight on Lw for noisy samples</cell></row><row><cell cols="2">CurriculumnNet [8] Density</cell><cell>Schedule noisy samples to later stages</cell></row><row><cell cols="3">Joint Optim. [32] Self-training Replace Lw with Ls</cell></row><row><cell>Self-Learning [9]</cell><cell>Density</cell><cell>Combine Lw and Ls with constant-ratio</cell></row><row><cell>Ours</cell><cell cols="2">Uncertainty Balance Lw and Ls sample-wisely</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Evaluations of SCC provided by different methods. Column 1-3 reports statistical metrics MSE, ECE and OCE. Column 4-7 reports model-based metric SAV</figDesc><table><row><cell>Confidence Provider</cell><cell>MSE ECE</cell><cell>OCE</cell><cell>SAV-WebVision SAV-ImageNet Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>Vanilla</cell><cell cols="3">0.2795 0.2371 0.1518 76.46 89.63 69.78 85.32</cell></row><row><cell>Label Smoothing</cell><cell cols="3">0.2786 0.2280 0.1200 76.82 89.86 70.06 85.76</cell></row><row><cell>Entropy Regularizer</cell><cell cols="3">0.4137 0.4138 0.0370 76.40 90.17 70.31 86.36</cell></row><row><cell cols="4">MC Dropout (p = 0.5) 0.2807 0.2431 0.1193 76.68 89.89 70.34 85.97</cell></row><row><cell>Mixup (α = 0.2)</cell><cell cols="3">0.2510 0.1828 0.0135 77.14 90.18 71.00 86.48</cell></row><row><cell>Augmix</cell><cell cols="3">0.2869 0.2366 0.1757 76.67 89.65 69.89 85.63</cell></row><row><cell>Ensemble (E = 5)</cell><cell cols="3">0.2687 0.2233 0.1537 76.49 89.68 70.06 85.86</cell></row><row><cell>Vanilla + GBA</cell><cell cols="3">0.2612 0.2494 0.0002 77.17 90.55 70.89 86.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The state-of-the-art results on WebVision-1000</figDesc><table><row><cell>Method</cell><cell>Backbone Network</cell><cell>WebVision ImageNet Top-1 Top-5 Top-1 Top-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The state-of-the-art results on Food-101N</figDesc><table><row><cell>Method</cell><cell>Top-1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Web label confidence and self-contained confidence are used interchangeably throughout the paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The work described in this paper was partially supported by Innovation and Technology Commission of the Hong Kong Special Administrative Region, China (Enterprise Support Scheme under the Innovation and Technology Fund B/E030/18).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ours (Vanilla)</title>
		<idno>ResNet50-D 75.36 89.38 67.93 84.77</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ours</surname></persName>
		</author>
		<idno>Vanilla+GBA) ResNet50-D 75.69 89.42 68.35 85.24</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<idno>ResNet50-D 75.54 90.36 68.77 86.59</idno>
		<title level="m">Initial Mixup Model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ours</surname></persName>
		</author>
		<idno>Mixup) ResNet50-D 75.74 90.78 70.38 88.25</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ours</surname></persName>
		</author>
		<idno>Mixup+GBA) ResNet50-D 75.78 91.07 70.66 88.46</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
	<note>References 1.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multiple Classifier Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">University of Cambridge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">AugMix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="6402" to="6413" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11384</idno>
		<title level="m">Product image recognition with guidance learning and noisy supervision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">When does label smoothing help? In: NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="13991" to="14002" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inferring context from pixels for multimodal image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fuxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="4077" to="4087" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="13888" to="13899" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from web data with self-organizing memory module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12846" to="12855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Are anchor points really indispensable in label-noise learning? In: NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6838" to="6849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An efficient and provable approach for mixture proportion estimation using linear independence assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4480" to="4489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Metacleaner: Learning to hallucinate clean representations for noisy-labeled visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7373" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
