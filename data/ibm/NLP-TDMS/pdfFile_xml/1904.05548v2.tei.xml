<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning Visual Dialogs with Structural and Partial Observations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
							<email>zilongzheng0318@ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<email>wenguanwang.ai@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
							<email>syqi@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<email>sczhu@stat.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning Visual Dialogs with Structural and Partial Observations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel model to address the task of Visual Dialog which exhibits complex dialog structures. To obtain a reasonable answer based on the current question and the dialog history, the underlying semantic dependencies between dialog entities are essential. In this paper, we explicitly formalize this task as inference in a graphical model with partially observed nodes and unknown graph structures (relations in dialog). The given dialog entities are viewed as the observed nodes. The answer to a given question is represented by a node with missing value. We first introduce an Expectation Maximization algorithm to infer both the underlying dialog structures and the missing node values (desired answers). Based on this, we proceed to propose a differentiable graph neural network (GNN) solution that approximates this process. Experiment results on the VisDial and VisDial-Q datasets show that our model outperforms comparative methods. It is also observed that our method can infer the underlying dialog structure for better dialog reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual Dialog has drawn increasing research interests at the intersection of computer vision and natural language processing. In such tasks, an image is given as context input, associated with a summarizing caption and a dialog history of question-answer pairs. The goal is to answer questions posed in natural language about images <ref type="bibr" target="#b8">[9]</ref>, or recover a follow-up question based on the dialog history <ref type="bibr" target="#b21">[22]</ref>. Despite its significance to artificial intelligence and human-computer interaction, it poses a richer set of challenges (see an example in <ref type="figure" target="#fig_1">Fig. 1</ref>) -requiring representing/understanding a series of multi-modal entities , and reasoning the rich semantic relations/structures among them. An ideal inference algorithm should be able to find out the * Equal contribution. C : A hot dog covered in mustard and cheese sits next to French fries. Q1: Are hot dogs in bun? A1: yes, there's only 1 though. Q2: Are they on plate? A2: No, it's in cardboard container. Q3: Are they steak fries? A3: No, they are shoestring fries. Q4: Are they on table? A4: They seem to be.  underlying semantic structure and give a reasonable answer based on this structure.</p><p>Previous studies have explored this task through embedding rich features from image representation learned from convolutional neural networks and language (i.e., questionanswer pairs, caption) representations learned from recurrent sequential models. Their impressive results well demonstrate the importance of mining and fusing multimodal information in this area. However, they largely neglect the key role of the rich relational information in dialog. Although a few <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b61">62]</ref> leveraged co-attention mechanisms to capture cross-modal correlations, their reasoning ability is still quite limited. They typically concatenate the multi-modal features together and directly project the concatenated feature into the answer feature space by a neural network. On one hand, their reasoning process does not fully utilize the rich relational information in this task due to their monolithic vector representations of dialog. On the other hand, their feed-forward network designs fail to deeply and iteratively mine and reason the information from different dialog entities over the inherent dialog structures.</p><p>In this work, we consider the problem of recovering the dialog structure and reasoning about the question/answer simultaneously. We represent the dialog as a graph, where the nodes are dialog entities and the edges are semantic dependencies between nodes. Given the dialog history as input, we have a partial observation of the graph. Then we formulate the problem as inferring about the values of unobserved nodes (e.g., the queried answer) and the graph structure. The challenge of the problem is that there is no label for dialog structures. For each individual dialog, we need to recover the underlying structure in an unsupervised manner.</p><p>The node values could then be inferred iteratively with the graph structure: we can reason about the nodes based on the graph structure, and further improve the structure based on the node values. To tackle this challenge, the insight is that a graph structure essentially specifies a joint probability distribution for all the nodes in the graph. Therefore we can view the queried dialog entities as missing values in the data, the dialog structure as unknown parameters of the distribution. Specifically, we encode the dialog as a Markov Random Field (MRF) where some nodes are observed, and the goal is to infer the edge weights between nodes as well as the value of unobserved nodes. We formulate a solution based on the Expectation-Maximization (EM) algorithm, and provide a graph neural network (GNN) approach to approximate this inference.</p><p>Our model provides a unified framework which is applicable to diverse dialog settings (detailed in § 4). Besides, it provides extra post-hoc interpretability to show the dialog structures through an implicit learning manner. We evaluate the performance of our method on VisDial v0.9 <ref type="bibr" target="#b8">[9]</ref>, VisDial v1.0 <ref type="bibr" target="#b8">[9]</ref> and VisDial-Q <ref type="bibr" target="#b21">[22]</ref> datasets. The experimental results prove that our model is able to automatically parse the dialog structure and infer reasonable answer, and further achieves promising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image Captioning aims to annotate images with natural language at the scene level automatically, which has been a long-term active research area in computer vision community. Early work <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b17">18]</ref> typically tackled this task as a retrieval problem, i.e., finding the best fitting caption from a set of predetermined caption templates. Modern methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b58">59]</ref> were mainly based on a CNN-RNN framework, where the RNN leverages the CNN-representation of an input image to output a word sequence as the caption. In this way, they were freed from dependence of the predefined, expression-limited caption candidate pool. After that, some methods <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35]</ref> tried to integrate the vanilla CNN-RNN architecture with neural attention mechanisms, like semantic attention <ref type="bibr" target="#b34">[35]</ref>, and bottom-up/top-down attention <ref type="bibr" target="#b0">[1]</ref>, to name a few representative ones. Another popular trend <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b5">6]</ref> in this area focuses on improving the discriminability of caption generations, such as stylized image captioning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6]</ref>, personalized image captioning <ref type="bibr" target="#b46">[47]</ref>, and context-aware image captioning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Visual Question Answering focuses on providing a natural language answer given an image and a free-form, openended question. It is a more recent (dated back to <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b1">2]</ref>) and challenging task (need to access information from both the question and image). With the availability of largescale datasets <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>, numerous VQA models were proposed to build multimodal representations upon the CNN-RNN architecture <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49]</ref>, and recently extended with differentiable attentions <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38]</ref>. Rather than above classification-based VQA models, there were some other work <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b2">3]</ref> leveraged answer representations into the VQA reasoning, i.e., predicting whether or not an image-question-answer triplet is correct. Teney et al. <ref type="bibr" target="#b56">[57]</ref> proposed to solve VQA with graph-structured representations of both visual content and questions, showing the advantages of graph neural network in such structurerich problems. Narasimhan et al. <ref type="bibr" target="#b43">[44]</ref> applied graph convolution networks for factual VQA. However, there are some notable differences between our model and <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b43">44]</ref> in the fundamental idea and theoretical basis, besides the specific tasks. First, we model the visual dialog task as a problem of inference over a graph with partially observed data and unknown dialog structures. This is one step further than propagating information over a fixed graph structure. Second, we emphasize both graph structure inference (in a unsupervised manner) and unobserved node reasoning. Last, the proposed model provides an end-to-end network architecture to approximate the EM solution and offers a new glimpse into the visual dialog task.</p><p>Visual Dialog refers to the task of answering a sequence of questions about an input image <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. It is the latest vision-and-language problem, after the popularity of image captioning and visual question answering. It requires to reason about the image, the on-going question, as well as the past dialog history. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b10">[11]</ref> represented two early attempts towards this direction, but with different dialog settings. In <ref type="bibr" target="#b8">[9]</ref>, a VisDial dataset is proposed and the questions in this dataset are free-form and may concern arbitrary content of the images. Differently, in <ref type="bibr" target="#b10">[11]</ref>, a 'Guess-What' game is designed to identify a secret object through a series of yes/no questions. Following <ref type="bibr" target="#b8">[9]</ref>, Lu et al. <ref type="bibr" target="#b33">[34]</ref> introduced a generator-discriminator architecture, where the generator are improved using a perceptual loss from the pre-trained discriminator. In <ref type="bibr" target="#b50">[51]</ref>, a neural attention mechanism, called Attention Memory, is proposed to resolve the current reference in the dialog. Das et al. <ref type="bibr" target="#b9">[10]</ref> then extended <ref type="bibr" target="#b8">[9]</ref> with an 'image guessing' game, i.e., finding a desired image from a lineup of images through multi-round dialog. Reinforcement Learning (RL) was used to tackle this task. Later methods to visual dialog include applying Parallel Attention to discover the object through dialog <ref type="bibr" target="#b66">[67]</ref>, learning a conditional variational auto-encoder for generating entire sequences of dialog <ref type="bibr" target="#b40">[41]</ref>, unifying visual question generation and visual question answering in a dual learning framework <ref type="bibr" target="#b21">[22]</ref>, combining RL and Generative Adversarial Networks (GANs) to generate more human-like answers <ref type="bibr" target="#b61">[62]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, a discriminative visual dialog model was proposed and a new evaluation protocol was designed to test the questioner side of visual dialog. More recently, <ref type="bibr" target="#b27">[28]</ref> used a neural module network to solve the problem of visual coreference resolution. Graph Neural Networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">50]</ref> draw a growing interest in the machine learning and computer vision communities, with the goal of combining structural representation of graph/graphical models with neural networks. There are two main stream of approaches. One stream is to design neural network operations to directly operate on graphstructured data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. Another stream is to build graphically structured neural networks to approximate the learning/inference process of graphical models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b7">8]</ref>. Our method falls into this category. Some of these methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48]</ref> implement every graph node as a small neural network and formulate the interactions between nodes as a message propagation process, which is designed to be end-to-end trainable. Some others <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8]</ref> tried to integrate CRFs and neural networks in a fully differentiable framework, which is quite meaningful for semantic segmentation.</p><p>In this work, for the first time, we generalize the task of visual dialog to such a setting that we have partial observation of the nodes (i.e., image, caption and dialog history), and the graph structure (relations in dialog) needs to be automatically inferred. In this setting, the answer is the essentially unobserved node needs to be inferred based on the dialog graph, where the graph structure describes the dependencies among given dialog entities. We propose an essential neural network approach as an approximation to the EM solution of this problem. The proposed GNN is significantly different from most previous GNNs, which consider problems that the node features are observable, and usually a graph structure is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We begin by describing the visual dialog task setup as introduced by Das et al. <ref type="bibr" target="#b8">[9]</ref>. Formally, a visual dialog agent is given a dialog tuple D = {(I, C, H t , Q t )} as input, including an image I, a caption C, a dialog history till round t − 1, H t = {(Q k , A k ), k = 1, · · · , t − 1}, and the current question Q t being asked at round t. The visual dialog agent is required to return a response A t to the question Q t , by ranking a list of 100 candidate answers.</p><p>In our approach, we represent the entire dialog by a graph, and we solve for the optimal queried answer by a GNN as an approximate inference (see <ref type="figure" target="#fig_2">Fig. 2</ref>). In this graph, the dialog entities H t = {(Q k , A k ), k = 1, · · · , t−1}, Q t , and A t are represented as nodes. The graph structure (i.e., edges) represents the semantic dependencies between those nodes. The joint distribution of all the question and answer nodes are described by a Markov Random Field, where the values for some nodes are observed (i.e., the history questions &amp; answers, the current question). The node value for the current answer is unknown, and the model needs to infer its value as well as the graph structure encoded by the edge weights in this MRF.</p><p>The joint distribution in this MRF over all the nodes is specified by its potential functions and the graph structure. The potential functions can be learned in the training phase to maximize the likelihood of the training data, and used for inference in the testing phase. However, we cannot learn a fixed graph structure for all dialogs since they are different from dialog to dialog. For dialogs in both training and testing, we need to automatically infer the semantic structures.</p><p>In addition, because there is no label (also is hard to obtain) for such graph structures, our model needs to infer them in an unsupervised manner. Viewing the input nodes (i.e., the history questions &amp; answers, the current question) as observed data, the queried answer node as missing data, we adopt an EM algorithm to recover both the distribution parameter (the edge weights) and the missing data (the current answer). In this algorithm, the edge weights and the queried answer node are inferred to maximize the expected log likelihood. Finally, we resemble this inference process by a GNN approach, in which the node values and edge weights are updated in an iterative fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dialog as Markov Random Field</head><p>We model a dialog as an MRF, in which the nodes represent questions and answers and the edges encode semantic dependencies. Specifically, in a fully connected MRF model, the joint probability of all the nodes v is:</p><formula xml:id="formula_0">p(v) = 1 Z exp {− i φu(vi)− (i,j)∈E φp(vi, vj)}, (1) where Z is a normalizing constant, φ u (v i ) is the unary potential function, and φ p (v i , v j )</formula><p>is the pairwise potential function.</p><p>In our task, we want to learn a general potential function for all dialogs. We also want to maintain soft relations between nodes (i.e., a connectivity between 0 and 1) instead of just binary relations. Hence we generalize the above form to an MRF with 0 ∼ 1 weighed edges:</p><formula xml:id="formula_1">p(v|W ) = 1 Z exp {− i wiφu(vi) − i,j wijφp(vi, vj)} = 1 Z exp {−Tr(W T Φ(v))},<label>(2)</label></formula><p>where w i and w i,j are the weights that compose the edge weight matrix W . Note that we write Φ(v) the potential matrix as a compact form of all the potentials between nodes, </p><formula xml:id="formula_2">where Φ i,i = φ u (v i ) and Φ i,j = φ p (v i , v j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference with Partial Observation</head><p>Next we briefly review EM as a typical approach to do inference with missing data. Suppose we have observed data x and unobserved data z, whose joint distribution is parametrized by θ. The goal is to infer the most likely parameter θ and random variable z. The EM algorithm optimizes the expected log likelihood:</p><formula xml:id="formula_3">Q(θ, θ old ) = z p(z|x, θ old ) log p(x, z|θ)dz.<label>(3)</label></formula><p>An EM algorithm is an iterative process of two steps: expectation (E-step) and maximization (M-step). In the Estep, the above expected likelihood is computed. In the Mstep, the parameter θ is optimized to maximize this objective:</p><formula xml:id="formula_4">θ = argmax θ Q(θ, θ old ).<label>(4)</label></formula><p>The EM iteration always increases the observed data likelihood and terminates when a local minimum is found. However, the expected log likelihood Eq. 3 is often intractable. In the visual dialog task, to compute this quantity we need to enumerate all possible answers to the current question in the entire language space. In practice, we can use an surrogate objective in the E-step, in which we compute the plug-in approximation <ref type="bibr" target="#b57">[58]</ref> by a maximum a posteriori (MAP) estimate:</p><formula xml:id="formula_5">Q(θ, θ old ) = max z p(z|x, θ old ) log p(x, z|θ).<label>(5)</label></formula><p>Then in the M-step we update the θ according to this surrogate objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MRF with Partial Observations</head><p>In the visual dialog task, the question &amp; answer history and the current question is given, hence we know the values for those nodes in the MRF. The task is to find out the missing value of the current answer node and the underlying sementic structure. Suppose in an MRF, we observe some nodes in the graph and we do not know the edge weights W . Denote the observed nodes as x and the unobserved nodes as z, where v = x ∪ z and x ∩ z = ∅. Here the weight matrix W parametrizes the joint distribution of x and z, hence it can be viewed as the θ in the previous section. To jointly infer W the graph structure (e.g., the semantic dependencies) and z the missing values (e.g., the queried answer), we run an EM algorithm: E-step: We compute z * = argmax z p(z|x, W old ) to obtaiñ Q(θ, θ old ) in Eq. 5. This is achieved by a max-product loopy belief propagation <ref type="bibr" target="#b60">[61]</ref>. At every iteration, each node sends a (different) message to each of its neighbors and receives a message from each neighbor. After receiving message from neighbors, the belief b(v i ) for each node v i is updated by the max-product update rule:</p><formula xml:id="formula_6">b(vi) = αφu(vi) v j ∈N (v i ) mji(vi),<label>(6)</label></formula><p>where α is a normalizing constant, N (v i ) denotes the neighbor nodes of v i , and m ji (v i ) is the message from v j to v i . The message is given by:</p><formula xml:id="formula_7">mji(vi) = max v j wij φp(vi, vj) v k ∈N (v j )\v i</formula><p>m kj (vj). <ref type="bibr" target="#b6">(7)</ref> where N (v j ) \ v i indicates the all the neighboring nodes of v j except v i . M-step: Based on the estimated z * in the E-step, we want to find the edge weights that maximizes the objective Eq. 5:</p><formula xml:id="formula_8">W = argmax WQ (W, W old ) = argmax W p(z * |x, W old ) log p(x, z * |W ) = argmax W log p(x, z * |W ).<label>(8)</label></formula><p>The M-step together with E-step forms a coordinate descent algorithm in the objective functionQ(W, W old ). This algorithm contains two loops: an outer loop of inferring z and θ alternatively, and an inner loop of inferring the missing values z by iterative belief propagation. <ref type="figure">Figure 3</ref>. A detailed illustration of our model. The left part shows feature extractions for each node, which serve as the initializations for node hidden states. After a few EM iterations, we obtain the hidden state (embedding) for the unobserved node (the queried answer). To choose the best answer from the pre-defined options, we use the dot product between the node and option embeddings as a similarity score. The scores are turned into probabilities by softmax activation, and a cross entropy loss is computed to train the network.</p><p>Note that in the partial observed case, for the E-step we fix the observed nodes v x ∈ x and only update the unobserved nodes v z ∈ z. Hence we also only need to compute messages from observed nodes to unobserved nodes. The message passing and belief update process iterate until convergence. When the iteration terminates, we obtain an MAP estimate z * for the missing values, conditioned on the observed nodes x and current estimated edge weights W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">GNN with Partial Observations</head><p>We design a GNN for the visual dialog task guided by the above formulations. The network is structured as an MRF, in which the caption and each question/answer pair is represented as a node embedding, and the semantic relations are represented by edges. The model contains three different neural modules: message functions, update functions, and link functions. These modules are called iteratively to emulate the above EM algorithm. E-step: We perform a neural message passing/belief propagation <ref type="bibr" target="#b16">[17]</ref> for an approximate inference of missing values z * . This process emulates the belief propagation in the Estep. For each node, we use an hidden state/embedding to represent its value. During belief propagation, the observed variables x and the edge weights W are fixed. The hidden states of the unobserved nodes are iteratively updated by communicating with other nodes. Specially, we use message functions M (·) to summarize messages to nodes coming from other nodes, and update functions U (·) to update the hidden node states according to the incoming messages.</p><p>At each iteration step s, the update function computes a new hidden state for a node after receiving incoming messages:</p><formula xml:id="formula_9">h s v i = U (h s−1 v i , m s v i ),<label>(9)</label></formula><p>where h s v is the hidden state/embedding for node v. m s v is the summarized incoming message for node v at s-th iteration. The messages are given by:</p><formula xml:id="formula_10">m s v i = v j ∈N (v i ) wijM (h s−1 v i , h s−1 v j ).<label>(10)</label></formula><p>The message passing phase runs for S iterations towards convergence. At the first iteration, the node hidden states h 0 v are initialized by node features F v . M-step: Based on the updated hidden states of all the nodes in the E-step, we update the edge weights W by link functions. A link function L(·) estimates the connectivity w ij between two nodes v i and v j based on their current hidden states:</p><formula xml:id="formula_11">wij = L(hv i , hv j ).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Network Architecture</head><p>At each round of the dialog, we aim to answer the query question based on the image, caption, and the question &amp; answer (QA) history. For dialog round t, we construct t+1 nodes in which one node represents the caption, t−1 nodes represents the history of t−1 rounds of QAs, and one last node represents the answer to the current query question. The embedding for each node is initialized by fusing the image feature and the language embedding of the corresponding sentence(s). As shown in <ref type="figure">Fig. 3</ref>, for the caption node we extract the language embedding of the caption, and fuse it with the image feature as an initialization. For the last node representing the queried answer, we use the corresponding question embedding fused with the image feature to initialize the hidden state. For the rest nodes, the hidden states are initialized by fusing the QA embedding and the image feature. The fusing of language embeddings and image features are achieved by co-attention techniques <ref type="bibr" target="#b35">[36]</ref>, and more details are introduced in §4. The goal of our approach is to infer the hidden state of the queried answer by the emulated EM algorithm.</p><p>After initializing the node hidden states with feature embeddings, we start the iterative inference by first estimating the edge weights. The edge weights are estimated by Eq. 11, where the link function is given by a dot product between transformed hidden states:</p><formula xml:id="formula_12">wij = L(hv i , hv j ) = fc(hv i ), fc(hv j )<label>(12)</label></formula><p>where ·, · denotes dot product, and fc(·) denotes multiple Initialize hv x to be Fv x 4: end for 5: for each unobserved node vz ∈ z do <ref type="bibr">6:</ref> Initialize hv z to be the question embedding 7: end for 8: /* Expectation-Maximization: outer loop */ 9: while not converged do To stabilize the training of the update function, we normalize the sum of weights for edges coming into one node to 1 by a softmax function. Then the node hidden state is update by a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_13">h s v i = U (h s−1 v i , m s v i ) = GRU(h s−1 v i , m s v i ).<label>(13)</label></formula><p>Here the GRU is chosen for two reasons. First, Eq. 13 has a natural recurrent form. GRU is one type of Recurrent Neural Networks (RNN) that known to be more computationally efficient than Long short-term memory (LSTM). Second, Li et al. <ref type="bibr" target="#b28">[29]</ref> has shown that GRU performs well in GNNs as update functions. The algorithm stops after several iterations of the outer loop for EM, in which the edge weights W and the node hidden states h v are updated alternatively. Inside each iteration, an inner loop is performed to update the node hidden states. The inner loop emulates the E-step, where a belief propagation is performed. The algorithm is illustrated in Alg. 1. For the visual dialog task, the set of unobserved nodes include only the node that represents the current queried answer.</p><p>Finally, we regard the hidden state of the last node as the embedding of the queried answer.</p><p>To choose one answer from the pre-defined options provided by the dataset, we compute h v , h o where h v is the node hidden state from the last node and h o is the language embedding for an option. A softmax activation function is applied to those dot products, and a multi-class cross entropy loss is computed to train the GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>4.1. Performance on VisDial v0.9 <ref type="bibr" target="#b8">[9]</ref> Dataset: We first evaluate the proposed approach on Vis-Dial v0.9 <ref type="bibr" target="#b8">[9]</ref>, which was collected via two Amazon Mechanical Turk (AMT) subjects chatting about an image. The first person is allowed to see only the image caption, and instructed to ask questions about the hidden image to better understand the scene. The second worker has access to both image and caption, and is asked to answer the first person's questions. Both are encouraged to talk in a natural manner. Their conversation is ended after 10 rounds of question answering. VisDial v0.9 contains a total of 1,232,870 dialog question-answer pairs on MSCOCO images <ref type="bibr" target="#b31">[32]</ref>. It is split into 80K for train, 3K for val and 40K as the test, in a manner consistent with <ref type="bibr" target="#b8">[9]</ref>. Evaluation Protocol: We follow <ref type="bibr" target="#b8">[9]</ref> to evaluate individual responses at each round (t = 1, 2, · · · , 10) in a retrieval setup. Specifically, at test time, every question is coupled with a list of 100 candidate answer options, which a VisDial model is asked to return a sorting of the candidate answers. The model is evaluated on standard retrieval metrics <ref type="bibr" target="#b8">[9]</ref>: Recall@1, Recall@5, Recall@10, Mean Reciprocal Rank (MRR), and Mean Rank of human response. Lower value for MR and higher values for all the other metrics are desirable. Data Preparation: To pre-process the data, we first resize each image into 224×224 resolution, and use the output of the last pooling layer (pool5) of VGG-19 <ref type="bibr" target="#b53">[54]</ref> as the image feature (512×7×7). For the text data, i.e., caption, questions and answers, we convert digits to words, and remove contractions, before tokenizing. The captions, questions, answers longer than 40, 20, 20 words respectively are truncated. All the texts in the experiment are lowercased. Each word is then turned into a vector representation with a look-up is implemented as a one-layer GRU with 512 hidden states.</p><p>We use a single Titan Xp GPU to train the network with a batch size of 32. In the experiments, we use the Adam optimizer with a base learning rate of 1e-3 further decreasing to 5e-5. The training converges after ∼5 epochs.</p><p>Quantitative Results: We compare our method with several state-of-the-art discriminative dialog models, i.e., LF <ref type="bibr" target="#b8">[9]</ref>, HRE <ref type="bibr" target="#b8">[9]</ref>, HREA <ref type="bibr" target="#b8">[9]</ref>, MN <ref type="bibr" target="#b8">[9]</ref>, SAN-QI <ref type="bibr" target="#b63">[64]</ref>, HieCoAtt-QI <ref type="bibr" target="#b35">[36]</ref>, AMEM <ref type="bibr" target="#b50">[51]</ref>, HCIAE-NP-ATT <ref type="bibr" target="#b33">[34]</ref>, SF <ref type="bibr" target="#b21">[22]</ref>, and SCA <ref type="bibr" target="#b61">[62]</ref>. <ref type="table">Table 1</ref> summarizes the quantitative results of above competitors and our model. Our model consistently outperforms most approaches, highlighting the importance of understanding the dependencies in visual dialog. Specifically, our R@k (k = 1, 5, 10) is at least 0.4 point higher than SF. Our method only performs slightly worse than SCA, which adopts adversarial learning techniques.</p><p>Qualitative Results: <ref type="figure" target="#fig_4">Fig. 4</ref> shows some qualitative results of our model. We summarize three key observations: (i) We compare our machine selected answer with human answer and show that our model is capable of selecting meaningful yet different answers compared with the ground-truth answer. (ii) We present our inferred dialog structure according to the edge weight between each pair of nodes. We show that the edge weight is relatively high when the correlation between the node pairs is strong. (iii) <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_4">Fig. 4</ref>  Quantitative Results: Five discriminative dialog models (i.e., LF <ref type="bibr" target="#b8">[9]</ref>, HRE <ref type="bibr" target="#b8">[9]</ref>, MN <ref type="bibr" target="#b8">[9]</ref>, LF-Att <ref type="bibr" target="#b8">[9]</ref>, MN-Att <ref type="bibr" target="#b8">[9]</ref>) were included in our experiments. <ref type="table" target="#tab_2">Table 2</ref> presents the overall quantitative comparison results. As seen, the suggested model consistently gaining promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance on</head><p>VisDial-Q Dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> Dataset: VisDial Dataset <ref type="bibr" target="#b8">[9]</ref> provides a solid foundation for assessing the performance of a visual dialog system answering questions. To test the questioner side of visual dialog, Jain et al. <ref type="bibr" target="#b21">[22]</ref> further propose a VisDial-Q dataset, which is built upon VisDial v0.9 <ref type="bibr" target="#b8">[9]</ref>. The dataset splitting is the same as VisDial v0.9. Evaluation Protocol: VisDial-Q dataset is companied with a retrieval based 'VisDial-Q evaluation protocol', analogous to the 'VisDial evaluation protocol' in VisDial dataset detailed before. A visual dialog system is required to choose one out of 100 next questions for a given questionanswer pair. Similar methodology in <ref type="bibr" target="#b8">[9]</ref> is adopted to collect the 100 follow-up question candidates. Therefore, the metrics described in § 4.1: Recall@k, MRR, and Mean Rank, are also used for quantitative evaluation. Data Preparation: We use the same text embedding techniques as we used for §4.1. Different from VisDial task, the first round of QA pair is given to predict next round of question. Thus the maximum round of dialog in the VisDial-Q task is set as 9. Similar as we illustrate in §3.5, we construct t + 1 node with caption and previous history as the first t nodes and the expected question as the last node. We initialize our question node with language embedding of the caption and set the language embedding of corresponding sentence as the embedding of the rest of nodes.</p><p>Quantitative Results: We follow the same protocol described in <ref type="bibr" target="#b21">[22]</ref> to evaluate our model. <ref type="table">Table 3</ref> shows the quantitative results for comparative methods and our ablative model variants. The ablative models include i) our model with constant graph (all edge weights are 1), and ii) our model without the EM iterations. Our full model with   <ref type="table">Table 3</ref>. Quantitative evaluation on VisDial-Q dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> with VisDial-Q evaluation protocol. See §4.3 for more details.</p><p>3 EM iterations outperforms the comparative method in all evaluation metrics. Particularly, we can see that our model with constant graph has a similar performance to the comparative method. This shows the effectiveness of our EMbased inference process. Experiment results on this dataset also shows the generality of our approach: it can infer the underlying dialog structure and reason accordingly about unobserved nodes (next question or current answer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Diagnostic Experiments</head><p>To assess the effect of some essential component of our model, we implement and test several variants: (i) constant graph that fixes edge weight between each pair of nodes to be 1; (ii) graph without EM iteration; and (iii) graph with n EM iterations. <ref type="table">Table 4</ref> shows the quantitative evaluations of these model variants on VisDial v0.9 <ref type="bibr" target="#b8">[9]</ref>. We summarize our observations here: (a) model without EM iterations performs the worst among all variants. This shows the importance of iteratively updating the node embeddings. (b) In our experiments, message passing with 3 iterations shows the best performance of our proposed model. (c) model using constant graph (3 iterations) performs better than worse than the model without EM iterations, since it allows iter-  <ref type="table">Table 4</ref>. Ablation study of the key components of our methods on VisDial v0.9 dataset <ref type="bibr" target="#b8">[9]</ref>. See §4.4 for more details.</p><formula xml:id="formula_14">Methods MRR ↑ R@1 ↑ R@5 ↑ R@10 ↑ Mean ↓ Ours (3 iter</formula><p>ative updates of node embeddings. However, it is outperformed by other iterative models with a dynamic structure, since all incoming messages are treated equally. This shows the importance of edge weights: they filter out misleading messages while allowing information flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we develop a novel model for the visual dialog task. The backbone of this model is a GNN, in which each node represents a dialog entity and the edge weights represent the semantic dependencies between nodes. An EM-style inference algorithm is proposed for this GNN to estimate the latent relations between nodes and the missing values of unobserved nodes. Experiments are performed on the VisDial and VisDial-Q dataset. Results show that our method is able to find and utilize underlying dialog structures for dialog inference in both tasks, demonstrating the generality and effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>C:</head><label></label><figDesc>A living area with music mixing equipment and a desk. Q1: Is the desk made of wood? A1: Yes. Q2: What color is it? A2: Brown. Q5: Do you see a chair? A5: Yes. Q7: Is it close to the desk? What color is it? A6: A black office chair. Predicted Answer: Yes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of the visual dialog task. Left: context image. Middle: image caption, dialog history, current query question, and the predicted answer. Right: the underlying semantic dependencies between nodes in the dialog (darker green links indicate higher dependencies).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The visual dialog is represented by a GNN, in which the dialog entities (i.e., caption, question &amp; answer pairs, and the unobserved queried answer) are represented by nodes (embeddings). The edges represent semantic dependencies between nodes. Some nodes's values are observed (i.e., nodes that represent the dialog history), and we need to infer the missing values for the unobserved node (i.e., the queried answer) based on the underlying dialog structure. The forward pass of the network emulates an EM algorithm, in which the M-step estimates the edge weights and E-step updates all hidden node states (embeddings) by neural message passing. After a few iterations, the hidden state for the unobserved node (answer) contains the inferred embedding for the missing value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for each node pair (vi, vj) do12:    wij = L(hv i , hv j ) = fc(hv i ), fc(hv j ) -step: inner loop for message passing */15:    for step s from 1 to S do16:    for each vz ∈ z do17: /* Compute incoming message for vi */ 18: m s vz = v j ∈N (vz ) wzjh s−1 v j 19: /* Update embedding for unobserved vi */ 20: h s vz = U (h s−1 vz , m s vz ) = GRU(h s−1 vz , m s vz ) end while fully connected layers with Rectified Linear Units (ReLU) in between the layers. Using M (h s−1 vi , h s−1 vj ) = h s−1 vj as the message function, the summarized message from all neighbor nodes is computes as m s vi = vj ∈N (vi) w ij h s−1 vj .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of our model on VisDial v0.9<ref type="bibr" target="#b8">[9]</ref>, comparing to human ground-truth answer. The last column presents the visual dialog structures inferred by our model, where the more darker green links indicate higher relations (predicted by link functions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 EM for Graph Neural NetworkInput: Extracted features Fv x for observed nodes vx ∈ x Output: Graph structure W , node embeddings hv z for unobserved nodes vz ∈ z 1: /* Initialization */ 2: for each observed node vx ∈ x do</figDesc><table><row><cell>3:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table ,</head><label>,</label><figDesc>We use 2 layers of fully connected layer in Eq. 12. The update function U (·) in Eq. 13Methods MRR ↑ R@1 ↑ R@5 ↑ R@10 ↑ Mean↓ LF [9] 0.5807 43.82 74.68 84.07 5.78 HRE [9] 0.5846 44.67 74.50 84.22 5.72 HREA [9] 0.5868 44.82 74.81 84.36 5.66 MN [9] 0.5965 45.55 76.22 85.37 5.46 SAN-QI [64] 0.5764 43.44 74.26 83.72</figDesc><table><row><cell>whose entries are 300-d vectors learned along other parameters during training. Thus for caption, each question and answer, we have the sequences of word embedding with size of 40×300, 20×300, and 20×300, re-spectively. The embedding of the caption, question or an-swer, is passed through a two-layered LSTM with 512 hid-den states and the output state is used as our final text em-beddings. We use the same LSTM and word embedding matrix across question, history, caption and options. Implementation Details: 5.88 5.84 4.99 4.81 4.70 4.47 4.57 Table 1. Quantitative evaluation of discriminative methods on HieCoAtt-QI [36] 0.5788 43.51 74.49 83.96 AMEM [51] 0.6160 47.74 78.04 86.84 HCIAE-NP-ATT [34] 0.6222 48.48 78.75 87.59 SF [22] 0.6242 48.55 78.96 87.75 SCA [62] 0.6398 50.29 80.71 88.81 Ours 0.6285 48.95 79.65 88.36 val split of VisDial v0.9 [9]. Our model outperforms most com-petitors. See  §4.1 for more details.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>In addition to the five evaluationMethods MRR ↑ R@1 ↑ R@5 ↑ R@10 ↑ Mean ↓ NDCG ↑LF<ref type="bibr" target="#b8">[9]</ref> 0.5542 40.95 72.45 82.83 Quantitative evaluation of discriminative methods on test-standard split of VisDial v1.0<ref type="bibr" target="#b8">[9]</ref>. Our model outperforms all other models across all metrics. See §4.2 for more details.</figDesc><table><row><cell>il-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Methods MRR ↑ R@1 ↑ R@5 ↑ R@10 ↑ Mean ↓ SF-QI [22] 0.3021 17.38 42.32 57.16 14.03 SF-QIH [22] 0.4060 26.76 55.17 70.39 9.32 Ours (w/o iter) 0.3977 25.69 54.52 70.33 9.38 Ours (const. graph) 0.4025 26.08 55.30 70.83 9.24 Ours (full, 3 iter) 0.4126 27.15 56.47 71.97 8.86</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep attention neural tensor network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Groupcap: Group-based image captioning with structured relevance and diversity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">factual&quot; or &quot;emotional&quot;: Stylized image captioning with adaptive learning and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CRF-CNN: Modeling structured information in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">and Dhruv Batra. Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guesswhat?! Visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stylenet: Generating attractive visual captions with styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? Dataset and methods for multilingual image question</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two can play this game: Visual dialog with discriminative question generation and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unnat</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ian Reid, and Anton van den Hengel. Deeply learning the messages in message passing inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Ian Reid</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminability objective for training descriptive captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning visual question answering by bootstrapping hard attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flipdial: A generative model for two-way visual dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semstyle: Learning to generate stylised image captions using unaligned text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<title level="m">Geometric deep learning on graphs and manifolds using mixture model cnns. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Out of the box: Reasoning with graph convolution nets for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medhini</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attend to you: Personalized image captioning with context sequence memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Cesc Chunseong Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NEURIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Anton van den Hengel. Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cambridge university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asymptotic statistics</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attentive fashion grammar network for fashion landmark detection and clothing category classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William T Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Are you talking to me? Reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ian D Reid, and Anton van den Hengel. Parallel attention: A unified framework for visual object discovery through dialogs and queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
