<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Switching Convolutional Neural Network for Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><forename type="middle">Babu</forename><surname>Sam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
							<email>shiv.surya314@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Switching Convolutional Neural Network for Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel crowd counting model that maps a given crowd scene to its density. Crowd analysis is compounded by myriad of factors like inter-occlusion between people due to extreme crowding, high similarity of appearance between people and background elements, and large variability of camera view-points. Current state-of-the art approaches tackle these factors by using multi-scale CNN architectures, recurrent networks and late fusion of features from multi-column CNN with different receptive fields. We propose switching convolutional neural network that leverages variation of crowd density within an image to improve the accuracy and localization of the predicted crowd count. Patches from a grid within a crowd scene are relayed to independent CNN regressors based on crowd count prediction quality of the CNN established during training. The independent CNN regressors are designed to have different receptive fields and a switch classifier is trained to relay the crowd scene patch to the best CNN regressor. We perform extensive experiments on all major crowd counting datasets and evidence better performance compared to current stateof-the-art methods. We provide interpretable representations of the multichotomy of space of crowd scene patches inferred from the switch. It is observed that the switch relays an image patch to a particular CNN column based on density of crowd.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crowd analysis has important geo-political and civic applications. Massive crowd gatherings are commonplace at candle-light vigils, democratic protests, religious gatherings and presidential rallies. Civic agencies and planners rely on crowd estimates to regulate access points and plan disaster contingency for such events. Critical to such analysis is crowd count and density.</p><p>In principle, the key idea behind crowd counting is self- * Equal contribution <ref type="figure">Figure 1</ref>. Sample crowd scenes from the ShanghaiTech dataset <ref type="bibr" target="#b21">[22]</ref> is shown. evident: density times area. However, crowds are not regular across the scene. They cluster in certain regions and are spread out in others. Typical static crowd scenes from the ShanghaiTech Dataset <ref type="bibr" target="#b21">[22]</ref> are shown in <ref type="figure">Figure 1</ref>. We see extreme crowding, high visual resemblance between people and background elements (e.g. Urban facade) in these crowd scenes that factors in further complexity. Different camera view-points in various scenes create perspective effects resulting in large variability of scales of people. Crowd counting as a computer vision problem has seen drastic changes in the approaches, from early HOG based head detections <ref type="bibr" target="#b7">[8]</ref> to CNN regressors <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref> predicting the crowd density. CNN based regressors have largely outperformed traditional crowd counting approaches based on weak representations from local features. We build on the performance of CNN based architectures for crowd counting and propose Switching Convolutional Neural Network (Switch-CNN) to map a given crowd scene to its density.</p><p>Switch-CNN leverages the variation of crowd density within an image to improve the quality and localization of the predicted crowd count. Independent CNN crowd density regressors are trained on patches sampled from a grid in a given crowd scene. The independent CNN regressors are chosen such that they have different receptive fields and field of view. This ensures that the features learned by each CNN regressor are adapted to a particular scale. This renders Switch-CNN robust to large scale and perspective variations of people observed in a typical crowd scene. A particular CNN regressor is trained on a crowd scene patch if the performance of the regressor on the patch is the best. A switch classifier is trained alternately with the training of multiple CNN regressors to correctly relay a patch to a particular regressor. The joint training of the switch and regressors helps augment the ability of the switch to learn the complex multichotomy of space of crowd scenes learnt in the differential training stage.</p><p>To summarize, in this paper we present:</p><p>• A novel generic CNN architecture, Switch-CNN trained end-to-end to predict crowd density for a crowd scene. • Switch-CNN maps crowd patches from a crowd scene to independent CNN regressors to minimize count error and improve density localization exploiting the density variation within a scene. • We evidence state-of-the-art performance on all major crowd counting datasets including ShanghaiTech dataset <ref type="bibr" target="#b21">[22]</ref>, UCF CC 50 dataset <ref type="bibr" target="#b7">[8]</ref> and World-Expo'10 dataset <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Crowd counting has been tackled in computer vision by a myriad of techniques. Crowd counting via head detections has been tackled by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> using motion cues and appearance features to train detectors. Recurrent network framework has been used for head detections in crowd scenes by <ref type="bibr" target="#b15">[15]</ref>. They use the deep features from Googlenet <ref type="bibr">[16]</ref> in an LSTM framework to regress bounding boxes for heads in a crowd scene. However, crowd counting using head detections has limitations as it fails in dense crowds, which are characterized by high inter-occlusion between people.</p><p>In crowd counting from videos, <ref type="bibr" target="#b2">[3]</ref> use image features like Tomasi-Kanade features into a motion clustering framework. Video is processed by <ref type="bibr" target="#b12">[12]</ref> into a set of trajectories using a KLT tracker. To prevent fragmentation of trajectories, they condition the signal temporally and spatially. Such tracking methods are unlikely to work for single image crowd counting due to lack of temporal information.</p><p>Early works in still image crowd counting like <ref type="bibr" target="#b7">[8]</ref> employ a combination of handcrafted features, namely HOG based detections, interest points based counting and Fourier analysis. These weak representations based on local features are outperformed by modern deep representations. In <ref type="bibr" target="#b20">[21]</ref>, CNNs are trained to regress the crowd density map. They retrieve images from the training data similar to a test image using density and perspective information as the similarity metric. The retrieved images are used to fine-tune the trained network for a specific target test scene and the density map is predicted. However, the model's applicability is limited by fine-tuning required for each test scene and perspective maps for train and test sequences which are not readily available. An Alexnet <ref type="bibr" target="#b8">[9]</ref> style CNN model is trained by <ref type="bibr" target="#b17">[18]</ref> to regress the crowd count. However, the application of such a model is limited for crowd analysis as it does not predict the distribution of the crowd. In <ref type="bibr" target="#b10">[11]</ref>, a multi-scale CNN architecture is used to tackle the large scale variations in crowd scenes. They use a custom CNN network, trained separately for each scale. Fully-connected layers are used to fuse the maps from each of the CNN trained at a particular scale, and regress the density map. However, the counting performance of this model is sensitive to the number of levels in the image pyramid as indicated by performance across datasets.</p><p>Multi-column CNN used by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> perform late fusion of features from different CNN columns to regress the density map for a crowd scene. In <ref type="bibr" target="#b21">[22]</ref>, shallow CNN columns with varied receptive fields are used to capture the large variation in scale and perspective in crowd scenes. Transfer learning is employed by <ref type="bibr" target="#b1">[2]</ref> using a VGG network employing dilated layers complemented by a shallow network with different receptive field and field of view. Both the model fuse the feature maps from the CNN columns by weighted averaging via a 1×1 convolutional layer to predict the density map of the crowd. However, the weighted averaging technique is global in nature and does not take in to account the intra-scene density variation. We build on the performance of multi-column CNN and incorporate a patch based switching architecture in our proposed architecture, Switch-CNN to exploit local crowd density variation within a scene (see Sec 3.1 for more details of architecture).</p><p>While switching architectures have not been used for counting, expert classifiers have been used by <ref type="bibr" target="#b13">[13]</ref> to improve single object image classification across depiction styles using a deep switching mechanism based on depiction style. However unlike <ref type="bibr" target="#b13">[13]</ref>, we do not have labels (For eg: Depiction styles like "art" and "photo") to train the switch classifier. To overcome this challenge, we propose a training regime that exploits CNN regressor's architectural differences (See Section 3.1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Convolutional architectures like <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref> have learnt effective image representations, which they leverage to perform crowd counting and density prediction in a regression framework. Traditional convolutional architectures have been modified to model the extreme variations in scale induced in dense crowds by using multi-column CNN architectures with feature fusion techniques to regress crowd density.</p><p>In this paper, we consider switching CNN architecture (Switch-CNN) that relays patches from a grid within a <ref type="figure">Figure 2</ref>. Architecture of the proposed model, Switch-CNN is shown. A patch from the crowd scene is highlighted in red. This patch is relayed to one of the three CNN regressor networks based on the CNN label inferred from Switch. The highlighted patch is relayed to regressor R3 which predicts the corresponding crowd density map. The element-wise sum over the entire density map gives the crowd count of the crowd scene patch. crowd scene to independent CNN regressors based on a switch classifier. The independent CNN regressors are chosen with different receptive fields and field-of-view as in multi-column CNN networks to augment the ability to model large scale variations. A particular CNN regressor is trained on a crowd scene patch if the performance of the regressor on the patch is the best. A switch classifier is trained alternately with the training of multiple CNN regressors to correctly relay a patch to a particular regressor. The salient properties that make this model excellent for crowd analysis are (1) the ability to model large scale variations (2) the facility to leverage local variations in density within a crowd scene. The ability to leverage local variations in density is important as the weighted averaging technique used in multi-column networks to fuse the features is global in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Switch-CNN</head><p>Our proposed architecture, Switch-CNN consists of three CNN regressors with different architectures and a classifier (switch) to select the optimal regressor for an input crowd scene patch. <ref type="figure">Figure 2</ref> shows the overall architecture of Switch-CNN. The input image is divided into 9 non-overlapping patches such that each patch is 1 3 rd of the image. For such a division of the image, crowd characteristics like density, appearance etc. can be assumed to be consistent in a given patch for a crowd scene. Feeding patches  We use three CNN regressors introduced in <ref type="bibr" target="#b21">[22]</ref>, R1 through R3, in Switch-CNN to predict the density of crowd. These CNN regressors have varying receptive fields that can capture people at different scales. The architecture of each of the shallow CNN regressor is similar: four convolutional layers with two pooling layers. R1 has a large initial filter size of 9×9 which can capture high level abstractions within the scene like faces, urban facade etc. R2 and R3 with initial filter sizes 7×7 and 5×5 capture crowds at lower scales detecting blob like abstractions.</p><formula xml:id="formula_0">input : N training image patches {Xi} N i=1 with ground truth density maps {D GT X i } N i=1</formula><formula xml:id="formula_1">for i = 1 to N do l best i = argmin k |C k i − C GT i |; end Strain = {(Xi, l best i ) | i ∈ [1, N ]} /*</formula><p>Patches are relayed to a regressor using a switch. The switch consists of a switch classifier and a switch layer. The switch classifier infers the label of the regressor to which the patch is to be relayed to. A switch layer takes the label inferred from the switch classifier and relays it to the correct regressor. For example, in <ref type="figure">Figure 2</ref>, the switch classifier relays the patch highlighted in red to regressor R3. The patch has a very high crowd density. Switch relays it to regressor R3 which has smaller receptive field: ideal for detecting blob like abstractions characteristic of patches with high crowd density. We use an adaptation of VGG16 <ref type="bibr" target="#b14">[14]</ref> network as the switch classifier to perform 3-way classification. The fully-connected layers in VGG16 are removed. We use global average pool (GAP) on Conv5 features to remove the spatial information and aggregate discriminative features. GAP is followed by a smaller fully connected layer and 3-class softmax classifier corresponding to the three regressor networks in Switch-CNN.</p><p>Ground Truth Annotations for crowd images are provided as point annotations at the center of the head of a person. We generate our ground truth by blurring each head annotation with a Gaussian kernel normalized to sum to one to generate a density map. Summing the resultant density map gives the crowd count. Density maps ease the difficulty of regression for the CNN as the task of predicting the exact point of head annotation is reduced to predicting a coarse location. The spread of the Gaussian in the above density map is fixed. However, a density map generated from a fixed spread Gaussian is inappropriate if the variation in crowd density is large. We use geometry-adaptive kernels <ref type="bibr" target="#b21">[22]</ref> to vary the spread parameter of the Gaussian depending on the local crowd density. It sets the spread of Gaussian in proportion to the average distance of k-nearest neighboring head annotations. The inter-head distance is a good substitute for perspective maps which are laborious to generate and unavailable for every dataset. This results in lower degree of Gaussian blur for dense crowds and higher degree for region of sparse density in crowd scene. In our experiments, we use both geometry-adaptive kernel method as well as fixed spread Gaussian method to generate ground truth density depending on the dataset. Geometry-adaptive kernel method is used to generate ground truth density maps for datasets with dense crowds and large variation in count across scenes. Datasets that have sparse crowds are trained using density maps generated from fixed spread Gaussian method.</p><p>Training of Switch-CNN is done in three stages, namely pretraining, differential training and coupled training described in Sec 5.4-3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pretraining</head><p>The three CNN regressors R1 through R3 are pretrained separately to regress density maps. Pretraining helps in learning good initial features which improves later finetuning stages. Individual CNN regressors are trained to minimize the Euclidean distance between the estimated density map and ground truth. Let DX i (·; Θ) represent the output of a CNN regressor with parameters Θ for an input image Xi. The l2 loss function is given by</p><formula xml:id="formula_2">L l 2 (Θ) = 1 2N N i=1 DX i (·; Θ) − D GT X i (·) 2 2 ,<label>(1)</label></formula><p>where N is the number of training samples and D GT</p><formula xml:id="formula_3">X i (·)</formula><p>indicates ground truth density map for image Xi. The loss L l 2 is optimized by backpropagating the CNN via stochastic gradient descent (SGD). Here, l2 loss function acts as a proxy for count error between the regressor estimated count and true count. It indirectly minimizes count error. The regressors R k are pretrained until the validation accuracy plateaus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Differential Training</head><p>CNN regressors R1−3 are pretrained with the entire training data. The count prediction performance varies due to the inherent difference in network structure of R1−3 like receptive field and effective field-of-view. Though we optimize the l2-loss between the estimated and ground truth density maps for training CNN regressor, factoring in count error during training leads to better crowd counting performance. Hence, we measure CNN performance using count error. Let the count estimated by kth regressor for ith image be</p><formula xml:id="formula_4">C k i = x DX i (x; Θ k ) . Let the reference count inferred from ground truth be C GT i = x D GT X i (x). Then count error for ith sample evaluated by R k is EC i (k) = |C k i − C GT i |,<label>(2)</label></formula><p>the absolute count difference between prediction and true count. Patches with particular crowd attributes give lower count error with a regressor having complementary network structure. For example, a CNN regressor with large receptive field capture high level abstractions like background elements and faces. To amplify the network differences, differential training is proposed (shown in blue in Algorithm 1). The key idea in differential training is to backpropagate the regressor R k with minimum count error for a given training crowd scene patch. For every training patch i, we choose the regressor l best i such that EC i (l best i ) is lowest across all regressors R1−3. This amounts to greedily choosing the regressor that predicts the most accurate count amongst k regressors. Formally, we define the label of chosen regressor l best i as:</p><formula xml:id="formula_5">l best i = argmin k |C k i − C GT i |<label>(3)</label></formula><p>The count error for ith sample is</p><formula xml:id="formula_6">EC i = min k |C k i − C GT i |.<label>(4)</label></formula><p>This training regime encourages a regressor R k to prefer a particular set of the training data patches with particular patch attribute so as to minimize the loss. While the backpropagation of independent regressor R k is still done with l2-loss, the choice of CNN regressor for backpropagation is based on the count error. Differential training indirectly minimizes the mean absolute count error (MAE) over the training images. For N images, MAE in this case is given by</p><formula xml:id="formula_7">EC = 1 N N i=1 min k |C k i − C GT i |,<label>(5)</label></formula><p>which can be thought as the minimum count error achievable if each sample is relayed correctly to the right CNN. However during testing, achieving this full accuracy may not be possible as the switch classifier is not ideal. To summarize, differential training generates three disjoint groups of training patches and each network is finetuned on its own group. The regressors R k are differentially trained until the validation accuracy plateaus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Switch Training</head><p>Once the multichotomy of space of patches is inferred via differential training, a patch classifier (switch) is trained to relay a patch to the correct regressor R k . The manifold that separates the space of crowd scene patches is complex and hence a deep classifier is required to infer the group of patches in the multichotomy. We use VGG16 <ref type="bibr" target="#b14">[14]</ref> network as the switch classifier to perform 3-way classification. The classifier is trained on the labels of multichotomy generated from differential training. The number of training patches in each group can be highly skewed, with the majority of patches being relayed to a single regressor depending on the attributes of crowd scene. To alleviate class imbalance during switch classifier training, the labels collected from the differential training are equalized so that the number of samples in each group is the same. This is done by randomly sampling from the smaller group to balance the training set of switch classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Coupled Training</head><p>Differential training on the CNN regressors R1 through R3 generates a multichotomy that minimizes the predicted count by choosing the best regressor for a given crowd scene patch. However, the trained switch is not ideal and the manifold separating the space of patches is complex to learn. To mitigate the effect of switch inaccuracy and inherent complexity of task, we co-adapt the patch classifier and the CNN regressors by training the switch and regressors in an alternating fashion. We refer to this stage of training as Coupled training (shown in green in Algorithm 1).</p><p>The switch classifier is first trained with labels from the multichotomy inferred in differential training for one epoch (shown in red in Algorithm 1). In, the next stage, the three CNN regressors are made to co-adapt with switch classifier (shown in blue in Algorithm 1). We refer to this stage of training enforcing co-adaption of switch and regressor R1−3 as Switched differential training.</p><p>In switched differential training, the individual CNN regressors are trained using crowd scene patches relayed by switch for one epoch. For a given training crowd scene patch Xi, switch is forward propagated on Xi to infer the choice of regressor R k . The switch layer then relays Xi to the particular regressor and backpropagates R k using the loss defined in Equation 1 and θ k is updated. This training regime is executed for an epoch.</p><p>In the next epoch, the labels for training the switch classifier are recomputed using criterion in Equation 3 and the switch is again trained as described above. This process of alternating switch training and switched training of CNN regressors is repeated every epoch until the validation accuracy plateaus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Testing</head><p>We evaluate the performance of our proposed architecture, Switch-CNN on four major crowd counting datasets At test time, the image patches are fed to the switch classifier which relays the patch to the best CNN regressor R k . The selected CNN regressor predicts a crowd density map for the relayed crowd scene patch. The generated density maps are assembled into an image to get the final density map for the entire scene. Because of the two pooling layers in the CNN regressors, the predicted density maps are 1 4 th size of the input.</p><p>Evaluation Metric We use Mean Absolute Error (MAE) and Mean Squared Error (MSE) as the metric for comparing the performance of Switch-CNN against the state-of-the-art crowd counting methods. For a test sequence with N images, MAE is defined as follows:</p><formula xml:id="formula_8">MAE = 1 N N i=1 |Ci − C GT i |,<label>(6)</label></formula><p>where Ci is the crowd count predicted by the model being evaluated, and C GT i is the crowd count from human labelled annotations. MAE is an indicator of the accuracy of the predicted crowd count across the test sequence. MSE is a metric complementary to MAE and indicates the robustness of the predicted count. For a test sequence, MSE is defined as follows:</p><formula xml:id="formula_9">MSE = 1 N N i=1 (Ci − C GT i ) 2 .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ShanghaiTech dataset</head><p>We perform extensive experiments on the ShanghaiTech crowd counting dataset <ref type="bibr" target="#b21">[22]</ref> that consists of 1198 annotated images. The dataset is divided into two parts named Part A and Part B. The former contains dense crowd scenes parsed from the internet and the latter is relatively sparse crowd scenes captured in urban surface streets. We use the traintest splits provided by the authors for both parts in our experiments. We train Switch-CNN as elucidated by Algorithm 1 on both parts of the dataset. Ground truth is generated using geometry-adaptive kernels method as the variance in crowd density within a scene due to perspective effects is high (See Sec 3.1 for details about ground truth generation). With an ideal switch (100% switching accuracy), Switch-CNN performs with an MAE of 51.4. However, the accuracy of the switch is 73.2% in Part A and 76.3% in Part B of the dataset resulting in a lower MAE.  <ref type="bibr" target="#b21">[22]</ref>. Switch-CNN also outperforms all other models on MSE metric indicating that the predictions have a lower variance than MCNN across the dataset. This is an indicator of the robustness of Switch-CNN's predicted crowd count.</p><p>We show sample predictions of Switch-CNN for sample test scenes from the ShanghaiTech dataset along with the ground truth in <ref type="figure" target="#fig_1">Figure 3</ref>. The predicted density maps closely follow the crowd distribution visually. This indicates that Switch-CNN is able to localize the spatial distribution of crowd within a scene accurately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">UCF CC 50 dataset</head><p>UCF CC 50 <ref type="bibr" target="#b7">[8]</ref> is a 50 image collection of annotated crowd scenes. The dataset exhibits a large variance in the crowd count with counts varying between 94 and 4543. The small size of the dataset and large variance in crowd count makes it a very challenging dataset. We follow the approach of other state-of-the-art models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref> and use 5fold cross-validation to validate the performance of Switch-CNN on UCF CC 50.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, we compare the performance of Switch-CNN with other methods using MAE and MSE as metrics. Switch-CNN outperforms all other methods and evidences a 15.7 point improvement in MAE over Hydra2s <ref type="bibr" target="#b10">[11]</ref>. Switch-CNN also gets a competitive MSE score compared to Hydra2s indicating the robustness of the predicted count. The accuracy of the switch is 54.3%. The switch accuracy is relatively low as the dataset has very few training examples and a large variation in crowd density. This limits the ability of the switch to learn the multichotomy of space of crowd scene patches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The UCSD dataset</head><p>The UCSD dataset crowd counting dataset consists of 2000 frames from a single scene. The scenes are characterized by sparse crowd with the number of people ranging from 11 to 46 per frame. A region of interest (ROI) is provided for the scene in the dataset. We use the train-test splits used by <ref type="bibr" target="#b3">[4]</ref>. Of the 2000 frames, frames 601 through 1400 are used for training while the remaining frames are held out for testing. Following the setting used in <ref type="bibr" target="#b21">[22]</ref>, we prune the feature maps of the last layer with the ROI provided. Hence, error is backpropagated during training for areas inside the ROI. We use a fixed spread Gaussian to generate ground truth density maps for training Switch-CNN as the crowd is relatively sparse. At test time, MAE is computed only for the specified ROI in test images for benchmarking Switch-CNN against other approaches. <ref type="table" target="#tab_4">Table 3</ref> reports the MAE and MSE results for Switch-CNN and other state-of-the-art approaches. Switch-CNN performs competitively compared to other approaches with an MAE of 1.62. The switch accuracy in relaying the patches to regressors R1 through R3 is 60.9%. However, the dataset is characterized by low variability of crowd density set in a single scene. This limits the performance gain achieved by Switch-CNN from leveraging intra-scene crowd density variation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">The WorldExpo'10 dataset</head><p>The WorldExpo'10 dateset consists of 1132 video sequences captured with 108 surveillance cameras. Five different video sequence, each from a different scene, are held out for testing. Every test scene sequence has 120 frames. The crowds are relatively sparse in comparison to other datasets with average number of 50 people per image. Region of interest (ROI) is provided for both training and test scenes. In addition, perspective maps are provided for all scenes. The maps specify the number of pixels in the image that cover one square meter at every location in the frame. These maps are used by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> to adaptively choose the spread of the Gaussian while generating ground truth density maps. We evaluate performance of the Switch-CNN using ground truth generated with and without perspective maps.</p><p>We prune the feature maps of the last layer with the ROI provided. Hence, error is backpropagated during training for areas inside the ROI. Similarly at test time, MAE is computed only for the specified ROI in test images for benchmarking Switch-CNN against other approaches.</p><p>MAE is computed separately for each test scene and averaged to determine the overall performance of Switch-CNN across test scenes. <ref type="table">Table 4</ref> shows that the average MAE of Switch-CNN across scenes is better by a margin of 2.2 point over the performance obtained by the state-of-theart approach MCNN <ref type="bibr" target="#b21">[22]</ref>. The switch accuracy is 52.72%.  <ref type="table">Table 4</ref>. Comparison of Switch-CNN with other state-of-the-art crowd counting methods on WorldExpo'10 dataset <ref type="bibr" target="#b20">[21]</ref>. Mean Absolute Error (MAE) for individual test scenes and average performance across scenes is shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of number of regressors on Switch-CNN</head><p>Differential training makes use of the structural variations across the individual regressors to learn a multichotomy of the training data. To investigate the effect of structural variations of the regressors R1 through R3, we train Switch-CNN with combinations of regressors (R1,R2), (R2,R3), (R1,R3) and (R1,R2,R3) on Part A of Shang-haiTech dataset. <ref type="table" target="#tab_6">Table 5</ref> shows the MAE performance of Switch-CNN for different combinations of regressors R k . Switch-CNN with CNN regressors R1 and R3 has lower MAE than Switch-CNN with regressors R1-R2 and R2-R3. This can be attributed to the former model having a higher switching accuracy than the latter. Switch-CNN with all three regressors outperforms both the models as it is able to model the scale and perspective variations better with three independent CNN regressors R1, R2 and R3 that are structurally distinct. Switch-CNN leverages multiple independent CNN regressors with different receptive fields. In Table 5, we also compare the performance of individual CNN regressors with Switch-CNN. Here each of the individual regressors are trained on the full training data from Part A of Shanghaitech dataset. The higher MAE of the individual CNN regressor is attributed to the inability of a single regressor to model the scale and perspective variations in the crowd scene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Switch Multichotomy Characteristics</head><p>The principal idea of Switch-CNN is to divide the training patches into disjoint groups to train individual CNN re-gressors so that overall count accuracy is maximized. This multichotomy in space of crowd scene patches is created automatically through differential training. We examine the underlying structure of the patches to understand the correlation between the learnt multichotomy and attributes of the patch like crowd count and density. However, the unavailability of perspective maps renders computation of actual density intractable. We believe inter-head distance between people is a candidate measure of crowd density. In a highly dense crowd, the separation between people is low and hence density is high. On the other hand, for low density scenes, people are far away and mean inter-head distance is large. Thus mean inter-head distance is a proxy for crowd density. This measure of density is robust to scale variations as the inter-head distance naturally subsumes the scale variations.   <ref type="bibr" target="#b21">[22]</ref> is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. We see that the multichotomy of space of crowd scene patches inferred from the switch separates patches based on latent factors correlated with crowd density.</p><p>To analyze the multichotomy in space of patches, we compute the average inter-head distance of each patch in Part A of ShanghaiTech test set. For each head annotation, the average distance to its 10 nearest neighbors is calculated. These distances are averaged over the entire patch representing the density of the patch. We plot a histogram of these distances in <ref type="figure" target="#fig_3">Figure 4</ref> and group the patches by color on the basis of the regressor R k used to infer the count of the patch. A separation of patch space based on crowd density is observed in <ref type="figure" target="#fig_3">Figure 4</ref>. R1, which has the largest receptive field of 9×9, evaluates patches of low crowd density (corresponding to large mean inter-head distance). An interesting observation is that patches from the crowd scene that have no people in them (patches in <ref type="figure" target="#fig_3">Figure 4</ref> with zero average inter-head distance) are relayed to R1 by the switch. We believe that the patches with no people are relayed to R1 as it has a large receptive field that helps capture background attributes in such patches like urban facade and foliage.   <ref type="bibr" target="#b21">[22]</ref> are shown. We see that the density of crowd in the patches increases from CNN regressor R1-R3.</p><p>in the patches increases from CNN regressor R1 through R3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Attribute Clustering Vs Differential Training</head><p>We saw in Sec 5.2 that differential training approximately divides training set patches into a multichotomy based on density. We investigate the effect of manually clustering the patches based on patch attribute like crowd count or density. We use patch count as metric to cluster patches. Training patches are divided into three groups based on the patch count such that the total number of training patches are equally distributed amongst the three CNN regressors R1−3. R1, having a large receptive field, is trained on patches with low crowd count. R2 is trained on medium count patches while high count patches are relayed to R3. The training procedure for this experiment is identical to Switch-CNN, except for the differential training stage. We repeat this experiment with average inter-head distance of the patches as a metric for grouping the patches. Patches with high mean inter-head distance are relayed to R1. R2 is relayed patches with low inter-head distance by the switch while the remaining patches are relayed to R3.   <ref type="table" target="#tab_8">Table 6</ref> reports MAE performance for the two clustering methods. Both crowd count and average inter-head distance based clustering give a higher MAE than Switch-CNN. Average inter-head distance based clustering performs comparably with Switch-CNN. This evidence reinforces the fact that Switch-CNN learns a multichotomy in the space of patches that is highly correlated with mean inter-head distance of the crowd scene. The differential training regime employed by Switch-CNN is able to infer this grouping automatically, independent of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effect of Coupled Training</head><p>Differential training on the CNN regressors R1 through R3 generates a multichotomy that minimizes the predicted count by choosing the best regressor for a given crowd scene patch. However, the trained switch is not ideal and the manifold separating the space of patches is complex to learn (see Section 5.2 of the main paper). To mitigate the effect of switch inaccuracy and inherent complexity of task, we perform coupled training of switch and CNN regressors. We ablate the effect of coupled training by training the switch classifier in a stand-alone fashion. For training the switch in a stand-alone fashion, the labels from differential training are held fixed throughout the switch classifier training.</p><p>The results of the ablation are reported in <ref type="table">Table 7</ref>. We see that training the switch classifier in a stand-alone fashion results in a deterioration of Switch-CNN crowd counting performance. While Switch-CNN with the switch trained in a stand-alone manner performs better than MCNN, it performs significantly worse than Switch-CNN with coupled training. This is reflected in the 13 point higher count MAE. Coupled training allows the patch labels to change in order to adapt to the ability of the switch classifier to relay a patch to the optimal regressor R k correctly. This co-adaption is absent when training switch alone leading to deterioration of crowd counting performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablations on UCF CC 50 dataset</head><p>We perform ablations referenced in Section 5.1 and 5.3 of the main paper on the UCF CC 50 dataset <ref type="bibr" target="#b7">[8]</ref>. The results of these ablations are tabulated in   <ref type="table">Table 9</ref>. Comparison of classification accuracy for different switch architectures on Part A of the ShanghaiTech dataset <ref type="bibr" target="#b21">[22]</ref>. The final switch-classifier selected for all Switch-CNN experiments is highlighted in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Choice of Switch Classifier</head><p>The switch classifier is used to infer the multichotomy of crowd patches learnt from differential training. The accuracy of the predicted count in Switch-CNN is critically dependent on the choice of the switch classifier. We repurpose different classifier architectures, from shallow CNN classifiers to state-of-the art object classifiers to choose the best classifier that strikes a balance between classification accuracy and computational complexity. <ref type="figure" target="#fig_6">Figure 6</ref> shows the different architectures of switch classifier that we evaluate. CNN-small is a shallow classifier derived from VGG-16 <ref type="bibr" target="#b14">[14]</ref>. We retain the first three convolutional layers from VGG-16 and add a 512 dimensional fully-connected layer along with a 3-way classifier. The convolutional layers in CNN-small are initialized from VGG-16. We also repurpose VGG-16 and VGG-19 <ref type="bibr" target="#b14">[14]</ref> by global average pooling the Conv 5 features and using a 512 dimensional fully-connected layer along with a 3-way classifier. All the convolutional layers in VGG-16 and VGG-19 are initialized from VGG models trained on Imagenet <ref type="bibr" target="#b5">[6]</ref>. The state-of-the-art object recognition classifiers, Resnet-50 and Resnet-101 <ref type="bibr" target="#b6">[7]</ref> are also evaluated. We replace the final 1000-way classifier layer with a 3-way classifier. For ResNet training, we do not update the Batch Normalization (BN) layers. The BN statistics from ResNet model trained for ILSCVRC challenge <ref type="bibr" target="#b5">[6]</ref> are retained during fine-tuning for crowd-counting. The BN layers behave as a linear activation function with constant scaling and offset. We do not update the BN layers as we use a batch size of 1 during SGD and the BN parameter update becomes noisy.</p><p>We train each of the classifier on image patch-label pairs, with labels generated from the differential training stage (see Section 3.3 of the main paper). The classifiers are trained using SGD in a stand-alone manner similar to Section 5.4. <ref type="table">Table 9</ref> shows the performance of the different switch classifiers on Part A of the ShanghaiTech dataset <ref type="bibr" target="#b21">[22]</ref>. CNN-small shows a 10% drop in classification accuracy over the other classifiers as it is unable to model the complex multichotomy inferred from differential training. We observe that the performance plateaus for the other classifiers despite using more powerful classifiers like ResNet. This can be attributed to complexity of manifold inferred from differential training. Hence, we choose the repurposed VGG-16 model for all our Switch-CNN experiments as it gives classification accuracy competitive with deeper models like ResNet, but with a lower computational cost. A lower computational cost is critical as it allows faster training during coupled training of the switch-classifier and CNN regressors R1−3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose switching convolutional neural network that leverages intra-image crowd density variation to improve the accuracy and localization of the predicted crowd count. We utilize the inherent structural and functional differences in multiple CNN regressors capable of tackling large scale and perspective variations by enforcing a differential training regime. Extensive experiments on multiple datasets show that our model exhibits state-of-theart performance on major datasets. Further, we show that our model learns to group crowd patches based on latent factors correlated with crowd density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Training switch for 1 epoch*/ Train switch with Strain and update Θsw; /*Switched Differential Training*/ for i = 1 to N do /*Infer choice of R k from switch*/ l sw i = argmax f switch (Xi; Θsw); Backpropagate R l switch i and update Θ l sw i ; end end Algorithm 1: Switch-CNN training algorithm is shown. The training algorithm is divided into stages coded by color. Color code index: Differential Training, Coupled Training, Switch Training as input to the network helps in regressing different regions of the image independently by a CNN regressor most suited to patch attributes like density, background, scale and perspective variations of crowd in the patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Sample predictions by Switch-CNN for crowd scenes from the ShanghaiTech dataset<ref type="bibr" target="#b21">[22]</ref> is shown. The top and bottom rows depict a crowd image, corresponding ground truth and prediction from Part A and Part B of dataset respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Histogram of average inter-head distance for crowd scene patches from Part A test set of ShanghaiTech dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5 displays some sample patches that are relayed to each of the CNN regressors R1 through R3. The density of crowd</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Sample crowd scene patches from Part A test set of ShanghaiTech dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>The architecture of different switch classifiers evaluated in Switch-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Trained parameters {Θ k } 3 k=1 for R k and Θsw for the switch Initialize Θ k ∀ k with random Gaussian weights</figDesc><table><row><cell cols="4">Pretrain {R k } 3 k=1 for Tp epochs : R k ← f k (·; Θ k ) ;</cell></row><row><cell cols="4">/*Differential Training for T d epochs*/</cell></row><row><cell cols="4">/*C k i is count predicted by R k for input Xi*/</cell></row><row><cell>/*C GT i</cell><cell cols="3">is ground truth count for input Xi*/</cell></row><row><cell cols="3">for t = 1 to T d do</cell></row><row><cell cols="3">for i = 1 to N do</cell></row><row><cell></cell><cell>l best i</cell><cell cols="2">= argmin |C k i − C GT i</cell><cell>|;</cell></row><row><cell></cell><cell></cell><cell>k</cell></row><row><cell></cell><cell cols="2">Backpropagate R l best i</cell><cell>and update Θ l best i</cell><cell>;</cell></row><row><cell cols="2">end</cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell></row><row><cell cols="4">/*Coupled Training for Tc epochs*/</cell></row><row><cell cols="4">Initialize Θsw with VGG-16 weights ;</cell></row><row><cell cols="3">for t = 1 to Tc do</cell></row><row><cell cols="4">/*generate labels for training switch*/</cell></row></table><note>output:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>shows that Switch-CNN outperforms all other state-of-the art methods by a significant margin on both the MAE and MSE metric. Switch-CNN shows a 19.8 point improvement in MAE on Part A and 4.8 point improvement in Part B of the dataset over MCNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison of Switch-CNN with other state-of-the-art crowd counting methods on ShanghaiTech dataset<ref type="bibr" target="#b21">[22]</ref>.</figDesc><table><row><cell></cell><cell>Part A</cell><cell cols="2">Part B</cell></row><row><cell>Method</cell><cell cols="3">MAE MSE MAE MSE</cell></row><row><cell cols="3">Zhang et al. [21] 181.8 277.7 32.0</cell><cell>49.8</cell></row><row><cell>MCNN [22]</cell><cell cols="2">110.2 173.2 26.4</cell><cell>41.3</cell></row><row><cell>Switch-CNN</cell><cell cols="2">90.4 135.0 21.6</cell><cell>33.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell cols="2">Lempitsky et al.[10] 493.4</cell><cell>487.1</cell></row><row><cell>Idrees et al.[8]</cell><cell>419.5</cell><cell>487.1</cell></row><row><cell>Zhang et al. [21]</cell><cell>467.0</cell><cell>498.5</cell></row><row><cell>CrowdNet [2]</cell><cell>452.5</cell><cell>-</cell></row><row><cell>MCNN [22]</cell><cell>377.6</cell><cell>509.1</cell></row><row><cell>Hydra2s [11]</cell><cell cols="2">333.73 425.26</cell></row><row><cell>Switch-CNN</cell><cell>318.1</cell><cell>439.2</cell></row></table><note>Comparison of Switch-CNN with other state-of-the-art crowd counting methods on UCF CC 50 dataset [8].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">MAE MSE</cell></row><row><cell>Kernel Ridge Regression [1]</cell><cell>2.16</cell><cell>7.45</cell></row><row><cell cols="2">Cumulative Attribute Regression [5] 2.07</cell><cell>6.86</cell></row><row><cell>Zhang et al. [21]</cell><cell>1.60</cell><cell>3.31</cell></row><row><cell>MCNN [22]</cell><cell>1.07</cell><cell>1.35</cell></row><row><cell>CCNN [11]</cell><cell>1.51</cell><cell>-</cell></row><row><cell>Switch-CNN</cell><cell>1.62</cell><cell>2.10</cell></row></table><note>. Comparison of Switch-CNN with other state-of-the-art crowd counting methods on UCSD crowd-counting dataset [4].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison of MAE for Switch-CNN variants and CNN regressors R1 through R3 on Part A of the ShanghaiTech dataset [22].</figDesc><table><row><cell>Method</cell><cell>MAE</cell></row><row><cell>R1</cell><cell>157.61</cell></row><row><cell>R2</cell><cell>178.82</cell></row><row><cell>R3</cell><cell>178.10</cell></row><row><cell>Switch-CNN with (R1,R3)</cell><cell>98.87</cell></row><row><cell>Switch-CNN with (R1,R2)</cell><cell>110.88</cell></row><row><cell>Switch-CNN with (R2,R3)</cell><cell>126.65</cell></row><row><cell cols="2">Switch-CNN with (R1,R2,R3) 90.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparison of MAE for Switch-CNN and manual clustering of patches based on patch attributes on Part A of the Shang-haiTech dataset<ref type="bibr" target="#b21">[22]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>The results follow the trend on ShanghaiTech dataset and reinforce the superiority of Switch-CNN (See Section 5.1 and 5.3 of the main paper for more details).</figDesc><table><row><cell>Method</cell><cell>MAE</cell></row><row><cell>Cluster by count</cell><cell>319.16</cell></row><row><cell cols="2">Cluster by mean inter-head distance 358.78</cell></row><row><cell>Switch-CNN(R1,R3)</cell><cell>369.58</cell></row><row><cell>Switch-CNN(R1,R2)</cell><cell>362.22</cell></row><row><cell>Switch-CNN(R3,R2)</cell><cell>334.66</cell></row><row><cell>Switch-CNN</cell><cell>318.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Additional results for ablations referenced in Section 5.1 and 5.3 of the main paper for UCF CC 50 dataset<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Method</cell><cell>Acc</cell></row><row><cell cols="2">CNN-small 64.39</cell></row><row><cell>VGG-16</cell><cell>73.75</cell></row><row><cell>VGG-19</cell><cell>74.3</cell></row><row><cell cols="2">ResNet-50 75.03</cell></row><row><cell cols="2">ResNet-101 74.95</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition using kernel ridge regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>4.4</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowdnet: A deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
	<note>2016. 2, 4.3</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised bayesian detection of independent motion in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>4.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
	<note>2013. 1, 2, 4.3, 2, 5.5, 8</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2016. 1, 2, 3, 4.3, 4.4</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Counting crowded moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="705" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for depiction invariant object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="187" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>3.1, 3.4, 5.6</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04878</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Going deeper with convolutions</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on Multimedia Conference</title>
		<meeting>the 2015 ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
	<note>2015. 1, 2, 3, 4.2, 4.3, 4.4, 4.5, 4.5, 4</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Singleimage crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
	<note>2016. 1, 1, 2, 3, 3.1, 3, 4.2, 4.2, 1, 4.3, 4.4, 4.5, 4.5, 5, 4, 5, 6, 5.4, 7, 9</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
