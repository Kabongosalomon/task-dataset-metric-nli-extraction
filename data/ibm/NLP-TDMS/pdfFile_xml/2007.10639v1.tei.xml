<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Transformer for Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
							<email>chensun@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
							<email>karteek.alahari@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<email>cordelias@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal Transformer for Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video</term>
					<term>language</term>
					<term>retrieval</term>
					<term>multi-modal</term>
					<term>cross-modal</term>
					<term>tem- porality</term>
					<term>transformer</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at http://thoth.inrialpes.fr/research/MMT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video is one of the most popular forms of media due to its ability to capture dynamic events and its natural appeal to our visual and auditory senses. Online video platforms are playing a major role in promoting this form of media. However, the billions of hours of video available on such platforms are unusable if we cannot access them effectively, for example, by retrieving relevant content through queries.</p><p>In this paper, we tackle the tasks of caption-to-video and video-to-caption retrieval. In the first task of caption-to-video retrieval, we are given a query in the form of a caption (e.g., "How to build a house") and the goal is to retrieve the videos best described by it (i.e., videos explaining how to build a house). In practice, given a test set of caption-video pairs, our aim is to provide, for each caption query, a ranking of all the video candidates such that the video associated with the caption query is ranked as high as possible. On the other hand, the task of video-to-caption retrieval focuses on finding among a collection of caption candidates the ones that best describe the query video. When matching a text query with videos, the inherent cross-modal and temporal information in videos needs to be leveraged effectively, for example, with a video encoder that handles all the constituent modalities (appearance, audio, speech) jointly across the entire duration of the video. In this example, a video encoder will be able to distinguish between "someone walking to" and "someone walking away" only if it exploits the temporal information of events occurring in the video (red arrows). Also, in order to understand that a "motorbike failed to start", it needs to use cross-modal information (e.g., absence of noise after someone tried to start the engine, orange arrow).</p><p>A common approach for the retrieval problem is similarity learning <ref type="bibr" target="#b28">[29]</ref>, where we learn a function of two elements (a query and a candidate) that best describes their similarity. All the candidates can then be ranked according to their similarity with the query. In order to perform this ranking, the captions as well as the videos are represented in a common multi-dimensional embedding space, wherein similarities can be computed as a dot product of their corresponding representations. The critical question here is how to learn accurate representations of both caption and video to base our similarity estimation on.</p><p>The problem of learning representation of text has been extensively studied, leading to various methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>, which can be used to encode captions. In contrast to these advances, learning effective video representation continues to be a challenge, and forms the focus of our work. This is in part due to the multimodal and temporal nature of video. Video data not only varies in terms of appearance, but also in possible motion, audio, overlaid text, speech, etc. Leveraging cross-modal relations thus forms a key to building effective video representations. As illustrated in <ref type="figure">Fig. 1</ref>, cues jointly extracted from all the constituent modalities are more informative than handling each modality independently. Hearing a motor sound right after seeing someone starting a bike tells us that the running bike is the visible one and not a background one. Another example is the case of a video of "a crowd listening to a talk", neither of the modali- ties "appearance" or "audio" can fully describe the scene, but when processed together, higher level semantics can be obtained. Recent work on video retrieval does not fully exploit such cross-modal highlevel semantics. They either ignore the multi-modal signal <ref type="bibr" target="#b14">[15]</ref>, treat modalities separately <ref type="bibr" target="#b15">[16]</ref>, or only use a gating mechanism to modulate certain modality dimensions <ref type="bibr" target="#b13">[14]</ref>. Another challenge in representing video is its temporality. Due to the difficulty in handling variable duration of videos, current approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> discard long-term temporal information by aggregating descriptors extracted at different moments in the video. We argue that this temporal information can be important to the task of video retrieval. As shown in <ref type="figure">Fig. 1</ref>, a video of "someone walking to an object" and "someone walking away from an object" will have the same representation once pooled temporally, however, the movement of the person relative to the object is potentially important in the query.</p><p>We address the temporal and multi-modal challenges posed in video data by introducing our multi-modal transformer. It performs the task of processing features extracted from different modalities at different moments in video and aggregates them in a compact representation. Building on the transformer architecture <ref type="bibr" target="#b24">[25]</ref>, our multi-modal transformer exploits the self-attention mechanism to gather valuable cross-modal and temporal cues about events occurring in a video. We integrate our multi-modal transformer in a cross-modal framework, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, which leverages both captions and videos, and estimates their similarity.</p><p>Contributions. In this work, we make the following three contributions: (i) First, we introduce a novel video encoder architecture for retrieval: Our multi-modal transformer processes effectively multiple modality features extracted at different times. (ii) We thoroughly investigate different architectures for language embed-ding, and show the superiority of the BERT model for the task of video retrieval. (iii) By leveraging our novel cross-modal framework we outperform prior state of the art for the task of video retrieval on MSRVTT <ref type="bibr" target="#b29">[30]</ref>, ActivityNet <ref type="bibr" target="#b11">[12]</ref> and LSMDC <ref type="bibr" target="#b20">[21]</ref> datasets. It is also the winning solution in the CVPR 2020 Video Pentathlon Challenge <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We present previous work on language and video representation learning, as well as on visual-language retrieval.</p><p>Language representations. Earlier work on language representations include bag of words <ref type="bibr" target="#b33">[34]</ref> and Word2Vec <ref type="bibr" target="#b17">[18]</ref>. A limitation of these representations is capturing the sequential properties in a sentence. LSTM <ref type="bibr" target="#b6">[7]</ref> was one of the first successful deep learning models to handle this. More recently, the transformer <ref type="bibr" target="#b24">[25]</ref> architecture has shown impressive results for text representation by implementing a self-attention mechanism where each word (or wordpiece <ref type="bibr" target="#b26">[27]</ref>) of the sentence can attend to all the others. The transformer architecture, consisting of self-attention layers alternatively stacked with fully-connected layers, forms the base of the popular language modeling network BERT <ref type="bibr" target="#b2">[3]</ref>. Burns et al. <ref type="bibr" target="#b0">[1]</ref> perform an analysis of the different word embeddings and language models (Word2Vec <ref type="bibr" target="#b17">[18]</ref>, LSTM <ref type="bibr" target="#b6">[7]</ref>, BERT <ref type="bibr" target="#b2">[3]</ref>, etc.) used in vision-language tasks. They show that the pretrained and frozen BERT model <ref type="bibr" target="#b2">[3]</ref> performs relatively poorly compared to a LSTM or even a simpler average embedding model. In this work, we show that for video retrieval, a pretrained BERT outperforms other language models, but it needs to be finetuned.</p><p>Video representations. With a two-stream network, Simonyan et al. <ref type="bibr" target="#b21">[22]</ref> have used complementary information from still frames and motion between frames to perform action recognition in videos. Carreira et al. <ref type="bibr" target="#b1">[2]</ref> incorporated 3D convolutions in a two-stream network to better attend the temporal structure of the signal. S3D <ref type="bibr" target="#b27">[28]</ref> is an alternative approach, which replaced the expensive 3D spatio-temporal convolutions by separable 2D and 1D convolutions. More recently, transformer-based methods, which leverage BERT pretraining <ref type="bibr" target="#b2">[3]</ref>, have been applied to S3D features in VideoBERT <ref type="bibr" target="#b23">[24]</ref> and CBT <ref type="bibr" target="#b22">[23]</ref>. While these works focus on visual signals, they have not studied how to encode the other multi-modal semantics, such as audio signals.</p><p>Visual-language retrieval. Harwath et al. <ref type="bibr" target="#b4">[5]</ref> perform image and audio-caption retrieval by embedding audio segments and image regions in the same space and requiring high similarity between each audio segment and its corresponding image region. The method presented in <ref type="bibr" target="#b12">[13]</ref> takes a similar approach for image-text retrieval by embedding images regions and words in a joint space. A high similarity is obtained for images that have matching words and image regions.</p><p>For videos, JSFusion <ref type="bibr" target="#b30">[31]</ref> estimates video-caption similarity through dense pairwise comparisons between each word of the caption and each frame of the video. In this work, we instead estimate both a video embedding and a caption embedding and then compute the similarity between them. Zhang et al. <ref type="bibr" target="#b32">[33]</ref> perform paragraph-to-video retrieval by assuming a hierarchical decomposition of the video and paragraph. Our method do not assume that the video can be decomposed into clips that align with sentences of the caption. A recent alternative is creating separate embedding spaces for different parts of speech (e.g., noun or verb) <ref type="bibr" target="#b25">[26]</ref>. In contrast to this method, we do not pre-process the sentences but encode them directly through BERT.</p><p>Another work <ref type="bibr" target="#b16">[17]</ref> leverages the large number of instructional videos in the HowTo100M dataset, but does not fully exploit the temporal relations. Our work instead relies on longer segments extracted from HowTo100M videos in order to learn temporal dependencies and address the problem of misalignment between speech and visual features. Mithun et al. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> use three experts (Object, Activity and Place) to compute three corresponding text-video similarities. These experts however do not collaborate together as their respective similarities are simply summed together. A related approach <ref type="bibr" target="#b15">[16]</ref> uses precomputed features from experts for text to video retrieval, where the overall similarity is obtained as a weighted sum of each expert's similarity. A recent extension <ref type="bibr" target="#b13">[14]</ref> to this mixture of experts model uses a collaborative gating mechanism for modulating each expert feature according to the other experts. However, this collaborative gating mechanism only strengthens (or weakens) some dimensions of the input signal in a single step, and is therefore not able to capture high level inter-modality information. Our multi-modal transformer overcomes this limitation by attending to all available modalities over multiple self-attention layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our overall method relies on learning a function s to compute the similarity between two elements: text and video, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We then rank all the videos (or captions) in the dataset, according to their similarity with the query caption (or video) in the case of text-to-video (or video-to-text) retrieval. In other words, given a dataset of n video-caption pairs {(v 1 , c 1 ), ..., (v n , c n )}, the goal of the learnt similarity function s(v i , c j ), between video v i and caption c j , is to provide a high value if i = j, and a low one if i = j. Estimating this similarity (described in Section 3.3) requires accurate representations for the video as well as the caption. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the two parts focused on producing these representations (presented in Sections 3.1 and 3.2 respectively) in our cross-modal framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video representation</head><p>The video-level representation is computed by our proposed multi-modal transformer (MMT). MMT follows the architecture of the transformer encoder presented in <ref type="bibr" target="#b24">[25]</ref>. It consists of stacked self-attention layers and fully collected layers. MMT's input Ω(v) is a set of embeddings, all of the same dimension d model . Each <ref type="figure">Fig. 3</ref>: Inputs to our multi-modal transformer. We combine feature semantics F , expert information E, and temporal cues T to form our video embeddings Ω(v), which are input to MMT. of them embeds the semantics of a feature, its modality, and the time in the video when the feature was extracted. This input is given by:</p><formula xml:id="formula_0">Video embeddings + Ω(v) F(v) T(v) E(v) ω rgb agg Features F rgb agg E rgb T agg + Expert embeddings Temporal embeddings + ω rgb 1 F rgb 1 E rgb T 1 + + ω rgb K F rgb K E rgb T D + + ω audio agg F audio agg E audio T agg + + ω audio 1 F audio 1 E audio T 1 + + ω audio K F audio K E audio T D + + ω speech agg F speech agg E speech T agg + + ω speech 1 F speech 1 E speech T 1 + + ω speech K F speech K E speech T D +</formula><formula xml:id="formula_1">Ω(v) = F (v) + E(v) + T (v),<label>(1)</label></formula><p>In the following, we describe those three components. Features F . In order to learn an effective representation from different modalities inherent in video data, we begin with video feature extractors called "experts" <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref>. In contrast to previous methods, we learn a joint representation leveraging both cross-modal and long-term temporal relationships among the experts. We use N pretrained experts {F n } N n=1 . Each expert is a model trained for a particular task that is then used to extract features from video. For a video v, each expert extracts a sequence F n (v) = [F n 1 , ..., F n K ] of K features. The features extracted by our experts encode the semantics of the video. Each expert F n outputs features in R dn . In order to project the different expert features into a common dimension d model , we learn N linear layers (one per expert) to project all the features into R d model .</p><p>A transformer encoder produces an embedding for each of its feature inputs, resulting in several embeddings for an expert. In order to obtain a unique embedding for each expert, we define an aggregated embedding F n agg that will collect and contextualize the expert's information. We initialize this embedding with a max pooling aggregation of all the corresponding expert's features as F n agg = maxpool({F n k } K k=1 ). The sequence of input features to our video encoder then takes the form:</p><formula xml:id="formula_2">F (v) = [F 1 agg , F 1 1 , ..., F 1 K , ..., F N agg , F N 1 , ..., F N K ].<label>(2)</label></formula><p>Expert embeddings E. In order to process cross-modality information, our MMT needs to identify which expert it is attending to. We learn N embeddings {E 1 , ..., E N } of dimension d model to distinguish between embeddings of different experts. Thus, the sequence of expert embeddings to our video encoder takes the form:</p><formula xml:id="formula_3">E(v) = [E 1 , E 1 , ..., E 1 , ..., E N , E N , ..., E N ].<label>(3)</label></formula><p>Temporal embeddings T . They provide temporal information about the time in the video where each feature was extracted to our multi-modal transformer.</p><p>Considering videos of a maximum duration of t max seconds, we learn D = |t max | embeddings {T 1 , ..., T D } of dimension d model . Each expert feature that has been extracted in the time range [t, t + 1) will be temporally embedded with T t+1 . For example, a feature extracted at 7.4s in the video will be temporally encoded with temporal embedding T 8 . We learn two additional temporal embeddings T agg and T unk , which encode aggregated features and unknown temporal information features (for experts whose temporal information is unknown), respectively. The sequence of temporal embeddings of our video encoder then takes the form:</p><formula xml:id="formula_4">T (v) = [T agg , T 1 , ..., T D , ..., T agg , T 1 , ..., T D ].<label>(4)</label></formula><p>Multi-modal Transformer. The video embeddings Ω(v) defined as the sum of features, expert and temporal embeddings in <ref type="formula" target="#formula_1">(1)</ref>, as shown in <ref type="figure">Fig. 3</ref>, are input to the transformer. They are given by:</p><formula xml:id="formula_5">Ω(v) = F (v) + E(v) + T (v) = [ω 1 agg , ω 1 1 , ..., ω 1 K , ..., ω N agg , ω N 1 , ..., ω N K ].</formula><p>MMT contextualises its input Ω(v) and produces the video representation Ψ agg (v). As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we only keep the aggregated embedding per expert. Thus, our video representation Ψ agg (v) consists of the output embeddings corresponding to the aggregated features, i.e.,</p><formula xml:id="formula_6">Ψ agg (v) = M M T (Ω(v)) = [ψ 1 agg , ..., ψ N agg ].<label>(5)</label></formula><p>The advantage of our MMT over the state-of-the-art collaborative gating mechanism <ref type="bibr" target="#b13">[14]</ref> is two-fold: First, the input embeddings are not simply modulated in a single step but iteratively refined through several layers featuring multiple attention heads. Second, we do not limit our video encoder with a temporally aggregated feature for each expert, but provide all the extracted features instead, along with a temporal encoding describing at what moment of the video they were extracted from. Thanks to its self-attention modules, each layer of our multi-modal transformer is able to attend to all its input embeddings, thus extracting semantics of events occurring in the video over several modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Caption representation</head><p>We compute our caption representation Φ(c) in two stages: first, we obtain an embedding h(c) of the caption, and then project it with a function g into N different spaces as Φ = g • h. For the embedding function h, we use a pretrained BERT model <ref type="bibr" target="#b2">[3]</ref>. Specifically, we extract our single caption embedding h(c) from the [CLS] output of BERT. In order to match the size of this caption representation with that of video, we learn for function g as many gated embedding modules <ref type="bibr" target="#b15">[16]</ref> as there are video experts. Our caption representation then consists of N embeddings, represented by Φ(c) = {φ i } N i=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Similarity estimation</head><p>We compute our final video-caption similarity s, as a weighted sum of each expert i's video-caption similarity φ i , ψ i agg . It is given by:</p><formula xml:id="formula_7">s(v, c) = N i=1 w i (c) φ i , ψ i agg ,<label>(6)</label></formula><p>where w i (c) represents the weight for the ith expert. To obtain these mixture weights, we follow <ref type="bibr" target="#b15">[16]</ref> and process our caption representation h(c) through a linear layer and then perform a softmax operation, i.e.,</p><formula xml:id="formula_8">w i (c) = e h(c) ai N j=1 e h(c) aj ,<label>(7)</label></formula><p>where (a 1 , ..., a N ) are the weights of the linear layer. The intuition behind using a weighted sum is that a caption may not describe all the inherent modalities in video uniformly. For example, in the case of a video with a person in a red dress singing opera, the caption "a person in a red dress" provides no information relevant for audio. On the contrary, the caption "someone is singing" should focus on the audio modality for computing similarity. Note that w i (c), φ i and ψ i agg can all be precomputed offline for each caption and for each video, and therefore the retrieval operation only involves dot product operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>We train our model with the bi-directional max-margin ranking loss <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_9">L = 1 B B i=1 j =i max(0, s ij − s ii + m) + max(0, s ji − s ii + m) ,<label>(8)</label></formula><p>where B is the batch size, s ij = s(v i , c j ), the similarity score between video v i and caption c j , and m is the margin. This loss enforces the similarity for true video-caption pairs s ii to be higher than the similarity of negative samples s ij or s ji , for all i = j, by at least m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>HowTo100M <ref type="bibr" target="#b16">[17]</ref>. It is composed of more than 1 million YouTube instructional videos, along with automatically-extracted speech transcriptions, which form the captions. These captions are naturally noisy, and often do not describe the visual content accurately or are temporally misaligned with it. We use this dataset only for pre-training.</p><p>MSRVTT <ref type="bibr" target="#b29">[30]</ref>. This dataset is composed of 10K YouTube videos, collected using 257 queries from a commercial video search engine. Each video is 10 to 30s long, and is paired with 20 natural sentences describing it, obtained from Amazon Mechanical Turk workers. We use this dataset for training from scratch and also for fine-tuning. We report results on the train/test splits introduced in [31] that uses 9000 videos for training and 1000 for test. We refer to this split as "1k-A". We also report results on the train/test split in <ref type="bibr" target="#b15">[16]</ref> that we refer to as "1k-B". Unless otherwise specified, our MSRVTT results are with "1k-A". ActivityNet Captions <ref type="bibr" target="#b11">[12]</ref>. It consists of 20K YouTube videos temporally annotated with sentence descriptions. We follow the approach of <ref type="bibr" target="#b32">[33]</ref>, where all the descriptions of a video are concatenated to form a paragraph. The training set has 10009 videos. We evaluate our video-paragraph retrieval on the "val1" split (4917 videos). We use ActivityNet for training from scratch and fine-tuning. LSMDC <ref type="bibr" target="#b20">[21]</ref>. It contains 118,081 short video clips (∼ 45s) extracted from 202 movies. Each clip is annotated with a caption, extracted from either the movie script or the audio description. The test set is composed of 1000 videos, from movies not present in the training set. We use LSMDC for training from scratch and also fine-tuning.</p><p>Metrics. We evaluate the performance of our model with standard retrieval metrics: recall at rank N (R@N , higher is better), median rank (MdR, lower is better) and mean rank (MnR, lower is better). For each metric, we report the mean and the standard deviation over experiments with 3 random seeds. In the main paper, we only report recall@5, median and mean ranks, and refer the reader to the supplementary material for additional metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Pre-trained experts. Recall that our video encoder uses pre-trained experts models for extracting features from each video modality. We use the following seven experts. Motion features are extracted from S3D <ref type="bibr" target="#b27">[28]</ref> trained on the Kinetics action recognition dataset. Audio features are extracted using VGGish model <ref type="bibr" target="#b5">[6]</ref> trained on YT8M. Scene embeddings are extracted from DenseNet-161 <ref type="bibr" target="#b8">[9]</ref> trained for image classification on the Places365 dataset <ref type="bibr" target="#b34">[35]</ref>. OCR features are obtained in three stages. Overlaid text is first detected using the pixel link text detection model. The detected boxes are then passed through a text recognition model trained on the Synth90K dataset. Finally, each character sequence is encoded with word2vec <ref type="bibr" target="#b17">[18]</ref> embeddings. Face features are extracted in two stages. An SSD face detector is used to extract bounding boxes, which are then passed through a ResNet50 trained for face classification on the VGGFace2 dataset. Speech transcripts are extracted using the Google Cloud Speech to Text API, with the language set to English. The detected words are then encoded with word2vec. Appearance features are extracted from the final global average pooling layer of SENet-154 <ref type="bibr" target="#b7">[8]</ref> trained for classification on ImageNet. For scene, OCR, face, speech and appearance, we use the features publicly released by <ref type="bibr" target="#b13">[14]</ref>, and compute the other features ourselves.</p><p>Training. For each dataset, we run a grid search on the corresponding validation set to estimate the hyperparameters. We use the Adam optimizer for all our experiments, and set the margin of the bidirectional max-margin ranking loss to 0.05. We also freeze our pre-trained expert models. When pre-training on HowTo100M, we use a batch size of 64 video-caption pairs, an initial learning rate of 5e-5, which we decay by a multiplicative factor 0.98 every 10K optimisation steps, and train for 2 million steps. Given the long duration of most of the HowTo100M videos, we randomly sample 100 consecutive words in the caption, and keep 100 consecutive seconds of video data, closest in time to the selected words.</p><p>When training from scratch or finetuning on MSRVTT or LSMDC, we use a batch size of 32 video-caption pairs, an initial learning rate of 5e-5, which we decay by a multiplicative factor 0.95 every 1K optimisation steps. We train for 50K steps. We use the same settings when training from scratch or finetuning on ActivityNet, except for 0.90 as the multiplicative factor.</p><p>To compute our caption representation h(c), we use the "BERT-base-cased" checkpoint of the BERT model and finetune it with a dropout probability of 10%. To compute our video representation Ψ agg (v), we use MMT with 4 layers and 4 attention heads, a dropout probability of 10%, a hidden size d model of 512, and an intermediate size of 3072.</p><p>For datasets with short videos (MSRVTT and LSMDC), we use all the 7 experts and limit video input to 30 features per expert, and BERT input to the first 30 wordpieces. For datasets containing longer videos (HowTo100M and ActivityNet), we only use motion and audio experts, and limit our video input to 100 features per expert and our BERT input to the first 100 wordpieces. In cases where an expert is unavailable for a given video, e.g., no speech was detected, we set the aggregated feature F n agg to a zero vector. We refer the reader to the supplementary material for a study of the model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies and comparisons</head><p>We will first show the advantage of pretraining our model on a large-scale, uncurated dataset. We will then perform ablations on the architecture used for our language and video encoders. Finally, we will present the relative importance of the pretrained experts used in this work, and compare with related methods. Pretraining. <ref type="table" target="#tab_0">Table 1</ref> shows the advantage of pretraining on HowTo100M, before finetuning on the target dataset (MSRVTT in this case). We also evaluated the impact of pretraining on ActivityNet and LSMDC; see <ref type="table" target="#tab_3">Table 5</ref> and <ref type="table" target="#tab_4">Table 6</ref>. Language encoder. We evaluated several architectures for caption representation, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Similar to the observation made in <ref type="bibr" target="#b0">[1]</ref>, we obtain poor results from a frozen, pretrained BERT. Using the [CLS] output from a pretrained and frozen BERT model is in fact the worst result. We suppose this is because the output was not trained for caption representation, but for a very different task: next sentence prediction. Finetuning BERT greatly improves performance; it is the best result. We also compare with GrOVLE [1] embeddings, frozen or finetuned, aggregated with a max-pooling operation or a 1-layer LSTM  and a fully-connected layer. We show that pretrained BERT embeddings aggregated by a max-pooling operation perform better than GrOVLE embeddings processed by a LSTM (best results from <ref type="bibr" target="#b0">[1]</ref> for the text-to-clip task).</p><p>We also analysed the impact of removing stop words from the captions in <ref type="table" target="#tab_0">Table 1</ref>. In a zero-shot setting, i.e., trained on HowTo100M, evaluated on MSRVTT without finetuning, removing the stop words helps generalize, by bridging the domain gap-HowTo100M speech is very different from MSRVTT captions. This approach was adopted in <ref type="bibr" target="#b14">[15]</ref>. However, we observe that when finetuning, it is better to keep all the words as they contribute to the semantics of the caption. Video encoder. We evaluated the influence of different architectures for computing video embeddings on the MSRVTT 1k-A test split.</p><p>In <ref type="table">Table 3a</ref>, we evaluate variants of our encoder architecture and its input. Similar to <ref type="bibr" target="#b15">[16]</ref>, we experiment with directly computing the caption-video similarities on each max-pooled expert features, i.e., no video encoder (NONE in the table). We compare this with the collaborative gating architecture (COLL) <ref type="bibr" target="#b13">[14]</ref> and our MMT variant using only the aggregated features as input. For the first two variants without MMT, we adopt the approach of <ref type="bibr" target="#b15">[16]</ref> to deal with missing modalities by re-weighting w i (c). We also show the superior performance of our multi-modal transformer in contextualising the different modality embeddings compared to the collaborative gating approach. We argue that our MMT is able to extract cross-modal information in a multi-stage architecture compared to collaborative gating, which is limited to modulating the input embeddings. Ta- <ref type="table">Table 3</ref>: Ablation studies on the video encoder of our framework with MSRVTT. (a) Influence of the architecture and input. With max-pooled features as input, we compare our transformer architecture (MMT) with the variant not using an encoder (NONE) and the one with Collaborative Gating <ref type="bibr" target="#b13">[14]</ref> (COLL). We also show that MMT can attend to all extracted features, as detailed in the text. (b) Importance of initializing F n agg features. We compare zero-vector initialisation, mean pooling and max pooling of the expert features. (c) Influence of the size of the multi-modal transformer. We compare different values for number-of-layers × number-of-attention-heads. ble 3a also highlights the advantage of providing MMT with all the extracted features, instead of only aggregated ones. Temporally aggregating each expert's features ignores information about multiple events occurring in a same video (see the last three rows). As shown by the influence of ordered and randomly shuffled features on the performance, MMT has the capacity to make sense of the relative ordering of events in a video. <ref type="table">Table 3b</ref> shows the importance of initialising the expert aggregation feature F n agg . Since the output of our video encoder is extracted from the "agg" columns, it is important to initialise them with an appropriate representation of the experts' features. The transformer being a residual network architecture, initializing F n agg input embeddings with a zero vector leads to a low performance. Initializing with max pooling aggregation of each expert performs better than mean pooling. Finally, we analyze the impact of the size of our multi-modal transformer model in <ref type="table">Table 3c</ref>. A model with 4 layers and 4 attention heads outperforms both a smaller model (2 layers and 2 attention heads) and a larger model (8 layers and 8 attention heads). Comparison of the different experts. In <ref type="figure" target="#fig_3">Figure 4</ref>, we show an ablation study when training our model on MSRVTT using only one expert (left), using all experts but one (middle), or gradually adding experts by greedy search (right). In the case of using only one expert, we note that the motion expert provides the best results. We attribute the poor performance of OCR, speech and face to the fact that they are absent from many videos, thus resulting in a zero vector input to our video encoder. While the scene expert shows a decent performance, if used alone, it does not contribute when used along other experts, perhaps due to the semantics it encodes being captured already by other experts like appearance or motion. On the contrary, the audio expert alone does not provide a good performance, but it contributes the most when used in conjunction with the others, most likely due to the complementary cues it provides, compared to the other experts. Comparison to prior state of the art. We compare our method on three datasets: MSRVTT <ref type="table" target="#tab_2">(Table 4</ref>), ActivityNet <ref type="table" target="#tab_3">(Table 5</ref>) and LSMDC <ref type="table" target="#tab_4">(Table 6</ref>). While MSRVTT and LSMDC contain short video-caption pairs (average video duration of 13s for MSRVTT, one-sentence captions), ActivityNet contains much longer videos (several minutes) and each video is captioned with multiple sentences. We consider the concatenation of all these sentences as the caption. We show that our method obtains state-of-the-art results on all the three datasets. The gains obtained through MMT's long term temporal encoding are particularly noticeable on the long videos of ActivityNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>We introduced multi-modal transformer, a transformer-based architecture capable of attending multiple features extracted at different moments, and from different modalities in video. This leverages both temporal and cross-modal cues, which are crucial for accurate video representation. We incorporate this video encoder along with a caption encoder in a cross-modal framework to perform   caption-video matching and obtain state-of-the-art results for video retrieval. As future work, we would like to improve temporal encoding for video and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary material</head><p>A.1 Model complexity Number of parameters. As shown below, using multiple modalities does not impact the number of parameters significantly. Interestingly, majority of the parameters correspond to the BERT caption encoding module. We also note that the difference in the video encoder comes from the projections. The number of parameters of a transformer encoder is independent of the number of input embeddings, as are the parameters of a CNN from the image size. Training and inference times. Training our full cross-modal architecture from scratch on MSRVTT takes about 4 hours on a single V100 16GB GPU.</p><p>If we replace our multi-modal transformer by collaborative gating <ref type="bibr" target="#b13">[14]</ref>, we reduce the number of parameters from 133.3M to 123.9M. However, the gain in inference time is minimal, from 1.1s to 0.8s, and is negligible compared to feature extraction, as detailed below.</p><p>Inference time for 1k videos and 1k text queries from MSRVTT on a single V100 GPU is as follows: approximately 3000s to extract features of 7 experts on 1k videos (480s just for S3D motion features), 1.1s to process videos with MMT, 0.9s to process 1k captions with BERT+gated embedding modules, 0.05s to compute similarities and rank the video candidates for the 1k queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results on additional metrics</head><p>Here, we report our results for the additional metrics R@1, R@10, R@50. <ref type="table" target="#tab_5">Table 7</ref> complements the results reported for the MSRVTT <ref type="bibr" target="#b29">[30]</ref> dataset in <ref type="table" target="#tab_2">Table 4</ref> of the main paper. Similarly, <ref type="table" target="#tab_6">Table 8</ref> and <ref type="table" target="#tab_7">Table 9</ref> report the additional evaluations for <ref type="table" target="#tab_3">Table 5</ref> and <ref type="table" target="#tab_4">Table 6</ref> of the main paper on ActivityNet <ref type="bibr" target="#b11">[12]</ref> and LSMDC <ref type="bibr" target="#b20">[21]</ref> datasets respectively. We observe that the results on these additional metrics are in line with the conclusions of the main paper.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France. arXiv:2007.10639v1 [cs.CV] 21 Jul 2020 Fig. 1:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Our cross-modal framework for similarity estimation. We use our Multimodal Transformer (MMT, right) to encode video, and BERT (left) for text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>9 (</head><label>9</label><figDesc>(a) Encoder architecture and input Text −→ Video Encoder Input R@5↑ MdR↓ MnR↓ NONE max pool 50.9±1.5 5.3±0.5 28.6±0.5 COLL max pool 51.3±0.8 5.0±0.0 29.5±1.8 MMT max pool 52.5±0.7 5.0±0.0 27.2±0.7 MMT shuffled feats 53.3±0.2 5.0±0.0 27.4±0.7 MMT ordered feats 54.0±0.2 4.0±0.0 26.7±0.9 (b) F n agg initialisation Text −→ Video F n agg init R@5↑ MdR↓ MnR↓ zero 50.2±0.9 5.7±0.5 28.5±1.3 mean pool 54.2±0.3 5.0±0.0 27.1±0.9 max pool 54.0±0.2 4.0±0.0 26.7±0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>MSRVTT performance (mean rank; lower is better) after training from scratch, when using only one expert (left), when using all experts but one (middle), when gradually adding experts by greedy search (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Our cross-modal architecture using 7 modalities has: 133.3M parameters, including caption encoder: 112.9M, video encoder: 20.4M (Projections: 3.3M, MMT: 17.1M). Our cross-modal architecture using 2 modalities has: 127.3M parameters, including caption encoder: 109.6M (decrease compared to 7 modalities due to using less gated embedding modules), video encoder: 17.7M (Projections: 0.6M, MMT: 17.1M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Advantage of pretraining on HowTo100M then finetuning on MSRVTT. Impact of removing the stop words. Performance reported on MSRVTT.</figDesc><table><row><cell>Text −→ Video</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different architectures for caption embedding when training from scratch on MSRVTT.</figDesc><table><row><cell>Text −→ Video</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Retrieval performance on the MSRVTT dataset. 1k-A and 1k-B denote test sets of 1000 randomly sampled caption-video pairs used in<ref type="bibr" target="#b30">[31]</ref> and<ref type="bibr" target="#b15">[16]</ref> resp.</figDesc><table><row><cell></cell><cell cols="2">Text −→ Video</cell><cell></cell><cell cols="3">Video −→ Text</cell></row><row><cell>Method</cell><cell cols="3">Split R@5↑ MdR↓ MnR↓</cell><cell cols="3">R@5↑ MdR↓ MnR↓</cell></row><row><cell cols="2">Random baseline 1k-A 0.5</cell><cell cols="2">500.0 500.0</cell><cell>0.5</cell><cell cols="2">500.0 500.0</cell></row><row><cell>JSFusion [31]</cell><cell>1k-A 31.2</cell><cell>13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HT [17]</cell><cell>1k-A 35.0</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [14]</cell><cell cols="6">1k-A 48.8±0.6 6.0±0.0 28.2±0.8 50.3±0.5 5.3±0.6 25.1±0.8</cell></row><row><cell>Ours</cell><cell cols="6">1k-A 54.0±0.2 4.0±0.0 26.7±0.9 56.0±0.9 4.0±0.0 23.6±1.0</cell></row><row><cell cols="2">HT-pretrained [17] 1k-A 40.2</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours-pretrained</cell><cell cols="6">1k-A 57.1±1.0 4.0±0.0 24.0±0.8 57.5±0.6 3.7±0.5 21.3±0.6</cell></row><row><cell cols="2">Random baseline 1k-B 0.5</cell><cell cols="2">500.0 500.0</cell><cell>0.5</cell><cell cols="2">500.0 500.0</cell></row><row><cell>MEE [16]</cell><cell>1k-B 37.9</cell><cell>10.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>JPose [26]</cell><cell>1k-B 38.1</cell><cell>9</cell><cell>-</cell><cell>41.3</cell><cell>8.7</cell><cell>-</cell></row><row><cell cols="2">MEE-COCO [16] 1k-B 39.2</cell><cell>9.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [14]</cell><cell cols="6">1k-B 46.0±0.4 7.0±0.0 35.3±1.1 46.0±0.5 6.5±0.5 30.6±1.2</cell></row><row><cell>Ours</cell><cell cols="6">1k-B 49.1±0.4 6.0±0.0 29.5±1.6 49.4±0.4 6.0±0.0 24.5±1.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Retrieval performance on the ActivityNet dataset. 0±0.0 20.8±0.4 54.8±0.4 4.3±0.5 21.2±0.5 Ours-pretrained 61.4±0.2 3.3±0.5 16.0±0.4 61.1±0.2 4.0±0.0 17.1±0.5</figDesc><table><row><cell></cell><cell cols="2">Text −→ Video</cell><cell></cell><cell cols="3">Video −→ Text</cell></row><row><cell>Method</cell><cell cols="3">R@5↑ MdR↓ MnR↓</cell><cell cols="3">R@5↑ MdR↓ MnR↓</cell></row><row><cell cols="2">Random baseline 0.1</cell><cell cols="2">2458.5 2458.5</cell><cell>0.1</cell><cell cols="2">2458.5 2458.5</cell></row><row><cell>FSE [33]</cell><cell cols="2">44.8±0.4 7</cell><cell>-</cell><cell cols="2">43.1±1.1 7</cell><cell>-</cell></row><row><cell>CE [14]</cell><cell cols="6">47.7±0.6 6.0±0.0 23.1±0.5 46.6±0.7 6.0±0.0 24.4±0.5</cell></row><row><cell>HSE [33]</cell><cell>49.3</cell><cell>-</cell><cell>-</cell><cell>48.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="2">54.2±1.0 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Retrieval performance on the LSMDC dataset. 2±0.8 21.0±1.4 76.3±1.9 29.3±1.1 22.5±0.4 77.1±2.6 Ours-pretrained 29.9±0.7 19.3±0.2 75.0±1.2 28.6±0.3 20.0±0.0 76.0±0.8</figDesc><table><row><cell></cell><cell cols="3">Text −→ Video</cell><cell cols="3">Video −→ Text</cell></row><row><cell>Method</cell><cell cols="3">R@5↑ MdR↓ MnR↓</cell><cell cols="3">R@5↑ MdR↓ MnR↓</cell></row><row><cell>Random baseline</cell><cell>0.5</cell><cell>500.0</cell><cell>500.0</cell><cell>0.5</cell><cell>500.0</cell><cell>500.0</cell></row><row><cell>CT-SAN [32]</cell><cell>16.3</cell><cell>46</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>JSFusion [31]</cell><cell>21.2</cell><cell>36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CCA [11] (rep. by [16]) 21.7</cell><cell>33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MEE [16]</cell><cell>25.1</cell><cell>27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MEE-COCO [16]</cell><cell>25.6</cell><cell>27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [14]</cell><cell cols="3">26.9±1.1 25.3±3.1 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>29.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Retrieval performance on the MSRVTT dataset. 1k-A and 1k-B denote test sets of 1000 randomly sampled caption-video pairs used in [31] and [16] resp. 9±1.2 48.8±0.6 62.4±0.8 6.0±0.0 28.2±0.8 20.6±0.6 50.3±0.5 64.0±0.2 5.3±0.6 25.1±0.8 Ours 1k-A 24.6±0.4 54.0±0.2 67.1±0.5 4.0±0.0 26.7±0.9 24.4±0.5 56.0±0.9 67.8±0.3 4.0±0.0 23.6±1.0 26.6±1.0 57.1±1.0 69.6±0.2 4.0±0.0 24.0±0.8 27.0±0.6 57.5±0.6 69.7±0.8 3.7±0.5 21.3±0.6 2±0.7 46.0±0.4 60.7±0.2 7.0±0.0 35.3±1.1 18.0±0.8 46.0±0.5 60.3±0.5 6.5±0.5 30.6±1.2 Ours 1k-B 20.3±0.5 49.1±0.4 63.9±0.5 6.0±0.0 29.5±1.6 21.1±0.4 49.4±0.4 63.2±0.4 6.0±0.0 24.5±1.8</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Text −→ Video</cell><cell></cell><cell></cell><cell cols="3">Video −→ Text</cell><cell></cell></row><row><cell>Method</cell><cell>Split R@1↑</cell><cell cols="4">R@5↑ R@10↑ MdR↓ MnR↓</cell><cell>R@1↑</cell><cell cols="4">R@5↑ R@10↑ MdR↓ MnR↓</cell></row><row><cell cols="2">Random baseline 1k-A 0.1</cell><cell>0.5</cell><cell>1.0</cell><cell cols="2">500.0 500.0</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell cols="2">500.0 500.0</cell></row><row><cell>JSFusion [31]</cell><cell>1k-A 10.2</cell><cell>31.2</cell><cell>43.2</cell><cell>13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HT [17]</cell><cell>1k-A 12.1</cell><cell>35.0</cell><cell>48.0</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CE [14] 1k-A 20.HT-pretrained [17] 1k-A 14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ours-pretrained 1k-A Random baseline 1k-B 0.1</cell><cell>0.5</cell><cell>1.0</cell><cell cols="2">500.0 500.0</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell cols="2">500.0 500.0</cell></row><row><cell>MEE [16]</cell><cell>1k-B 13.6</cell><cell>37.9</cell><cell>51.0</cell><cell>10.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>JPose [26]</cell><cell>1k-B 14.3</cell><cell>38.1</cell><cell>53.0</cell><cell>9</cell><cell>-</cell><cell>16.4</cell><cell>41.3</cell><cell>54.4</cell><cell>8.7</cell><cell>-</cell></row><row><cell cols="2">MEE-COCO [16] 1k-B 14.2</cell><cell>39.2</cell><cell>53.8</cell><cell>9.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [14]</cell><cell>1k-B 18.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Retrieval performance on the ActivityNet dataset. 2±0.3 47.7±0.6 91.4±0.4 6.0±0.0 23.1±0.5 17.7±0.6 46.6±0.7 90.9±0.2 6.0±0.0 24.7±0.2 54.2±1.0 93.2±0.4 5.0±0.0 20.8±0.4 22.9±0.8 54.8±0.4 93.1±0.2 4.3±0.5 21.2±0.5 Ours-pretrained 28.7±0.2 61.4±0.2 94.5±0.0 3.3±0.5 16.0±0.4 28.9±0.2 61.1±0.2 94.3±0.4 4.0±0.0 17.1±0.5</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Text −→ Video</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Video −→ Text</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>R@1↑</cell><cell cols="4">R@5↑ R@50↑ MdR↓ MnR↓</cell><cell>R@1↑</cell><cell cols="4">R@5↑ R@50↑ MdR↓ MnR↓</cell></row><row><cell cols="2">Random baseline 0.02</cell><cell>0.1</cell><cell cols="3">1.02 2458.5 2458.5</cell><cell>0.02</cell><cell>0.1</cell><cell cols="3">1.02 2458.5 2458.5</cell></row><row><cell>FSE [33]</cell><cell cols="4">18.2±0.2 44.8±0.4 89.1±0.3 7</cell><cell>-</cell><cell cols="4">16.7±0.8 43.1±1.1 88.4±0.3 7</cell><cell>-</cell></row><row><cell>CE [14]</cell><cell cols="10">18.4±0.5</cell></row><row><cell>HSE [33]</cell><cell>20.5</cell><cell>49.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.7</cell><cell>48.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>22.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Retrieval performance on the LSMDC dataset. 2±0.4 29.2±0.8 38.8±0.9 21.0±1.4 76.3±1.9 12.1±0.1 29.3±1.1 37.9±1.1 22.5±0.4 77.1±2.6 Ours-pretrained 12.9±0.1 29.9±0.7 40.1±0.8 19.3±0.2 75.0±1.2 12.3±0.2 28.6±0.3 38.9±0.8 20.0±0.0 76.0±0.8</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Text −→ Video</cell><cell></cell><cell></cell><cell cols="3">Video −→ Text</cell><cell></cell></row><row><cell>Method</cell><cell>R@1↑</cell><cell cols="4">R@5↑ R@10↑ MdR↓ MnR↓</cell><cell>R@1↑</cell><cell cols="4">R@5↑ R@10↑ MdR↓ MnR↓</cell></row><row><cell>Random baseline</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500.0</cell><cell>500.0</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500.0</cell><cell>500.0</cell></row><row><cell>CT-SAN [32]</cell><cell>5.1</cell><cell>16.3</cell><cell>25.2</cell><cell>46</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>JSFusion [31]</cell><cell>9.1</cell><cell>21.2</cell><cell>34.1</cell><cell>36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CCA [11] (rep. by [16]) 7.5</cell><cell>21.7</cell><cell>31.0</cell><cell>33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MEE [16]</cell><cell>9.3</cell><cell>25.1</cell><cell>33.4</cell><cell>27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MEE-COCO [16]</cell><cell>10.1</cell><cell>25.6</cell><cell>34.6</cell><cell>27</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [14]</cell><cell cols="5">11.2±0.4 26.9±1.1 34.8±2.0 25.3±3.1 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>13.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank the authors of <ref type="bibr" target="#b13">[14]</ref> for sharing their codebase and features, and Samuel Albanie, in particular, for his help with implementation details. This work was supported in part by the ANR project AVENUE.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language Features Matter: Effective language representations for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CVPR 2020 video pentathlon challenge: Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Video Pentathlon Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Jointly discovering visual objects and spoken words from raw sensory input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Surs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ArXiv abs/1907.13487</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06430</idno>
		<title level="m">Endto-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno>abs/1804.02516</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICMR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint embeddings with multimodal cues for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJMIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">Learning video representations using contrastive bidirectional transformer. arXiv 1906</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5743</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fine-grained action retrieval through multiple parts-of-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>ArXiv 1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Distance metric learning, with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding bag-of-words model: a statistical framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
