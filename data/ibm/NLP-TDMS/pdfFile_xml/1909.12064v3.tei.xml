<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Set Functions for Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Horn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bock</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
						</author>
						<title level="a" type="main">Set Functions for Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that commonly occur in real-world datasets, especially in healthcare applications. This paper proposes a novel approach for classifying irregularly-sampled time series with unaligned measurements, focusing on high scalability and data efficiency. Our method SeFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable with a beneficial memory footprint, thus scaling well to large datasets of long time series and online monitoring scenarios. Furthermore, our approach permits quantifying per-observation contributions to the classification outcome. We extensively compare our method with existing algorithms on multiple healthcare time series datasets and demonstrate that it performs competitively whilst significantly reducing runtime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the increasing digitalization, measurements over extensive time periods are becoming ubiquitous. Nevertheless, in many application domains, such as healthcare <ref type="bibr" target="#b42">(Yadav et al., 2018)</ref>, measurements might not necessarily be observed at a regular rate or could be misaligned. Moreover, the presence or absence of a measurement and its observation frequency may carry information of its own <ref type="bibr" target="#b22">(Little &amp; Rubin, 2014)</ref>, such that imputing the missing values is not always desired.</p><p>While some algorithms can be readily applied to datasets with varying lengths, these methods usually assume regular sampling of the data and/or require the measurements 1 Department of Biosystems Science and Engineering, ETH Zurich, 4058 Basel, Switzerland 2 SIB Swiss Institute of Bioinformatics, Switzerland. Correspondence to: Karsten Borgwardt &lt;karsten.borgwardt@bsse.ethz.ch&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). across modalities to be aligned/synchronized, preventing their application to the aforementioned settings. By contrast, existing approaches, in particular in clinical applications, for unaligned measurements, typically rely on imputation to obtain a regularly-sampled version of a dataset for classification <ref type="bibr" target="#b5">(Desautels et al., 2016</ref><ref type="bibr" target="#b27">, Moor et al., 2019</ref>. Learning a suitable imputation scheme, however, requires understanding the underlying dynamics of a system; this task is significantly more complicated and not necessarily required when classification or pattern detection is the main goal. Furthermore, even though a decoupled imputation scheme followed by classification is generally more scalable, it may lose information (in terms of "missingness patterns") that could be crucial for prediction tasks. The fact that decoupled schemes perform worse than methods that are trained end-to-end was empirically demonstrated by <ref type="bibr" target="#b18">Li &amp; Marlin (2016)</ref>. Approaches that jointly optimize both tasks add a large computational overhead, thus suffering from poor scalability or high memory requirements.</p><p>Our method is motivated by the understanding that, while RNNs and similar architectures are well suited for capturing and modelling the dynamics of a time series and thus excel at tasks such as forecasting, retaining the order of an input sequence can even be a disadvantage in some scenarios <ref type="bibr" target="#b38">(Vinyals et al., 2016)</ref>. We show that by relaxing the condition that a sequence must be processed in order, we can naturally derive an architecture that directly accounts for (i) irregular sampling, and (ii) unsynchronized measurements. Our method SEFT: Set Functions for Time Series, extends recent advances in set function learning to irregular sampled time series classification tasks, yields favourable classification performance, is highly scalable, and improves over current approaches by almost an order of magnitude in terms of runtime. With SEFT, we propose to rephrase the problem of classifying time series as classifying a set of observations. We show how set functions can be used to create classifiers that are applicable to unaligned and irregularly-sampled time series, leading to favourable performance in classification tasks. Next to being highly parallelizable, thus permitting ready extensions to online monitoring setups with thousands of patients, our method also yields importance values for each observation and each modality. This makes it possible to interpret predictions, providing much-needed insights into the decision made by the model. <ref type="bibr">arXiv:1909.12064v3 [cs.</ref>LG] 14 Sep 2020 2. Related Work This paper focuses on classifying time series with irregular sampling and potentially unaligned measurements. We briefly discuss recent work in this field; all approaches can be broadly grouped into the following three categories.</p><p>Irregular sampling as missing data: While the problem of supervised classification in the presence of missing data is closely related to irregular sampling on time series, there are some core differences. Missing data is usually defined with respect to a number of features that could be observed, whereas time series themselves can have different lengths and a "typical" number of observed values might not exist. Generally, an irregularly-sampled time series can be converted into a missing data problem by discretizing the time axis into non-overlapping intervals, and declaring intervals in which no data was sampled as missing <ref type="bibr" target="#b2">(Bahadori &amp; Lipton, 2019)</ref>. This approach is followed by <ref type="bibr" target="#b25">Marlin et al. (2012)</ref>, who used a Gaussian Mixture Model for semi-supervised clustering on electronic health records. Similarly, <ref type="bibr" target="#b21">Lipton et al. (2016)</ref> discretize the time series into intervals, aggregate multiple measurements within an interval, and add missingness indicators to the input of a Recurrent Neural Network. By contrast, <ref type="bibr" target="#b4">Che et al. (2018)</ref> present several variants of the Gated Recurrent Unit (GRU) combined with imputation schemes. Most prominently, the GRU-model was extended to include a decay term (GRU-D), such that the last observed value is decayed to the empirical mean of the time series via a learnable decay term. While these approaches are applicable to irregularly-sampled data, they either rely on imputation schemes or empirical global estimates on the data distribution (our method, by contrast, requires neither), without directly exploiting the global structure of the time series.</p><p>Frameworks supporting irregular sampling: Some frameworks support missing data. For example, <ref type="bibr" target="#b23">Lu et al. (2008)</ref> directly defined a kernel on irregularlysampled time series, permitting subsequent classification and regression with kernel-based classifiers or regression schemes. Furthermore, Gaussian Processes <ref type="bibr" target="#b40">(Williams &amp; Rasmussen, 2006)</ref> constitute a common probabilistic model for time series; they directly permit modelling of continuous time data using mean and covariance functions. Along these lines, <ref type="bibr" target="#b17">Li &amp; Marlin (2015)</ref> derived a kernel on Gaussian Process Posteriors, allowing the comparison and classification of irregularly-sampled time series using kernel-based classifiers. Nevertheless, all of these approaches still rely on separate tuning/training of the imputation method and the classifier so that structures supporting the classification could be potentially missed in the imputation step. An emerging line of research employs Hawkes processes <ref type="bibr" target="#b10">(Hawkes, 1971</ref><ref type="bibr" target="#b20">, Liniger, 2009</ref>), i.e. a specific class of self-exciting point processes, for time se-ries modelling and forecasting <ref type="bibr" target="#b26">(Mei &amp; Eisner, 2017</ref><ref type="bibr" target="#b41">, Xiao et al., 2017</ref>. While Hawkes processes exhibit extraordinary performance in these domains, there is no standardised way of using them for classification. Previous work <ref type="bibr" target="#b24">(Lukasik et al., 2016)</ref> trains multiple Hawkes processes (one for each label) and classifies a time series by assigning it the label that maximises the respective likelihood function. Since this approach does not scale to our datasets, we were unable to perform a fair comparison. We conjecture that further research will be required to make Hawkes processes applicable to general time series classification scenarios.</p><p>End-to-end learning of imputation schemes: Methods of this type are composed of two modules with separate responsibilities, namely an imputation scheme and a classifier, where both components are trained in a discriminative manner and end-to-end using gradient-based training. Recently, <ref type="bibr" target="#b18">Li &amp; Marlin (2016)</ref> proposed the Gaussian Process Adapters (GP Adapters) framework, where the parameters of a Gaussian Process Kernel are trained alongside a classifier. The Gaussian Process gives rise to a fixed-size representation of the irregularly-sampled time series, making it possible to apply any differentiable classification architecture. This approach was further extended to multivariate time series by <ref type="bibr" target="#b6">Futoma et al. (2017)</ref> using Multi-task Gaussian Processes (MGPs) <ref type="bibr" target="#b3">(Bonilla et al., 2008)</ref>, which allow correlations between the imputed channels. Moreover, <ref type="bibr" target="#b6">Futoma et al. (2017)</ref> made the approach more compatible with time series of different lengths by applying a Long Short Term Memory (LSTM) <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref> classifier. Motivated by the limited scalability of approaches based on GP Adapters, <ref type="bibr" target="#b34">Shukla &amp; Marlin (2019)</ref> suggest an alternative imputation scheme, the interpolation prediction networks. It applies multiple semi-parametric interpolation schemes to obtain a regularly-sampled time series representation.</p><p>The parameters of the interpolation network are trained with the classifier in an end-to-end setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Our paper focuses on the problem of time series classification of irregularly sampled and unaligned time series. We first define the required terms before describing our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation &amp; Requirements</head><p>Definition 1 (Time series). We describe a time series of an instance i as a set S i of M := |S i | observations s j such that S i := {s 1 , . . . , s M }. We assume that each observation s j is represented as a tuple (t j , z j , m j ), consisting of a time value t j ∈ R + , an observed value z j ∈ R, and a modality indicator m j ∈ {1 . . . D}, where D represents the dimensionality of the time series. We write  <ref type="figure">Figure 1</ref>. Schematic overview of SEFT's architecture. The first panel exemplifies a potential input, namely a multivariate time series, consisting of 3 modalities m1, m2, m3. We treat the j th observation as a tuple (tj, zj, mj), comprising of a time tj, a value zj, and a modality indicator mj. All observations are summarized as a set of such tuples. The elements of each set are summarized using a set function f . Conditional on the summarized representation and the individual set elements an attention mechanism (as described in Section 3.3) is applied to learn the importance of the individual observations. Respective query vectors for 2 attentions head are illustrated in purple and orange blocks. The results of each attention head are then concatenated and used as the input for the final classification layers.</p><p>Ω ⊆ R + × R × N + to denote the domain of observations. An entire D-dimensional time series can thus be represented as</p><formula xml:id="formula_0">S i := {(t 1 , z 1 , m 1 ) , . . . , (t M , z M , m M )} ,<label>(1)</label></formula><p>where for notational convenience we omitted the index i from individual measurements.</p><p>We leave this definition very general on purpose, in particular allowing the length of each time series to differ, since our models are inherently capable of handling this. Likewise, we neither enforce nor expect all time series to be synchronized, i.e. being sampled at the same time, but rather we are fully agnostic to non-synchronized observations in the sense of not having to observe all modalities at each time point 1 . We collect all time series and their associated labels in a dataset D.</p><p>Definition 2 (Dataset). We consider a dataset D to consist of n time series. Elements of D are tuples, i.e. D := {(S 1 , y 1 ), . . . , (S N , y N )}, where S i denotes the i th time series and y i ∈ {1, . . . , C} its class label.</p><p>For an online monitoring scenario, we will slightly modify Definition 2 and only consider subsets of time series that have already been observed. <ref type="figure">Figure 1</ref> gives a high-level overview of our method, including the individual steps required to perform classification. To get a more intuitive grasp of these definitions, we briefly illustrate our time series notation with an example. Let instance i be an inhospital patient, while the time series represent measurements of two channels of vital parameters during a hospi- <ref type="bibr">1</ref> We make no assumptions about the time values tj and merely require them to be positive real-valued numbers because our time encoding procedure (see below) is symmetric with respect to zero. In practice, positive time values can always be achieved by applying a shift transformation. tal stay, namely heart rate (HR) and mean arterial blood pressure (MAP). We enumerate those channels as modalities 1 and 2. Counting from admission time, a HR of 60 and 65 beats per minute (bpm) was measured after 0.5 h and 3.0 h, respectively, whereas MAP values of 80, 85, and 87 mmHg were observed after 0.5 h, 1.7 h, and 2.5 h. According to Definition 1, the time series is thus represented as S i := {(0.5, 60, 1), (3, 65, 1), (0.5, 80, 2), (1.7, 85, 2), <ref type="bibr" target="#b33">(3,</ref><ref type="bibr">87</ref>, 2)}. In this example, observations are ordered by modality to increase readability; in practice, we are dealing with unordered sets. This does not imply, however, that we "throw away" any time information; we encode time values in our model, thus making it possible to maintain the temporal ordering of observations. Our model, however, does not assume that all observations are stored or processed in the same ordering-this assumption was already shown <ref type="bibr" target="#b38">(Vinyals et al., 2016)</ref> to be detrimental with respect to classification performance in some scenarios. Therefore, our model does not employ a "sequentialness prior": instead of processing a sequence conditional on previouslyseen elements (such as in RNNs or other sequence-based models), it processes values of a sequence all at oncethrough encoding and aggregation steps-and retains all information about event occurrence times.</p><p>In our experiments, we will focus on time series in which certain modalities-channels-are not always observed, i.e. some measurements might be missing. We call such time series non-synchronized.</p><p>Definition 3 (Non-synchronized time series). A D-dimensional time series is non-synchronized if there exists at least one time point t j ∈ R + at which at least one modality is not observed, i.e. if there is t j ∈ R + such that</p><formula xml:id="formula_1">|{(t k , z k , m k ) | t k = t j }| = D.</formula><p>Furthermore, we require that the measurements of each modality satisfy t i = t j for i = j, such that no two measurements of of the same modality occur at the same time. This assumption is not required for technical reasons but for consistency; moreover, it permits interpreting the results later on.</p><p>Summary To summarize our generic setup, we do not require M , the number of observations per time series, to be the same, i.e. |S i | = |S j | for i = j is permitted, nor do we assume that the time points and modalities of the observations are the same across time series. This setting is common in clinical and biomedical time series. Since typical machine learning algorithms are designed to operate on data of a fixed dimension, novel approaches to this non-trivial problem are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Model</head><p>In the following, we describe an approach inspired by differentiable learning of functions that operate on sets <ref type="bibr" target="#b44">(Zaheer et al., 2017</ref><ref type="bibr" target="#b39">, Wagstaff et al., 2019</ref>. The following paragraphs provide a brief overview of this domain, while describing the building blocks of our model. Specifically, we phrase the problem of classifying time series on irregular grids as learning a function f on a set of arbitrarily many time series observations following Defini-</p><formula xml:id="formula_2">tion 1, i.e. S = {(t 1 , z 1 , m 1 ), . . . , (t M , z M , m M )}, such that f : S → R C , where</formula><p>S represents a generic time series of arbitrary cardinality and R C corresponds to the logits of the C classes in the dataset. As we previously discussed, we interpret each time series as an unordered set of measurements, where all information is conserved because the observation time is included for each set element. Specifically, we define f to be a set function, i.e. a function that operates on a set and thus has to be invariant to the ordering of the elements in the set. Multiple architectures are applicable to constructing set functions such as Transformers <ref type="bibr" target="#b16">(Lee et al., 2019</ref><ref type="bibr" target="#b37">, Vaswani et al., 2017</ref>, or Deep Sets <ref type="bibr" target="#b44">(Zaheer et al., 2017)</ref>. Given its exceptional scalability properties, we base this work on the framework of <ref type="bibr" target="#b44">Zaheer et al. (2017)</ref>. Intuitively, this amounts to computing multivariate dataset-specific summary statistics, which are optimized to maximize classification performance. Thus, we sum-decompose the set function f into the form</p><formula xml:id="formula_3">f (S) = g   1 |S| sj ∈S h(s j )   (2)</formula><p>where h : Ω → R d and g : R d → R C are neural networks, d ∈ N + determines the dimensionality of the latent representation, and s j represents a single observation of the time series S. We can view the averaged representations 1/|S| sj ∈S h(s j ) in general as a dataset-specific summary statistic learned to best distinguish the class labels.</p><p>Equation 2 also implies the beneficial scalability properties of our approach: each embedding can be calculated independently of the others; hence, the constant computational cost of passing a single observation through the function h is scaled by the number of observations, resulting in a runtime of O(M ) for a time series of length M .</p><p>Recently, <ref type="bibr" target="#b39">Wagstaff et al. (2019)</ref> derived requirements for a practical universal function representation of sumdecomposable set functions, i.e the requirements necessary for a sum-decomposable function to represent an arbitrary set-function given that h and g are arbitrarily expressive.</p><p>In particular, they show that a universal function representation can only be guaranteed provided that d ≥ max i |S i | is satisfied. During hyperparameter search, we therefore independently sample the dimensionality of the aggregation space, and allow it to be in the order of the number of observations that are to be expected in the dataset. Further, we explored the utilization of max, sum, and mean as alternative aggregation functions inspired by Zaheer et al. <ref type="formula" target="#formula_0">(2017)</ref>, <ref type="bibr" target="#b7">Garnelo et al. (2018)</ref>.</p><p>Intuitively, our method can be related to Takens's embedding theorem <ref type="bibr" target="#b36">(Takens, 1981)</ref> for dynamical systems: we also observe a set of samples from some unknown (but deterministic) dynamical process; provided the dimensionality of our architecture is sufficiently large 2 , we are capable of reconstructing the system up to diffeomorphism. The crucial difference is that we do not have to construct a time-delay embedding (since we are not interested in being able to perfectly reproduce the dynamics of the system) but rather, we let the network learn an embedding that is suitable for classification, which is arguably a simpler task due to its restricted scope.</p><p>Time Encoding To represent the time point of an observation on a normalized scale, we employ a variant of positional encodings <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>. Preliminary results indicated that this encoding scheme reduces the sensitivity towards initialization and training hyperparameters of a model. Specifically, the time encoding converts the 1-dimensional time axis into a multi-dimensional input by passing the time t of each observation through multiple trigonometric functions of varying frequencies. Given a dimensionality τ ∈ N + of the time encoding, we refer to the encoded position as x ∈ R τ , where with k ∈ {0, . . . , τ /2} and t representing the maximum time scale that is expected in the data. Intuitively, we select the wavelengths using a geometric progression from 2π to t · 2π, and treat the number of steps and the maximum timescale t as hyperparameters of the model. We used time encodings for all experiments, such that an observation is represented as s j = (x (t j ) , z j , m j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention-based Aggregation</head><p>So far, our method permits encoding sets of arbitrary sizes into a fixed-size representation. For increasingly large set sizes, however, many irrelevant observations could influence the result of the set function. The mean aggregation function is particularly susceptible to this because the influence of an observation to the embedding shrinks proportionally to the size of the set. We thus suggest to use a weighted mean in order to allow the model to decide which observations are relevant and which should be considered irrelevant. This is equivalent to computing an attention over the set input elements, and subsequently, computing the sum over all elements in the set.</p><p>Our approach is based on scaled dot-product attention with multiple heads i ∈ {1, . . . , m} in order to be able to cover different aspects of the aggregated set 3 . We define a(S, s j ), i.e. the attention weight function of an individual time series, to depend on the overall set of observations S, and the value of the set element s j . This is achieved by computing an embedding of the set elements using a smaller set function f , and projecting the concatenation of the set representation and the individual set elements into a d-dimensional space. Specifically, we have</p><formula xml:id="formula_4">K j,i = [f (S), s j ] T W i where W i ∈ R (im(f )+|sj |)×d and K ∈ R |S|×d .</formula><p>Furthermore, we define a matrix of query points Q ∈ R m×d , which allow the model to summarize different aspects of the dataset via</p><formula xml:id="formula_5">e j,i = K j,i · Q i √ d and a j,i = exp(e j,i ) j exp(e j,i )</formula><p>where a j,i represents the amount of attention that head i gives to set element j. The head-specific row Q i of the query matrix Q allows a head to focus on individual aspects (such as the distribution of one or multiple modalities) of a time series. For each head, we multiply the set element embeddings computed via the function h with the attentions derived for the individual instances, i.e. r i = j a j,i h(s j ). The computed representation is concatenated and passed to the aggregation network g θ as in a regular set function, i.e. r * = [r 1 . . . r m ]. In our setup, we initialize Q with zeros, such that at the beginning of train-ing, the attention mechanism is equivalent to computing the unweighted mean over the set elements.</p><p>Overall, this aggregation function is similar to Transformers <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, but differs from them in a few key aspects. Commonly, Transformer blocks use the information from all set elements to compute the embedding of an individual set element, leading to a runtime and space complexity of O(n 2 ). By contrast, our approach computes the embeddings of set elements independently, leading to lower runtime and memory complexity of O(n). This is particularly relevant as set elements in our case are individual observations, so that we obtain set sizes that are often multiples of the time series length. Furthermore, we observed that computing embeddings with information from other set elements (as the Transformer does) actually decreases generalization performance in several scenarios (please refer to <ref type="table">Table 1</ref> for details).</p><p>Online monitoring scenario In an online monitoring scenario, we compute all variables of the model in a cumulative fashion. The set of observations used to predict at the current time point is therefore a subset of the total observations available at the time point at which the prediction is made. If this were computed naïvely, the attention computation would result in O(|S|) runtime and memory complexity, where |S| is the number of observations. Instead we rearrange the computation of the weighted mean as follows, while discarding the head indices for simplicity:</p><formula xml:id="formula_6">f (S i ) = j≤i exp(e j ) k≤i exp(e k ) h(s j ) = j≤i exp(e j )h(s j ) k≤i exp(e k )</formula><p>In the previous equation, both numerator and denominator can be computed in a cumulative fashion and thus allow reusing computations from previous time points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">LOSS FUNCTION</head><p>If not mentioned otherwise, we choose h and g in Equation 2 to be multilayer perceptron deep neural networks, parametrized by weights θ and ψ, respectively. We thus denote these neural networks by h θ and g ψ ; their parameters are shared across all instances per dataset. Our training setup follows <ref type="bibr" target="#b44">Zaheer et al. (2017)</ref>; we apply the set function to the complete time series, i.e. to the set of all observations for each time series. Overall, we optimize a loss function that is defined as</p><formula xml:id="formula_7">L(θ, ψ) := E (S,y)∈D y; g ψ s j ∈S a(S, s j )h θ (s j ) ,</formula><p>where (·) represents a task-specific loss function. In all of our experiments, we utilize the binary cross-entropy loss in combination with a sigmoid activation function in the last layer of g ψ for binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We executed all experiments and implementations in a unified and modular code base, which we make available to the community. We provide two dedicated packages (i) for automatic downloading and preprocessing of the datasets according to the splits used in this work and (ii) for training the introduced method and baselines to which we compare in the following. We make both publicly available 4 . While some of the datasets used in the following have access restrictions, anybody can gain access after satisfying the defined requirements. This ensures the reproducibility of our results. Please consult Appendix A.3 for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In order to benchmark the proposed method, we selected three datasets with irregularly-sampled and nonsynchronized measurements. We are focusing on two tasks with different challenges: first, we predict patient mortality on two datasets; this task is exacerbated by the high imbalance in the datasets. Second, we predict the onset of sepsis 5 in an online scenario.</p><p>MIMIC-III Mortality Prediction MIMIC-III <ref type="bibr" target="#b12">(Johnson et al., 2016</ref>) is a widely-used, freely-accessible dataset consisting of distinct ICU stays of patients. The median length of a stay is 2.1 d; a wide range of physiological measurements (e.g. MAP and HR) are recorded with a resolution of 1 h. Furthermore, laboratory test results, collected at irregular time intervals, are available. Recently, <ref type="bibr" target="#b9">Harutyunyan et al. (2019)</ref> defined a set of machine learning tasks, labels, and benchmarks using a subset of the MIMIC-III dataset. We trained and evaluated our method and competing methods on the binary mortality prediction task (M3-Mortality), while discarding the binning step and applying additional filtering described in Appendix A.1. The goal of the mortality prediction task (which we abbreviate as M3-Mortality) is to predict whether a patient will die during their hospital stay using only data from the first 48 h of the ICU stay. In total, the dataset contains around 21, 000 stays of which approximately 10 % result in death.</p><p>Physionet 2012 Mortality Prediction Challenge The 2012 Physionet challenge dataset <ref type="bibr" target="#b8">(Goldberger et al., 2000)</ref>, 4 https://github.com/BorgwardtLab/Set_ Functions_for_Time_Series 5 An organ dysfunction caused by a dysregulated host response to an infection. Sepsis is potentially life-threatening and is associated with high mortality of patients. which we abbreviate P-Mortality, contains 12, 000 ICU stays each of which lasts at least 48 h. For each stay, a set of general descriptors (such as gender or age) are collected at admission time. Depending on the course of the stay and patient status, up to 37 time series variables were measured (e.g. blood pressure, lactate, and respiration rate). While some modalities might be measured in regular time intervals (e.g. hourly or daily), some are only collected when required; moreover, not all variables are available for each stay. Similar to M3-Mortality, our goal is to predict whether a patient will die during the hospital stay. The training set comprises 8, 000 stays, while the testing set comprises 4, 000 ICU visits. The datasets are similarly imbalanced, with a prevalence of around 14 %. Please refer to <ref type="table">Table A.2, Table A</ref>.1, and Table A.3 in the appendix for a more detailed enumeration of samples sizes, label distributions, and the handling of demographics data. The total number of samples slightly deviates from the originallypublished splits, as time series of excessive length precluded us from fitting some methods in reasonable time, and we therefore excluded them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physionet 2019 Sepsis Early Prediction Challenge</head><p>Given the high global epidemiological burden of sepsis, <ref type="bibr" target="#b30">Reyna et al. (2020)</ref> launched a challenge for the early detection of sepsis from clinical data. Observations from over 60, 000 intensive care unit patients from three different U.S. hospitals were aggregated. Up to 40 variables (e.g. vitals signs and lab results) were recorded hourly, with each hour being labelled with a binary variable indicating whether an onset of sepsis-according to the Sepsis-3 definition (Seymour et al., 2016)-occurred. Our task is the hourly prediction of sepsis onset within the next 6 h to 12 h. In our work we refer to this task as P-Sepsis. To account for clinical utility of a model, the authors introduced a novel evaluation metric (see <ref type="bibr" target="#b30">Reyna et al. (2020)</ref> for more details), which we also report in our experiments: at each prediction time point t, a classifier is either rewarded or penalized using a utility function U (p, t), which depends on the distance to the actual sepsis onset for patient p. The total utility function is the sum over all patients P and the predictions at all time points T , i.e. U total := p∈P t∈T U (p, t). The score is then normalized (U norm ) such that a perfect classifier receives a score of 1, while a classifier with no positive predictions at all receives a score of 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Competitor Methods</head><p>In order to achieve a thorough comparison, we compare our method to the following six approaches: 1. GRUsimple <ref type="bibr" target="#b4">(Che et al., 2018)</ref>  All methods except LATENT-ODE were implemented in the same framework using the same training pipeline. Due to differences in implementation and limited comparability, we highlight the results of LATENT-ODE with † . In particular, for LatentODE we were unable to run an extensive hyperparameter search using the provided codebase, as runtime was considerable higher compared to any other method. This results in an underestimation of performance for LATENT-ODE compared to the other methods. For a detailed description of the differences between the implementations, we refer the reader to Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Setup</head><p>To mitigate the problem of unbalanced datasets, all models were trained on balanced batches of the training data rather than utilizing class weights. This was done in order to not penalize models with a higher memory footprint 6 . Due to oversampling, the notion of an epoch is different from common understanding. In our experiments we set the number of optimizer steps per epoch to be the minimum of the number of steps required for seeing all samples from the majority class and the number of steps required to see each samples from the minority class three times. Training was stopped after 30 epochs without improvement of the area under the precision-recall curve (AUPRC) on the validation data for the mortality prediction tasks, whereas balanced accuracy was utilized for the online predictions scenario. The hyperparameters with the best overall validation performance were selected for quantifying the performance on the test set. The train, validation, and test splits were the same for all models and all evaluations. To permit a fair comparison between the methods, we executed hyperparameter searches for each model on each dataset, composed of uniformly sampling 20 parameters according to Appendix A.4. Final performance on the test set was calculated by 3 independent runs of the models; evaluation took place after the model was restored to the state with the best validation AUPRC / balanced accuracy. In all subsequent benchmarks, we use the standard deviation of the test performance of these runs as generalization performance estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>Table 1 depicts the results on the two mortality prediction tasks. For each metric, we use bold font to indicate the best value, and italics to indicate the second-best. Overall, our proposed method SEFT-ATTN exhibits competi- <ref type="table">Table 1</ref>. Performance comparison of methods on mortality prediction datasets. "AUROC" denotes the area under the Receiver Operating Characteristic (ROC) curve; "AUPRC" denotes the area under the precision-recall curve. Evaluation metrics were scaled to 100 in order to increase readability. † denotes that the performance could be underestimated due to limited hyperparameter tuning compared to other methods. tive performance. In terms of AUPRC, arguably the most appropriate metric for unbalanced datasets such as these, we consistently rank among the first four methods. For M3-Mortality, abbreviated as M3M in the table, our runtime performance is lower than that of TRANSFORMER, but we outperform it in terms of AUPRC. Here both GRU-D and IP-NETS outperform the devised approach, while exhibiting considerably higher runtimes. The favourable trade-offs of SEFT-ATTN between runtime and AUPRC are further underscored by <ref type="figure" target="#fig_1">Figure 2</ref>. On P-Mortality, abbreviated as P12 in the table, our method is comparable to the performance of GRU-D and TRANSFORMER and shows comparable or lower runtime. This picture is similar for the Area under the ROC curve (AUROC), where IP-NETS show a slightly higher performance than our approach, at a cost of almost three-fold higher runtime.</p><p>Opening the black box In the medical domain, it is of particular interest to understand the decisions a model makes based on the input it is provided with. The for- <ref type="figure">Figure 3</ref>. Visualizations of a single attention head on an instance of the P-Mortality dataset. We display a set of blood pressure variables which are most relevant for assessing patient stability: non-invasive diastolic arterial blood pressure (NIDiasABP), non-invasive systolic arterial blood pressure (NISysABP), and invasively measured systolic arterial blood pressure (SysABP). Darker colors represent higher attention values. In the invasive channel showing high time resolution (right most panel), our model attends foremost to the region after a sudden increase in blood pressure. In the non-invasive, intermittently observed channels, the model additionally focuses on regions of high observation density reflecting the clinicians concern. mulation of our model and its per-observation perspective on time series gives it the unique property of being able to quantify to which extent an individual observation contributed to the output of the model. We exemplify this in <ref type="figure">Figure 3</ref> with a patient time series that was combined with the attention values of our model, displayed for a set of clinically relevant variables. After reviewing these records with our medical expert, we find that in channels showing frequent and regularly-spaced observations, the model attends to drastic changes. For instance, see the sudden increase in continuously-monitored invasive systolic blood pressure. Interestingly, in channels that are observed only intermittently (due to manual intervention, such as noninvasive blood pressure measurements), we observe that our model additionally attends to regions of high observation density, thereby reflecting the increased concern of clinicians for the circulatory stability of patients.</p><p>Online Prediction Scenario In order to provide models with potential clinical applicability, it is instrumental to cover online monitoring scenario to potentially support clinicians. We present the results of the Sepsis early prediction online monitoring scenario P-Sepsis in <ref type="table">Table 2</ref>. In this scenario the TRANSFORMER and IP-NETS yield the highest performance and outperform all other methods almost two-fold. These results are very surprising, given that the best out of 853 submissions to the Physionet 2019 challenge only achieved a test utility score of 0.36 <ref type="bibr" target="#b30">(Reyna et al., 2020)</ref>. In order to investigate this issue further, we designed a second evaluation scenario, where future information of each instance is guaranteed to be unavailable to the model, by splitting each instance into multiple cumulative versions of increasing length. We then ran all trained models in this scenario and included results for models where the performance metrics differ in <ref type="table">Table 2</ref>, highlighted with an additional * . It is clearly recognizable that the performance of both IP-NETS and TRANSFORMER decrease in the second evaluation scenario indicating the models' reliance on leaked future information.</p><p>For IP-NETS, information can leak through the nonparametric imputation step prior to the application of the downstream recurrent neural network. It is infeasible to train the vanilla IP-NETS approach on slices of the time series up until the time point of prediction, as we cannot reuse computations from previous imputation steps. While it would be possible to construct an IP-NETS variant that does not rely on future information during the imputation step, for example using smoothing techniques, we deem this beyond the scope of this work.</p><p>Similar effects occur in the case of the TRANSFORMER: While observations from the future are masked in the attention computation, preventing access to future values results in a detrimental reduction in performance. Even though the source of dependence on future values is quite probable to reside in the layer normalization applied in the TRANS-FORMER model, the performance drop can have multiple explanations, i.e. 1. the absence of future time points leads to high variance estimates of mean and variance in the layer norm operation, resulting in bad performance in the initial time points of the time series, or 2. the model actively exploits future information though the layer normalization. This could for example be possible by the model looking for indicative signals in future time points and when present returning very high norm outputs. The signature of these high norm outputs can then, through the layer norm operation, be observed in earlier time points. While one could construct variants where the TRANSFORMER model can by no means access future information, for example by replacing the layer norm layer with and alternative normalization scheme <ref type="bibr" target="#b29">(Nguyen &amp; Salazar, 2019</ref><ref type="bibr" target="#b1">, Bachlechner et al., 2020</ref>, we reserve a more thorough investigation of this issue for future work.</p><p>By contrast, our model does not contain any means of leaking future information into the prediction of the current time point and thus exhibits the same performance in both evaluation scenarios, while remaining competitive with alternative approaches. Surprisingly, the model with <ref type="table">Table 2</ref>. Results of the online prediction scenario on the P-Sepsis task. The dataset is highly imbalanced, such that we only report measures which are sensitive to class imbalance. Further, if the results between the evaluation scenarios differ, we highlight results without masked future information in gray, and the performance achieved with masking with * . † indicates that the results might be underestimating the true performance due to limited hyperparameter tuning compared to the other methods. 28.5 TRANSFORMER * 53.6 ± 1.7 3.63 ± 0.95 65.8 ± 3.7 −43.9 ± 10.0 28.5 SEFT-ATTN 70.9 ± 0.8 4.84 ± 0.22 76.8 ± 0.9 25.6 ± 1.9 77.5 the highest performance in this scenario is GRU-SIMPLE, which could be explained by the very regular sampling character of the P-Sepsis dataset. Here the measurements were already binned into hours, such that it cannot be considered completely irregularly sampled. This explains the high performance of GRU-SIMPLE, as compared to models which were specifically designed to cope with the irregular sampling problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>We presented a novel approach for classifying time series with irregularly-sampled and unaligned-i.e. nonsynchronized-observations. Our approach yields competitive performance on real-world datasets with low runtime compared to many competitors. While it does not out perform state-of-the-art models, it shows that shifting the perspective to individual observations represents a promising avenue for models on irregularly-sampled data in the future. Further, as the model does not contain any "sequential" inductive biases compared to RNNs, it indicates that for time series classification tasks, this bias might not be of high importance. This is in line with recent research on Transformer architectures for NLP <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, where order is solely retained through positional embeddings and not inherent to the processing structure of the model. Our experiments demonstrated that combining the perspective of individual observations with an attention mechanism permits increasing the interpretability of the model. This is particularly relevant for medical and healthcare applications.</p><p>Along these lines, we also want to briefly discuss a phenomenon that we observed on M3-Mortality: the performance we report on this task is often lower than the one reported in the original paper <ref type="bibr" target="#b9">(Harutyunyan et al., 2019)</ref> and follow-up work <ref type="bibr" target="#b35">(Song et al., 2018)</ref>. We suspect that this is most likely caused by a distribution shift between the validation dataset and the test dataset: in fact, model performance is on average 6.3% higher (in terms of AUPRC) on the validation data set than on the test dataset, which might indicate that the validation dataset is not representative here. We also notice that previous work <ref type="bibr" target="#b9">(Harutyunyan et al., 2019)</ref> uses heavily-regularised and comparatively small models for this specific scenario, making them more impervious to distribution shifts. Finally, the fact that we do not bin the observations prior to the application of the models could make our task more difficult compared to the original setup.</p><p>Additionally, we would like to discuss the low performance of the LATENT-ODE model. This can be partially attributed to the fact that we did not perform an extensive hyperparameter search with this model. Furthermore, all runs of the LATENT-ODE model contain an additional reconstruction loss term, so that it is necessary to define the trade-off between reconstruction and classification.</p><p>In this work we used the same parameters as <ref type="bibr" target="#b31">Rubanova et al. (2019)</ref>, which (due to potentially different characteristics of the datasets) could lead to a less than optimal tradeoff. For example, some modalities in the datasets we considered might be harder to reconstruct, such that the value of the reconstruction term could be higher, leading to gradients which prioritize reconstruction over classification. Nevertheless, despite these differences the performance of LATENT-ODE in terms of AUC on the P-Mortality dataset measured in this work is actually higher than the performance reported in the original publication.</p><p>In future work, we plan to explore attention mechanisms specifically designed for sets of very high cardinalities.</p><p>We also strive to make the attention computation more robust so that elements with low attentions values do not get neglected due to numerical imprecision of aggregation operations; this is also known as catastrophic cancellation <ref type="bibr" target="#b15">(Knuth, 1998)</ref>, in our case, summation. GPU implementations of algorithms such as <ref type="bibr">Kahan summation (Kahan, 1965)</ref> would represent a promising avenue for further improving performance of attention mechanisms for set functions.</p><p>A. Appendix In the case of the P-Mortality dataset, some instances did not contain any time series information at all and were thus removed. This led to the exclusion of the following 12 patients: <ref type="bibr">140501,</ref><ref type="bibr">150649,</ref><ref type="bibr">140936,</ref><ref type="bibr">143656,</ref><ref type="bibr">141264,</ref><ref type="bibr">145611,</ref><ref type="bibr">142998,</ref><ref type="bibr">147514,</ref><ref type="bibr">142731,</ref><ref type="bibr">150309,</ref><ref type="bibr">155655,</ref><ref type="bibr">156254</ref>.</p><p>For P-Sepsis some instances did not contain static values or were lacking time series information all together. We thus excluded the following files: p013777.psv, p108796.psv, p115810.psv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static variables</head><p>The datasets often also contain information about static variables, such as age and gender. Table A.4 lists all the static variables for each of them. Splits All datasets were partitioned into three subsets training, validation and testing. For the M-Mortality dataset, the same splits as in <ref type="bibr" target="#b9">(Harutyunyan et al., 2019)</ref> were used to ensure comparability of the obtained results. For both Physionet datasets (P-Mortality and P-Sepsis), we did not have access to the held-out test set used in the challenges and thus defined our own splits. For this, the full dataset was split into a training split (80%) and a testing split (20%), while stratifying such that the splits have (approximately) the same class imbalance. This procedure was repeated on the training data to additionally create a validation split. In the case of the online task P-Sepsis, stratification was based on whether the patient develops sepsis or not.</p><p>Implementation We provide the complete data preprocessing pipeline including the splits used to generate the results in this work as a separate Python package medical-ts-datasets, which integrates with tensorflow-datasets(TFD). This permits other researchers to directly compare to the results in this work. By doing so, we strive to enable more rapid progress in the medical time series community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Comparison partners</head><p>The following paragraphs give a brief overview of the methods that we used as comparison partners in our experiments.</p><p>GRU-simple GRU-SIMPLE <ref type="bibr" target="#b4">(Che et al., 2018)</ref> augments the input at time t of a Gated-Recurrent-Unit RNN with a measurement mask m d t and a δ t matrix, which contains the time since the last measurement of the corresponding modality d, such that</p><formula xml:id="formula_8">δ t =      s t − s t−1 + δ d t−1 t &gt; 1, m d t−1 = 0 s t − s t−1 t &gt; 1, m d t−1 = 1 0 t = 0</formula><p>where s t represents the time associated with time step t.</p><p>GRU-D GRU-D or GRU-Decay <ref type="bibr" target="#b4">(Che et al., 2018)</ref> contains modifications to the GRU RNN cell, allowing it to decay past observations to the mean imputation of a modality using a learnable decay rate. By additionally providing the measurement masks as an input the recurrent neural network the last feed in value. Learns how fast to decay back to a mean imputation of the missing data modality.</p><p>Phased-LSTM The PHASED-LSTM <ref type="bibr" target="#b28">(Neil et al., 2016)</ref> introduced a biologically inspired time dependent gating mechanism of a Long short-term RNN cell <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref>. This allows the network to handle event-based sequences with irregularly spaced observations, but not unaligned measurements. We thus additionally augment the input in a similar fashion as described for the GRU-SIMPLE approach.</p><p>Interpolation Prediction Networks IP-NETWORKS <ref type="bibr" target="#b34">(Shukla &amp; Marlin, 2019)</ref> apply multiple semi-parametric interpolation schemes to irregularlysampled time series to obtain regularly-sampled representations that cover long-term trends, transients, and also sampling information. The parameters of the interpolation network are trained with the classifier in an end-to-end fashion.</p><p>Transformer In the TRANSFORMER architecture <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, the elements of a sequence are encoded simultaneously and information between sequence elements is captured using Multi-Head-Attention blocks. Transformers are typically used for sequenceto-sequence modelling tasks. In our setup, we adapted them to classification tasks by mean-aggregating the final representation. This representation is then fed into a one-layer MLP to predict logits for the individual classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Implementation details</head><p>All experiments were run using tensorflow 1.15.2 and training was performed on NVIDIA Geforce GTX 1080Ti GPUs. In order to allow a fair comparison between methods, the input processing pipeline employed caching of model-specific representations and transformations of the data.</p><p>In contrast, due to the high complexity of the LATENT-ODE model, we relied on the implementation provided by the authors and introduced our datasets into their code. This introduces the following differences between the evaluation of LATENT-ODE compared to the other methods: 1. input processing pipeline is not cached 2. model code is written in PyTorch 3. due to an order of magnitude higher runtime, a thorough hyperparameter search was not feasi-ble . This can introduce biases both in terms of runtime and performance compared to the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Training, Model Architectures, and Hyperparameter Search</head><p>General All models were trained using the Adam optimizer <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2015)</ref>, while log-uniformly sampling the learning rate between 0.01 and 0.0001. Further, the batch size of all methods was sampled from the values <ref type="bibr">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512)</ref>.</p><p>Recurrent neural networks For the RNN based methods (GRU-SIMPLE, PHASED-LSTM, GRU-D and IP-NETS), the number of units was sampled from the values <ref type="bibr">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024)</ref>. Further, recurrent dropout and input dropout were sampled from the values (0.0, 0.1, 0.2, 0.3, 0.4). For the PHASED-LSTM method, however, we did not apply dropout to the recurrent state and the inputs, as the learnt frequencies were hypothesized to fulfil a similar function as dropout <ref type="bibr" target="#b28">(Neil et al., 2016)</ref>. We additionally sample parameters that are specific to PHASED-LSTM: if peephole connections should be used, the leak rate from (0.001, 0.005, 0.01) and the maximal wavelength for initializing the hidden state phases from the range (10, 100, 1000). For IP-NETS, we additionally sample the imputation stepsize uniformly from the range (0.5, 1., 2.5, 5.) and the fraction of reconstructed data points from (0.05, 0.1, 0.2, 0.5, 0.75).</p><p>Static variables were handled by computing the initial hidden state of the RNNs conditional on the static variables. For all methods, the computation was performed using a one-hidden-layer neural network with the number of hidden units set to the number of hidden units in the RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEFT-Attn</head><p>We vary the number of layers, dropout in between the layers and the number of nodes per layer for both the encoding network h θ and the aggregation network g ψ from the same ranges. The number of layers is randomly sampled between 1 and 5, the number of nodes in a layer are uniformly sampled from the range <ref type="bibr">(16,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512)</ref> and the dropout fraction is sampled from the values (0.0, 0.1, 0.2, 0.3). The width of the embedding space prior to aggregation is sampled from the values <ref type="bibr">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048)</ref>. The aggregation function was set to be sum as described in the text. The number of dimensions used for the positional embedding τ is selected uniformly from (4, 8, 16) and t, i.e. the maximum time scale, was selected from the values <ref type="figure">(10, 100, 1000)</ref>. The attention network f was set to always use mean aggregation. Furthermore, we use a constant architecture for the attention network f with 2 layers, 64 nodes per layer, 4 heads and a dimensionality of the dot product space d of 128. We sample the amount of attention dropout uniformly from the values (0.0, 0.1, 0.25, 0.5).</p><p>Transformer We utilize the same model architecture as defined in <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref>, where we use an MLP with a single hidden layer as a feed-forward network, with dimensionality of the hidden layer selected to be twice the model dimensionality. The Transformer architecture was applied to the time series by concatenating the vectors of each time point with a measurement indicator. If no value was measured, input was set to zero for this modality. The parameters for the Transformer network were sampled according to the following criteria: the dimensionality of the model was sampled uniformly from the values <ref type="bibr">(64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024)</ref>, the number of attention heads per layer from the values <ref type="bibr">(2,</ref><ref type="bibr">4,</ref><ref type="bibr">8)</ref>, and the number of layers from the range [1, 6] ∈ N. Moreover, we sampled the amount of dropout of the residual connections and the amount of attention dropout uniformly from the values (0.0, 0.1, 0.2, 0.3, 0.5), and the maximal timescale for the time embedding from the values (10, 100, 1000) (similar to the SEFT approach). Further, 1000 steps of warmup were applied, where the learning rate was linearly scaled from lr min = 0 to the learning rate lr max sampled by the hyperparameter search.</p><p>Latent-ODE We utilize the implementation from <ref type="bibr" target="#b31">Rubanova et al. (2019)</ref> and extended the evaluation metrics and datasets to fit our scenario. Due to the long training time almost an order of magnitude longer than any other method considered a thorough hyperparameter search as executed for the other methods was not possible. We thus rely on the hyperparameters selected by the authors. In particular, we use their physionet 2012 dataset settings for all datasets. For further details see <ref type="table">Table A</ref>.5.</p><p>Selected hyperparameters In order to ensure reproducibility, the parameters selected by the hyperparameter search are shown in <ref type="table">Table A</ref>.5 for all model dataset combinations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2. GRU-Decay<ref type="bibr" target="#b4">(Che et al., 2018)</ref> 3. Phased-LSTM<ref type="bibr" target="#b28">(Neil et al., 2016)</ref> 4. Interpolation Prediction Networks<ref type="bibr" target="#b34">(Shukla &amp; Marlin, 2019)</ref> 5. Trans-former<ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> 6. Latent-ODE<ref type="bibr" target="#b31">(Rubanova et al., 2019)</ref> .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>0.7 48.3 ± 0.4 83.2 ± 0.5 81.2 ± 8.5 PHASED-LSTM 73.8 ± 3.3 37.1 ± 0.5 80.3 ± 0.4 166 ± 7 TRANSFORMER 77.4 ± 5.6 42.6 ± 1.0 82.1 ± 0.3 20.1 ± 0.1 LATENT-ODE † 72.8 ± 1.7 39.5 ± 0.5 80.9 ± 0.2 4622 SEFT-ATTN 79.0 ± 2.2 46.3 ± 0.5 83.9 ± 0.4 14.5 ± 0.5 P12 GRU-D 80.0 ± 2.9 53.7 ± 0.9 86.3 ± 0.3 8.67 ± 0.49 GRU-SIMPLE 82.2 ± 0.2 42.2 ± 0.6 80.8 ± 1.1 30.0 ± 2.5 IP-NETS 79.4 ± 0.3 51.0 ± 0.6 86.0 ± 0.2 25.3 ± 1.8 PHASED-LSTM 76.8 ± 5.2 38.7 ± 1.5 79.0 ± 1.0 44.6 ± 2.3 TRANSFORMER 83.7 ± 3.5 52.8 ± 2.2 86.3 ± 0.8 6.06 ± 0.06 LATENT-ODE † 76.0 ± 0.1 50.7 ± 1.7 85.7 ± 0.6 3500 SEFT-ATTN 75.3 ± 3.5 52.4 ± 1.1 85.1 ± 0.4 7.62 ± 0Runtime vs. AUPRC trade-offs for all methods on the two mortality prediction tasks. LATENT-ODE is not shown as its runtime is significantly higher compared to the other models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table A.1. M3-Mortality prevalence of labels for the binary classification task</figDesc><table><row><cell>Train Test</cell><cell>Val</cell></row><row><cell cols="2">In-hospital deaths 0.135 0.116 0.135</cell></row><row><cell cols="2">Table A.2. P-Mortality prevalence of labels for the binary</cell></row><row><cell>classification task</cell><cell></cell></row><row><cell>Train Test</cell><cell>Val</cell></row><row><cell cols="2">In-hospital deaths 0.142 0.142 0.142</cell></row><row><cell cols="2">Table A.3. P-Sepsis prevalence of labels for the online predic-</cell></row><row><cell>tion task</cell><cell></cell></row><row><cell>Train Test</cell><cell>Val</cell></row><row><cell cols="2">Sepsis occurrence 0.018 0.018 0.018</cell></row><row><cell>A.1. Dataset preprocessing</cell><cell></cell></row><row><cell cols="2">Filtering Due to memory requirements of some of the</cell></row><row><cell cols="2">competitor methods, it was necessary to excluded time</cell></row><row><cell cols="2">series with an extremely large number of measurements.</cell></row><row><cell cols="2">For M3-Mortality, patients with more than 1000 time</cell></row><row><cell cols="2">points were discarded as they contained dramatically dif-</cell></row><row><cell cols="2">ferent measuring frequencies compared to the rest of the</cell></row><row><cell cols="2">dataset. This led to the exclusion of the following 32 pa-</cell></row><row><cell cols="2">tient records: 73129 2, 48123 2, 76151 2, 41493 1,</cell></row><row><cell cols="2">65565 1, 55205 1, 41861 1, 58242 4, 54073 1,</cell></row><row><cell cols="2">46156 1, 55639 1, 89840 1, 43459 1, 10694 2,</cell></row><row><cell cols="2">51078 2, 90776 1, 89223 1, 12831 2, 80536 1,</cell></row><row><cell cols="2">78515 1, 62239 2, 58723 1, 40187 1, 79337 1,</cell></row><row><cell cols="2">51177 1, 70698 1, 48935 1, 54353 2, 19223 2,</cell></row><row><cell>58854 1, 80345 1, 48380 1.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table A.4. Static variables used for each of the datasets in the experiments. Categorical variables are shown in italics and were expanded to one-hot encodings.</figDesc><table><row><cell>Dataset</cell><cell>Static Variables</cell></row><row><cell cols="2">M-Mortality Height</cell></row><row><cell cols="2">P-Mortality Age, Gender, Height, ICUType</cell></row><row><cell>P-Sepsis</cell><cell>Age, Gender, HospAdmTime</cell></row><row><cell cols="2">Time series variables For all datasets, we used all avail-</cell></row><row><cell cols="2">able time series variables including vitals, lab measure-</cell></row><row><cell cols="2">ments, and interventions. All variables were treated as con-</cell></row><row><cell cols="2">tinuous, and no additional transformations were applied.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">x 2k (t) := sin t t 2k/τ(3)x 2k+1 (t) := cos t t 2k/τ (4) 2 In Takens's embedding theorem, d &gt; dB is required, where dB refers to the fractal box counting dimension<ref type="bibr" target="#b19">(Liebovitch &amp; Toth, 1989)</ref>, which is typically well below the size of typical neural network architectures.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since we are dealing only with a single instance (i.e. time series) in this section, we use i and j to denote a head and an observation, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">These models would only allow training with small batch sizes, which combined with the unbalanced nature of the datasets would lead to high variance in the gradients.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Dr. Felipe Llinares Lpez for comments and discussions during the progress of this work. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow Datasets, a collection of ready-to-use datasets</title>
		<ptr target="https://www.tensorflow.org/datasets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rezero is all you need: Fast convergence at large depth</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Temporal-clustering invariance in irregular healthcare time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12206</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-task gaussian process prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prediction of sepsis in the intensive care unit with minimal electronic health record data: A machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Desautels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Calvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kerem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shimabukuro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Chettipally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to detect sepsis with a Multitask Gaussian Process RNN classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1174" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1690" to="1699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Physiobank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">physiotoolkit, and physionet: components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multitask learning and benchmarking with clinical time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>2052-4463</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectra of some self-exciting and mutually exciting point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimic-Iii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pracniques: Further remarks on reducing truncation errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="41" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Art of Computer Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Knuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Addison-Wesley</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attentionbased permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classification of sparse and irregularly sampled time series with mixtures of expected gaussian kernels and random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="484" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast algorithm to determine fractal dimensions by box counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Liebovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="386" to="390" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multivariate Hawkes processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Liniger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Zurich</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Directly modeling missing data in sequences with rnns: Improved classification of clinical time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wetzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="253" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Statistical analysis with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A reproducing kernel hilbert space framework for pairwise time series distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="624" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hawkes processes for continuous time sequence classification: an application to rumour stance classification in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Srijith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2064</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-2064" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="393" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised pattern discovery in electronic health care data using probabilistic clustering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wetzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium</title>
		<meeting>the 2nd ACM SIGHIT International Health Informatics Symposium</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The neural Hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M ;</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6754" to="6764" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Early recognition of sepsis with gaussian process temporal convolutional networks and dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roqueiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fackler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Machine Learning for Healthcare Conference</title>
		<editor>Wiens, J.</editor>
		<meeting>the 4th Machine Learning for Healthcare Conference<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="2" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phased lstm: Accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05895</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Early prediction of sepsis from clinical data: the physionet/computing in cardiology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Reyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Shashikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical care medicine</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="217" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Latent odes for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03907</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Assessment of clinical criteria for sepsis: for the third international consensus definitions for sepsis and septic shock</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Iwashyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Brunkhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Rea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scherag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rubenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shankar-Hari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>sepsis-</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="762" to="774" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interpolation-prediction networks for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attend and diagnose: Clinical time series analysis using attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamical Systems and Turbulence</title>
		<editor>Rand, D. and Young, L.-S.</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1981" />
			<biblScope unit="page" from="366" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osborne</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09006</idno>
		<title level="m">On the limitations of representing functions on sets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Joint modeling of event sequence and time series with attentional twin recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08524</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mining electronic health records (EHRs): a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online learning for multivariate hawkes processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Etesami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">;</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4937" to="4946" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
