<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Visual Attention Grounding Neural Model for Multimodal Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhou</surname></persName>
							<email>minzhou@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxiang</forename><surname>Cheng</surname></persName>
							<email>samcheng@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
							<email>yongjaelee@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Visual Attention Grounding Neural Model for Multimodal Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our model jointly optimizes the learning of a shared visuallanguage embedding and a translator. The model leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this dataset, our visual attention grounding model outperforms other methods by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal machine translation is the problem of translating sentences paired with images into a different target language <ref type="bibr" target="#b9">(Elliott et al., 2016)</ref>. In this setting, translation is expected to be more accurate compared to purely text-based translation, as the visual context could help resolve ambiguous multi-sense words. Examples of real-world applications of multimodal (vision plus text) translation include translating multimedia news, web product information, and movie subtitles.</p><p>Several previous endeavours <ref type="bibr" target="#b15">(Huang et al., 2016;</ref><ref type="bibr" target="#b2">Calixto et al., 2017a;</ref><ref type="bibr" target="#b10">Elliott and Kádár, 2017)</ref> have demonstrated improved translation quality when utilizing images. However, how to effectively integrate the visual information still remains a challenging problem. For instance, in the WMT 2017 multimodal machine translation challenge , methods that incorporated visual information did not outperform pure text-based approaches with a big margin.</p><p>In this paper, we propose a new model called Visual Attention Grounding Neural <ref type="bibr">Ma-</ref>chine Translation (VAG-NMT) to leverage visual information more effectively. We train VAG-NMT with a multitask learning mechanism that simultaneously optimizes two objectives: (1) learning a translation model, and (2) constructing a vision-language joint semantic embedding. In this model, we develop a visual attention mechanism to learn an attention vector that values the words that have closer semantic relatedness with the visual context. The attention vector is then projected to the shared embedding space to initialize the translation decoder such that the source sentence words that are more related to the visual semantics have more influence during the decoding stage. When evaluated on the benchmark Multi30K and the Ambiguous COCO datasets, our VAG-NMT model demonstrates competitive performance compared to existing state-of-the-art multimodal machine translation systems.</p><p>Another important challenge for multimodal machine translation is the lack of a large-scale, realistic dataset. To our knowledge, the only existing benchmark is Multi30K <ref type="bibr" target="#b9">(Elliott et al., 2016)</ref>, which is based on an image captioning dataset, Flickr30K <ref type="bibr" target="#b28">(Young et al., 2014)</ref> with manual German and French translations. There are roughly 30K parallel sentences, which is relatively small compared to text-only machine translation datasets that have millions of sentences such as WMT14 EN→DE. Therefore, we propose a new multimodal machine translation dataset called IKEA to simulate the real-world problem of translating product descriptions from one language to another. Our IKEA dataset is a collection of parallel English, French, and German product descriptions and images from IKEA's and UNIQLO's websites. We have included a total of 3,600 products so far and will include more in the future. arXiv:1808.08266v2 [cs.CL] 28 Aug 2018</p><p>In the machine translation literature, there are two major streams for integrating visual information: approaches that (1) employ separate attention for different (text and vision) modalities, and (2) fuse visual information into the NMT model as part of the input. The first line of work learns independent context vectors from a sequence of text encoder hidden states and a set of location-preserving visual features extracted from a pre-trained convnet, and both sets of attentions affect the decoder's translation <ref type="bibr" target="#b2">(Calixto et al., 2017a;</ref><ref type="bibr" target="#b14">Helcl and Libovický, 2017)</ref>. The second line of work instead extracts a global semantic feature and initializes either the NMT encoder or decoder to fuse the visual context <ref type="bibr" target="#b3">(Calixto et al., 2017b;</ref><ref type="bibr" target="#b20">Ma et al., 2017)</ref>. While both approaches demonstrate significant improvement over their Text-Only NMT baselines, they still perform worse than the best monomodal machine translation system from the WMT 2017 shared task <ref type="bibr" target="#b29">(Zhang et al., 2017)</ref>.</p><p>The model that performs best in the multimodal machine translation task employed image context in a different way. <ref type="bibr" target="#b15">(Huang et al., 2016)</ref> combine region features extracted from a region-proposal network <ref type="bibr" target="#b23">(Ren et al., 2015)</ref> with the word sequence feature as the input to the encoder, which leads to significant improvement over their NMT baseline. The best multimodal machine translation system in WMT 2017 <ref type="bibr" target="#b1">(Caglayan et al., 2017)</ref> performs element-wise multiplication of the target language embedding with an affine transformation of the convnet image feature vector as the mixed input to the decoder. While this method outperforms all other methods in WMT 2017 shared task workshop, the advantage over the monomodal translation system is still minor.</p><p>The proposed visual context grounding process in our model is closely related to the literature on multimodal shared space learning. <ref type="bibr" target="#b18">(Kiros et al., 2014)</ref> propose a neural language model to learn a visual-semantic embedding space by optimizing a ranking objective, where the distributed representation helps generate image captions. (Karpathy and Li, 2014) densely align different objects in the image with their corresponding text captions in the shared space, which further improves the quality of the generated caption. In later work, multimodal shared space learning was extended to multimodal multilingual shared space learning. <ref type="bibr" target="#b4">(Calixto et al., 2017c</ref>) learn a multi-modal multilin-gual shared space through optimization of a modified pairwise contrastive function, where the extra multilingual signal in the shared space leads to improvements in image-sentence ranking and semantic textual similarity task. <ref type="bibr" target="#b11">(Gella et al., 2017)</ref> extend the work from <ref type="bibr" target="#b4">(Calixto et al., 2017c</ref>) by using the image as the pivot point to learn the multilingual multimodal shared space, which does not require large parallel corpora during training. Finally, <ref type="bibr" target="#b10">(Elliott and Kádár, 2017)</ref> is the first to integrate the idea of multimodal shared space learning to help multimodal machine translation. Their multi-task architecture called "imagination" shares an encoder between a primary task of the classical encoder-decoder NMT and an auxiliary task of visual feature reconstruction.</p><p>Our VAG-NMT mechanism is inspired by (Elliott and Kádár, 2017), but has important differences. First, we modify the auxiliary task as a visual-text shared space learning objective instead of the simple image reconstruction objective. Second, we create a visual-text attention mechanism that captures the words that share a strong semantic meaning with the image, where the grounded visual-context has more impact on the translation. We show that these enhancements lead to improvement in multimodal translation quality over <ref type="bibr" target="#b10">(Elliott and Kádár, 2017)</ref>. </p><formula xml:id="formula_0">{x i } N i=1 ∈ X in language X to sen- tences {y i } N i=1 ∈ Y in language Y with the assis- tance of images {v i } N i=1 ∈ V .</formula><p>We treat the problem of multimodal machine translation as a joint optimization of two tasks: (1) learning a robust translation model and (2) constructing a visual-language shared embedding that grounds the visual semantics with text. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of our VAG-NMT model. We adopt a state-of-the-art attention-based sequenceto-sequence structure <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> for translation. For the joint embedding, we obtain the text representation using a weighted sum of hidden states from the encoder of the sequenceto-sequence model and we obtain the image representation from a pre-trained convnet. We learn the weights using a visual attention mechanism, which represents the semantic relatedness between the image and each word in the encoded text. We learn the shared space with a ranking loss and the translation model with a cross entropy loss.</p><p>The joint objective function is defined as:</p><formula xml:id="formula_1">J(θ T , φ V ) = αJ T (θ T ) + (1 − α)J V (φ V ),</formula><p>(1) where J T is the objective function for the sequence-to-sequence model, J V is the objective function for joint embedding learning, θ T are the parameters in the translation model, and φ V are the parameters for the shared vision-language embedding learning, and α determines the contribution of the machine translation loss versus the visual grounding loss. Both J T and J V share the parameters of the encoder from the neural machine translation model. We describe details of the two objective functions in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>We first encode an n-length source sentence {x}, as a sequence of tokens x = {x 1 , x 2 , . . . , x n }, with a bidirectional GRU <ref type="bibr" target="#b25">(Schuster and Paliwal, 1997;</ref>. Each token is represented by a one-hot vector, which is then mapped into an embedding e i through a pre-trained embedding matrix. The bidirectional GRU processes the embedding tokens in two directions: left-toright (forward) and right-to-left (backward). At every time step, the encoder's GRU cell generates two corresponding hidden state vectors:</p><formula xml:id="formula_2">− → h i = − −−−−−−−−− → GRU (h i−1 , e i ) and ← − h i = ← −−−−−−−−− − GRU (h i−1 , e i ).</formula><p>The two hidden state vectors are then concatenated together to serve as the encoder hidden state vector of the source token at step i:</p><formula xml:id="formula_3">h i = [ ← − h i , − → h i ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shared embedding objective</head><p>After encoding the source sentence, we project both the image and text into the shared space to find a good distributed representation that can capture the semantic meaning across the two modalities. Previous work has shown that learning a multimodal representation is effective for grounding knowledge between two modalities <ref type="bibr" target="#b18">(Kiros et al., 2014;</ref><ref type="bibr" target="#b6">Chrupala et al., 2015)</ref>. Therefore, we expect the shared encoder between the two objectives to facilitate the integration of the two modalities and positively influence translation during decoding.</p><p>To project the image and the source sentence to a shared space, we obtain the visual embedding (v) from the pool5 layer of ResNet50 <ref type="bibr" target="#b12">(He et al., 2015a)</ref> pre-trained on ImageNet classification <ref type="bibr" target="#b24">(Russakovsky et al., 2015)</ref>, and the source sentence embedding using the weighted sum of encoder hidden state vectors ({h i }) to represent the entire source sentence (t). We project each {h i } to the shared space through an embedding layer. As different words in the source sentence will have different importance, we employ a visual-language attention mechanism-inspired by the attention mechanism applied in sequenceto-sequence models <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>-to emphasize words that have the stronger semantic connection with the image. For example, the highlighted word "cat" in the source sentence in <ref type="figure" target="#fig_0">Fig. 1</ref> has the more semantic connection with the image.</p><p>Specifically, we produce a set of weights β = {β 1 , β 2 , . . . , β n } with our visual-attention mechanism, where the attention weight β i for the i'th word is computed as:</p><formula xml:id="formula_4">β i = exp(z i ) N l=1 exp(z l ) ,<label>(2)</label></formula><p>and</p><formula xml:id="formula_5">z i = tanh(W v v) · tanh(W h h i )</formula><p>is computed by taking the dot product between the transformed encoder hidden state vector h i and the transformed image feature vector v, and W v and W h are the association transformation parameters. Then, we can get a weighted sum of the encoder hidden state vectors t = n i=1 β i h i to represent the semantic meaning of the entire source sentence. The next step is to project the source sentence feature vector t and the image feature vector v into the same shared space. The projected vector for text is: t emb = tanh(W t emb t + b t emb ) and the projected vector for image is:</p><formula xml:id="formula_6">v emb = tanh(W v emb v + b v emb ).</formula><p>We follow previous work on visual semantic embedding <ref type="bibr" target="#b18">(Kiros et al., 2014)</ref> to minimize a pairwise ranking loss to learn the shared space:</p><formula xml:id="formula_7">JV (φV ) = p k max{0, γ − s(vp, tp) + s(vp, t k =p )} + k p max{0, γ − s(t k , v k ) + s(t k , v p =k )},<label>(3)</label></formula><p>where γ is a margin, and s is the cosine distance between two vectors in the shared space. k and p are the indexes of the images and text sentences, respectively. t k =p and v p =k are the contrastive examples with respect to the selected image and the selected source text, respectively. When the loss decreases, the distance between a paired image and sentence will drop while the distance between an unpaired image and sentence will increase. In addition to grounding the visual context into the shared encoder through the multimodal shared space learning, we also initialize the decoder with the learned attention vector t such that the words that have more relatedness with the visual semantics will have more impact during the decoding (translation) stage. However, we may not want to solely rely on only a few most important words. Thus, to produce the initial hidden state of the decoder, we take a weighted average of the attention vector t and the mean of encoder hidden states:</p><formula xml:id="formula_8">s 0 = tanh(W init (λt + (1 − λ) 1 N N i h i )),<label>(4)</label></formula><p>where λ determines the contribution from each vector. Through our experiments, we find the best value for λ is 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Translation objective</head><p>During the decoding stage, at each time step j, the decoder generates a decoder hidden state s j from a conditional GRU cell  whose input is the previously generated translation token y j−1 , the previous decoder hidden state s j−1 , and the context vector c j at the current time step:</p><formula xml:id="formula_9">s j = cGRU(s j−1 , y j−1 , c j )<label>(5)</label></formula><p>The context vector c j is a weighted sum of the encoder hidden state vectors, and captures the relevant source words that the decoder should focus on when generating the current translated token y j . The weight associated with each encoder hidden state is determined by a feed-forward network.</p><p>From the hidden state s j we can predict the conditional distribution of the next token y j with a fullyconnected layer W o given the previous token's language embedding e j−1 , the current hidden state d j and the context vector for current step c j :</p><formula xml:id="formula_10">p(y j |y j−1 , x) = softmax(W o o t ),<label>(6)</label></formula><formula xml:id="formula_11">where o t = tanh(W e e j−1 + W d d j + W c c j ).</formula><p>The three inputs are transformed with W e , W d , and W c , respectively and then summed before being fed into the output layer. We train the translation objective by optimizing a cross entropy loss function:</p><formula xml:id="formula_12">J T (θ T ) = − j log p(y j |y j−1 , x)<label>(7)</label></formula><p>By optimizing the objective of the translation and the multimodal shared space learning tasks jointly along with the visual-language attention mechanism, we can simultaneously learn a general mapping between the linguistic signals in two languages and grounding of relevant visual content in the text to improve the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IKEA Dataset</head><p>Previous available multimodal machine translation models are only tested on image caption datasets, we, therefore, propose a new dataset, IKEA, that has the real-world application of international online shopping. We create the dataset by crawling commercial products' descriptions and images from IKEA and UNIQLO websites. There are 3,600 products and we plan to expand the data in the future. Each sample is composed of the web-crawled English description of a product, an image of the product, and the web-crawled German or French description of the product. Different than the image caption datasets, the German or French sentences in the IKEA dataset is not an exact parallel translation of its English sentence. Commercial descriptions in different languages can be vastly different in surface form but still keep the core semantic meaning. We think IKEA is a good data set to simulate real-world multimodal translation problems. The sentence in the IKEA dataset contains 60-70 words on average, which is five times longer than the average text length in Multi30K <ref type="bibr" target="#b9">(Elliott et al., 2016)</ref>. These sentences not only describe the visual appearance of the product, but also the product usage. Therefore, capturing the connection between  <ref type="bibr" target="#b1">(Caglayan et al., 2017)</ref> 31.1 ± 0.7 52.2 ± 0.4 52.7 ± 0.9 69.5 ± 0.7 Text-Only NMT 31.6 ± 0.5 52.2 ± 0.3 53.5 ± 0.7 70.0 ± 0.7 VAG-NMT 31.6 ± 0.3 52.2 ± 0.3 53.8 ± 0.3 70.3 ± 0.5  <ref type="bibr" target="#b10">(Elliott and Kádár, 2017)</ref> 28.0 48.1 N/A N/A LIUMCVC <ref type="bibr" target="#b1">(Caglayan et al., 2017)</ref> 27.1 ± 0.9 47.2 ± 0.6 43.5 ± 1.2 63.2 ± 0.9 Text-Only NMT 27.9 ± 0.6 47.8 ± 0.6 44.6 ± 0.6 64.2 ± 0.5 VAG-NMT</p><p>28.3 ± 0.6 48.0 ± 0.5 45.0 ± 0.4 64.7 ± 0.4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate our proposed model on three datasets: Multi30K <ref type="bibr" target="#b9">(Elliott et al., 2016)</ref>, Ambiguous COCO , and our newly-collected IKEA dataset. The Multi30K dataset is the largest existing human-labeled dataset for multimodal machine translation. It consists of 31,014 images, where each image is annotated with an English caption and manual translations of image captions in German and French. There are 29,000 instances for training, 1,014 instances for validation, and 1,000 for testing. Additionally, we also evaluate our model on the Ambiguous COCO Dataset collected in the WMT2017 multimodal machine translation challenge . It contains 461 images from the MSCOCO dataset <ref type="bibr" target="#b19">(Lin et al., 2014)</ref>, whose captions contain verbs with ambiguous meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Setting</head><p>We pre-process all English, French, and German sentences by normalizing the punctuation, lower casing, and tokenizing with the Moses toolkit. A Byte-Pair-Encoding (BPE) <ref type="bibr" target="#b27">(Sennrich et al., 2015)</ref> operation with 10K merge operations is learned from the pre-processed data and then applied to segment words. We restore the original words by concatenating the subwords segmented by BPE in post-processing. During training, we apply early stopping if there is no improvement in BLEU score on validation data for 10 validation steps. We apply beam search decoding to generate translation with beam size equal to 12. We evaluate the performance of all models using BLEU <ref type="bibr" target="#b21">(Papineni et al., 2002)</ref> and METEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014)</ref>. The setting used in IKEA dataset is the same as Multi30K, except that we lower the default batch size from 32 to 12; since IKEA dataset has long sentences and large variance in sentence length, we use smaller batches to make the training more stable. Full details of our hyperparameter choices can be found in Appendix B. We run all models five times with different random seeds and report the mean and standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We compare the performance of our model against the state-of-the-art multimodal machine translation approaches and the text-only baseline. The idea of our model is inspired by the "Imagination" model <ref type="bibr" target="#b10">(Elliott and Kádár, 2017)</ref>, which unlike our models, simply averages the encoder hidden states for visual grounding learning. As "Imagination" does not report its performance on Multi30K 2017 and Ambiguous COCO in its original paper, we directly use their result reported in the WMT 2017 shared task as a comparison. LIUMCVC is the best multimodal machine translation model in WMT 2017 multimodal machine translation challenge and exploits visual information with several different methods. We always compare our VAG-NMT with the method that has been reported to   <ref type="table" target="#tab_2">Table 2</ref>). The METEOR score of our VAG-NMT is slightly worse than that of "Imagination" for English -&gt; German on Ambiguous COCO Dataset. This is likely because the "Imagination" result was produced by ensembling the result of multiple runs, which typically leads to 1-2 higher BLEU and METEOR points compared to a single run. Thus, we expect our VAG-NMT to outperform the "Imagination" baseline if we also use an ensemble.</p><p>We observe that our multimodal VAG-NMT model has equal or slightly better result compared to the text-only neural machine translation model on the Multi30K dataset. On the Ambiguous COCO dataset, our VAG-NMT demonstrates clearer improvement over this text-only baseline. We suspect this is because Multi30K does not have many cases where images can help improve translation quality, as most of the image captions are short and simple. In contrast, Ambiguous COCO was purposely curated such that the verbs in the captions can have ambiguous meaning. Thus, visual context will play a more important role in Ambiguous COCO; namely, to help clarify the sense of the source text and guide the translation to select the correct word in the target language.</p><p>We then evaluate all models on the IKEA dataset. <ref type="table" target="#tab_4">Table 3</ref> shows the results. Our VAG-NMT has a higher value in BLEU and a comparable value in METEOR compared to the Text-only NMT baseline. Our VAG-NMT outperforms LI-UMCVC's multimodal system by a large margin, which shows that the LIUMCVC's multimodal's good performance on Multi30K does not generalize to this real-world product dataset. The good performance may come from the visual attention mechanism that learns to focus on text segments that are related to the images. Such attention there-fore teaches the decoder to apply the visual context to translate those words. This learned attention is especially useful for datasets with long sentences that have irrelevant text information with respect to the image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multimodal embedding results</head><p>To assess the learned joint embedding, we perform an image retrieval task evaluated using the Recall@K metric <ref type="bibr" target="#b18">(Kiros et al., 2014)</ref> on the Multi30K dataset.</p><p>We project the image feature vectors V = {v 1 , v 2 , . . . , v n } and their corresponding captions S = {s 1 , s 2 , . . . , s n } into a shared space. We follow the experiments conducted by the previous visual semantic embedding work <ref type="bibr" target="#b18">(Kiros et al., 2014)</ref>, where for each embedded text vector, we find the closest image vectors around it based on the cosine similarity. Then we can compute the recall rate of the paired image in the top K nearest neighbors, which is also known as R@K score. The shared space learned with VAG-NMT achieves 64% R@1, 88.6% R@5, and 93.8% R@10 on Multi30K, which demonstrates the good quality of the learned shared semantics. We also achieved 58.13% R@1, 87.38% R@5 and 93.74% R@10 on IKEA dataset; 41.35% R@1, 85.48% R@5 and 92.56% R@10 on COCO dataset. Besides the quantitative results, we also show several qualitative results in <ref type="figure" target="#fig_2">Figure 2</ref>. We show the top five images retrieved by the example captions. a person is skiing or snowboarding down a mountainside . a mountain climber trekking through the snow with a pick and a blue hat . the snowboarder is jumping in the snow . two women are water rafting . three people in a blue raft on a river of brown water .</p><p>people in rafts watch as two men fall out of their own rafts . <ref type="figure">Figure 3</ref>: Visual-text attention score on sample data from Multi30K. The first and second rows show the three closest images to the caption a person is skiing or snowboarding down a mountainside and two woman are water rafting, respectively. The original caption is listed under each image. We highlight the three words with highest attention in red.</p><p>The images share "cyclist", "helmet", and "dog" mentioned in the caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Human evaluation</head><p>We use Facebook to hire raters that speak both German and English to evaluate German translation quality. As Text-Only NMT has the highest BLEU results among all baseline models, we compare the translation quality between the Text-Only and the VAG-NMT on all three datasets. We randomly selected 100 examples for evaluation for each dataset. The raters are informed to focus more on semantic meaning than grammatical correctness when indicating the preference of the two translations. They are also given the option to choose a tie if they cannot decide. We hired two raters and the inter-annotator agreement is 0.82 in Cohen's Kappa. We summarize the results in <ref type="table" target="#tab_6">Table 4</ref>, where we list the number of times that raters prefer one translation over another or think they are the same quality. Our VAG-NMT performs better than Text-Only NMT on MSCOCO and IKEA dataset, which correlates with the automatic evaluation metrics. However, the result of VAG-NMT is slightly worse than the Text-Only NMT on the Multi30K test dataset. This also correlates with the result of automatic evaluation metrics.</p><p>Finally, we also compare the translation quality between LIUMCVC multimodal and VAG-NMT on 100 randomly selected examples from the IKEA dataset. VAG-NMT performs better than LIUMCVC multimodal. The raters prefer our VAG-NMT in 91 cases, LIUMCVC multimodal in 68 cases, and cannot tell in 47 cases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>To demonstrate the effectiveness of our visual attention mechanism, we show some qualitative examples in <ref type="figure">Figure 3</ref>. Each row contains three images that share similar semantic meaning, which are retrieved by querying the image caption using our learned shared space. The original caption of each image is shown below each image. We highlight the three words in each caption that have the highest weights assigned by the visual attention mechanism.</p><p>In the first row of <ref type="figure">Figure 3</ref>, the attention mechanism assigns high weights to the words "skiing", "snowboarding", and "snow". In the second row, it assigns high attention to "rafting" or "raft" for every caption of the three images. These examples demonstrate evidence that our attention mechanism learns to assign high weights to words that have corresponding visual semantics in the image.</p><p>We also find that our visual grounding attention captures the dependency between the words that Source Caption: a tennis player is moving to the side and is gripping his racquet with both hands .</p><p>Text-Only NMT: ein tennisspieler bewegt sich um die seite und greift mit beiden händen an den boden .</p><p>VAG-NMT: ein tennisspieler bewegt sich zur seite und greift mit beiden händen den schläger .</p><p>Source Caption: three skiers skiing on a hill with two going down the hill and one moving up the hill .</p><p>Text-Only NMT: drei skifahrer fahren auf skiern einen hügel hinunter und eine person fährt den hügel hinunter .</p><p>VAG-NMT: drei skifahrer auf einem hügel fahren einen hügel hinunter und ein bewegt sich den hügel hinauf .</p><p>Source Caption: a blue , yellow and green surfboard sticking out of a sandy beach .</p><p>Text-Only NMT: ein blau , gelb und grünes surfbrett streckt aus einem sandstrand .</p><p>VAG-NMT: ein blau , gelb und grüner surfbrett springt aus einem sandstrand . <ref type="figure">Figure 4</ref>: Translations generated by VAG-NMT and Text-Only NMT. VAG-NMT performs better in the first two examples, while Text-Only NMT performs better in the third example. We highlight the words that distinguish the two systems' results in red and blue. Red words are marked for better translation from VAG-NMT and blue words are marked for better translation from Text-Only NMT.</p><p>have strong visual semantic relatedness. For example, in <ref type="figure">Figure 3</ref>, words, such as "raft","river", and "water", with high attention appear in the image together. This shows that the visual dependence information is encoded into the weighted sum of attention vectors which is applied to initialize the translation decoder. When we apply the sequence-to-sequence model to translate a long sentence, the encoded visual dependence information strengthens the connection between the words with visual semantic relatedness. Such connections mitigate the problem of standard sequenceto-sequence models tending to forget distant history. This hypothesis is supported by the fact that our VAG-NMT outperforms all the other methods on the IKEA dataset which has long sentences.</p><p>Lastly, in <ref type="figure">Figure 4</ref> we provide some qualitative comparisons between the translations from VAG-NMT and Text-Only NMT. In the first example, our VAG-NMT properly translates the word "racquet" to "den schläger", while the Text-Only NMT mistranslated it to "den boden" which means "ground" in English. We suspect the attention mechanism and visual shared space capture the visual dependence between the word "tennis" and "racquet". In the second example, our VAG-NMT model correctly translates the preposition "up" to "hinauf" but Text-Only NMT mistranslates it to "hinunter" which means "down" in English. We consistently observe that VAG-NMT translates prepositions better than Text-Only NMT. We think it is because the pre-trained convnet features captured the relative object position that leads to a better preposition choice. Finally, in the third example, we show a failure case where Text-Only NMT generates a better translation. Our VAG-NMT mistranslates the verb phrase "sticking out" to "springt aus" which means "jump out" in German, while Text-Only NMT translates to "streckt aus", which is correct. We find that VAG-NMT often makes mistakes when translating verbs. We think it is because the image vectors are pretrained on an object classification task, which does not have any human action information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We proposed a visual grounding attention structure to take advantage of the visual information to perform machine translation. The visual attention mechanism and visual context grounding module help to integrate the visual content into the sequence-to-sequence model, which leads to better translation quality compared to the model with only text information. We achieved stateof-the-art results on the Multi30K and Ambiguous COCO dataset. We also proposed a new product dataset, IKEA, to simulate a real-world online product description translation challenge.</p><p>In the future, we will continue exploring different methods to ground the visual context into the translation model, such as learning a multimodal shared space across image, source language text, as well as target language text. We also want to change the visual pre-training model from an image classification dataset to other datasets that have both objects and actions, to further improve translation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IKEA Dataset Stats and Examples</head><p>We summarize the statistics of our IKEA dataset in <ref type="figure" target="#fig_4">Figure 5</ref>, where we demonstrate the information about the number of tokens, the length of the product description and the vocabulary size. We provide one example of the IKEA dataset in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameter Settings</head><p>In this Appendix, we share details on the hyperparameter settings for our model and the training process. The word embedding size for both encoder and decoder are 256. The Encoder is a onelayer bidirectional recurrent neural network with Gated Recurrent Unit (GRU), which has a hidden size of 512. The decoder is a recurrent neural network with conditional GRU of the same hidden size. The visual representation is a 2048dim vector extracted from the pool5 layer of a pre-trained ResNet-50 network. The dimension of the shared visual-text semantic embedding space is 512. We set the decoder initialization weight value λ to 0.5.</p><p>During training, we use Adam (Kingma and Ba, 2014) to optimize our model with a learning rate of 4e − 4 for German Dataset and 1e − 3 for French dataset. The batch size is 32. The total gradient norm is clipped to 1 <ref type="bibr" target="#b22">(Pascanu et al., 2012)</ref>. Dropout is applied at the embedding layer in the encoder, context vectors extracted from the encoder and the output layer of the decoder. For Multi30K German dataset the dropout probabilities are (0.3, 0.5, 0.5) and for Multi30K French dataset the dropout probabilities are (0.2, 0.4, 0.4). For the Multimodal shared space learning objective function, the margin size γ is set to 0.1. The objective split weight α is set to 0.99. We initialize the weights of all the parameters with the method introduced in <ref type="bibr" target="#b13">(He et al., 2015b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Analysis on Visual-Text Attention</head><p>We conducted an ablation test to further evaluate the effectiveness of our visual-text attention mechanism. We created two comparison experiments where we reduced the impact of visual-text attention with different design options. In the first experiment, we remove the visual-attention mechanism in our pipeline and simply use the mean of the encoder hidden states to learn the shared embedding space. In the second experiment, we initialize the decoder with just the mean of encoder hidden states without the weighted sum of encoder states using the learned visual-text attention scores. We run both experiments on Multi30K German Dataset five times and demonstrate the results in table 5. As can be seen, the performance of the changed translation model is obviously worse than the full VAG-NMT in both experiments. This observation suggests that the visual-attention mechanism is critical in improving the translation performance. The model improvement comes from the attention mechanism influencing the model's objective function and decoder's initialization.</p><p>English → German Method BLEU METEOR -attention in shared embedding 30.5 ± 0.6 51.7 ± 0.4 -attention in initialization 30.8 ± 0.8 51.9 ± 0.5 VAG-NMT 31.6 ± 0.6 52.2 ± 0.3 <ref type="table">Table 5</ref>: Ablation analysis on visual-text attention mechanism in the Multi30K German dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the VAG-NMT structure 3 Visual Attention Grounding NMT Given a set of parallel sentences in language X and Y , and a set of corresponding images V paired with each sentence pair, the model aims to translate sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) a cyclist is wearing a helmet (b) a black dog and his favorite toys.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Top five images retrieved using the given caption. The original corresponding image of the caption is highlighted with a red bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Statistic of the IKEA dataset (a) Product image (b) Source description (c) Target description in German</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>An example of product description and the corresponding translation in German from the IKEA dataset. Both descriptions provide an accurate caption for the commercial characteristics of the product in the image, but the details in the descriptions are different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Translation results on the Multi30K dataset</figDesc><table><row><cell>English → German</cell><cell>English → French</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Translation results on the Ambiguous COCO dataset visual semantics and the text is more challenging on this dataset. The dataset statistics and an exam- ple of the IKEA dataset is in Appendix A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>± 1.2 65.7 ± 0.1 65.8 ± 1.2 68.9 ± 1.4</figDesc><table><row><cell></cell><cell cols="2">English → German</cell><cell cols="2">English → French</cell></row><row><cell>Method</cell><cell>BLEU</cell><cell>METEOR</cell><cell>BLEU</cell><cell>METEOR</cell></row><row><cell cols="2">LIUMCVC-Multi 59.9 ± 1.9</cell><cell>63.8 ± 0.4</cell><cell>58.4 ± 1.6</cell><cell>64.6 ± 1.8</cell></row><row><cell>Text-Only NMT</cell><cell>61.9 ± 0.9</cell><cell>65.6 ± 0.9</cell><cell>65.2 ± 0.7</cell><cell>69.0 ± 0.2</cell></row><row><cell>VAG-NMT</cell><cell>63.5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Translation results on the IKEA dataset</figDesc><table><row><cell>have the best performance on each dataset.</cell></row><row><cell>Our VAG-NMT surpasses the results of the</cell></row><row><cell>"Imagination" model and the LIUMCVC's model</cell></row><row><cell>by a noticeable margin in terms of BLEU score on</cell></row><row><cell>both the Multi30K dataset (Table 1) and the Am-</cell></row><row><cell>biguous COCO dataset (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Human evaluation results</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We would like to thank Daniel Boyla for providing insightful discussions to help with this research. We also want to thank Chunting Zhou and Ozan Caglayan for suggestions on machine translation model implementation. This work was supported in part by NSF CAREER IIS-1751206.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">LIUM-CVC submissions for WMT17 multimodal translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<idno>abs/1707.04481</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Doubly-attentive decoder for multi-modal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno>abs/1702.01287</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Incorporating global visual features into attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno>abs/1701.06521</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multilingual multi-modal embeddings for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno>abs/1702.01101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning language through pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ákos</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
		<idno>abs/1506.03694</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Findings of the second shared task on multimodal machine translation and multilingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno>abs/1710.07177</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi30k: Multilingual englishgerman image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno>abs/1605.00459</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagination improves multimodal translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ákos</forename><surname>Kádár</surname></persName>
		</author>
		<idno>abs/1705.04350</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Image pivoting for learning multilingual multimodal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>abs/1707.07601</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindrich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindrich</forename><surname>Libovický</surname></persName>
		</author>
		<idno>abs/1707.04550</idno>
		<title level="m">CUNI system for the WMT17 multimodal translation task. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention-based multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1412.2306</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">OSU multimodal machine translation system report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1710.02718</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ima-geNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
		<idno>abs/1703.04357</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>abs/1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nictnaist system for wmt17 multimodal translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
