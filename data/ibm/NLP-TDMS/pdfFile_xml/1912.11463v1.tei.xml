<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Khan</surname></persName>
							<email>zeeshank606@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Gandhinagar</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukul</forename><surname>Khanna</surname></persName>
							<email>mukul18khanna@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Indian Institute of Technology Gandhinagar</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmuganathan</forename><surname>Raman</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Indian Institute of Technology Gandhinagar</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-HDR imaging</term>
					<term>Feedback Networks</term>
					<term>RNN</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High dynamic range (HDR) image generation from a single exposure low dynamic range (LDR) image has been made possible due to the recent advances in Deep Learning. Various feed-forward Convolutional Neural Networks (CNNs) have been proposed for learning LDR to HDR representations. To better utilize the power of CNNs, we exploit the idea of feedback, where the initial low level features are guided by the high level features using a hidden state of a Recurrent Neural Network. Unlike a single forward pass in a conventional feed-forward network, the reconstruction from LDR to HDR in a feedback network is learned over multiple iterations. This enables us to create a coarse-to-fine representation, leading to an improved reconstruction at every iteration. Various advantages over standard feed-forward networks include early reconstruction ability and better reconstruction quality with fewer network parameters. We design a dense feedback block and propose an end-to-end feedback network-FHDR for HDR image generation from a single exposure LDR image. Qualitative and quantitative evaluations show the superiority of our approach over the stateof-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-High dynamic range (HDR) image generation from a single exposure low dynamic range (LDR) image has been made possible due to the recent advances in Deep Learning. Various feed-forward Convolutional Neural Networks (CNNs) have been proposed for learning LDR to HDR representations. To better utilize the power of CNNs, we exploit the idea of feedback, where the initial low level features are guided by the high level features using a hidden state of a Recurrent Neural Network. Unlike a single forward pass in a conventional feed-forward network, the reconstruction from LDR to HDR in a feedback network is learned over multiple iterations. This enables us to create a coarse-to-fine representation, leading to an improved reconstruction at every iteration. Various advantages over standard feed-forward networks include early reconstruction ability and better reconstruction quality with fewer network parameters. We design a dense feedback block and propose an end-to-end feedback network-FHDR for HDR image generation from a single exposure LDR image. Qualitative and quantitative evaluations show the superiority of our approach over the stateof-the-art methods.</p><p>Index Terms-HDR imaging, Feedback Networks, RNN, Deep Learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Common digital cameras can not capture the wide range of light intensity levels in a natural scene. This can lead to a loss of pixel information in under-exposed and overexposed regions of an image, resulting in a low dynamic range (LDR) image. To recover the lost information and represent the wide range of illuminance in an image, high dynamic range (HDR) images need to be generated. There has been active research going on in the area of deep learning for HDR imaging. The advances in deep learning for image processing tasks have paved way for various approaches for HDR image reconstruction using feed-forward convolutional neural network (CNN) architectures <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b4">[5]</ref>. The above methods specifically transform a single exposure LDR image into an HDR image. HDRCNN <ref type="bibr" target="#b0">[1]</ref> proposed a deep autoencoder for HDR image reconstruction which uses a weighted mask to recover only the over-exposed regions of an LDR image. The authors of DRTMO <ref type="bibr" target="#b2">[3]</ref> designed a framework with two networks for generating up-exposure and downexposure LDR images, which are merged to form an HDR image. The network is not end-to-end trainable and uses a large number of parameters. Unlike the others, the proposed FHDR model provides an end-to-end trainable solution and is able to comprehensively learn the LDR-HDR mapping while outperforming the existing methods.</p><formula xml:id="formula_0">[2] [3] [4]</formula><p>Deeper networks are known to learn more complex nonlinear relationships like the LDR to HDR mapping. The caveat with deeper networks is that they consume a lot of computational resources and tend to over-fit the training data. To overcome this problem, we exploit the power of feedback mechanisms, inspired by <ref type="bibr" target="#b5">[6]</ref>, for the task of HDR image reconstruction. A feedback block is an RNN whose output is fed back to guide its input via a hidden state. A feedback network can run for many iterations for a single training example. Considering the number of iterative operations on the shared network parameters, a feedback network is virtually deeper than the corresponding feed-forward network with the same physical depth. Here, virtual depth = physical depth × number of iterations.</p><p>For improving the reconstruction at every iteration, the loss is calculated for the output of each iteration. By doing so, the network is forced to create a coarse-to-fine representation, being able to reconstruct HDR content right from the first iteration and improve with every subsequent iteration. We propose a global feedback block which consists of smaller local feedback blocks of densely connected layers, inspired by the Dense-Net architecture <ref type="bibr" target="#b6">[7]</ref>. Dense connections allow the network to reuse features. This helps in learning robust image representations, even with lesser network parameters.</p><p>The performance of our framework is evaluated on the standard City Scene dataset <ref type="bibr" target="#b7">[8]</ref> and another dataset prepared from the list of HDR image datasets suggested in <ref type="bibr" target="#b0">[1]</ref>. Qualitative and quantitative assessments of the network suggest that even with fewer network parameters, the proposed FHDR model outperforms the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HDR RECONSTRUCTION FRAMEWORK A. Feedback system</head><p>Feedback systems are adopted to influence the input based on the generated output, unlike the conventional feed-forward networks, where information flow is unidirectional and is not directly influenced by the generated output. The authors in <ref type="bibr" target="#b5">[6]</ref> exploited the feedback mechanism using an RNN, where the output of the feedback block travels back to guide its input via a hidden state. Their architecture uses ConvLSTM cells as the basic RNN units and is designed for the task of image classification. Even with lesser network parameters as compared to other feed-forward networks, such networks are able to learn better representations. Recently, authors of <ref type="bibr" target="#b8">[9]</ref> designed a feedback network specifically for the task of image super-resolution which achieved state-of-the-art performance.</p><p>Inspired by the success of feedback networks, we designed a feedback network for learning the LDR to HDR mapping that has been explained in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1: FHDR Architecture</head><p>Our architecture consists of three blocks similar to <ref type="bibr" target="#b8">[9]</ref>, as shown in <ref type="figure">Fig. 1</ref>. The first block is the Feature Extraction block (FEB), followed by the Feedback block (FBB) and an HDR reconstruction block (HRB). Inspired by <ref type="bibr" target="#b9">[10]</ref>, we use a global residual skip connection for bypassing low level LDR features at every iteration to guide the HDR reconstruction block in the final layers. For every training example, the network runs for n iterations. Here each iteration from t = 1 to t = n is a forward pass in time in an unfolded RNN. The FEB is responsible for extracting the low-level feature information F in from the input LDR image I LDR .</p><formula xml:id="formula_1">F in = f F EB (I LDR ).<label>(1)</label></formula><p>Here, f F EB represents the operations of the FEB. To achieve the feedback mechanism, F in is fed to the FBB, combined with the output of the FBB from the previous iteration, using a global hidden state as below.</p><formula xml:id="formula_2">F t F BB = f F BB (F in , F t−1 F BB ).<label>(2)</label></formula><p>Here, F t F BB represents the output of the feedback block at iteration t. At t = 1, when there is no feedback, the hidden state is initialised with the values of the extracted features F in .</p><p>At every iteration, the low level LDR features from the first convolutional layer in FEB are added to the output of the FBB using a global residual skip connection as below.</p><formula xml:id="formula_3">F t res = F in(1) + F t F BB .<label>(3)</label></formula><p>Here, F t res represents the global residual feature map learned at iteration t. F in(1) stands for the low level LDR features from the first convolutional layer in FEB. F t res is passed to the HRB to generate an HDR image at every iteration as below.</p><formula xml:id="formula_4">H t gen = f HRB (F t res )<label>(4)</label></formula><p>Here, f HRB represents the operations of the HRB and H t gen represents the HDR image generated at t th iteration. For every LDR image, a forward pass can run for n iterations, therefore generating n HDR images. Each generated image is coupled with a loss, hence resulting in improved reconstruction at each iteration through back-propagation in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feedback block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: Feedback block</head><p>We have designed a novel feedback block for the task of learning LDR-to-HDR representations, as shown in <ref type="figure">Fig. 2</ref>. The basic unit of the feedback block is a Dilated Dense Block (DDB) shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. It is a modification of the Dense block proposed in <ref type="bibr" target="#b6">[7]</ref>. Dilated convolutions help in increasing the receptive field of the network <ref type="bibr" target="#b10">[11]</ref>. A DDB helps in utilising all the hierarchical features from the input. Other than the two 1 × 1 convolutional layers for feature compression, each DDB houses four dilated 3 × 3 convolutional layers, each of which uses the information from all the previous layers using dense skip connections. This reuse of features due to the dense forward connections allows for reduced network parameters and improves the learning ability. Three of such DDBs come together to form the feedback block of the network.</p><p>We implement global and local feedback mechanisms described as follows.</p><p>1) Global Feedback: The global feedback block, FBB is considered as an RNN with a global hidden state. High level features are transferred from the output of the feedback block at the t − 1 th iteration to its input at the t th iteration. The hidden state is concatenated with F T in and a 1 × 1 compression convolution layer is applied for high-level and low-level feature fusion as shown in <ref type="figure">Fig. 2</ref>. The fused features are passed to the dilated dense blocks, followed by a 3 × 3 convolution layer for further processing.</p><p>2) Local Feedback: We argue that a feedback connection is always beneficial as it helps to guide the low-level features which are in some way blind to the higher level features. Hence, we have implemented local feedback connections over each DDB which aim to improve the features generated locally. These connections run parallel to the global feedback connections and increase the overall effectiveness of the network. Each DDB can be considered as an RNN similar to the global feedback block, transferring features from its output to its input via a local hidden state. Loss calculated directly on HDR images is misrepresented due to the dominance of high intensity values of images with a wide dynamic range. Therefore, we tonemap the generated and the ground truth HDR images to compress the wide intensity range before calculating the loss function value. We use the µ-law for tonemapping, as suggested by <ref type="bibr" target="#b11">[12]</ref>. The µ-law is represented as below.</p><formula xml:id="formula_5">T (H t gen ) = log(1 + µH t gen ) log(1 + µ)<label>(5)</label></formula><p>Here, T represents the tonemapping operation and µ defines the amount of compression, which is set to 5000 for the experiments. In addition to the L1 loss suggested by previous feedback networks, we use a perceptual loss <ref type="bibr" target="#b12">[13]</ref> for improving the visual quality of the generated image. We calculate the L1 loss and the perceptual loss at every iteration t and take an average over all the iterations. The average L1 loss L L1 is given below.</p><formula xml:id="formula_6">L L1 = 1 n n t=1 T (H t gen ) − T (H gt )<label>(6)</label></formula><p>Here, H gt represents the ground truth image. The average perceptual loss L p can be represented as below.</p><formula xml:id="formula_7">L p = 1 n n t=1 f V GG19 T (H t gen ), T (H gt )<label>(7)</label></formula><p>Here, f V GG19 represents the perceptual loss calculated between the tonemapped ground truth and generated images. The final loss function L is given below.</p><formula xml:id="formula_8">L = L p + λL L1<label>(8)</label></formula><p>Here, λ is set to 0.1 for all the experiments. We have observed that applying only the L1 distance produces dark artifacts. However, a combination of L1 loss and perceptual loss has resulted in improved visual quality of the generated image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation details</head><p>All the convolutional layers (conv2d) in the network have 3×3 kernels, followed by a ReLU activation, unless mentioned otherwise. There are 2 conv2d layers in the FEB, 20 (6×3+2) conv2d layers in FBB, and 2 conv2d layers in the HRB. The size of the feature maps remains same throughout the network. The depth of the feature maps also remain same i.e. 64, except for in the DDBs. A growth rate of 32 has been implemented for the DDBs, meaning that 3 × 3 conv2d layers of each dense block output a 32 channel feature map that gets concatenated with features from all the preceding layers. This accumulated feature map depth is brought down using a 1 × 1 conv2d layer at the end of the dense block.</p><p>We trained our network on Geforce RTX 2070 GPU with a batch-size of 16 for the City Scene dataset and 6 for the curated HDR dataset. Adam optimizer <ref type="bibr" target="#b13">[14]</ref> was adopted with momentum parameters β 1 = 0.5 and β 2 = 0.999. All the variants of the proposed model were trained for 200 epochs with an initial learning rate of 2 × 10 −4 for first 100 epochs, decayed linearly over the next 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS A. Dataset</head><p>We have trained and evaluated the performance of our network on the standard City Scene dataset <ref type="bibr" target="#b7">[8]</ref> and another HDR dataset curated from various sources shared in <ref type="bibr" target="#b0">[1]</ref>. The City Scene dataset is a low resolution HDR dataset that contains pairs of LDR and ground truth HDR images of size 128 × 64. We trained the network on 39460 image pairs provided in the training set and evaluated it on 1672 randomly selected images from the testing set. The curated HDR dataset consists of 1010 HDR images of high resolution. Training and testing pairs were created by producing LDR counterparts of the HDR images by exposure alteration and application of different camera curves from <ref type="bibr" target="#b17">[18]</ref>. Data augmentation was done using random cropping and resizing. The final training set consists of 11262 images of size 256×256. The testing set consists of 500 images of size 512×512 to evaluate the performance of the network on high resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>For the quantitative evaluation of the proposed method, we use the HDRVDP-2 Q-score metric, specifically designed for evaluating reconstruction of HDR images based on human perception <ref type="bibr" target="#b18">[19]</ref>. We also use the commonly adopted PSNR and SSIM image-quality metrics. The PSNR score is calculated in dB between the µ-law tonemapped ground truth and generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feedback mechanism analysis</head><p>To study the influence of feedback mechanisms, we compare the results of four variants of the network based on the number of feedback iterations performed -(i) FHDR/W (feed-forward / without feedback), (ii) FHDR-2 iterations, (iii) FHDR-3 iterations, and (iv) FHDR-4 iterations. To visualise the overall performance, we plot the PSNR score values against the number of epochs for the four variants, shown in <ref type="figure" target="#fig_1">Fig. 5</ref>. The significant impact of feedback connections can be easily observed. The PSNR score increases as the number of iterations increase. Also, early reconstruction can be seen in the network with feedback connections. Based on this, we decided to implement an iteration count of 4 for the proposed FHDR network.</p><p>IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p>Qualitative and quantitative evaluations are performed against two non-learning based inverse tonemapping methods-(AKY) <ref type="bibr" target="#b14">[15]</ref> and (KOV) <ref type="bibr" target="#b15">[16]</ref>, two deep learning methods-LDR AKY <ref type="bibr" target="#b14">[15]</ref> KOV <ref type="bibr" target="#b15">[16]</ref> DRTMO <ref type="bibr" target="#b2">[3]</ref> HDRCNN <ref type="bibr" target="#b0">[1]</ref> FHDR/W FHDR GROUND TRUTH <ref type="figure">Fig. 4</ref>: Qualitative evaluation against five described methods on the curated HDR dataset. (HDR images have been tonemapped using Reinhard tonemapping algorithm <ref type="bibr" target="#b16">[17]</ref> for displaying.) HDRCNN <ref type="bibr" target="#b0">[1]</ref> and DRTMO <ref type="bibr" target="#b2">[3]</ref> and the feed-forward counterpart of the proposed FHDR method. We use the HDR toolbox <ref type="bibr" target="#b19">[20]</ref> for evaluating the non-learning based methods. The deep learning methods were trained and evaluated on the described datasets, as mentioned in earlier sections. DRHT <ref type="bibr" target="#b3">[4]</ref> is the state-of-the-art deep neural network for the image correction task. It reconstructs HDR content as an intermediate output and transforms it back into an LDR image. Due to unavailability of DRHT network implementation, results of their network on the curated dataset are not presented. Performance metrics of their LDR-to-HDR network, trained on the City Scene dataset has been sourced from their paper because of the same experiment setup in terms of the training and testing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative evaluation</head><p>A quantitative comparison of the above mentioned HDR reconstruction methods has been presented in <ref type="table">Table 1</ref>. The proposed network outperforms the state-of-the-art methods in evaluations performed on both the datasets. Results of DRTMO <ref type="bibr" target="#b2">[3]</ref> could not be calculated on the City Scene dataset because the network does not accept low resolution images. Even though the feed-forward counterpart of FHDR outperforms other methods, the proposed FHDR network (4iterations) performs far better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative evaluation</head><p>As can be seen in <ref type="figure">Fig. 4</ref>, non-learning based methods are unable to recover highly over-exposed regions. DRTMO brightens the whole image and is able to recover only the under-exposed, regions. HDRCNN, on the other hand, is designed to recover only the saturated regions and thus under performs in recovering information in under-exposed dark regions of the image. Unlike others, our FHDR/W pays equal attention to both under-exposed and over-exposed regions of the image. The proposed FHDR network with the feedback connections enhances the overall sharpness of the image and performs an even better reconstruction of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>City Scene Dataset Curated HDR Dataset PSNR SSIM Q-score PSNR SSIM Q-score AKY <ref type="bibr" target="#b14">[15]</ref> 15  V. CONCLUSION We propose a novel feedback network FHDR, to reconstruct an HDR image from a single exposure LDR image. The dense connections in the forward-pass enable feature-reuse, thus learning robust representations with minimum parameters. Local and global feedback connections enhance the learning ability, guiding the initial low level features from the high level features. Iterative learning forces the network to create a coarse-to-fine representation which results in early reconstructions. Extensive experiments demonstrate that the FHDR network is successfully able to recover the underexposed and overexposed regions outperforming state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Dilated Dense Block D. Loss function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Convergence analysis for four variants of FHDR, evaluated on the City Scene dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative comparison against state-of-the-art methods.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hdr image reconstruction from a single exposure using deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Expandnet: A deep convolutional neural network for high dynamic range expansion from low dynamic range content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marnerides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bashford-Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hatchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="2018" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep reverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="177" to="178" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image correction via deep reciprocating hdr transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep chain hdri: Reconstructing a high dynamic range image from a single low dynamic range image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="49913" to="49924" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feedback networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning high dynamic range from outdoor panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4519" to="4528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep high dynamic range imaging of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="144" to="145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do hdr displays support ldr content?: a psychophysical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Akyüz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Riecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-quality reverse tone mapping for a wide range of exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kovaleski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 27th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What is the space of camera response functions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">602</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hdr-vdp-2.2: a calibrated method for objective quality prediction of high-dynamic range and standard images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10501</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Advanced high dynamic range imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AK Peters/CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
