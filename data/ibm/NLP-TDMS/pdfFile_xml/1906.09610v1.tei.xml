<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Description-based Person Re-identification by Multi-granularity Image-text Alignments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wanli</forename><forename type="middle">Ouyang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Improving Description-based Person Re-identification by Multi-granularity Image-text Alignments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Description-based person re-identification</term>
					<term>multi-granularity image-text alignments</term>
					<term>step training strategy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Description-based person re-identification (Re-id) is an important task in video surveillance that requires discriminative cross-modal representations to distinguish different people. It is difficult to directly measure the similarity between images and descriptions due to the modality heterogeneity (the crossmodal problem). And all samples belonging to a single category (the fine-grained problem) makes this task even harder than the conventional image-description matching task. In this paper, we propose a Multi-granularity Image-text Alignments (MIA) model to alleviate the cross-modal fine-grained problem for better similarity evaluation in description-based person Re-id. Specifically, three different granularities, i.e., global-global, global-local and local-local alignments are carried out hierarchically. Firstly, the global-global alignment in the Global Contrast (GC) module is for matching the global contexts of images and descriptions. Secondly, the global-local alignment employs the potential relations between local components and global contexts to highlight the distinguishable components while eliminating the uninvolved ones adaptively in the Relation-guided Global-local Alignment (RGA) module. Thirdly, as for the local-local alignment, we match visual human parts with noun phrases in the Bi-directional Fine-grained Matching (BFM) module. The whole network combining multiple granularities can be end-to-end trained without complex preprocessing. To address the difficulties in training the combination of multiple granularities, an effective step training strategy is proposed to train these granularities step-by-step. Extensive experiments and analysis have shown that our method obtains the state-of-the-art performance on the CUHK-PEDES dataset and outperforms the previous methods by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COCO</head><p>CUHK-PEDES <ref type="figure">Fig. 1</ref>. The fine-grained problem of description-based person re-identification. Images in description-based person Re-id (CUHK-PEDES <ref type="bibr" target="#b23">[24]</ref> dataset, six different people) are much less distinguishable than the ones in imagedescription matching task (MS-COCO <ref type="bibr" target="#b18">[19]</ref> dataset), because they all belong to the same category, i.e., pedestrian category. (Best viewed in colors.) and can provide adequate and comprehensive information including semantic components and their potential relations to retrieve the matched person. Although studied from various perspectives <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b22">[23]</ref>, there are still challenging problems that need to be better addressed.</p><p>Description-based person Re-id is a challenging task because the existing modality heterogeneity makes it difficult to directly measure the cross-modal similarity between images and descriptions. Although the conventional image and description matching problem has been widely studied <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b42">[43]</ref>, there is a specific difference in the task of description-based person Re-id. All images in this task belong to the same category, i.e., pedestrian category (the fine-grained problem), making this task harder than only dealing with the modality heterogeneity. As shown in <ref type="figure">Figure  1</ref>, six different people have similar suits, and it is obviously harder to distinguish them compared with the images in the conventional image-description matching problem which have various topics, scenes, styles and so on. And the handannotated descriptions are also similar among different people due to the same category. Therefore, it is more difficult to solve the cross-modal fine-grained problem in the description-based person Re-id.</p><p>To address the fine-grained problem, one straightforward idea is to apply the existing fine-grained component matching methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b29">[30]</ref> to the new scenario of descriptionbased person Re-id to enhance the discrimination of different features. But there are still some problems that have not been  First, methods <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b25">[26]</ref> based on pre-processing with external cues (e.g., pose) for local component extraction need to be further fine-tuned or even re-trained additionally beforehand on the dataset of pedestrian. So they can provide more accurate components for the subsequent fine-grained matching in person identification. Unfortunately, there is no annotation of body part or body segment in the dataset of descriptionbased person Re-id, which makes the fine-tuning or re-training impossible. As for the region-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>, they need attribute-level annotations (as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a)) to fine-tune the region proposal generation approaches for preprocessing due to the the fine-grained problem in person Re-id. But attribute-based annotations are also not available in the pedestrian dataset. And complex pre-processing approaches may lead to some difficulties in end-to-end training.</p><p>Second, fine-grained part-based methods <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref> have shown great strength in the conventional image-based person Re-id, which do not require additional part labeling annotations for pre-processing. However, it is non-trivial to carry out part-based fine-grained matching in the descriptionbased person Re-id, and no work has been investigated from this aspect to the best of our knowledge. The difficulties compared with the conventional image-based person Re-id mainly lie in two aspects. On the one hand, a single image part may correspond to multiple separate words in the description, as shown by the part 'yellow bag slung' in <ref type="bibr">Figure 2 (b)</ref>. Thus the simple textual partition, i.e., naturally dividing the sentence to separate words, is not appropriate for imagedescription fine-grained matching. On the other hand, there are also some ambiguities due to the modality heterogeneity when using textual words to retrieve the matched image part. Specifically, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c), 'white skirt' can refer to several visual components from different people which contain skirts in different styles and sizes, covering different regions and parts of human body. And these ambiguities in finegrained matching may cause confusion and harm the retrieval accuracy in the description-based person Re-id. Therefore, adaptive local component alignment is necessary for crossmodal fine-grained matching.</p><p>In addition, only employing fine-grained component matching is not enough because it neglects the potential relations between local components and global contexts, which can also be used to improve the cross-modal similarity evaluation. To be more specific, the cross-modal global-local relations can be used as filters to eliminate the uninvolved components in the other modality in an image-sentence pair. For example in <ref type="figure" target="#fig_1">Figure 2</ref> (b), the legs and shoes are not mentioned in description, thus these attributes should not contribute to the visual semantic representations accordingly. In this example, sentence description provides cross-modal mutual information that helps to ignore uninvolved visual cues, and this also applies when visual information is used for discarding uninvolved textual components. Based on the relation-guided filtering process, we can obtain better aggregated representations for measuring more accurate cross-modal similarities.</p><p>More than the fine-grained component matching and relation-guided matching that consider the fine-grained problem to enhance the discrimination of features, the global contexts are also important in person identification. It is because that global contexts consist of more information including not only local components but their spatial relations (mainly in images) and orders (mainly in descriptions) comprehensively. These potential semantic aspects also contribute to identifying a pedestrian more accurately. On the whole, as explained in <ref type="figure">Figure 3</ref>, we consider the foregoing finegrained component matching, relation-guided matching, and the global context matching as different granularities to carry out multi-granularity cross-modal alignments hierarchically. The three granularities can complement each other and provide comprehensive cross-modal similarity evaluation.</p><p>Although our method can be end-to-end trained, it does not mean that training all modules simultaneously is a good training strategy. In fact, the combination of multiple granularities brings some difficulties in training. For one thing, local components and global contexts lie in different semantic levels and there are some differences in the objectives to use in training. To be more specific, global contexts contain more than local components but their potential dependency (e.g., spatial relations in images and word orders in descriptions), so they have tighter relevance to person identity than local components. And they are appropriate to be trained under the supervision of person identity more than only the cross-modal matching. For another, local component extraction approaches may inevitably bring some ambiguities in fine-grained component representations. For example, multiple attributes or incomplete attribute may be divided into a single image part, and this problem is likely to harm the global feature extraction when trained together. Therefore, we empirically find that it is more effective to train the global contexts and the local <ref type="figure">Fig. 3</ref>. The multi-granularity image-text alignments. There are three different granularities in our solution, i.e., global-global, global-local and local-local alignments. The tick icon in the global-local granularity means that the component is mentioned in global context in the other modality, and the cross icon means that it is not mentioned. For example, 'shoes' are not mentioned in the corresponding description, thus the last image part is with a cross icon, which indicates that this image part is less important according to the textual description. (Best viewed in colors.) components hierarchically and step-by-step.</p><p>In summary, this paper proposes a Multi-granularity Image-text Alignments (MIA) model to alleviate the crossmodal fine-grained problem for better similarity evaluation in tha task of description-based person Re-id. Specifically, three different granularities, i.e., global-global, global-local and local-local alignments are carried out hierarchically as shown in <ref type="figure">Figure 3</ref>. Firstly, the global-global alignment in the Global Contrast (GC) module is for matching the global contexts of images and descriptions. Secondly, the global-local alignment employs the potential relations between local components and global contexts to highlight the distinguishable components while eliminating the uninvolved ones adaptively in the Relation-guided Global-local Alignment (RGA) module. Thirdly, as for the local-local alignment, we match visual human parts with noun phrases in the Bi-directional Finegrained Matching (BFM) module. And the whole network with multiple granularities can be end-to-end trained without complex pre-processing. To address the difficulties in training the combination of multiple granularities, an effective step training strategy is proposed to train these granularities stepby-step. We have obtained the state-of-the-art performance on the CUHK-PEDES <ref type="bibr" target="#b23">[24]</ref> dataset, and outperformed the previous methods by a significant margin. The main contributions are as follows:</p><p>• To the best of our knowledge, we are probably the first to match visual human parts with noun phrases for description-based person Re-id. • To alleviate the cross-modal fine-grained problem, we propose a multi-granularity image-text alignments model for description-based person Re-id. Three different granularities, i.e., global-global, global-local and local-local alignments are carried out hierarchically. They consider matching the global contexts, using global-local relations to filter the uninvolved components and the bi-directional fine-grained matching, respectively, for more accurate cross-modal matching. And the proposed model is endto-end trainable. • To better train the combination of multiple granularities, an effective step training strategy is proposed to train the whole model step-by-step. <ref type="bibr">•</ref> We obtain the state-of-the-art performance on the CUHK-PEDES dataset, significantly outperforming other previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Visual-Semantic Embedding</head><p>With the rapid growth of multi-modal data, the relations and alignments among different modalities have drawn much attention recently. Frome et al. <ref type="bibr" target="#b12">[13]</ref> propose the Visual Semantic Embedding (VSE) framework, which aligns global image features with sentence features by using ranking loss. Faghri et al. <ref type="bibr" target="#b11">[12]</ref> penalize the VSE model according to the hardest negative examples in the loss function and further improve the image-sentence alignment. Huang et al. <ref type="bibr" target="#b16">[17]</ref> propose a selective multi-modal Long Short Term Memory network (sm-LSTM) for image-text matching. The sm-LSTM includes a multi-modal context-modulated attention scheme which can selectively attend to a pair of instances of image and sentence at each timestep. Lee et al. <ref type="bibr" target="#b21">[22]</ref> propose the Stacked Cross Attention Network (SCAN), which discovers the cross-modal alignments by a fine-grained attention scheme on regions in image and words in sentence. Beyond the fundamental imagetext matching, there are more emerging and attractive applications related to visual-semantic embedding, such as image captioning <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b1">[2]</ref> and visual question answering <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Anderson et al. Unlike them, the fine-grained problem is the major difficulty in distinguishing different people in the description-based person Re-id, which needs to be carefully addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Person Re-identification</head><p>Person re-identification has gained increasing attention from both academia and industry recently, and state-of-the-art approaches are mostly dominated by the emerging deep learning techniques. Su et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Contrast</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation-guided Global-local Alignment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-directional Fine-grained Matching</head><p>He has dark slacks on and a dark jacket, and he is wearing glasses and also carrying a case. <ref type="figure">Fig. 4</ref>. The overall framework of our solution. There are mainly two parts inside the framework: the (a) global and local representation extraction and the (b) multi-granularity image-text alignments model. The numbers on different blocks show the steps that they are trained in our step training strategy, respectively. And the proposed step training strategy will be explained in detail in the following Section III-B2. (Best viewed in colors.) contrastive attention model to learn features separately from the body and background regions with region-level triplet loss.</p><p>Different from the previous approaches that exploit additional uni-modal cues, e.g., pose and mask, for discriminative representation learning, the most severe difficulty in description-based person Re-id is the modality heterogeneity. And it is non-trivial to address the cross-modal fine-grained problem in the description-base person Re-id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Description-based Person Re-identification</head><p>Li et al. <ref type="bibr" target="#b23">[24]</ref> propose the first large-scale person description dataset, CUHK PErson DEScription dataset (CUHK-PEDES), which contains person images with detailed natural language annotations. They also provide the Recurrent Neural Network with Gated Neural Attention mechanism model (GNA-RNN) with unit-level attentions and word-level gates to determine the cross-modal affinity with only matching objective. After that, Li et al. <ref type="bibr" target="#b22">[23]</ref> further propose an identity-aware two-stage framework for textual-visual matching. They first adopt Cross-Modal Cross-Entropy loss (CMCE) in stage-1 for screening easy incorrect matchings, which uses person identity information as supervision. Then they verify hard matchings with a co-attention mechanism, which jointly learns the visual spatial attention and latent semantic attention in stage-2. These two solutions mentioned above regard each hidden state from LSTM as word-level textual representation, which may incur some noise because a hidden state contains a complex semantic mixture of the current word and previous words. And they only pay attention to one single direction when using the fine-grained matching or attention scheme for representation enhancement, i.e., only using text for weighting different visual components. Chen et al. <ref type="bibr" target="#b4">[5]</ref> improve visual representations by global and local cross-modal associations. The global image-language association is established according to the identity labels, and the local association focuses on improving the visual representations by phrase reconstruction.</p><p>Different from them, we are probably the first to match visual human parts with noun phrases for description-based person Re-id, and further propose a multi-granularity imagedescription alignments model to carry out more accurate person identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>The overall framework with our Multi-granularity Imagetext Alignments (MIA) model is shown in <ref type="figure">Figure 4</ref>. There are mainly two parts inside the framework: (a) global and local representations extraction and (b) multi-granularity image-text alignments model.</p><p>In part (a), we use convolutional neural networks (CNN) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b14">[15]</ref> to extract the visual feature maps in the image path. Then the path is divided into two for global context features and image part features, respectively. We use a global mean pooling layer and a fully connected layer (FC layer) in sequence to obtain the global visual representation. We employ 1 × 1 convolution and local mean pooling on respective image parts to obtain part features. As for the textual path, sentence encoding and phrase encoding share the same bi-directional gated recurrent unit (Bi-GRU) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> and then have different FC layers.</p><p>For the MIA model in part (b), there are mainly three modules corresponding to the three granularities. To be more specific, the Global Contrast (GC) module is used to carry out global-global alignment. And it uses the global visual and textual context representations to obtain a fundamental globalglobal similarity. The Relation-guided Global-local Alignment (RGA) module is for global-local relation filtering, which exploits cross-modal relation alignments to filter the uninvolved attributes for better aggregated representations. And an intermediate global-local similarity is computed in the RGA module. Then the Bi-directional Fine-grained Matching (BFM) module is for local-local alignment based on the trained finegrained local components. By combining these three modules for different granularities hierarchically, we can obtain more comprehensive cross-modal similarity evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-granularity Image-text Alignments (MIA)</head><p>We present the details of the three different modules in our MIA model as follows.</p><p>1) Global Contrast (GC): For the fundamental globalglobal granularity, we use the conventional CNNs for visual encoding and Bi-GRU for textual encoding. For the image I, the feature maps from CNN are sequentially passed through a global mean pooling layer and a FC layer to obtain the global visual context representation, I ∈ R V . As for the description T , we first embed each word w ∈ R W inside T to a vector</p><formula xml:id="formula_0">x ∈ R E by x = W e × w,<label>(1)</label></formula><p>where W e ∈ R E×W . Then we input all these vectors sequentially through a Bi-GRU,</p><formula xml:id="formula_1">− → h t = − −− → GRU (x t , − − → h t−1 ) ← − h t = ← −− − GRU (x t , ← − − h t−1 ).<label>(2)</label></formula><p>After that, we concatenate the forward hidden state − → h F ∈ R H and backward ←− h F ∈ R H at separate last time-step F and F . And the sentence FC layer is employed for getting the final representation T ∈ R C of the description T , as shown by</p><formula xml:id="formula_2">T = W g × [ − → h F , ←− h F ] + b g ,<label>(3)</label></formula><p>where W g ∈ R C×2H and b g ∈ R C are the parameters in the sentence FC layer for the description T . And [·, ·] means the concatenation of two vectors. The global-global similarity between image I and description T is computed by</p><formula xml:id="formula_3">s G = sim(I, T),<label>(4)</label></formula><p>where sim(·, ·) denotes the similarity function between the two feature vectors.</p><p>2) Relation-guided Global-local Alignment (RGA): For relational modeling, we first have to extract the fine-grained image parts and noun phrases. Then the global-local relations can be modeled based on the fine-grained component features and global contexts. The approach in <ref type="bibr" target="#b36">[37]</ref> splits the feature maps from visual CNN into several non-overlapping parts equally along the vertical direction. And it introduces almost no extra computational cost, better than the pose-based or region-based approaches which require complex pre-processing beforehand. Therefore, visual partition can be done along with the training procedure and enables our method to be end-to-end trainable. As for the textual description, noun phrases have more semantic concepts and more precise correspondences with image parts than separate words. Therefore, we focus on noun phrases and encode them separately, different from <ref type="bibr" target="#b21">[22]</ref> that models all words in sentence by their hidden states from LSTM. The reason is that a hidden state contains a complex semantic mixture of the current word and previous words, which may bring noise in the following cross-modal matching.</p><p>After obtaining the visual human parts and textual noun phrases, directly using these fine-grained local components for cross-modal matching is inappropriate, because there exist some ambiguities due to the modality heterogeneity and imperfect partition approaches, and the local component representations are not well trained yet. Therefore, we employ the attention mechanism to first carry out relation-guided globallocal alignment to improve the quality of local component representations. And an intermediate cross-modal similarity can be obtained based on the attentively aggregated representations and the global contexts.</p><p>For the image I, we obtain n local features corresponding to different non-overlapping image parts following <ref type="bibr" target="#b36">[37]</ref>, which are denoted as P 1 , P 2 , ..., P n ∈ R P . As for description T , we use the Natural Language ToolKit (NLTK) <ref type="bibr" target="#b3">[4]</ref> for syntactic analysis, word segmentation and part-of-speech tagging, and then obtain several noun phrases. This extraction procedure can be handled dynamically along with the training procedure, which benefits the end-to-end training. Similar to the whole description encoding, we use Equations 1 and 2, and employ another FC layer in</p><formula xml:id="formula_4">N = W l × [ − → h F , ←− h F ] + b l<label>(5)</label></formula><p>for getting the representation of a noun phrase. W l ∈ R N ×2H and b l ∈ R N are the parameters in the noun phrase FC layer. We do not restrict the number m of noun phrases extracted from a sentence, i.e., m is different for different descriptions, and obtain the features of</p><formula xml:id="formula_5">N 1 , N 2 , ..., N m ∈ R N .</formula><p>Based on the image part representations P 1 , P 2 , ..., P n and noun phrase representations N 1 , N 2 , ..., N m , we have two opposite directions in relation-guided global-local alignment, i.e., image-guided phrase alignment (T → I) and sentenceguided part alignment (I → T ). <ref type="figure" target="#fig_4">Figure 5</ref> shows the I → T direction as an example. We first employ a cross-modal attention method to determine the relations v i between all the image parts P 1 , P 2 , ..., P n and the global textual context T.</p><formula xml:id="formula_6">Specifically, each v i is computed by v i = exp(sim(M LP V (P i ), T)) n i=1 exp(sim(M LP V (P i ), T)) .<label>(6)</label></formula><p>M LP V (·) means multi-layer perceptron for visual parts, i.e., MLP-V-RGA in <ref type="figure">Figure 4</ref>, and sim(·, ·) denotes the similarity function between P i and T. Then we use</p><formula xml:id="formula_7">I R = n i=1 v i · P i<label>(7)</label></formula><p>to selectively aggregate the part representations P i to the relation-guided visual representation I R ∈ R P . This feature aggregation process is supervised by the global-local relation indicator v i , which indicates the importance between different image parts and the whole description. After that, the intermediate cross-modal similarity in the I → T direction is</p><formula xml:id="formula_8">s I = sim(I R , T),<label>(8)</label></formula><p>which is regarded as the global-local similarity inside the intermediate hierarchy RGA of our MIA model. Similarly, we can obtain the relation-guided textual representation T R ∈ R N in the opposite T → I direction by</p><formula xml:id="formula_9">t j = exp(sim(M LP T (N j ), I)) m j=1 exp(sim(M LP T (N j ), I)) ,<label>(9)</label></formula><formula xml:id="formula_10">T R = m j=1 t j · N j .<label>(10)</label></formula><formula xml:id="formula_11">N 1 N 2 N 3 N 4 P 1 P 2 P 3 P 4 P 5 P 6</formula><p>dark jacket glasses case dark slacks </p><formula xml:id="formula_12">I 1 I 2 I 3 I 4 mean sim1</formula><formula xml:id="formula_13">s T = sim(T R , I).<label>(11)</label></formula><p>3) Bi-directional Fine-grained Matching (BFM): In the top level of our MIA model, we carry out bi-directional finegrained matching between visual human parts and textual noun phrases for local-local granularity similarities. The local feature extraction of image parts and noun phrases employs the modules that have been trained in the foregoing GC and RGA modules. And another two MLPs (MLP-T-BFM and MLP-V-BFM in <ref type="figure">Figure 4</ref>) in the BFM module provide adaptation for appropriate parts-phrases matching. Based on the component representations, the BFM module obtains distinguishable finegrained cross-modal similarities and identifies a pedestrian having small differences with others.</p><p>To better exploit cross-modal correspondences, we have two opposite directions in fine-grained matching, i.e., nounphrase-related direction (P → N ) and part-related direction (N → P ). <ref type="figure" target="#fig_5">Figure 6</ref> shows an example of the P → N direction. We first select a noun phrase, e.g., N 1 of dark slack, and evaluate the similarity between N 1 and all the image parts, P 1 , P 2 , ..., P n . Then we refer to these similarity values and use an attention mechanism to adaptively obtain the combined single-noun-phrase-related visual representation, I 1 ∈ R P , as shown by the path in golden color in <ref type="figure" target="#fig_5">Figure 6</ref>. Using the same steps for each noun-phrase feature N i , we can obtain all the combined single-noun-phrase-related visual representations I 1 , I 2 , ..., I m ∈ R P by</p><formula xml:id="formula_14">I i = n k=1 α i,k · P k ,<label>(12)</label></formula><formula xml:id="formula_15">α i,k = exp(sim(M LP T (N i ), M LP V (P k ))) n k=1 exp(sim(M LP T (N i ), M LP V (P k )))</formula><p>.</p><p>M LP T (·) means the multi-layer perceptron MLP-T-BFM for noun phrases, and M LP V (·) indicates the MLP-V-BFM for visual parts, as shown in <ref type="figure">Figure 4</ref>. sim(·, ·) denotes the similarity function between the two feature vectors, and exp(·) means the exponential operation. After obtaining I 1 , I 2 , ..., I m , the local-local similarity in the P → N direction is</p><formula xml:id="formula_17">s P = 1 m m k=1 sim(I k , N k ).<label>(14)</label></formula><p>Similarly in the opposite N → P direction, we can obtain the combined single-image-part-related textual representations T 1 , T 2 , ..., T n ∈ R N by</p><formula xml:id="formula_18">T j = m k=1 β j,k · N k ,<label>(15)</label></formula><formula xml:id="formula_19">β j,k = exp(sim(M LP V (P j ), M LP T (N k ))) m k=1 exp(sim(M LP V (P j ), M LP T (N k )))</formula><p>. <ref type="formula" target="#formula_0">(16)</ref> And the local-local similarity in the N → P direction is</p><formula xml:id="formula_20">s N = 1 n n k=1 sim(T k , P k ).<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Similarity Fusion:</head><p>From the aforementioned multigranularity alignments in separate modules, we obtain five similarities which can be divided into three different categories. Specifically, s G from the GC module can be regarded as the global-global similarity, and s I and s T are the intermediate global-local similarities in the RGA module. In the toplevel BFM module, s P and s N are regarded as the local-local similarities. To properly fuse these similarities, we introduce two hyper-parameters to adjust their proportions,</p><formula xml:id="formula_21">s F = s G + λ 1 · s R + λ 2 · s L , s R = (s I + s T ) /2, s L = (s P + s N ) /2,<label>(18)</label></formula><p>where s F indicates the final cross-modal similarity, and s R and s L indicate the similarities in RGA and BFM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Procedure</head><p>We employ two kinds of objectives and propose an effective step training strategy. In the following, we provide the details of the objectives and the training strategy. 1) Objectives: Two different objectives are used in training, i.e., Identity objective and Matching objective. The identity objective comes from that identities (ID) of pedestrians can be regarded as categories for classifying images and descriptions, and the matching objective is commonly used in the conventional cross-modal retrieval.</p><p>Identity objective: We regard different IDs in the training set as the number of categories, and classify images and descriptions to the corresponding ID category separately. The loss value of the identity objective is</p><formula xml:id="formula_22">P I = sof tmax(W s · I + b s ), L I = −log(P I (ID)), P T = sof tmax(W s · T + b s ), L T = −log(P T (ID)).<label>(19)</label></formula><p>W s ∈ R ID×G is the shared matrix for mapping the features of images and descriptions into the same space, whose dimension ID depends on the number of different IDs in the training set, and G = V = C. b s ∈ R ID means the shared bias vector. Subscript I means image and T means description. P · (ID) is the predicted probability of the right person ID, and the loss value L · is the negative logarithm of the probability P · (ID).</p><p>Matching objective: The hinge-based triplet matching objective has shown its strength in the task of image-text matching <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Referring to <ref type="bibr" target="#b11">[12]</ref>, we adopt the Sum of Hinge (SH) loss L M as the matching objective: </p><formula xml:id="formula_23">L M = T max[0,</formula><p>where I means image and T means description. (I, T ) and (T, I) mean the matched image and description pairs, and (I, T ), (T, I) indicate the mismatched pairs. S(·, ·) means the similarity function between two samples. Parameter α is for the margin between matched and mismatched pairs.</p><p>Discussion. The mentioned two objectives have different concerns. The identity objective classifies the descriptions corresponding to different images while referring to the same person into the same ID category. However, the descriptions of an image may have some kind of mismatch to other images inside the same ID category. In other words, the identity objective is a little weak in handling the fine-grained matching. Therefore, the identity objective is more like a loose constraint, which is fit for initialization in training to eliminate obvious mismatched pairs. As for the matching objective, it is stricter because it regards the descriptions annotated for one image as negative matchings for other images that belong to the same person ID. So the matching objective can be used to learn more accurate cross-modal relations between an image and its corresponding descriptions, which is more suitable to be exploited for finetuning.</p><p>2) Training Strategy: The step training strategy contains three steps, corresponding to the three modules in our MIA model, i.e., the GC, RGA and BFM modules, respectively.</p><p>In the first step, we only use the identity objective to initialize the parameters related to global representations, which are annotated by number 1 in <ref type="figure">Figure 4</ref>. As explained in Section III-B1, the identity objective is more like a loose constraint and fits for initialization, thus we do not fine-tune the pre-trained visual CNN but focus on training the textual path and global visual FC layer from scratch. The overall loss function of the first step is</p><formula xml:id="formula_25">L 1 =L I + L T .<label>(21)</label></formula><p>In the second step, we aim to train the fine-grained component representations under the reference of trained global contexts, thus we additionally use the matching objective which is more suitable for accurate fine-tuning. As annotated by numbers 1 and 2 in <ref type="figure">Figure 4</ref>, the parameters (including the visual CNN) are fine-tuned by the identity and matching objectives together, and the overall loss function is</p><formula xml:id="formula_26">L 2 =L 1 + L G M + (L I−T M + L T −I M ),<label>(22)</label></formula><p>where L G M means the matching objective for global representations in the GC module. Finally, we fix other parameters except for the two MLPs in the BFM module for parts and phrases in training, as annotated by number 3 in <ref type="figure">Figure 4</ref>. The loss function is</p><formula xml:id="formula_27">L 3 =L P −N M + L N −P M .<label>(23)</label></formula><p>L P −N M and L N −P M are for the matching objectives in the two opposite directions in the BFM module.</p><p>Discussion. In the proposed step training strategy, the identity objective is only used to train the global contexts rather than local components, and the reason is that only global representations have tighter relevance to person ID. Specifically, different people could have similar local components, i.e., local components are not tightly relevant to person ID. Thus it is a little inappropriate to use identity objective for classifying the local components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Protocols</head><p>We evaluate our MIA model based on the CUHK-PEDES <ref type="bibr" target="#b23">[24]</ref> dataset, which is currently the only one for descriptionbased person Re-id. It contains 40,206 images from 13,003 different pedestrians, and each image comes with two handannotated descriptions. The textual descriptions are generally longer than 23 words, and the vocabulary contains 9,408 unique words in total. We follow the same protocol in <ref type="bibr" target="#b23">[24]</ref>. Specifically, there are 34,054 images with 68,108 captions of 11,003 different pedestrians in the training set. The validation set has 3,078 images, 6,156 sentences of 1,000 pedestrians, and the testing set has 3,074 images with 6,148 captions of another 1,000 pedestrians.</p><p>We measure performance by R@K, i.e., recall at K-the fraction of queries for which the correct item is retrieved in the closest K points to the query. Following <ref type="bibr" target="#b23">[24]</ref>, a successful search is achieved if any image of the corresponding person is among the top-K images. We report R@1, R@5 and R@10 criteria and their summation (Total) for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For an image I, the global visual representation I has the dimension of V = 1024. To have fair comparisons with the previous methods, we use the pre-trained VGG-16 <ref type="bibr" target="#b33">[34]</ref> and ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as visual CNN, respectively. Following <ref type="bibr" target="#b36">[37]</ref>, we resize all images to the size of 384×128, and these images are randomly mirrored for image augumentation before sent to the visual encoder. For VGG-16, we extract the feature maps before the first FC layer, whose size is 12×4×512. As for ResNet-50, we extract the feature maps before the average pooling layer, whose size is 24×8×2048. As for the text encoding, the global textual representation T has the dimension of C = 1024. Dimension of the word embeddings that are input to the GRU is E = 300, and the textual forward and backward final hidden states − → h F and ←− h F from GRU are of H = 1024, respectively.</p><p>For image parts, we split feature maps from visual CNN to six parts equally along the vertical direction, i.e., n = 6 in all the experiments, which has been proved to be the optimized </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Granularity</head><p>Training Strategy Results Context Relation Component Identity Matching S1 S2 S3 S1 S2 S3 S1 S2 S3 S1 S2 S3 S1 S2 S3 number of parts in <ref type="bibr" target="#b36">[37]</ref>. The two visual MLPs, i.e., MLP-V-BFM and MLP-V-RGA, map the features of image parts from the dimension of 256 to P = 1024. As for noun phrases, the output dimension of the textual MLPs, i.e., MLP-T-BFM and MLP-T-RGA, are set to N = 1024. All the four MLPs consist of two linear mapping layers and an activation function of ReLU <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b39">[40]</ref> between them. We employ the Cosine similarity as the similarity function sim(·, ·) everywhere in our MIA model, which has been proved to be effective and widely used in the field of crossmodal retrieval. In training, the Adam <ref type="bibr" target="#b19">[20]</ref> optimizer is employed to train the models with a batch size of 96. We start training with a learning rate of 0.001 for 10 epochs in step-1. In step-2, the initial learning rate is set to 0.0002, and it scales down to one tenth after each 10 epochs. After training for 15 epochs in step-2, we fix all the modules except for the two MLPs in the BFM module, i.e., MLP-V-BFM and MLP-T-BFM, for training another 5 epochs with a learning rate of 0.0002 in step-3. The margin α is set to 0.2 in the matching objective L M in Equation 20.</p><formula xml:id="formula_28">s G s R s L s F GC (G) √ √ √ √ √ √ GC + RGA (G) √ √ √ √ √ √ √ GC + BFM (G) √ √ √ √ √ √ √ GC + RGA + BFM (G) √ √ √ √ √ √ √ √ MIA (G) √ √ √ √ √ √ √ √ √ GC + RGA (R) √ √ √ √ √ √ √ GC + RGA + BFM (R) √ √ √ √ √ √ √ √ MIA (R) √ √ √ √ √ √ √ √ √ Fine (L) √ √ √ GC + BFM (L) √ √ √ √ √ √ √ GC + RGA + BFM (L) √ √ √ √ √ √ √ √ MIA (L) √ √ √ √ √ √ √ √ √ GC √ √ √ √ √ √ GC + BFM √ √ √ √ √ √ √ GC + RGA √ √ √ √ √ √ √ GC + RGA + BFM √ √ √ √ √ √ √ √ MIA √ √ √ √ √ √ √ √ √</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of Ablation Models</head><p>We carry out extensive experiments for evaluation of ablation models by taking VGG-16 as the visual CNN. To systematically evaluate the contributions of different model components and training configurations, we design various ablation models. As shown in <ref type="table" target="#tab_0">Table I</ref>, 'Context', 'Relation' and 'Component' are for the global-global, global-local and local-local alignments, respectively. The S1, S2 and S3 mean separate steps in training. The last column 'Results' in <ref type="table" target="#tab_0">Table I</ref> indicates the retrieval results of which granularity are reported. When a specific granularity is not trained, the corresponding parameter λ will be set to 0 in s F in Equation <ref type="bibr" target="#b17">18</ref>. For example, the entry 'GC' is provided with λ 1 = 0 and λ 2 = 0. For the model which needs to combine multiple granularities, we set λ 1 = 1 and λ 2 = 0.5 in Equation 18 to report the s F . 1) Granularities and Their Combination: As our MIA model is a solution that combines multiple granularities of cross-modal similarities, we provide analysis on the effectiveness of separate granularities and their combinations, as shown in <ref type="table" target="#tab_0">Table II</ref>. Specifically, entries in the upper block show the retrieval performance when combining different granularities in training. In the lower block, we train the whole MIA model with our step training strategy and provide the performance of different granularities individually, i.e., s G , s R , s L , and their combinations s F . The detailed analyses are as follows:</p><p>1-1) The 'GC + RGA' and 'GC + BFM' both obtain performance improvements compared with the 'GC', which proves that employing multi-granularity alignments can facilitate more accurate cross-modal similarity evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-2)</head><p>The RGA module is more effective than the BFM module. The reason is that using trained global contexts as reference in RGA tends to have the appropriate optimization direction for fine-grained feature extraction, resulting in better retrieval performance. In contrast, directly using fine-grained components for cross-modal matching in the BFM module may suffer from the problem of ambiguous local components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-3)</head><p>The 'GC + RGA + BFM' performs better than the 'GC + BFM' but worse when compared to the 'GC + RGA', which again validates the superiority of relation-guided filtering in RGA. And it also proves that employing fine-grained component matching in BFM for training together may lead to a wrong optimizing direction and harm the retrieval accuracy.</p><p>1-4) The 'MIA (R)' is better than the 'MIA (G)'. This indicates that considering relations to adaptively highlight the The young woman is wearing a tan shirt with a grey jacket on top, black pants and tan shoes. She is also smiling and talking to somebody.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Somebody Tan shoes Black pants Grey jacket Tan shirt Young woman Dark slacks Dark jacket Glasses Case</head><p>Legs and shoes are not mentioned</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Legs and shoes are not mentioned</head><p>A young girl is walking and looking at her phone wearing a red t shirt and jean shorts with a yellow bag slung over her shoulder.</p><p>She has long, blonde hair and is dressed in a sparkly vest, pink knit shirt and darker pink skirt with sparkles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second maximum</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimum</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second minimum</head><p>Color -Weight Legend <ref type="figure">Fig. 7</ref>. Visualization analysis on ablation studies of our method. (a) The effectiveness of the relation-guided attention in the RGA module. We provide the I → T direction as an example, i.e., the relations between image parts and the global textual context. Red color means the part with the maximal weight after attention and yellow color means the second maximum. Green and blue colors are for the parts with the smallest weights. The underlined attributes in description are relevant to the two parts with the largest weights in image, which are the most distinguishable attributes for more accurate person identification. In contrast, the image parts that are relevant to the uninvolved components in description have the smallest weights.  <ref type="figure">Fig. 8</ref>. Comparisons of the retrieval results among different granularities. The 'GC + BFM' and 'GC + RGA' models outperform the 'GC' model, and our 'MIA' method obtains the best retrieval results by combining multiple granularities. Taking the upper one for instance, using the 'yellow short sleeve shirt' can retrieve many people with a yellow shirt, but the fine-grained attribute 'reading a small pamphlet' is the key semantic concept to distinguish the correct person from others. As shown by the image in right (same color meanings as in <ref type="figure">Figure 7)</ref>, the 'reading a small pamphlet' part has the largest weight and the 'yellow short sleeve shirt' part has the second largest weight after the relation-guided attention. On the contrary, the bottom two parts have the smallest weights because the legs and shoes are not mentioned in the query description. And the lower example can be explained similarly. (Best viewed in colors.) significant components while filtering the uninvolved ones can obtain improvements than the global context matching. The global contexts consider every component equally by the global pooling, ignoring their different significance and bringing some noise from the uninvolved components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-5)</head><p>The full model 'MIA' outperforms any one of the 'MIA (G)', 'MIA (R)' and 'MIA (L)', which indicates that combining multi-granularity image-text alignments can provide comprehensive cross-modal similarity evaluation, leading to better retrieval performance.</p><p>To show the effectiveness of different granularities more clearly, we provide visualization analysis in <ref type="figure">Figure 7</ref>. In part (a), we illustrate some representative examples in 'GC + RGA' to show the strength of the relation-guided attention in the RGA module. Red color means that the part is the most relevant one to the textual context (i.e., maximal weight in attention), which should be highlighted heavily in the visual parts aggregation guided by the global-local relations. And the part in yellow color has the second maximum weight. The words that related to these two marked visual parts (with the largest weights) are underlined in description, and we can find that they cover the major attributes in description, which are the key semantic concepts that could be used to distinguish the person from others. In contrast, the parts in green and blue colors have the smallest weights after relation-guided attention, and the reason is that these two parts are for the attributes which are not mentioned in the description. Taking the first image-description pair as an example, 'looking at her phone', 'wearing a red t-shirt' and 'with a yellow bag slung over her shoulder' are the major attributes that make the person distinguishable from other people. Our method properly focuses on the two image parts containing these key attributes and can further achieve more accurate matching. The legs and shoes are not mentioned in the description, thus the bottom two image parts have the smallest weights.</p><p>In part (b), we illustrate some representative examples in 'GC + BFM' to show the effectiveness of the fine-grained matching in BFM. Red color means the part is the most relevant one (with maximal weight) to the phrase after attention, and yellow color means the second most relevant one. Unlike part (a), we omit the parts with the smallest weights in image, and the reason is that it is meaningless to tell which image part has the most discrepancy to the noun phrase. In the left example in <ref type="figure">Figure 7</ref> (b), we can find that our method can obtain correct matching for all the four noun phrases which have explicit referring attributes in image.</p><p>As for the example in the right, the first five noun phrases with explicit referring attributes are correctly referred. But the last phrase 'Somebody', which is ambiguous and involves almost all parts, cannot be accurately located to specific parts in the image. Based on the accurate fine-grained component matching, the major difficulty in description-based Re-id, i.e., the cross-modal fine-grained problem, can be alleviated. And the retrieval performance can be further improved. We provide comparisons of retrieval results in <ref type="figure">Figure 8</ref>. There are two aspects which are worth noting: First, the 'GC + BFM' and 'GC + RGA' models both outperform the 'GC' model, and 'GC + RGA' is better. Taking the upper one for example, we visualize the relation attention in RGA and find that our model focuses on 'reading a small pamphlet' (in red with the maximal weight) and 'wearing a yellow short sleeve shirt' (in yellow with the second maximal weight), which are the most distinguishable attributes for accurate retrieval. Between these two attributes, using 'yellow short sleeve shirt' can retrieve many people with a yellow shirt, but 'reading a small pamphlet' is the key attribute to distinguish the correct person from others. Therefore, the part in red is the most distinguishable one, i.e., having maximal weight in our relation-guided attention filtering. Second, by combining the RGA and BFM modules, the MIA model can obtain the best retrieval results than either RGA or BFM individually, validating the effectiveness of combining multi-granularity alignments in cross-modal similarity evaluation.</p><p>2)</p><p>Step Training Strategy Analysis: In <ref type="table" target="#tab_0">Table III</ref>, we provide the detailed results in different granularities individually, i.e., s G , s R , s L and the final similarity s F , to carry out analysis on the proposed step training strategy. 2-1) For the global context similarity s G , we can find that directly employing the BFM module for training together tends to harm the performance based on the comparisons between 'GC + BFM (G)' and 'GC (G)' as well as 'GC + RGA + BFM (G)' and 'GC + RGA (G)'. The reason is due to ambiguities of local components, which may lead to a wrong optimizing direction and influence the quality of feature extractions.</p><p>2-2) Compared between 'MIA (G)' and 'GC + RGA + BFM (G)', we can find that our step training strategy can alleviate the problem of ambiguous local component extraction, and keep the s G from decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-3)</head><p>The 'MIA (G)' has the same results as the 'GC + RGA (G)'. The reason is that we fix the parameters related to the GC and RGA modules but focus on training the two MLPs (MLP-V-BFM and MLP-T-BFM) in step-3. So the parameters related to the GC and RGA modules remain unchanged.</p><p>2-4) Models in the s R block have the similar behaviors as in the s G block, and the reasons are the same as well.</p><p>2-5) In the block of s L , we can find that the performance of s L improves gradually. From 'Fine (L)' to 'MIA (L)', our step training strategy introduces the GC, RGA and BFM modules step-by-step with appropriate objectives in respective training steps. This will enhance the shared backbone networks and further improve the fine-grained component feature extraction, leading to a better fine-grained similarity s L .</p><p>2-6) As for the last block of s F , our step training ('MIA') outperforms the together training ('GC + RGA + BFM') by 1.3% in terms of R@1. In addition, compared with the 'GC + RGA' in <ref type="table" target="#tab_0">Table II</ref>, 'MIA' further obtains 0.8% improvements in terms of R@1 despite the problem of ambiguities from finegrained component extraction. The reason is that we first use GC and RGA to train the backbone modules for better feature extraction in the step-1 and step-2, avoiding the influence of inappropriate optimizing direction in BFM. After that, we fix the backbone modules and only train the two MLPs in BFM, which provide adaptation for more accurate s L . And this can results in a better s F finally by similarity combination.</p><p>3) Objective Analysis: Some varieties of employing different objectives in separate training steps are carried out and the results are in Table IV (Id is for identity objective and Mat means matching objective). We choose the 'GC + RGA' model for comparison, because the BFM module only contains the MLP-V-BFM and MLP-T-BFM to be trained in step-3, and the gradients are not back-propagated through other modules. So the performance of 'GC + RGA' remains unchanged.</p><p>3-1) Compared between the first two entries, we can find that using the identity objective and matching objective together in step-2 obtains better results than only using the matching objective. The identity objective fits for the goal of person identification, and the matching objective is effective and commonly used in the cross-modal image-sentence matching. Therefore, combining these two objectives together can further improve the representations in training and obtain better retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-2)</head><p>Considering the last two entries, using the identity objective in step-1 obtains better performance than the matching objective. It is more suitable to use the identity objective that is consistent with the testing protocol <ref type="bibr" target="#b23">[24]</ref> to initialize the parameters from scratch. Specifically, a successful search is achieved if any image of the corresponding person is among the top images. In contrast, the matching objective is too strict to provide an appropriate initial training orientation, because it regards the descriptions annotated for one image as negative matchings for other images that belong to the same person identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Step Combination: We also combine the step-1 and step-2 together in training (using a learning rate of 0.0002 to train the whole model, including the visual CNN), and the results of the 'GC + RGA' model are shown in <ref type="table" target="#tab_4">Table V</ref>. We can find its performance is much worse than our step training strategy. This validates the importance of only using the identity objective for parameters initialization in step-1, i.e., initializing the textual paths and global visual FC layer from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) All Words vs Noun Phrases:</head><p>To validate the superiority of using noun phrases rather than all words in description, we provide the results in <ref type="table" target="#tab_0">Table VI</ref>. The two entries both employ the 'MIA' model in <ref type="table" target="#tab_0">Table I</ref> with λ 1 = 1 and λ 2 = 0.5 for similarity combination. We can find that the proposed MIA model obtains better performance with only noun phrases rather than all words. The reason is that some words may not have the explicit corresponding components in the image, and these words will introduce some noise to the cross-modal finegrained matching. Besides, a single image part may correspond to multiple separate words in description, while a single word is not able to completely describe the image part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Other State-of-the-art Methods</head><p>The comparisons with other state-of-the-art methods are shown in <ref type="table" target="#tab_0">Table VII</ref>, and the best results are in bold. All the methods follow the same protocol in <ref type="bibr" target="#b23">[24]</ref> for fair comparison, as stated in Section IV-A. These methods can be divided into two categories by visual CNNs, i.e., VGG-16 and ResNet-50.  <ref type="bibr" target="#b4">[5]</ref> introduces the patchphrase matching, but only focuses on improving the visual representations by phrase reconstruction in local association. Both using ResNet-50, our MIA method outperforms the GLIA by 9.52% in terms of R@1, which again proves the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effect of the Hyper-parameters</head><p>In Equation 18, there are two balancing hyper-parameters λ 1 and λ 2 to adjust the proportions of separate similarities coming from different granularities. We carry out experiments to analyze the effect of these hyper-parameters, and the detailed results are in <ref type="table" target="#tab_0">Tables VIII and IX</ref>. Specifically, we employ the 'MIA' model in <ref type="table" target="#tab_0">Table I</ref>, and adjust the λ 1 or λ 2 when fixing the other one to figure out their effect individually. We can find that the performance is improved when λ 1 increases, and reaches the peak at about λ 1 = 1.5. After that, it goes down a little when λ 1 keeps increasing. As for the hyper-parameter  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Man is wearing a blue t-shirt with khaki shorts. He has on a black watch on his left wrist.</head><p>He is wearing a camera bag over his left shoulder that is resting on his right back hip. Query</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Top-10 retrieval results Top-1 Top-10</p><p>Top-1 Top-10 Top-10 retrieval results <ref type="figure">Fig. 9</ref>. Failure cases analysis. We provide some failure cases that our MIA model cannot retrieve the ground truth image within the top-10 results. These failure circumstances can be roughly divided into two different scenarios: (a) incomplete coverage and (b) fuzzy description. (Best viewed in colors.) λ 2 , the performance increases before around 0.7, then a little decrease will be seen as λ 2 continues to increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Failure Cases Analysis</head><p>Although our MIA model has achieved significant improvements than other state-of-the-art methods, there are still some circumstances that cannot be well handled. Some failure cases are shown in <ref type="figure">Figure 9</ref>. Specifically, we select the cases where the true matching pedestrian images are out of the top-10 retrieval results, and they can be roughly divided into two different scenarios. (a) Incomplete coverage. The query description does match some images which are not the ground-truth, but it cannot cover all the attributes in these images. Regarding these images as correct matchings will make mistakes. As shown in <ref type="figure">Figure 9</ref> (a), the description does match the pedestrian image in red box, but cannot cover the attribute 'black bag in right shoulder', which is the key attribute to be distinguished. And the ground truth image is mistakenly placed out of the top-10 retrieval results. (b) Fuzzy description. The sentence may contain some fuzzy attributes, which will harm the accurate retrieval. For example in <ref type="figure">Figure  9</ref> (b), the man's t-shirt is a little fuzzy between gray color and blue color, and the description finally regards it in blue color. On the contrary, the trained model considers the t-shirt is in gray color and places the ground truth image out of the top-10 retrieval results. The pedestrian image in red box is more likely to have blue t-shirt in the view of the trained model which has seen thousands of images in training.</p><p>To address these problems, we consider making more improvements to our method in the future work. To be more specific, we may refine the image partition approach, and make the number of image parts relevant to the attributes of a pedestrian. After that, the constraints between the number of image parts and the number of noun phrases can be exploited to alleviate the problem of incomplete coverage. As for the fuzzy description, data cleaning and image quality enhancement (e.g., super-resolution approaches) methods are likely to solve this problem.</p><p>V. CONCLUSION In this paper, we have proposed an end-to-end Multigranularity Image-text Alignments (MIA) model for the description-based person re-identification. The MIA model addresses the cross-modal fine-grained problem based on three different granularities hierarchically, i.e., global-global, globallocal and local-local alignments. Specifically, the global-global alignment in the Global Contrast (GC) module is for matching the global contexts of images and descriptions. The globallocal alignment exploits the relations between local components and global contexts to highlight the distinguishable components while eliminating the uninvolved ones adaptively in the Relation-guided Global-local Alignment (RGA) module. And the visual human parts and noun phrases are matched in the Bi-directional Fine-grained Matching (BFM) module for the local-local alignment. For better training the combination of multiple granularities, an effective step training strategy has been proposed to train these granularities step-by-step. Extensive experiments and analysis have been provided to validate the effectiveness of our MIA model and the step training strategy. This work has obtained the state-of-the-art performance on the CUHK-PEDES <ref type="bibr" target="#b23">[24]</ref> dataset, outperforming other previous methods significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Components exist in an image but are not mentioned in the description (c) Ambiguity in the textual description is walking and looking at her phone wearing a red t shirt and jean shorts with a yellow bag slung over her shoulder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Illustration of the fine-grained attribute-level regions in description-based person Re-id. (b) Illustration of the uninvolved components in imagesentence pair. The legs and shoes are not mentioned in the description and should not contribute to the visual representation. (c) Ambiguity due to the modality heterogeneity when using textual words to retrieve the matched image components. The 'white skirt' can refer to several visual components from different people which contain skirts in different styles and sizes, covering different regions and parts of human body. (Best viewed in colors.) well settled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b0">[1]</ref> propose a combined bottom-up and top-down attention mechanism for image captioning and visual question answering. The bottomup attention proposes features of salient image regions by Faster R-CNN<ref type="bibr" target="#b31">[32]</ref>, while the top-down attention determines feature weightings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b35">[36]</ref> propose a pose-driven deep convolutional model to learn improved features, which leverages the human parts to alleviate the pose variations and learn robust feature representations from both the global image and different local parts. Liu et al.[26] propose a posetransferrable person Re-id framework which utilizes pose transferred sample augmentations (with identity supervision) to enhance Re-id model training. Song et al. [35] introduce the binary segmentation masks and further design a mask-guided Global and local representations extraction (b) Multi-granularity image-text alignments model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>AggregationFig. 5 .</head><label>5</label><figDesc>He has dark slacks on and a dark jacket, and he is wearing glasses and also carrying a case. Illustration of obtaining the global-local similarity in the I → T direction in the RGA module. (Best viewed in colors.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of obtaining the local-local similarity in the P → N direction in the BFM module. (Best viewed in colors.) M LP T (·) means multi-layer perceptron for noun phrases, i.e., MLP-T-RGA in Figure 4. And the corresponding intermediate similarity after relation-guided global-local alignment is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>α − S(I, T ) + S(I, T )] + I max[0, α − S(T, I) + S(T, I)],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>L I−T M and L T −I M indicate the matching objective for the two opposite directions in RGA, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Analysis of GC + RGA (b) Analysis of GC + BFM He has dark slacks on and a dark jacket, and he is wearing glasses and also carrying a case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(b) The effectiveness of the fine-grained matching in the BFM module. The two examples are using noun phrases to attend to image parts (P → N direction). Red color means the part is the most similar one (with the maximal weight) to the phrase after the part-phrase attention, and yellow color means the second similar one. (Best viewed in colors.)The woman is reading a small pamphlet. The woman is wearing a yellow short sleeve shirtwears a black and white t-shirt and black shorts with blue sneakers he carries a yellow bag walking in front of the man</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>a blue shirt, a pair of blue jeans and a pair of white and balck shoes. Query</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETAILED</head><label>I</label><figDesc>CONFIGURATIONS IN EXPERIMENTS (S1, S2 AND S3 MEAN SEPARATE STEPS IN TRAINING).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY OF THE MIA MODEL.</figDesc><table><row><cell>Models</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Total</cell></row><row><cell>GC</cell><cell>43.4</cell><cell>67.4</cell><cell>76.7</cell><cell>167.2</cell></row><row><cell>GC + BFM</cell><cell>44.7</cell><cell>69.0</cell><cell>77.6</cell><cell>191.3</cell></row><row><cell>GC + RGA</cell><cell>47.2</cell><cell>70.5</cell><cell>79.1</cell><cell>196.8</cell></row><row><cell>GC + RGA + BFM</cell><cell>46.7</cell><cell>70.5</cell><cell>79.1</cell><cell>196.3</cell></row><row><cell>MIA (G)</cell><cell>45.9</cell><cell>69.1</cell><cell>78.1</cell><cell>193.1</cell></row><row><cell>MIA (R)</cell><cell>46.3</cell><cell>69.5</cell><cell>78.2</cell><cell>194.0</cell></row><row><cell>MIA (L)</cell><cell>36.1</cell><cell>59.8</cell><cell>69.5</cell><cell>165.4</cell></row><row><cell>MIA</cell><cell>48.0</cell><cell>70.7</cell><cell>79.3</cell><cell>198.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ANALYSIS</head><label>III</label><figDesc>OF THE PROPOSED STEP TRAINING STRATEGY.</figDesc><table><row><cell>Models</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Total</cell></row><row><cell></cell><cell>s G</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GC (G)</cell><cell>43.4</cell><cell>67.4</cell><cell>76.7</cell><cell>187.8</cell></row><row><cell>GC + BFM (G)</cell><cell>42.8</cell><cell>66.2</cell><cell>75.7</cell><cell>184.7</cell></row><row><cell>GC + RGA (G)</cell><cell>45.9</cell><cell>69.1</cell><cell>78.1</cell><cell>193.1</cell></row><row><cell>GC + RGA + BFM (G)</cell><cell>44.1</cell><cell>68.2</cell><cell>77.0</cell><cell>189.3</cell></row><row><cell>MIA (G)</cell><cell>45.9</cell><cell>69.1</cell><cell>78.1</cell><cell>193.1</cell></row><row><cell></cell><cell>s R</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GC + RGA (R)</cell><cell>46.3</cell><cell>69.5</cell><cell>78.2</cell><cell>194.0</cell></row><row><cell>GC + RGA + BFM (R)</cell><cell>44.2</cell><cell>68.9</cell><cell>78.2</cell><cell>191.3</cell></row><row><cell>MIA (R)</cell><cell>46.3</cell><cell>69.5</cell><cell>78.2</cell><cell>194.0</cell></row><row><cell></cell><cell>s L</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fine (L)</cell><cell>32.0</cell><cell>55.5</cell><cell>66.0</cell><cell>153.5</cell></row><row><cell>GC + BFM (L)</cell><cell>34.0</cell><cell>57.4</cell><cell>67.9</cell><cell>159.3</cell></row><row><cell>GC + RGA + BFM (L)</cell><cell>34.7</cell><cell>58.6</cell><cell>69.4</cell><cell>162.7</cell></row><row><cell>MIA (L)</cell><cell>36.1</cell><cell>59.8</cell><cell>69.5</cell><cell>165.4</cell></row><row><cell></cell><cell>s F</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GC + RGA + BFM</cell><cell>46.7</cell><cell>70.5</cell><cell>79.1</cell><cell>196.3</cell></row><row><cell>MIA</cell><cell>48.0</cell><cell>70.7</cell><cell>79.3</cell><cell>198.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ANALYSIS</head><label>IV</label><figDesc>OF EMPLOYING DIFFERENT OBJECTIVES IN THE TWO STEPS (GC + RGA RESULTS).</figDesc><table><row><cell cols="2">Methods Step-1 Step-2</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Total</cell></row><row><cell>Mat</cell><cell>Mat</cell><cell>40.6</cell><cell>64.9</cell><cell>74.3</cell><cell>179.8</cell></row><row><cell>Mat</cell><cell cols="2">Id + Mat 42.1</cell><cell>66.4</cell><cell>75.8</cell><cell>184.3</cell></row><row><cell>Id</cell><cell cols="2">Id + Mat 47.2</cell><cell>70.5</cell><cell>79.1</cell><cell>196.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISONS</head><label>V</label><figDesc>BETWEEN COMBINING STEP-1 AND STEP-2 AND OUR STEP TRAINING STRATEGY (GC + RGA RESULTS).</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="3">R@1 R@5 R@10</cell><cell>Total</cell></row><row><cell cols="2">Combining step-1 &amp; step-2</cell><cell>39.7</cell><cell>64.0</cell><cell>73.9</cell><cell>177.6</cell></row><row><cell>Step training strategy</cell><cell></cell><cell>47.2</cell><cell>70.5</cell><cell>79.1</cell><cell>196.8</cell></row><row><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="6">COMPARISONS BETWEEN USING ALL WORDS AND ONLY NOUN PHRASES</cell></row><row><cell></cell><cell cols="3">(MIA RESULTS).</cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Total</cell></row><row><cell>All words</cell><cell>44.2</cell><cell>66.3</cell><cell>75.9</cell><cell>186.4</cell></row><row><cell>Noun phrases</cell><cell>48.0</cell><cell>70.7</cell><cell>79.3</cell><cell>198.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII COMPARISONS</head><label>VII</label><figDesc>WITH THE STATE-OF-THE-ART METHODS.Using VGG-16, we achieve significant 15.85% improvements in terms of R@1 compared with the best Dual-Path model<ref type="bibr" target="#b49">[50]</ref>. PWM-ATH<ref type="bibr" target="#b5">[6]</ref> employs fine-grained patch-word matching, but does not have the global-local relations for supervising the learning of fine-grained component features or filtering the uninvolved components in image-description pair. And the multi-granularity similarity combination is not considered, either. Our MIA model achieves 20.86% increase compared with the PWM-ATH in terms of R@1, validating the great advantage of the global-local relations and multi-granularity alignments. As for using the stronger ResNet-50 as visual CNN, our MIA model beats the best Dual-Path [50] model by 8.70% in terms of R@1. The GLIA</figDesc><table><row><cell>Methods</cell><cell></cell><cell>R@1</cell><cell>R@5</cell><cell>R@10</cell><cell>Total</cell></row><row><cell></cell><cell cols="3">VGG-16 as visual CNN</cell><cell></cell></row><row><cell cols="2">CNN-RNN (2016) [31]</cell><cell>8.07</cell><cell>-</cell><cell>32.47</cell><cell>-</cell></row><row><cell cols="2">Neural Talk (2015) [39]</cell><cell>13.66</cell><cell>-</cell><cell>41.72</cell><cell>-</cell></row><row><cell cols="2">GNA-RNN (2017) [24]</cell><cell>19.05</cell><cell>-</cell><cell>53.64</cell><cell>-</cell></row><row><cell cols="2">IATVM (2017) [23]</cell><cell>25.94</cell><cell>-</cell><cell>60.48</cell><cell>-</cell></row><row><cell cols="2">PWM-ATH (2018) [6]</cell><cell cols="2">27.14 49.45</cell><cell>61.02</cell><cell>137.61</cell></row><row><cell cols="2">Dual Path (2017) [50]</cell><cell cols="2">32.15 54.42</cell><cell>64.30</cell><cell>150.87</cell></row><row><cell cols="4">MIA (λ 1 = 1, λ 2 = 0.5) 48.00 70.70</cell><cell>79.30</cell><cell>198.00</cell></row><row><cell></cell><cell cols="3">ResNet-50 as visual CNN</cell><cell></cell></row><row><cell cols="2">Dual Path (2017) [50]</cell><cell cols="2">44.40 66.26</cell><cell>75.07</cell><cell>185.73</cell></row><row><cell>GLIA (2018) [5]</cell><cell></cell><cell cols="2">43.58 66.93</cell><cell>76.26</cell><cell>186.77</cell></row><row><cell cols="4">MIA (λ 1 = 1, λ 2 = 0.3) 53.10 75.00</cell><cell>82.90</cell><cell>211.00</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell></row><row><cell cols="6">ANALYSIS OF HYPER-PARAMETER λ 1 (MIA RESULTS, λ 2 = 0).</cell></row><row><cell>λ 1</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Total</cell></row><row><cell>0.1</cell><cell>46.2</cell><cell>69.8</cell><cell>77.9</cell><cell>193.9</cell></row><row><cell>0.3</cell><cell>46.6</cell><cell>70.0</cell><cell>78.7</cell><cell>195.3</cell></row><row><cell>0.5</cell><cell>46.9</cell><cell>70.4</cell><cell>78.9</cell><cell>196.2</cell></row><row><cell>0.7</cell><cell>47.0</cell><cell>70.4</cell><cell>79.1</cell><cell>196.5</cell></row><row><cell>0.9</cell><cell>47.1</cell><cell>70.4</cell><cell>79.1</cell><cell>196.6</cell></row><row><cell>1.0</cell><cell>47.2</cell><cell>70.5</cell><cell>79.1</cell><cell>196.8</cell></row><row><cell>1.1</cell><cell>47.2</cell><cell>70.6</cell><cell>79.3</cell><cell>197.1</cell></row><row><cell>1.3</cell><cell>47.3</cell><cell>70.5</cell><cell>79.3</cell><cell>197.1</cell></row><row><cell>1.5</cell><cell>47.6</cell><cell>70.5</cell><cell>79.2</cell><cell>197.3</cell></row><row><cell>1.7</cell><cell>47.5</cell><cell>70.5</cell><cell>79.3</cell><cell>197.3</cell></row><row><cell>1.9</cell><cell>47.5</cell><cell>70.5</cell><cell>79.2</cell><cell>197.2</cell></row><row><cell>2.0</cell><cell>47.4</cell><cell>70.5</cell><cell>79.1</cell><cell>197.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IX ANALYSIS</head><label>IX</label><figDesc>OF HYPER-PARAMETER λ 2 (MIA RESULTS, λ 1 = 0).</figDesc><table><row><cell>λ 2</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Total</cell></row><row><cell>0.1</cell><cell>46.5</cell><cell>69.8</cell><cell>78.4</cell><cell>194.7</cell></row><row><cell>0.3</cell><cell>47.2</cell><cell>70.1</cell><cell>78.9</cell><cell>196.2</cell></row><row><cell>0.5</cell><cell>47.6</cell><cell>70.2</cell><cell>79.2</cell><cell>197.0</cell></row><row><cell>0.7</cell><cell>47.8</cell><cell>70.1</cell><cell>79.3</cell><cell>197.2</cell></row><row><cell>0.9</cell><cell>47.7</cell><cell>70.0</cell><cell>79.4</cell><cell>197.1</cell></row><row><cell>1.0</cell><cell>47.5</cell><cell>69.9</cell><cell>79.2</cell><cell>196.6</cell></row><row><cell>1.1</cell><cell>47.4</cell><cell>69.8</cell><cell>79.1</cell><cell>196.3</cell></row><row><cell>1.3</cell><cell>46.7</cell><cell>69.7</cell><cell>78.8</cell><cell>195.2</cell></row><row><cell>1.5</cell><cell>46.3</cell><cell>69.5</cell><cell>78.5</cell><cell>194.3</cell></row><row><cell>1.7</cell><cell>45.9</cell><cell>69.1</cell><cell>78.3</cell><cell>193.3</cell></row><row><cell>1.9</cell><cell>45.5</cell><cell>68.6</cell><cell>78.1</cell><cell>192.2</cell></row><row><cell>2.0</cell><cell>45.3</cell><cell>68.5</cell><cell>78.1</cell><cell>191.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person re-identification by global and local image-language association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving text-based person search by spatial matching and adaptive threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSST</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pamm: Pose-aware multi-shot matching for improving person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3739" to="3752" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video person reidentification by temporal residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1366" to="1377" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">W/sup 4: real-time surveillance of people and their activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Haritaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="809" to="830" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic-based surveillance video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1168" to="1181" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identity-aware textualvisual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Integrating graph partitioning and matching for trajectory analysis in video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4844" to="4857" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring questionguided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning discriminative binary codes for large-scale cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2494" to="2507" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unifying the video and question attentions for open-ended video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5656" to="5666" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2860" to="2871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Background-modeling-based adaptive prediction for surveillance video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="769" to="784" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
