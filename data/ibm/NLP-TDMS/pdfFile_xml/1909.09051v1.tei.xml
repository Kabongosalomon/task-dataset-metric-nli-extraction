<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Monocular Depth Hints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niantic</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Monocular Depth Hints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular depth estimators can be trained with various forms of self-supervision from binocular-stereo data to circumvent the need for high-quality laser scans or other ground-truth data. The disadvantage, however, is that the photometric reprojection losses used with selfsupervised learning typically have multiple local minima. These plausible-looking alternatives to ground truth can restrict what a regression network learns, causing it to predict depth maps of limited quality. As one prominent example, depth discontinuities around thin structures are often incorrectly estimated by current state-of-the-art methods.</p><p>Here, we study the problem of ambiguous reprojections in depth prediction from stereo-based self-supervision, and introduce Depth Hints to alleviate their effects. Depth Hints are complementary depth suggestions obtained from simple off-the-shelf stereo algorithms. These hints enhance an existing photometric loss function, and are used to guide a network to learn better weights. They require no additional data, and are assumed to be right only sometimes. We show that using our Depth Hints gives a substantial boost when training several leading self-supervised-from-stereo models, not just our own. Further, combined with other good practices, we produce state-of-the-art depth predictions on the KITTI benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">We demonstrate that our selective training using Depth</head><p>Hints is a general enhancement that can improve multiple leading self-supervised training algorithms, allowing our implementations to reach better minima. The Depth Hints can come from the same stereo image data, via, e.g. OpenCV's stereo estimates <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>3. We show that our selective training with Depth Hints, coupled with sensible network design choices, leads us to outperform most other algorithms. We achieve state-of-the-art results on the KITTI dataset [8], outperforming both our baseline model and previously published results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the accuracy of depth-from-color algorithms improves, new opportunities are unlocked in augmented reality, robotics, and autonomous driving. Per-pixel, ground truth depth supervision is difficult to acquire, requiring cumbersome and expensive depth-sensing devices <ref type="bibr" target="#b7">[8]</ref>. As an alternative, there is an active search for self-supervised depth-estimation models, where a training signal is derived from data captured using commodity color cameras. In such self-supervised settings, training involves adjusting a network's depth predictions to minimize a photometric loss. This loss is usually the distance between a reference image and the depth-guided reprojection of other views into that reference viewpoint. Depth regression is optimized and relative poses come from stereo camera calibration in a training-from-stereo setting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>, while depth values and camera poses can be optimized jointly when training on videos <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>The photometric distance between the reference and depth-reprojected images could be measured with L 1 or L 2 distance, more complicated structural dissimilarity distances (DSSIM <ref type="bibr" target="#b33">[34]</ref>), or a combination of DSSIM+L 1 distances <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref> used in state-of-the-art methods. A drawback of self-supervision is that finding the optimal depth value is normally difficult, especially where the photometric loss can be low for multiple depth values (e.g. due to repeating structures and uniformly textures areas). Consequently, training is harder, which leads to lower accuracy predictions.</p><p>When training depth-from-color models, our Depth Hints offer a specific alternative to the model's current depth predictions. Where the alternative's reprojection is better, the training proceeds in following the "hint." Surprisingly, simply using our Depth Hints as labels for direct supervision already gives a nearly state of the art baseline. Overall, our contributions are:</p><p>1. We show that existing self-supervised regression methods can struggle during training to find the global optimum when minimizing photometric reprojection loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A neural network that predicts depth from a single image could be trained with supervised depth data, or using self-supervision by exploiting photometric consistency. The many flavors of self-supervision differ by design, opting for pre-training, cropping vs. scaling, use of synthetic data, online vs. batch pose-estimation, etc. Here we discuss the current leading methods, and where we expect Depth Hints are and are not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-supervised depth prediction</head><p>Self-supervised approaches can exploit photometric consistency in binocular stereo pairs, in consecutive video frames, or in consecutive frames of a stereo video.</p><p>Stereo training: Garg et al. <ref type="bibr" target="#b6">[7]</ref> formulated the selfsupervised training of monocular depth estimation with photometric consistency loss between stereo pairs. They chose an L 2 loss, which tends to generate blurry results. Godard et al. <ref type="bibr" target="#b8">[9]</ref> (Monodepth) used a weighted sum of DSSIM <ref type="bibr" target="#b33">[34]</ref> and L 1 measures between correspondences. They regularized network predictions with left-right consistency between left and right disparity maps and introduced a post-processing technique that boosts depth quality, where the final depth map is a weighted average of network predictions generated from the original and horizontally flipped images. The left-right consistency was extended to a trinocular assumption by <ref type="bibr" target="#b26">[27]</ref> for improved results.</p><p>Computing reprojection loss at a higher resolutions has been shown to improve depth map quality <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20]</ref>. Pillai et al. <ref type="bibr" target="#b25">[26]</ref> also introduced differentiable flip augmentation and subpixel convolutions for increased fidelity of depth maps. Depth Hints are computed from binocular stereo data, so should be able to enhance training for any of these stereo-derived models that use the very effective DSSIM+L 1 photometric loss.</p><p>Monocular training: SfMLearner by Zhou et al. <ref type="bibr" target="#b41">[42]</ref> was the first method to train a depth prediction network from monocular video only. Their network jointly predicts depth and relative camera pose changes from a frame at time t to frame t − 1, and from frame t to t + 1. Using these predictions, both the future and past frames are reprojected into the current frame, and an L 1 loss is applied. Additionally, this per-pixel loss is multiplied by a predicted mask to enable occluded pixels to be ignored.</p><p>Godard et al. <ref type="bibr" target="#b9">[10]</ref> build upon this, proposing that instead of averaging the loss from the reprojected future and past frames, the minimum of reprojection losses should be minimized. They also propose during training to detect and ignore pixels that appear to be stationary with respect to ego-motion. Multiple works propose additional regularization of predicted depths, such as surface normal consistency <ref type="bibr" target="#b36">[37]</ref>, edge consistency <ref type="bibr" target="#b35">[36]</ref> and 3D pointcloud consistency <ref type="bibr" target="#b21">[22]</ref>. Recently, multiple works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref> have proposed to model the relationship of pixels in the consecutive frames of a video with joint estimation of optical flow, depth and camera poses with loss terms that supervise the different estimates to be consistent. Depth Hints are not naturally compatible with monocular-video only data; extensions are left as future work.</p><p>It is also possible to train from both monocular video (forward and backward in time) and stereo pairs for improved pose and depth estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Additional supervision</head><p>Following the work of Eigen et al. <ref type="bibr" target="#b4">[5]</ref>, many others have trained using forms of per-pixel ground-truth depth labels. Training with ground truth is almost always a good idea when it is available, and we strive to push self-supervised performance closer to this ceiling.</p><p>With LiDAR Depth: Kuznietsov et al. <ref type="bibr" target="#b17">[18]</ref> optimize a fused loss, which sums a supervised loss based on sparse Li-DAR pointclouds and a self-supervised loss from stereo images. They follow Godard et al. <ref type="bibr" target="#b8">[9]</ref> by using DSSIM+L 1 as the photometric reprojection loss, and they follow Laina et al. <ref type="bibr" target="#b18">[19]</ref> by using berHu loss (inverse Huber) <ref type="bibr" target="#b18">[19]</ref> on the Li-DAR pointcloud.</p><p>Fu et al. <ref type="bibr" target="#b5">[6]</ref> showed that framing the regression of depths as ordinal classification can bring significant improvements to supervised prediction, though this concept is difficult to adopt for self-supervised training.</p><p>With Synthetic Depth: Synthetic data is an interesting source of ground-truth depths and/or stereo pairs. Instead of the usual photometric loss, domain adaptation is possible using generative adversarial networks <ref type="bibr" target="#b24">[25]</ref>, or by leveraging the ability of stereo matching networks to better generalize to real world data <ref type="bibr" target="#b10">[11]</ref>. Luo et al. <ref type="bibr" target="#b20">[21]</ref> demonstrate how synthetic data can be incorporated into single-image depth estimation with a two stage process. First, a network synthesizes a right view from the left view. Then, a second network performs stereo matching to recover depth from the half-synthetic stereo pair. Both networks can be trained on stereo+synthetic data, and optionally fine-tuned with ground truth.</p><p>With SLAM Depth: Yang et al. <ref type="bibr" target="#b34">[35]</ref> train a monocular depth estimation network with both self-supervision from stereo pairs, and supervision from sparse depths estimated in batch by the Stereo DSO <ref type="bibr" target="#b32">[33]</ref> algorithm. They demonstrate that a depth estimator network can improve visual odometry for monocular videos, resolving some scale ambiguity.</p><p>Klodt and Vedaldi <ref type="bibr" target="#b16">[17]</ref> use sparse depths and poses from a traditional SLAM system as a supervisory signal to train depth and pose prediction networks. They train from monocular videos (in contrast to <ref type="bibr" target="#b34">[35]</ref>), which requires special consideration of scale, and modeling of uncertainty in the depth and poses.</p><p>With Semantic Labels: Ramirez et al. <ref type="bibr" target="#b38">[39]</ref> show that a depth estimation network can be improved by jointly predicting depth and semantic labels. They propose a novel cross domain discontinuity loss to help align depth discontinuities with semantic boundaries.</p><p>With Estimated Depth:</p><p>The concurrent work monoResMatch by Tosi et al. <ref type="bibr" target="#b29">[30]</ref> also exploits proxy ground truth labels generated with a traditional stereo matching method <ref type="bibr" target="#b12">[13]</ref>. The inclusion of the proxy supervision is shown to greatly improve accuracy over using a standard self-supervised loss. Our proposed loss is different from theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In monocular depth estimation, the task is to train a neural network to predict a depth map d from a single input image I. In the self-supervised setting, the training data consists of pairs of images I and I † with known camera intrinsics K and K † , and relative camera pose (R, t). The network is trained to reconstruct the reference image I by reprojecting the other image into the reference view, sõ</p><formula xml:id="formula_0">I = π(I † , K † , R, t, K, d).<label>(1)</label></formula><p>Hence, pixel i at predicted depth d i gets a color valueĨ i . Under idealized training conditions, the predicted colorĨ i would perfectly match I i for all i.</p><p>When training from stereo, the only unknown parameter in π() is the estimated depth d. For monocular or stereo video, in addition to d, the network also needs to predict the camera pose (R, t). Presently, we do not pursue hints for pose, though this is a natural extension of our method.</p><p>Many leading algorithms now use a differentiable photometric consistency loss to measure how well the warped image approximates the reference image. We focus on the DSSIM+L 1 loss, a photometric consistency loss used in many self-supervised monocular depth estimation methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20]</ref>. This loss is computed per pixel as</p><formula xml:id="formula_1">l r (d i ) = α 1 − SSIM(I i ,Ĩ i ) 2 + (1 − α)|I i −Ĩ i |,<label>(2)</label></formula><p>where SSIM() is computed over a 3x3 pixel window, with α set to 0.85. If we were training with supervision, we would minimize the distance between continuous depth d i predicted by the network at pixel i, and depth d i procured by a Li-DAR system, Kinect sensor, a stereo algorithm, or a SLAM system, depending on the training context. Note that the last two contexts could count as a form of self-supervision, in that the labels d i are inferred, and not ground-truth measurements. There are several supervised losses l s used and compared in the literature e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref>, such as L 1 , L 2 and (names in superscripts):</p><formula xml:id="formula_2">l log L1 s (d i , d i ) = log(1 + |d i − d i |); (3) l berHu s (d i , d i ) = |d i − d i |, if |d i − d i | ≤ δ, (di−d i ) 2 +δ 2 2δ , otherwise.<label>(4)</label></formula><p>Typically</p><formula xml:id="formula_3">δ = 0.2 max i=0..N (|d i −d i |)</formula><p>. Similarly, the same losses are often applied on inverse depth (i.e. disparity). We found that l log L1 s works well with estimated depths (and <ref type="bibr" target="#b14">[15]</ref> favors it for Kinect data), while l berHu s is an established choice for accurate LiDAR and SLAM depths <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and disparities <ref type="bibr" target="#b34">[35]</ref>. We can see that the network failed to converge to the correct solution, with many thin structures missing in the predicted depth map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Need for Depth Hints</head><p>How do these mistakes come about? It is not failure to generalize or the result of overfitting, as this is an image from the training set. Another explanation could be that the depth map's artifacts are due to a poor choice of photometric reprojection loss, where failures on thin structures are not penalized enough. However, <ref type="figure" target="#fig_0">Figure 1</ref> (bottom) shows DSSIM+L 1 loss for a pixel on a thin object, and we can see that the loss is lower still for more appropriate depth values.</p><p>We hypothesize that, in the absence of a ground-truth depth label, the network becomes stuck, learning to regress depth for a local minimum of the reprojection loss and failing to seek the global minimum. To escape such bad minima, we propose to consult an alternative depth value in case it can offer a more plausible reprojection, and if so, incorporate it into the objective function. We refer to these alternative depth values as Depth Hints. Depth Hints, born from noisy estimates, can be more or less accurate than our current network prediction, and therefore we expect the iterative training of a CNN to gradually change its uptake of these hints as it converges. In contrast to supervised depth prediction, though, our main focus is to converge to the best minimum using a standard self-supervised reprojection loss. Depth Hints are only used, when needed, to guide the network out of local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Method</head><p>We assume that stereo data is being used to train a CNN to regress a depth map from a color image. We start from an existing loss function, designed for self-supervised training from such stereo images, that uses a photometric reprojection measure like DSSIM+L 1 . We propose to adaptively modify the existing training process only where the currently estimated depth map is worse than the Depth Hint. A  Depth Hint is essentially a depth map estimated by a thirdparty binocular stereo algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training from stereo pairs</head><p>During training, we provide our network with a per-pixel Depth Hint, i.e. a potential alternative hypothesis to the network's own depth estimate. Our key idea is that we only want to provide a supervisory signal from the Depth Hints in places where they make for a superior reprojected imagẽ I, compared to using the network prediction. Else the hint is ignored. To be clear, the proposed objective is not learning to regress a map of hinted depth values. That would be a supervised loss, and is indeed one of our baselines. Interestingly, <ref type="bibr" target="#b6">[7]</ref> explored that baseline and found it disappointing, because L 2 was in favor at the time. Rather, our objective remains to optimize a given algorithm's existing loss, and to consult a pixel's Depth Hint only when the reprojection loss can be improved upon.</p><p>In light of this, we reformulate our loss for pixel i as:</p><formula xml:id="formula_4">l ours (d i ) = l r (d i ) + l log L1 s (d i , h i ) if l r (h i ) &lt; l r (d i ) l r (d i ) otherwise,<label>(5)</label></formula><p>for an inferred network depth d i and a depth hint h i , with an associated self-supervised loss function l r from (2) judging the photometric quality of the depth estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computing Depth Hints:</head><p>We propose to generate Depth Hints using stereo pairs. Depth Hints with perfect accuracy are unattainable, and it would be extremely expensive to sweep discrete per-pixel depth values to find those that generate the optimal DSSIM+L 1 reprojection. Instead, we use a standard heuristically-designed stereo method to compute depth. It is tempting to use a state-of-the-art stereo algorithm instead, e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29]</ref>, however most modern stereo algorithms are supervised using the LiDAR ground truth from the KITTI dataset. Using one of these would cause us to be implicitly learning from laser-scanned ground-truth data. Further, generating multiple depth maps is not trivial with most stereo methods.</p><p>Semi-Global Matching (SGM) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> is an off-theshelf stereo matching algorithm available in OpenCV. SGM allows generation of different depth maps, depending on the hyperparameters used. For example, one can specify the size of the block to match between images, and the number of discrete disparities to evaluate. Hence, at training time, we can randomly choose hyperparameters for SGM to gen-erate Depth Hints on the fly. We refer to such Depth Hints as "Random SGM." Alternatively, for each training image pair, we can generate a collection of depth maps by running SGM with every possible hyperparameter choice. We discretize this space into 12 parameter choices, formed of combinations of three block sizes with four resolutions of disparities. We call this version of Depth Hints "Fused SGM," because it checks that collection of depth maps and chooses the depth value at each pixel based on the DSSIM+L 1 score. Fused SGM Depth Hints are pre-computed just once for the training corpus. Unless specified, we use Fused SGM depths as hints in our models.</p><p>Finally, SGM's depth maps can contain holes where the matching cost is ambiguous. All losses associated with SGM's depth maps are set to infinity for such pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training from stereo video</head><p>We can also apply this same method in the stereo video self-supervised task, where training data is a video of binocular pairs. In addition to the depth prediction for the current frame at time t, the network also produces two camera poses for the forward t + 1 and backward t − 1 frames. The input to the depth prediction network is just the current frame t, while the pose prediction network is given 3 frames at times t, t − 1 and t + 1. Similarly to Godard et al. <ref type="bibr" target="#b9">[10]</ref>, we warp all three other views (other image of the stereo pair, forward frame and backward frame) into the reference viewpoint, and select the photometric reprojection loss as the minimum of the 3 associated losses at each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>Our network architecture and training regime closely follow Godard et al. <ref type="bibr" target="#b9">[10]</ref>, and can be viewed in the supplementary materials. Unless otherwise specified, we use Resnet-18 <ref type="bibr" target="#b11">[12]</ref> as the encoder, pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>, also following <ref type="bibr" target="#b9">[10]</ref>. We specify the resolution of the input images explicitly, as it was shown to impact accuracy <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Depth map post-processing <ref type="bibr" target="#b8">[9]</ref> improves the quality of the final depth maps, so, for the quantitative results in Tables 1, 2 and 3, we add a "PP" column to indicate if postprocessing was applied.</p><p>Due to GPU memory restrictions, some methods train the network with a random crop of the full resolution image e.g. as in DORN <ref type="bibr" target="#b5">[6]</ref>. At test time, the full resolution image is tiled into suitable crops, then each crop is processed by the network and the depth maps are averaged to produce the full resolution output. Training on crops has the potential to improve most models, because the network processes more data and is able to 'see' finer details. We specify if a network was trained with crops instead of downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Our validation consists of four sets of experiments, all exploring the task of training a CNN to predict depth from a single color image, using binocular stereo data instead of ground-truth labels. Depending on the experiment, we compare against known leading baselines that supplement, and pre-and post-process the input stereo pairs and output depths to various degrees. The four experiments are:</p><p>1. Section 6.1 illustrates that local minima exist when photometric reconstruction loss is used for selfsupervision, and that Depth Hints can help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Section 6.2 reports ablation-type experiments on</head><p>Depth Hints, showing the negative impact of using the same SGM-computed stereo depths in more traditional loss functions.</p><p>3. Section 6.3 shows how Depth Hints usually help other modern self-supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Section 7 pits Depth</head><p>Hints against other state of the art algorithms, grouped by preconditions.</p><p>We run experiments on the KITTI dataset <ref type="bibr" target="#b7">[8]</ref> which consists of calibrated stereo video registered to LiDAR measurements of a city, captured from a moving car. The depth evaluation is done on the LiDAR pointcloud, and we report all seven of the standard metrics. See <ref type="bibr" target="#b4">[5]</ref> for evaluation details, but broadly, lower numbers are better in red columns, while higher numbers are better in blue columns. To enable direct comparison with recent works, we use the Eigen split of KITTI <ref type="bibr" target="#b4">[5]</ref> and evaluate with Garg's crop <ref type="bibr" target="#b6">[7]</ref>, using the standard cap of 80m <ref type="bibr" target="#b8">[9]</ref>. We note that there are potential evaluation issues with the KITTI ground-truth data due to a translational offset between the color camera used to record images and the LiDAR scanner. In the supplementary material we also present some evaluations on the updated KITTI ground truth data provided by <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Solution with Depth Hints</head><p>The experiment described in <ref type="figure" target="#fig_0">Figure 1</ref> is typical, showing that recent self-supervision approaches can get by without ground truth depths for most pixels, because a DSSIM+L 1 loss trains the CNN to regress reasonable depths. However, even seemingly distinct structures like a tree induce local minima that are plausible, and hard for the training process to escape. Supervised training with LiDAR data would yield an excellent photometric match, but in its absence, a Depth Hint can provide an alternative that our loss function <ref type="bibr" target="#b4">(5)</ref> incorporates in a gradual way: the hint isn't trusted explicitly, and as training progresses, the hint may be ignored.</p><p>In experiments, the network initially makes use of Depth Hints for 85% of available pixels, dropping to 50% at the end of training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Baseline Loss Functions</head><p>Besides our proposed loss in <ref type="bibr" target="#b4">(5)</ref>, there are various alternative strategies for incorporating Depth Hints in the objective function. Here we discuss such alternatives and compare them experimentally in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>First, we start with a simple baseline, where a neural network is trained to predict depth labels produced by an offthe-shelf stereo algorithm. This baseline is trained with loss</p><formula xml:id="formula_5">l ps (d i ) = l log L1 s (d i , h i ),<label>(6)</label></formula><p>where "ps" indicates proxy-supervised losses. Here h i is estimated by the SGM algorithm. We train three baselines with this loss. The first uses depth maps generated on the fly with a random selection of hyperparameters (Random SGM) to avoid the influence of DSSIM+L 1 loss. The second baseline uses the same method, but with a left-right consistency check to reduce noise by invalidating pixels which have disagreeing depth values in the two views (Random SGM LR). The last baseline uses the single Fused SGM depth maps from Section 5.1 that give an indirect signal from the DSSIM+L 1 loss. Another approach is to optimize the sum of selfsupervised and supervised losses, so</p><formula xml:id="formula_6">l sum (d i ) = l r (d i ) + l log L1 s (d i , h i ).<label>(7)</label></formula><p>This baseline is similar to the additional supervision from SLAM found in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>. Similarly, Zhu et al. <ref type="bibr" target="#b42">[43]</ref> add a supervised loss <ref type="bibr" target="#b0">[1]</ref> to solve for optical flow and Kuznietsov et al. <ref type="bibr" target="#b17">[18]</ref> add a supervised loss for depth estimation from LiDAR. Concurrently proposed monoResMatch <ref type="bibr" target="#b29">[30]</ref> uses this method to incorporate a proxy-supervised signal, albeit using a reverse Huber loss <ref type="bibr" target="#b18">[19]</ref> as opposed to log L 1 . The addition of supervised losses change the objective function that is being minimized; one could view the additional term as a form of regularization, constraining the network prediction to adhere to the proposed depth values. However, this strategy can struggle to contend with noise in the depths estimated by stereo algorithms. A different way of incorporating Depth Hints is to pretrain a network using l ps on the fused Depth Hints and finetune using l r . In <ref type="table" target="#tab_1">Table 1</ref>, this method is denoted as "l ps Fused SGM → l r ". We train l ps for 10 epochs followed by l r for another 10 epochs with the original learning rate.</p><p>Since the fused SGM depths may be a noisy estimate of depth, we could enable our model to train from them more robustly by explicitly modeling uncertainty <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. In these prior works, imperfections in the supervisory signal are modelled as part of the training loss; in addition to disparity, the network predicts a per-pixel data-dependent estimate of the residual error of the supervised loss. For pixels where the network expects that it will not be able to accurately satisfy the main training loss, it can pay a 'penalty' by predicting a higher residual error. This method was exploited by Klodt and Vedaldi <ref type="bibr" target="#b16">[17]</ref> to make learning from potentially noisy SLAM depths and poses more robust.</p><p>Referring to <ref type="table" target="#tab_1">Table 1</ref>, we note the clear benefit of treating the Depth Hints as noisy and only incorporating their estimates when they are superior to the network prediction. Surprisingly, our various baselines are competitive when compared to state of the art methods in <ref type="table">Table 3</ref>. For example, even "l ps Fused SGM" scores better than 3Net <ref type="bibr" target="#b26">[27]</ref> and SuperDepth <ref type="bibr" target="#b25">[26]</ref>, and is highly competitive with Mon-odepth2 (S and MS) <ref type="bibr" target="#b9">[10]</ref> on all metrics, albeit with pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Depth Hints for Existing Methods</head><p>Here we demonstrate the benefits of using Depth Hints to improve existing methods. As most existing methods do not provide training code, we have implemented a selection of them that are trained with self-supervised loss. Hence, we modify our loss functions to closely match the selected methods, while keeping our network architecture, image resolution, optimization parameters, and number of epochs consistent across experiments. <ref type="table" target="#tab_3">Table 2</ref> shows quantitative results of existing methods that were augmented with Depth Hints. We see noticeable improvements in all methods which are trained using stereo (S) and stereo video (MS), demonstrating the effectiveness of incorporating Depth Hints. Additionally, we do not observe an improvement for the semi-supervised case <ref type="bibr" target="#b17">[18]</ref>, nor do the comparatively noisy Depth Hints hurt its results. Please see supplementary material for additional information regarding these implementations.</p><p>Finally, Depth Hints show substantial improvements when trained and evaluated on synthetic FlyingThings3D Sceneflow dataset <ref type="bibr" target="#b23">[24]</ref>. The improvements are significant due to many objects with thin structures present in the   <ref type="bibr" target="#b22">[23]</ref>. Highlighted methods are augmented with Depth Hints , and score better than their regular counterparts almost universally. <ref type="bibr" target="#b17">[18]</ref> is an exception, possibly because it already uses LiDAR data. We also show results for <ref type="bibr" target="#b9">[10]</ref> without ImageNet <ref type="bibr" target="#b3">[4]</ref> pretraining, denoted as 'Monodepth2 no pt'. Data column (data source used for training): D refers to methods that use depth supervision at training time, S is for self-supervised training on stereo images, and MS is for models trained with stereo video.</p><p>dataset. These results demonstrate that Depth Hints can improve monocular depth estimation in various domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Depth From Color Tournament</head><p>Although it only represents one application domain, the KITTI dataset has been established as the dominant benchmark for measuring the accuracy of depth inferred from color. Broadly, our Depth Hints approach produces better looking results (see <ref type="figure">Figure 2</ref>) and scores indicating that we are the new state of the art across three major competition "categories." Please see <ref type="table">Table 3</ref>. Of course there are more or less flattering ways to cluster the competition, so we present "Our" method in multiple forms, for better compatibility within each category. In doing so, we show that Depth Hints are useful across multiple settings (stereo vs. mono+stereo, low vs. high resolution, with/without pretraining), making the difference between first and second place.</p><p>Rows in <ref type="table">Table 3</ref> are color-coded by category, with the winning score for each of seven measures marked in bold.</p><p>Low-res Stereo is the classic category, with the longest history of competitors (we show the highest scorers). Our full method ("Ours Resnet50") wins decisively on every metric. One could argue about two "outside" advantages: we pre-train on Imagenet and our SGM step gets the benefit of a time-tested heuristic. Our ablation experiments in Sec 6.2 show the difference between using SGM naively and incorporating its output as a Depth Hint. For completeness, we present results for our method with no pretraining ("Ours Resnet50 w/o pretraining"). When we compare this to the highest scoring non pretrained network 3Net <ref type="bibr" target="#b26">[27]</ref>, we show better scores in all seven metrics.</p><p>High-res allows for processing of larger inputs. Again our method ("Ours HR Resnet50") shows a considerable improvement over existing methods in all metrics. Similar to before, we also show results for our method without pretraining ("Ours HR Resnet50 w/o pretraining"). Our non-pretrained model beats SuperDepth <ref type="bibr" target="#b25">[26]</ref> in six out of seven metrics (tied in one), and compares favourably to the concurrent work monoResMatch <ref type="bibr" target="#b29">[30]</ref>, which makes use of a significantly more complex network compared to our encoder-decoder architecture.</p><p>Stereo Video MS could theoretically be the category with the strongest scores, because each self-supervised algorithm has access to time series movies (M) in stereo (S), with the opportunity to match occluded regions by searching elsewhere in time. Interestingly, in this category we see smaller improvements by using our approach over <ref type="bibr" target="#b9">[10]</ref> for lower resolution ("Ours"), but observe a substantial boost in the high resolution case ("Ours HR").</p><p>Overall, we note that error metrics like SqRel and RMSE, which penalize large errors in a few pixels, benefit most from Depth Hints. Depth Hints help to recover thin structures and to more accurately delineate object boundaries ( <ref type="figure">Figure 2</ref>). The AbsRel metric has smaller gains, since only a minority of pixels in each image are improved. The Depth Supervised category is one we cannot compete in. The clear winner here is DORN <ref type="bibr" target="#b5">[6]</ref>, who avoid selfsupervision entirely, training directly from LiDAR data. SVSM <ref type="bibr" target="#b20">[21]</ref> uses outside synthetic data, and LiDAR data for finetuning. DVSO <ref type="bibr" target="#b34">[35]</ref> obtains depth supervision through an excellent SLAM system, yielding LiDAR-like pointclouds, and combines them with self-supervision to achieve scores similar to ours in their "SimpleNet" model. However, their paper introduces an important enhancement that we lack, namely a depth refinement network. Input 3Net <ref type="bibr" target="#b26">[27]</ref> Ours SuperDepth <ref type="bibr" target="#b25">[26]</ref> Monodepth2 HR <ref type="bibr" target="#b9">[10]</ref> Ours HR <ref type="figure">Figure 2</ref>. Qualitative comparison with existing methods.  <ref type="table">Table 3</ref>. Quantitative results. Adjusting our model slightly, we compare it to the top performers in three different categories on KITTI 2015 <ref type="bibr" target="#b7">[8]</ref>, using the Eigen split. Data column (data source used for training): D refers to methods that use KITTI depth supervision at training time, D* use auxiliary depth supervision from synthetic data, D † use auxiliary depth supervision from SLAM, S is for selfsupervised training on stereo images, MS is for models trained with both M (forward and backward frames) and S data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We investigated current issues with reprojection losses in the self-supervised monocular depth estimation setting. Based on these observations, we introduced Depth Hints as a practical approach to help escape from local minima, and to guide the network toward a better overall solution. The depth proposals make for a strong baseline themselves, but our training mechanism reverts to the default reprojection loss when the proposals are unhelpful. Qualitatively, Depth Hints seem to help most with thin structures and sharp boundaries. Extensive experimentation supports this. Further, Depth Hints provide a boost when applied to existing self-supervision schemes. Combined with a common network architecture, without but preferably with pre-training, our Depth Hints model achieves the top-scores on the selfsupervised KITTI Eigen benchmark by a significant margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 (</head><label>1</label><figDesc>top) shows an input image from the training set, and the corresponding depth map produced by Godard et al. [10]'s network, trained on stereo data with DSSIM+L 1 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Top row: Image from the training set and corresponding depth maps produced by neural networks trained without and with Depth Hints (Godard et al.<ref type="bibr" target="#b9">[10]</ref> (Monodepth2) architecture and loss). Middle row, left to right: Crop of the image centered around a thin structure with the center pixel circled, the scanline in the other image for the circled pixel, LiDAR pointcloud, fused depth map from SGM, crop of the depth map produced by a network trained without Depth Hints, our result, and the color coding illustrating pixel disparities. Bottom row: On the left is the plot of DSSIM+L1 cost of the pixel on the thin structure for every pixel disparity. Plots on the right show the predictions made by the network after q epochs when trained with and without Depth Hints. The network trained without Depth Hints gets stuck in a local minimum and does not escape even after 20 epochs. On the other hand, the network trained with Depth Hints is in the vicinity of the correct solution (disparity of 64.63 according to LiDAR) after the first epoch. We visualize depths as disparities in pixel space for clarity. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ours vs. Baselines. Comparison of baselines evaluated on KITTI 2015<ref type="bibr" target="#b7">[8]</ref> using the Eigen split. All methods here were trained on stereo pairs only.</figDesc><table><row><cell>Method</cell><cell>PP</cell><cell>H × W</cell><cell>Abs Rel</cell><cell>Sq Rel</cell><cell>RMSE</cell><cell>RMSE log</cell><cell>δ&lt;1.25</cell><cell>δ&lt;1.25 2</cell><cell>δ&lt;1.25 3</cell></row><row><cell>lps Random SGM</cell><cell></cell><cell>192 × 640</cell><cell>0.110</cell><cell>0.901</cell><cell>4.816</cell><cell>0.193</cell><cell>0.871</cell><cell>0.958</cell><cell>0.981</cell></row><row><cell>lps Random SGM LR</cell><cell></cell><cell>192 × 640</cell><cell>0.109</cell><cell>0.877</cell><cell>4.800</cell><cell>0.193</cell><cell>0.870</cell><cell>0.958</cell><cell>0.981</cell></row><row><cell>lps Fused SGM</cell><cell></cell><cell>192 × 640</cell><cell>0.109</cell><cell>0.850</cell><cell>4.741</cell><cell>0.193</cell><cell>0.873</cell><cell>0.956</cell><cell>0.980</cell></row><row><cell>lsum Fused SGM</cell><cell></cell><cell>192 × 640</cell><cell>0.108</cell><cell>0.841</cell><cell>4.754</cell><cell>0.194</cell><cell>0.871</cell><cell>0.957</cell><cell>0.980</cell></row><row><cell>lps Fused SGM → lr</cell><cell></cell><cell>192 × 640</cell><cell>0.109</cell><cell>0.916</cell><cell>4.910</cell><cell>0.203</cell><cell>0.866</cell><cell>0.952</cell><cell>0.977</cell></row><row><cell>Klodt [17] uncertainty</cell><cell></cell><cell>192 × 640</cell><cell>0.108</cell><cell>0.905</cell><cell>4.815</cell><cell>0.196</cell><cell>0.871</cell><cell>0.955</cell><cell>0.979</cell></row><row><cell>Ours</cell><cell></cell><cell>192 × 640</cell><cell>0.106</cell><cell>0.780</cell><cell>4.695</cell><cell>0.193</cell><cell>0.875</cell><cell>0.958</cell><cell>0.980</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Depth Hints with Existing Methods. Comparison of our implementations of existing methods with and without Depth Hints. The data used to train/test is defined in the Dataset column, whereby 'K' is for KITTI 2015<ref type="bibr" target="#b7">[8]</ref> using the Eigen split, and 'SF' is for the FlyingThings3D Sceneflow dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Top row: Four test set images. Each subsequent row: Depth maps generated by a stereo-only method. Notice how Ours and Ours HR capture thin structures such as traffic lights, traffic signs, lampposts, etc.</figDesc><table><row><cell>Cit.</cell><cell>Method</cell><cell>PP</cell><cell>Data</cell><cell>H × W</cell><cell>Abs Rel</cell><cell>Sq Rel</cell><cell>RMSE</cell><cell>RMSE log</cell><cell>δ&lt;1.25</cell><cell>δ&lt;1.25 2</cell><cell>δ&lt;1.25 3</cell></row><row><cell>[18]</cell><cell>Kuznietsov</cell><cell></cell><cell>DS</cell><cell>187 × 621</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell>[6]</cell><cell>DORN</cell><cell></cell><cell>D</cell><cell>385 × 513 crop</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell></row><row><cell>[35]</cell><cell>DVSO SimpleNet</cell><cell></cell><cell>D  † S</cell><cell>256 × 512</cell><cell>0.107</cell><cell>0.852</cell><cell>4.785</cell><cell>0.199</cell><cell>0.866</cell><cell>0.950</cell><cell>0.978</cell></row><row><cell>[35]</cell><cell>DVSO</cell><cell></cell><cell>D  † S</cell><cell>256 × 512</cell><cell>0.097</cell><cell>0.734</cell><cell>4.442</cell><cell>0.187</cell><cell>0.888</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>[11]</cell><cell>Guo StereoUnsupFT → Mono pt</cell><cell></cell><cell>D*S</cell><cell>256 × 512</cell><cell>0.099</cell><cell>0.745</cell><cell>4.424</cell><cell>0.182</cell><cell>0.884</cell><cell>0.963</cell><cell>0.983</cell></row><row><cell>[21]</cell><cell>SVSM w/o finetuning</cell><cell></cell><cell>D*S</cell><cell>192 × 640 crop</cell><cell>0.102</cell><cell>0.700</cell><cell>4.681</cell><cell>0.200</cell><cell>0.872</cell><cell>0.954</cell><cell>0.978</cell></row><row><cell>[11]</cell><cell>Guo StereoSupFTAll → Mono pt</cell><cell></cell><cell>D*DS</cell><cell>256 × 512</cell><cell>0.097</cell><cell>0.653</cell><cell>4.170</cell><cell>0.170</cell><cell>0.889</cell><cell>0.967</cell><cell>0.986</cell></row><row><cell>[21]</cell><cell>SVSM finetuned</cell><cell></cell><cell>D*DS</cell><cell>192 × 640 crop</cell><cell>0.094</cell><cell>0.626</cell><cell>4.252</cell><cell>0.177</cell><cell>0.891</cell><cell>0.965</cell><cell>0.984</cell></row><row><cell>[9]</cell><cell>Monodepth</cell><cell></cell><cell>S</cell><cell>256 × 512</cell><cell>0.138</cell><cell>1.186</cell><cell>5.650</cell><cell>0.234</cell><cell>0.813</cell><cell>0.930</cell><cell>0.969</cell></row><row><cell>[25]</cell><cell>StrAT</cell><cell></cell><cell>S</cell><cell>256 × 512</cell><cell>0.128</cell><cell>1.019</cell><cell>5.403</cell><cell>0.227</cell><cell>0.827</cell><cell>0.935</cell><cell>0.971</cell></row><row><cell>[10]</cell><cell>Monodepth2 (w/o pretraining)</cell><cell></cell><cell>S</cell><cell>192 × 640</cell><cell>0.128</cell><cell>1.089</cell><cell>5.385</cell><cell>0.229</cell><cell>0.832</cell><cell>0.934</cell><cell>0.969</cell></row><row><cell>[27]</cell><cell>3Net (Resnet50)</cell><cell></cell><cell>S</cell><cell>256 × 512</cell><cell>0.126</cell><cell>0.961</cell><cell>5.205</cell><cell>0.220</cell><cell>0.835</cell><cell>0.941</cell><cell>0.974</cell></row><row><cell></cell><cell>Ours Resnet50 w/o pretraining</cell><cell></cell><cell>S</cell><cell>192 × 640</cell><cell>0.118</cell><cell>0.941</cell><cell>5.055</cell><cell>0.210</cell><cell>0.850</cell><cell>0.948</cell><cell>0.976</cell></row><row><cell>[10]</cell><cell>Monodepth2</cell><cell></cell><cell>S</cell><cell>192 × 640</cell><cell>0.108</cell><cell>0.842</cell><cell>4.891</cell><cell>0.207</cell><cell>0.866</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>S</cell><cell>192 × 640</cell><cell>0.106</cell><cell>0.780</cell><cell>4.695</cell><cell>0.193</cell><cell>0.875</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell></cell><cell>Ours Resnet50</cell><cell></cell><cell>S</cell><cell>192 × 640</cell><cell>0.102</cell><cell>0.762</cell><cell>4.602</cell><cell>0.189</cell><cell>0.880</cell><cell>0.960</cell><cell>0.981</cell></row><row><cell>[26]</cell><cell>SuperDepth</cell><cell></cell><cell>S</cell><cell>384 × 1024</cell><cell>0.112</cell><cell>0.875</cell><cell>4.958</cell><cell>0.207</cell><cell>0.852</cell><cell>0.947</cell><cell>0.977</cell></row><row><cell></cell><cell>Ours HR Resnet50 w/o pretraining</cell><cell></cell><cell>S</cell><cell>320 × 1024</cell><cell>0.112</cell><cell>0.857</cell><cell>4.807</cell><cell>0.203</cell><cell>0.861</cell><cell>0.952</cell><cell>0.978</cell></row><row><cell>[30]</cell><cell>monoResMatch</cell><cell></cell><cell>S</cell><cell>256 × 512 crop</cell><cell>0.111</cell><cell>0.867</cell><cell>4.714</cell><cell>0.199</cell><cell>0.864</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>[10]</cell><cell>Monodepth2</cell><cell></cell><cell>S</cell><cell>320 × 1024</cell><cell>0.105</cell><cell>0.822</cell><cell>4.692</cell><cell>0.199</cell><cell>0.876</cell><cell>0.954</cell><cell>0.977</cell></row><row><cell></cell><cell>Ours HR</cell><cell></cell><cell>S</cell><cell>320 × 1024</cell><cell>0.099</cell><cell>0.723</cell><cell>4.445</cell><cell>0.187</cell><cell>0.886</cell><cell>0.962</cell><cell>0.981</cell></row><row><cell></cell><cell>Ours HR Resnet50</cell><cell></cell><cell>S</cell><cell>320 × 1024</cell><cell>0.096</cell><cell>0.710</cell><cell>4.393</cell><cell>0.185</cell><cell>0.890</cell><cell>0.962</cell><cell>0.981</cell></row><row><cell>[40]</cell><cell>Zhan</cell><cell></cell><cell>MS</cell><cell>160 × 608</cell><cell>0.135</cell><cell>1.132</cell><cell>5.585</cell><cell>0.229</cell><cell>0.820</cell><cell>0.933</cell><cell>0.971</cell></row><row><cell>[20]</cell><cell>EPC++</cell><cell></cell><cell>MS</cell><cell>256 × 832</cell><cell>0.128</cell><cell>0.935</cell><cell>5.011</cell><cell>0.209</cell><cell>0.831</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>[10]</cell><cell>Monodepth2</cell><cell></cell><cell>MS</cell><cell>192 × 640</cell><cell>0.104</cell><cell>0.786</cell><cell>4.687</cell><cell>0.194</cell><cell>0.876</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>MS</cell><cell>192 × 640</cell><cell>0.105</cell><cell>0.769</cell><cell>4.627</cell><cell>0.189</cell><cell>0.875</cell><cell>0.959</cell><cell>0.982</cell></row><row><cell>[10]</cell><cell>Monodepth2</cell><cell></cell><cell>MS</cell><cell>320 × 1024</cell><cell>0.104</cell><cell>0.775</cell><cell>4.562</cell><cell>0.191</cell><cell>0.878</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell></cell><cell>Ours HR</cell><cell></cell><cell>MS</cell><cell>320 × 1024</cell><cell>0.098</cell><cell>0.702</cell><cell>4.398</cell><cell>0.183</cell><cell>0.887</cell><cell>0.963</cell><cell>0.983</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Aron Monszpart and Galen Han for helping to run our experiments, and our anonymous reviewers for their positive comments and helpful suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02695</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li-Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning SFM from SFM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06125</idno>
		<title level="m">Every pixel counts++: Joint learning of geometry and motion with 3D holistic understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3D geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Structured adversarial training for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishit</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Sakurikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Edgestereo: An effective multi-task learning network for stereo matching and edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01700</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant CNNs. In 3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stereo dso: Large-scale direct sparse visual odometry with stereo cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schworer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Rahim</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LEGO: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometry meets semantic for semi-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computational Imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Guided optical flow learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Zhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><forename type="middle">D</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
