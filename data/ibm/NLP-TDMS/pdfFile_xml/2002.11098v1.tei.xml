<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward fast and accurate human pose estimation via soft-gated skip connections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian@adrianbulat.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
							<email>jean.kossaifi@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>georgios.t@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<email>maja.pantic@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward fast and accurate human pose estimation via soft-gated skip connections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is on highly accurate and highly efficient human pose estimation. Recent works based on Fully Convolutional Networks (FCNs) have demonstrated excellent results for this difficult problem. While residual connections within FCNs have proved to be quintessential for achieving high accuracy, we re-analyze this design choice in the context of improving both the accuracy and the efficiency over the state-ofthe-art. In particular, we make the following contributions: (a) We propose gated skip connections with per-channel learnable parameters to control the data flow for each channel within the module within the macro-module. (b) We introduce a hybrid network that combines the HourGlass and U-Net architectures which minimizes the number of identity connections within the network and increases the performance for the same parameter budget. Our model achieves state-of-the-art results on the MPII and LSP datasets. In addition, with a reduction of 3× in model size and complexity, we show no decrease in performance when compared to the original HourGlass network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Being one of the most challenging computer vision problems with a multitude of applications, human pose estimation has been one of the primary research areas that the computer vision community tried to solve with Deep Learning and Convolutional Neural Networks (CNNs). Given that the results produced by existing state-of-the-art methods look at least impressive both qualitatively and quantitatively, it is natural to question how much progress can be expected on this problem over the next years and whether there is room for further improvement.</p><p>Yet, from a practical perspective, many applications cannot fully enjoy the high accuracy demonstrated by recent advances. The reason is for this is twofold: (a) the bulk of current work assumes the abundance of computational resources (e.g. GPUs, memory, power) to run these models which for many applications are not available. (b) In many application domains (e.g. autonomous driving) accuracy is absolutely essential, and there is very little room for accuracy drop when, for example, more lightweight, compact, and memory efficient methods are used.</p><p>Hence, although there are more and more methods proposed recently that achieve top performance in difficult benchmarks like the MPII <ref type="bibr" target="#b0">[1]</ref> and LSP <ref type="bibr" target="#b16">[17]</ref> and COCO <ref type="bibr" target="#b21">[22]</ref>, we argue that there is a significant gap literature as there are no methods which can get any close to this accuracy when there are memory (in terms of # of parameters) and computational power (in terms of flops) constraints. The focus of this work is to offer an improvement over the stateof-the-art under this setting.  Besides being challenging, the problem of human pose estimation under low memory and computing capacity has received little attention from the research community so far. To our knowledge there are only two very recent papers that make an attempt towards this direction: the works of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b33">[34]</ref>. Both methods aim at improving human pose estimation accuracy by introducing novel architectural changes at block <ref type="bibr" target="#b3">[4]</ref> and network <ref type="bibr" target="#b33">[34]</ref> level. Our work improves upon these two works by looking into a component of the network architecture that is used "as is" in most of recent works: skip connections. While residual connections proved to be quintessential for achieving high accuracy within Fully Convolutional Networks, in this work we re-analyze this design choice in the context of human pose estimation and show that with simple improvements significant gains can be achieved both in terms of accuracy and efficiency for the whole complexity (memory, and flops) spectrum.</p><p>In particular, we make the following contributions:</p><p>• We propose gated skip connections with per-channel learnable parameters to control the data flow for each channel within the module. This has the simple effect to learn how much information from the previous stage is propagated into the next one per channel and encourages each module learn more complicated functions. • We introduce a hybrid network structure that combines the HourGlass <ref type="bibr" target="#b22">[23]</ref> and U-Net <ref type="bibr" target="#b27">[28]</ref> architectures. The newly proposed architecture minimizes the number of identity connections within the network, and is shown to increase the performance within the same number of parameters budget. <ref type="bibr">•</ref> We report state-of-the-art results across the whole spectrum of # parameters and FLOPS. Our method is capable of producing a reduction of 65% in model size and complexity (i.e. more twice as fast) with no decrease in performance when compared to the original HourGlass network. A larger version of our model achieves state-of-the-art results on the MPII and LSP datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Here we review the related work, first for efficient neural networks, where we define efficiency in terms of number of FLOPs II-A and then for human pose estimation II-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Efficient neural networks</head><p>It is now widely accepted that the advent of more powerful computational resources, and availability of large amount of annotated data are at the root of the recent successes of deep learning. However, aside from these, it is architectural improvements that have made it possible to reach the remarkable levels of performance attained on a variety of tasks, ranging from image classification <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> to fine-grained recognition <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p><p>In particular, the depth of deep networks is a crucial aspect of their performance. Training these has been possible since AlexNet <ref type="bibr" target="#b19">[20]</ref>, and then VGG <ref type="bibr" target="#b29">[30]</ref>, the success of which has led to deeper neural networks. However, this ever increasing depth also makes for harder to train networks. This can be explained by several factors: i) increase in the number of parameters, ii) gradient vanishing or exploding. However, recent architectural changes made it possible to train very deep networks. The most important of these change was the introduction of skip connections within neural networks, which allow information to flow more easily within the network, both at forward time (activation) or during the backward pass (during which gradient can flow more easily, thus alleviating vanishing or exploding gradients phenomena).</p><p>ResNet <ref type="bibr" target="#b9">[10]</ref> uses these within blocks of convolution -with or without bottleneck-. Finally, pushing this to the extreme, more recently, DenseNet <ref type="bibr" target="#b12">[13]</ref> proposes to introduce one to all connections between a convolutional block and its successors within the same block.</p><p>While the above methods focus on performance (typically classification accuracy), neural networks can also be made more efficient in terms of computation. For instance, in <ref type="bibr" target="#b13">[14]</ref>, the authors propose to leverage 1 × 1 convolutions and skip connections to achieve performance similar to AlexNet on ImageNet but with a fraction of the parameters. MobileNet <ref type="bibr" target="#b11">[12]</ref> builds on the same principle but explicitly parametrizes the convolutions as separable ones (e.g. their kernels can be expressed as the sum of rank one tensors).</p><p>The same concept has been instrumental in improving the state-of-the-art for human pose estimation, e.g. by introducing skip connections between the encoder and decoder parts of U-Nets. We detail these changes in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human pose estimation</head><p>Current state-of-the-art on single person pose estimation is held by variants of the so-called HourGlass <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b5">[6]</ref> and U-Net architectures <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Both HourGlass and U-Net architectures consist of a stack of encoder-decoder Fully Convolutional Networks (see <ref type="figure">Fig. 2</ref>) with skip connections between the encoder and the decoder part. On each skip connection between them a residual block is usually placed. The resolution is decreased, and respectively increased, 4 times (from 64 × 64px to 4 × 4px.</p><p>In <ref type="bibr" target="#b36">[37]</ref>, the authors propose a coarse-to-fine learning mechanism where an initial coarse prediction is refined at a later stage by zooming-in into a region of interest and predicting a correction (expressed as an offset from the coarse detection). The work of Lifshitz et al. <ref type="bibr" target="#b20">[21]</ref> proposes to localize the landmarks using a dense voting technique where joint probabilities are learned from relative keypoint locations. In <ref type="bibr" target="#b1">[2]</ref>, the authors introduce a hybrid architecture that combines a normal feed-forward model with a recurrent block. In a similar spirit with [23], Wei et al. <ref type="bibr" target="#b37">[38]</ref> propose a 6-stack neural network to detect and gradually refine the keypoint predictions. In <ref type="bibr" target="#b2">[3]</ref>, the authors attempt to improve the localization process by diving the keypoint detection task into two sup-problems: detection and regression. At the first stage they detect only the visible landmarks using a part detection network, while at the second one they regress jointly the position of all keypoints, both visible and occluded.</p><p>More recent methods attempt to combine the HG based architecture with attention mechanisms <ref type="bibr" target="#b7">[8]</ref>, feature pyramids <ref type="bibr" target="#b38">[39]</ref> and adversarial training <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The work of <ref type="bibr" target="#b7">[8]</ref> uses a Conditional Random Field in order to model the correlations among nearby regions combining a holistic attention model with a body part one. This way the network learns to focus on both global and local details. In order to enforce a stronger model, in <ref type="bibr" target="#b6">[7]</ref>, the authors propose an adversarial training approach where the pose estimator has the role of a generator. At training time a discriminator is used to assess the quality of the produced heatmaps. A similar approach is followed in <ref type="bibr" target="#b5">[6]</ref>, where a discriminator is used to discern between feasible and biologically unfeasible poses. Yang et al. <ref type="bibr" target="#b38">[39]</ref> proposes a Pyramid Residual Module to improve the scale invariance of the models. The Pyramid Residual Module learns a series of convolutional filters at various input scales, on features obtained using different sub-sampling ratios. In <ref type="bibr" target="#b18">[19]</ref>, the authors propose a series of architectural enhancements aimed at improving the overall network robustness such as the addition of a multi-scale supervision, a structure-aware loss and a keypoint masking technique aimed at increasing the accuracy of the occluded . The features coming from the encoder are merged in the decoder using element-wise summation, resulting in the same dimensionality N .</p><formula xml:id="formula_0">c N 2×N (b) Proposed.</formula><p>The features are first concatenated; a convolutional layer with a 3× 3 kernel then reduces their dimensionality back to N .</p><formula xml:id="formula_1">c N/2 N/2 G r o u p e d C o n - v o lu t io n 2×N (c) Proposed.</formula><p>The features are concatenated and then processed using a grouped convolutional layer with a kernel of size 3 × 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Overall network architecture. We depict the overall network architecture (top-row) and detail the various ways in which features coming from the skip connections are agregated i) in existing work (2a), (ii) our proposed concatenation approach (2b) and (iii) our proposed concatenation with grouped convolution (2c). Each yellow rectangular cuboid depicts the hierarchical residual module shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the red one a max-pooling layer and the blue one a nearest neighbour upsampling operation. For our method the number of parameters is varied by changing the width (i.e number of channels) and the number of stacks.</p><p>points. With the goal of strengthening the intrinsic human body model learned by the network, <ref type="bibr" target="#b32">[33]</ref> introduces a compositional model that learns the relation between various body parts.</p><p>While most of the prior works focus on increasing the network accuracy without enforcing any strict computational requirements, herein we attempt to obtain a neural network that performs well across different level of computational resources. To our knowledge, the only papers that have similar aims with our work are <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b34">[35]</ref>. <ref type="bibr" target="#b3">[4]</ref> proposes to improve the speed of human pose estimation by fully binarizing the features and the weights of a given network. The work of <ref type="bibr" target="#b34">[35]</ref> combines dense connections with the HG model improving the overall accuracy and speed.</p><p>Our work is different to both <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b34">[35]</ref> (as well to all aforementioned papers on human pose estimation) because it improves upon skip connections, one of the most fundamental component of deep architectures which is used "as is" by all methods for human pose estimation. We show that the proposed enhancements significantly improves the overall network performance within the same computational budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Here we introduce in detail our method (Section III-A and III-B), the network architecture used as well as the implementation and training details (Section III-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Soft-gated residual connections</head><p>As detailed in Section II-A, one of the key aspects that made training very deep neural networks possible was the introduction of residual networks <ref type="bibr" target="#b9">[10]</ref>. Residual connections have since become a ubiquitous part of current state-of-theart neural network architectures and are often considered a quintessential aspect that drives their accuracy. Despite this, we argue here that, at least for some cases, the presence of an identity connection may have undesirable effects and hinder the performance of the model.</p><p>In the quest for training very deep neural networks, the work of <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b30">[31]</ref> explores the effect of using a hard gating</p><formula xml:id="formula_2">function g(x) = σ(W g x + b g ), where σ(x) = 1</formula><p>1−e −x is the sigmoid function, W g and b g the weights and respectively the bias of a given hard gating transformations. Typically g(x) is implemented using a convolutional layer with a kernel size of 1 × 1. When tested on ResNet architectures, these changes lead to sub-par results or even fail to converge <ref type="bibr" target="#b10">[11]</ref>.  (b) Features distribution at the output of the residual block that uses the proposed soft gating function (see <ref type="figure" target="#fig_1">Fig. 1</ref>), immediately after summation. <ref type="figure">Fig. 4</ref>: Comparison of the distribution of the features at the output of the residual block for the baseline <ref type="figure">(Fig. 4a</ref>) and the proposed approach <ref type="figure">(Fig. 4b)</ref>. Notice that the proposed method, with the help of soft gating function, can preserve the function learned by the residual module l. In contrast, the baseline module is forced to incorporate all of the information coming from the previous module, limiting as a consequence its representational power.</p><p>Currently, despite their relatively shallow nature at a stack level, all HourGlass-like architectures take the benefit of using skip connections for granted. Herein, we argue that this is not universally true and explore their effect in the context of human pose estimation, showing that our method can outperform across various computational budgets, architectures that make extensive use of skip connections <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b34">[35]</ref> (see Section IV).</p><p>It is already widely accepted that ResNets learn an unrolled iterative estimation <ref type="bibr" target="#b8">[9]</ref> with each residual unit learning a small correction with respect to the previous unit. As such, removing an arbitrary residual unit inside a macro-module leads to an insignificant drop in performance <ref type="bibr" target="#b15">[16]</ref> as long as the first and the last units are kept. Since in an HourGlass stack, typically, at each resolution level inside the encoder and decoder a very small number of residual blocks are used (usually one), forcing the blocks to learn a correction with respect to the input hinders the learning process and goes against the finding from <ref type="bibr" target="#b15">[16]</ref> that suggest that at the transition level between resolutions novel functions need to be learned by the network. By addressing this, we will show bellow that a 4 stacks HG network matches and outperforms all   . Notice that after applying the scaling factors α to the features from the skip connection (second row) most of the values are scaled toward 0 (third row). This suggests that only a handful of information (i.e. "residual information") is actually useful at the next stage. Combining directly the features coming from the module and the skip connection may have undesirable effect and hinder the overall performance since the function will have to learn a more incremental step due to the avalanche of information from the step t − 1.</p><p>of the previous methods that normally use 8 or more stacks which suggests that our network can learn stronger and more diverse functions.</p><p>In this work, we propose a novel way of improving the residual module using a channel-wise soft gating mechanism defined as bellow:</p><formula xml:id="formula_3">x l+1 = αx l + F(x l , W l ),<label>(1)</label></formula><p>where x l ∈ R C×w×h are the input features from the previous layer, W l is a set of weights associated with the lth residual block and F a residual function implemented using a set of convolutional layers (in this work using the module depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>). α ∈ R C×1 is a channel-wise soft gate (scaling factor) that is learned via backpropagation.  Herein, we apply the newly proposed soft gating mechanism to the state-of-the-art module from <ref type="bibr" target="#b3">[4]</ref>, previously used for quantized neural networks. In the process we explore two different settings: (a) using a single soft gate for all channels and (b) learning a value for each input channel. As the results from <ref type="table" target="#tab_1">Table II</ref> show, since different channels encode different types of information, the best results can be obtained using the channel-wise version that improves the overall performance against the baseline consisting of simple using the identity transformation (i.e. α = 1) by up to 1%.</p><p>To visualize the effect of the scaling factor we added in the skip connection to allow for soft-gating, we plot in <ref type="figure" target="#fig_3">Figure 3</ref> the output distribution of the scaling factors. Interestingly, we notice that the majority of the values are clustered around 0, which means that most of the information is not needed, or potentially even harmful for training. This phenomenon is observed across all layers of the network, regardless of the depth. These observations confirm the importance of this soft-gating parameter and its ability to filter redundant information. This is further reinforced in <ref type="figure">Fig. 4</ref> where the features learned by the proposed residual module variation are preserved after summation, since the scaling factors allows the module to select only the useful information from the previous stage. We also visualize how this scaling factors affects the distribution of the weights in the supplementary material. Notice that most of the features coming from the previous block are filtered by the introduced channel-wise scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Improved network architecture</head><p>Here we introduce a new hybrid network structure that combines the HourGlass <ref type="bibr" target="#b22">[23]</ref> and U-Net <ref type="bibr" target="#b27">[28]</ref> architectures.  By minimizing the number of identity connections within the network, we are able to obtain superior performance with the same number of parameters as existing networks. The HourGlass architecture as introduced in <ref type="bibr" target="#b22">[23]</ref> and depicted in <ref type="figure">Fig. 2</ref>, consists of a series of encoder-decoder macro-modules, where the predictions are gradually refined at each stage. Each residual module from a particular resolution level in the encoder is connected with its counterpart in the decoder. The connection can be realized either using an Identity function (U-Net) or using another residual module (HG). Typically in HG this data is fused using an elementwise summation.</p><p>However, herein we argue that directly adding the features from two different distribution is suboptimal, as such we explore various way of aggregating the data coming from different sources (i.e. places in the network). As such we explore the following options: a) Concatenating the features and then processing them inside the residual module using a convolutional layer with 3 × 3 filters <ref type="figure">(Fig. 2b)</ref>, b) Concatenating the features and the combining them inside the residual module using a grouped convolutional layers, where the number of groups corresponds with the number of data sources, 2 in this case. Finally, we explore various block choices on the skip connection between the encoder and the decoder parts.</p><p>As the results from <ref type="table" target="#tab_1">Table IV</ref> show, for the same parameters budget (approx. 3.4M distributed across 2 stacks) concatenating the features and analyzing them jointly (groups=1) leads to the best results. improving on top of the baseline by 0.5% when evaluated on the MPII validation set.</p><p>While we explored a series of different choices for the transform layer placed on the skip connections between the encoder and the decoder such as:</p><formula xml:id="formula_4">[BatchNorm → ReLU → 1 × 1 Conv2D], [BatchNorm → ReLU → 3 × 3 Conv2D], [1 × 1</formula><p>Conv2D] etc we found no noticeable differences between them across multiple runs as long as the number of parameters across the entire network stayed roughly the same. This suggests that the layers found on the big-skip connections simple learn a feature projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>For training, all images were center cropped to 256 × 256px around the labeled torso point. For LSP, since such point is not provided, we simple used the center of the tight bounding box. During training we randomly augmented the data on-the-fly by applying random rotation (from −30 • to   <ref type="bibr" target="#b34">[35]</ref> across the high speed regime domain. Notice that our method consistently outperforms <ref type="bibr" target="#b34">[35]</ref> up to 0.5% on the MPII validation set, while been less computationally demanding. The number of FLOPs for both networks is estimated using an input image 256 × 256px.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection type PCKh</head><p>Baseline <ref type="figure">(Fig. 2a</ref>) 87.5% Proposed concat. <ref type="figure">(Fig. 2b)</ref> 88.0% Proposed concat. and grouped <ref type="figure">(Fig. 2c)</ref> 87.8% 30 • )), scaling (from 0.75× to 1.25×), flipping and color jittering. On MPII, we trained the models for 200 epochs using RMSprop <ref type="bibr" target="#b35">[36]</ref> and a batch size of 24. During this time, the learning rate was varied from 2.5e − 4 to 1e − 5, dropping it at epochs 75, 100 and 150, while the weight decay was set to 0. The weights were initialized from an uniform distribution while the scaling factor α with 0. At the end of every HG stack we apply a pixel-wise MSE loss defined as:</p><formula xml:id="formula_5">l = 1 N N n=1 ij Ŷ n (i, j) − Y n (i, j) 2 ,<label>(2)</label></formula><p>where Y n (i, j) andŶ n (i, j) represent the ground truth score map at location (i, j) for the nth keypoint. For LSP, we followed the best practices and finetuned the models pretrained on MPII for 100 epochs using the LSP training set + LSP-extended using the same learning rate as previously, except that on this occasion we dropped the learning rate every 25 epochs. The two missing points from LSP were obtained by simply interpolating between the annotated ones.</p><p>All of our models were implemented using pytorch <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>In this section, we thoroughly experiment on all the parameters of our model, validate our claims and compare to the state-of-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We perform experiments on the two most challenging datasets for single person human-pose estimation: MPII and LSP. a) MPII: <ref type="bibr" target="#b0">[1]</ref> is one the most challenging datasets available to-date for articulated human pose estimation. The dataset consists of 25,000 images containing more than 40,000 annotated persons with up to 16 keypoints and occlusion labels. The images portrait humans across a large set of activities and natural scenarios collected from youtube. Out of this, and following <ref type="bibr" target="#b36">[37]</ref>, 25,000 persons were used for training and 3,000 for validation. b) LSP: <ref type="bibr" target="#b16">[17]</ref> is a single person human pose estimation dataset consisting of 2000 images, equally split between training and validation that contains humans performing various sport activities. Each image is annotated with up to 14 keypoints. The dataset was later expanded in <ref type="bibr" target="#b17">[18]</ref> with 10,000 more images for training (LSP-Extended). However, many of these annotations were noisy and were re-annotated in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with state-of-the-art</head><p>Herein, we compare the performance of the proposed approach against that of other state-of-the-art methods on the MPII and LSP datasets. Despite being significantly shallower (4 stacks vs 8 <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b34">[35]</ref>) and computationally lighter (see <ref type="table" target="#tab_1">Table VII</ref>), our model achieves top performance, surpassing many larger and heavier models. Furthermore, a wider version of it, where we simple increase the number of channels from 144 to 256 while keeping the number of stacks equal to 4, reaches state-of-the-art results on both MPII and LSP datasets.</p><p>As the results from <ref type="table" target="#tab_8">Table V</ref> show, our smaller model (8.5M parameters, 9.9 × 10 9 FLOPs, 4 stacks) surpasses the HG baseline <ref type="bibr" target="#b22">[23]</ref>(25.5M parameters, 40 × 10 9 FLOPs, 8 stacks) and the recent efficient architecture proposed in <ref type="bibr" target="#b34">[35]</ref>(10.1M, 12.7 × 10 9 FLOPs, 8 stacks) while being less computationally demanding and having a smaller memory footprint. Furthermore, a wider version of our model(26M parameters, 33.5 × 10 9 FLOPs, 4 stacks) sets a new stateof-the art results, improving upon the previously best results by up to 0.5% on certain categories. It is important to note that even the larger version of our model is computationally comparable to the original HG network (33.5 × 10 9 vs 28.4 × 10 9 FLOPs) despite being 1.5% better. Similarly, on LSP, our method achieves state-of-the-art results (see supplimentary material for numerical results).    In addition to this, in order to asses the efficiency of our method across the whole spectrum of computational resources we compare our approach against that of CU-Net <ref type="bibr" target="#b34">[35]</ref>. As the results from <ref type="table" target="#tab_1">Table III</ref> show, our method matches or outperforms that of CU-Net in the high speed regime (0.7-4 GFlops). Since we didn't reduce the number of features in the base blocks that precedes the first HourGlass stack as in <ref type="bibr" target="#b34">[35]</ref>, our method performs similarly with CU-Net when only 0.5M parameters are used. As we move into the medium speed regime the improvements offered by the proposed method become significantly larger, our approach being able to match the performance offered by CU-Net using 50% less HG stacks, 20% less parameters and FLOPs (see <ref type="table" target="#tab_1">Table I</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we revisited residual units and introduced a new learnable soft gated skip connections. Specifically, our proposed block has gated per-channel skip connections, where each channel has a learnable parameter that controls the data flow between the current and previous residual module. In addition, we introduce a hybrid network that combines the HG and U-Net architectures which minimizes the number of identity connections within the network and increases the performance for the same parameter budget. We demonstrate superior performance and efficiency on the challenging task of human body-pose estimation. Specifically, our model obtains state-of-the-art results on the MPII and LSP datasets. In addition, with a reduction of 65% in model size and complexity, we show no decrease in performance when compared to the original HG network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The proposed gated skip connections with perchannel learnable parameters for controlling the data flow for each channel within the module (N is the number of input channels to each conv. layer).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Baseline<ref type="bibr" target="#b22">[23]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Output distribution of the scaling factor α ∈ R N for various blocks through the network from the bottom to top (left to right). Notice that most of the values are clustered around 0 and contained in the interval [−0.1, 0.1]. Features distribution at the output of the baseline residual block (i.e. the skip connection is implemented using the Identity function), immediately after summation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Features distribution after the concatenation stage inside the residual block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Features distribution on the skip connection before scaling them using α. Features distribution on the skip connection after scaling them using α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of the distribution of the features in various scenarios for two layers from the bottom(left column) and top of the network(right column)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>PCKh-based on the MPII validation set for the medium speed regime domain.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>PCKh-based comparison with state-of-the-art on the MPII validation set for different values and methods of computing the scaling factor α.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison against the state-of-the-art method of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>: PCKh-based comparison on the MPII validation</cell></row><row><cell>set for different methods of combining the features between</cell></row><row><cell>the decoder and the encoder parts of the network. The</cell></row><row><cell>results are reported on a model that contains approx. 3.4M</cell></row><row><cell>parameters (2 stacks with 128 features per block).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>PCKh-based comparison with state-of-the-art on the MPII test set. Notice that our method matches and surpasses the performance of the next top-performing method. Ours* was pre-trained on the HSSK dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">Head Shoulder Elbow Wrist</cell><cell>Hip</cell><cell cols="3">Knee Ankle Total</cell></row><row><cell>Yang et al., CVPR'16 [40]</cell><cell>90.6</cell><cell>78.1</cell><cell>73.8</cell><cell>68.8</cell><cell>74.8</cell><cell>69.9</cell><cell>58.9</cell><cell>73.6</cell></row><row><cell>Rafi et al., BMVC'16 [27]</cell><cell>95.8</cell><cell>86.2</cell><cell>79.3</cell><cell>75.0</cell><cell>86.6</cell><cell>83.8</cell><cell>79.8</cell><cell>83.8</cell></row><row><cell>Yu et al., ECCV'16 [41]</cell><cell>87.2</cell><cell>88.2</cell><cell>82.4</cell><cell>76.3</cell><cell>91.4</cell><cell>85.8</cell><cell>78.7</cell><cell>84.3</cell></row><row><cell>Belagiannis&amp;Zisserman, FG'17 [2]</cell><cell>95.2</cell><cell>89.0</cell><cell>81.5</cell><cell>77.0</cell><cell>83.7</cell><cell>87.0</cell><cell>82.8</cell><cell>85.2</cell></row><row><cell>Lifshitz et al., ECCV'16 [21]</cell><cell>96.8</cell><cell>89.0</cell><cell>82.7</cell><cell>79.1</cell><cell>90.9</cell><cell>86.0</cell><cell>82.5</cell><cell>86.7</cell></row><row><cell>Pishchulin et al., CVPR'16 [26]</cell><cell>97.0</cell><cell>91.0</cell><cell>83.8</cell><cell>78.1</cell><cell>91.0</cell><cell>86.7</cell><cell>82.0</cell><cell>87.1</cell></row><row><cell>Insafutdinov et al., ECCV'16 [15]</cell><cell>97.4</cell><cell>92.7</cell><cell>87.5</cell><cell>84.4</cell><cell>91.5</cell><cell>89.9</cell><cell>87.2</cell><cell>90.1</cell></row><row><cell>Wei et al., CVPR'16 [38]</cell><cell>97.8</cell><cell>92.5</cell><cell>87.0</cell><cell>83.9</cell><cell>91.5</cell><cell>90.8</cell><cell>89.9</cell><cell>90.5</cell></row><row><cell>Bulat&amp;Tzimiropoulos, ECCV'16 [3]</cell><cell>97.2</cell><cell>92.1</cell><cell>88.1</cell><cell>85.2</cell><cell>92.2</cell><cell>91.4</cell><cell>88.7</cell><cell>90.7</cell></row><row><cell>Chu et al., CVPR'17 [8]</cell><cell>98.1</cell><cell>93.7</cell><cell>89.3</cell><cell>86.9</cell><cell>93.4</cell><cell>94.0</cell><cell>92.5</cell><cell>92.6</cell></row><row><cell>Yang et al., ICCV'17 [39]</cell><cell>98.3</cell><cell>94.5</cell><cell>92.2</cell><cell>88.9</cell><cell>94.7</cell><cell>95.0</cell><cell>93.7</cell><cell>93.9</cell></row><row><cell>Ning et al., TMM'18 [24]</cell><cell>98.2</cell><cell>94.4</cell><cell>91.8</cell><cell>89.3</cell><cell>94.7</cell><cell>95.0</cell><cell>93.5</cell><cell>93.9</cell></row><row><cell>Chou et al., CVPR-W'17 [7]</cell><cell>98.2</cell><cell>94.9</cell><cell>92.2</cell><cell>89.5</cell><cell>94.2</cell><cell>95.0</cell><cell>94.1</cell><cell>94.0</cell></row><row><cell>Ours</cell><cell>98.7</cell><cell>95.7</cell><cell>93.1</cell><cell>90.3</cell><cell>95.8</cell><cell>95.6</cell><cell>94.8</cell><cell>94.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>PCK-based comparison with state-of-the-art on the LSP test set.</figDesc><table><row><cell>Method</cell><cell>Yang et al. [39]</cell><cell>Wei et al. [38]</cell><cell>Bulat&amp;Tzimiropoulos [3]</cell><cell>Chu et al. [8]</cell><cell>Newell et al. [23]</cell><cell>Tang et al [35]</cell><cell>Ours</cell></row><row><cell># parameters</cell><cell>28M</cell><cell>29.7M</cell><cell>58.1M</cell><cell>58.1M</cell><cell>25.5M</cell><cell>10.1M</cell><cell>8.5M</cell></row><row><cell>PCKh</cell><cell>92.0%</cell><cell>88.5%</cell><cell>89.7%</cell><cell>91.5%</cell><cell>90.9%</cell><cell>90.8%</cell><cell>91.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison in terms of number of parameters and PCKh accuracy with state-of-the-art on MPII testing set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial learning of structure-aware fully convolutional networks for landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1705.00389</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02439</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Highway and residual networks learn unrolled iterative estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07771</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Densely connected convolutional networks. arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzkbski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04773</idno>
		<title level="m">Residual connections encourage iterative inference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge-guided deep fractal neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1246" to="1259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Anchor loss: Modulating loss scale based on prediction difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09212</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cu-net: Coupled u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep deformation network for object landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
