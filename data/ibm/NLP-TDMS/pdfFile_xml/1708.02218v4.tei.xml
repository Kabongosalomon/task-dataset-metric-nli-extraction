<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Classification with 2D Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><forename type="middle">J</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-P</forename><surname>Tixier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
							<email>nikolentzos@lix.polytechnique.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
							<email>pmeladianos@aueb.gr</email>
							<affiliation key="aff1">
								<orgName type="department">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>1É cole Polytechnique</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Classification with 2D Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph learning is currently dominated by graph kernels, which, while powerful, suffer some significant limitations. Convolutional Neural Networks (CNNs) offer a very appealing alternative, but processing graphs with CNNs is not trivial. To address this challenge, many sophisticated extensions of CNNs have recently been introduced. In this paper, we reverse the problem: rather than proposing yet another graph CNN model, we introduce a novel way to represent graphs as multichannel image-like structures that allows them to be handled by vanilla 2D CNNs. Experiments reveal that our method is more accurate than state-of-the-art graph kernels and graph CNNs on 4 out of 6 real-world datasets (with and without continuous node attributes), and close elsewhere. Our approach is also preferable to graph kernels in terms of time complexity. Code and data are publicly available 3 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph classification. Graphs, or networks, are rich, flexible, and universal structures that can accurately represent the interaction among the components of many natural and human-made complex systems. A central graph mining task is that of graph classification (not to be mistaken with node classification). The instances are full graphs and the goal is to predict the category they belong to. The applications of graph classification are numerous and range from determining whether a protein is an enzyme or not in bioinformatics, to categorizing documents in NLP, and social network analysis. Graph classification is the task of interest in this study. Limitations of graph kernels. The state-of-the-art in graph classification has traditionally been dominated by graph kernels. Graph kernels compute the similarity between two graphs as the sum of the pairwise similarities between some of their substructures, and then pass the similarity matrix computed on the entire dataset to a kernel-based supervised algorithm such as the Support Vector Machine <ref type="bibr" target="#b6">[7]</ref> to learn soft classification rules. Graph kernels mainly vary based on the substructures they use, which include random walks <ref type="bibr" target="#b10">[11]</ref>, shortest paths <ref type="bibr" target="#b1">[2]</ref>, and subgraphs <ref type="bibr" target="#b29">[30]</ref>, to cite only a few. While graph kernels have been very successful, they suffer significant limitations: L1: High time complexity. This problem is threefold: first, populating the kernel matrix requires computing the similarity between every two graphs in the training set (say of size N ), which amounts to N (N −1) /2 operations. The cost of training therefore increases much more rapidly than the size of the dataset. Second, computing the similarity between a pair of graphs (i.e., performing a single operation) is itself polynomial in the number of nodes. For instance, the time complexity of the shortest path graph kernel is O(|V 1 | 2 |V 2 | 2 ) for two graphs (V 1 , V 2 ), where |V i | is the number of nodes in graph V i . Processing large graphs can thus become prohibitive, which is a serious limitation as big networks abound in practice. Finally, finding the support vectors is O(N 2 ) when the C parameter of the SVM is small and O(N 3 ) when it gets large <ref type="bibr" target="#b3">[4]</ref>, which can again pose a problem on big datasets.</p><p>L2: Disjoint feature and rule learning. With graph kernels, the computation of the similarity matrix and the learning of the classification rules are two independent steps. In other words, the features are fixed and not optimized for the task.</p><p>L3: Graph comparison is based on small independent substructures. As a result, graph kernels focus on local properties of graphs, ignoring their global structure <ref type="bibr" target="#b23">[24]</ref>. They also underestimate the similarity between graphs and suffer unnecessarily high complexity (due to the explosion of the feature space), as substructures are considered to be orthogonal dimensions <ref type="bibr" target="#b37">[38]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed method</head><p>Overview. We propose a simple approach to turn a graph into a multi-channel image-like structure suitable to be processed by a traditional 2D CNN. The process (summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>) can be broken down into 3 steps: (1) graph node embedding, (2) embedding space compression, (3) repeated extraction of 2D slices from the compressed space and computation of a 2D histogram for each slice.</p><p>The "image" representation of the graph is finally given by the stack of its 2D histograms (each histogram making for a channel). Note that the dimensionality of the final representation of a graph does not depend on its number of nodes or edges. Big and small graphs are represented by images of the same size.</p><p>Our method addresses the limitations of graph kernels in the following ways: L1. By converting all graphs in a given dataset to representations of the same dimensionality, and by using a classical 2D CNN architecture for processing those graph representations, our method offers constant time complexity at the instance level, and linear time complexity at the dataset level. Moreover, stateof-the-art node embeddings can be obtained for a given graph in linear time w.r.t. the number of nodes in the graph, for instance with node2vec <ref type="bibr" target="#b12">[13]</ref>.</p><p>L2. Thanks to the 2D CNN classifier, features are learned directly from the raw data during training such that classification accuracy is maximized.</p><p>L3. Our approach capitalizes on state-of-the-art graph node embedding techniques that capture both local and global properties of graphs. In addition, we remove the need for handcrafted features. How to represent graphs as structures that verify the spatial dependence property? Convolutional Neural Networks (CNNs) are feedforward neural networks specifically designed to work on regular grids <ref type="bibr" target="#b18">[19]</ref>. A regular grid is the d-dimensional Euclidean space discretized by parallelotopes (rectangles for d = 2, cuboids for d = 3, etc.). Regular grids satisfy the spatial dependence <ref type="bibr" target="#b3">4</ref> property, which is the fundamental premise on which local receptive fields and hierarchical composition of features in CNNs hold. Traditionally, a graph G(V, E) is encoded as its adjacency matrix A or Laplacian matrix L. A is a square matrix of dimensionality |V | × |V |, symmetric in the case of undirected graphs, whose (i, j) th entry A i,j is equal to the weight of the edge e i,j between nodes v i and v j , if such an edge exists, or to 0 otherwise.</p><p>On the other hand, the Laplacian matrix L is equal to D − A, where D is the diagonal degree matrix. One could initially consider passing one of those structures as input to a 2D CNN. However, unlike in images, where close pixels are more strongly correlated than distant pixels, adjacency and Laplacian matrices are not associated with spatial dimensions and the notion of Euclidean distance, and thus do not satisfy the spatial dependence property. As will be detailed next, we capitalize on graph node embeddings to address this issue.</p><p>Step 1: Graph node embeddings. There is local correlation in the node embedding space. In that space, the Euclidean distance between two points is meaningful: it is inversely proportional to the similarity of the two nodes they represent. For instance, two neighboring points in the embedding space might be associated with two nodes playing the same structural role (e.g., of flow control), belonging to the same community, or sharing some other common property.</p><p>Step 2: Alignment and compression with PCA. As state-of-the-art node embedding techniques (such as node2vec) are neural, they are stochastic. Dimensions are recycled from run to run, which means that a given dimension will not be associated with the same latent concepts across graphs, or across several runs on the same graph. Therefore, to ensure that the embeddings of all the graphs in the collection are comparable, we apply PCA and retain the first d D principal components (where D is the dimensionality of the original node embedding space). PCA also serves an information maximization (compression) purpose. Compression is desirable in terms of complexity, as it greatly reduces the shape of the tensors fed to the CNN (and thus the number of channels, for reasons that will become clear in what follows), at the expense of a negligible information loss.</p><p>Step 3: Computing and stacking 2D histograms. We finally repeatedly extract 2D slices from the d-dimensional PCA node embedding space, and turn those planes into regular grids by discretizing them into a finite, fixed number of equally-sized bins, where the value associated with each bin is the count of the number of nodes falling into that bin. In other words, we represent a graph as a stack of d/2 2D histograms of its (compressed) node embeddings 5 . As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the first histogram is computed from the coordinates of the nodes in the plane made of the first two principal directions, the second histogram from directions 3 and 4, and so forth. Note that using adjacent and following PCA dimensions is an arbitrary choice. It ensures at least that channels are sorted by decreasing order of informativeness.</p><p>Using computer vision vocabulary, bins can be viewed as pixels, and the 2D slices of the embedding space as channels. However, in our case, instead of having 3 channels (R,G,B) like with color images, we have d/2 of them. That is, each pixel (each bin) is associated with a vector of size d/2, whose entries are the counts of the nodes falling into that bin in the corresponding 2D slice of the embedding space. Finally, the resolution of the image is determined by the number of bins of the histograms, which is constant for a given dataset across all channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>2D CNN Architecture. We implemented a variant of LeNet-5 <ref type="bibr" target="#b18">[19]</ref> with which we reached 99.45% accuracy on the MNIST handwritten digit classification dataset. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref> for an input of shape <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b27">28)</ref>, this simple architecture deploys four convolutional-pooling layers (each repeated twice) in parallel, with respective region sizes of 3, 4, 5 and 6, followed by two fullyconnected layers. Dropout <ref type="bibr" target="#b32">[33]</ref> is employed for regularization at every hidden layer. The activations are ReLU functions (in that, our model differs from LeNet-5), except for the ultimate layer, which uses a softmax to output a probability distribution over classes. For the convolution-pooling block, we employ 64 filters at the first level, and as the signal is halved through the (2,2) max pooling layer, the number of filters in the subsequent convolutional layer is increased to 96 to compensate for the loss in resolution.</p><p>Input <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b27">28)</ref> ConvPoolDropx2 filter shape: <ref type="bibr">(</ref>  Random graphs. To quickly test the viability of our pipeline, we created a synthetic 5-class dataset containing 2000 undirected and unweighted networks in each category. For the first and second classes, we generated graphs with the Stochastic Block Model, featuring respectively 2 and 3 communities of equal sizes. The in-block and cross-block probabilities were respectively set to 0.1 and 0.7, and the size |V | of each graph was randomly sampled from the Normal distribution with mean 150 and standard deviation 30, i.e., N (150, 30). The third category was populated with scale-free networks, that is, graphs whose degree distributions follow a power law, using the Barabási-Albert model. The number of incident edges per node was sampled for each graph from N (5, 2), and the size of each graph was drawn from N (150, 30) like for the first two classes. Finally, the fourth and fifth classes were filled with Erdös-Rényi graphs whose sizes were respectively sampled from N (300, 30) and N (150, 30), and whose edge probabilities were drawn from N (0.3, 0.15). This overall, gave us a large variety of graphs. Spectral embeddings. We started with the most naive way to embed the nodes of a graph, that is, using the eigenvectors of its adjacency matrix (we also experimented with the Laplacian but observed no difference). Here, no PCAbased compression was necessary. We retained the eigenvectors associated with the 10 largest eigenvalues in magnitude, thus embedding the nodes into a 10dimensional space.</p><p>Image resolution and channels. As we computed the unit-normed eigenvectors, the coordinates of any node in any dimension belonged to the [−1, 1] range. Furthermore, inspired by the MNIST images which are 28 × 28 in size, and on which we initially tested our 2D CNN architecture, we decided to learn 2D histograms featuring 28 bins in each direction. This gave us a resolution of 28 /(1−(−1)), that is, 14 pixels per unit, which we write 14:1 for brevity in the remainder of this paper. Finally, we decided to make use of the information carried out by all 10 eigenvectors, and thus kept 5 channels. Any graph in the dataset was thus represented as a tensor of shape <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b27">28)</ref>.</p><p>Results. In a 10-fold cross validation setting where each fold was repeated 3 times, we reached a mean classification accuracy of 99.08%(±3.21), usually within 3 epochs, which is a very good performance. Even though we injected some variance, categories are associated with specific patterns (see <ref type="figure" target="#fig_3">Fig. 4</ref>) which are easily captured by the CNN. Real-world graphs. We conducted experiments on 6 real-world datasets whose statistics are summarized in <ref type="table" target="#tab_2">Table 1</ref>. In all datasets, graphs are unweighted, undirected, with unlabeled nodes, and the task is to predict the class they belong to. Classes are mutually exclusive. The first five datasets, REDDIT-B, REDDIT-5K, REDDIT-12K, COLLAB, and IMDB-B, come from <ref type="bibr" target="#b37">[38]</ref> and contain social networks. The sixth dataset, PROTEINS full, is a bioinformatics one and comes from <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>. In this dataset, each node is associated with a 29-dimensional continuous vector. To incorporate this extra information into our images, we compressed the attribute vectors with PCA, retaining the same number of di-mensions as for the node embeddings, and normalized them to have same range as the node embeddings. Finally, each node was represented by a single vector made of its compressed node embedding concatenated with its compressed and normalized continuous attribute vector. That is, we had d/2 channels for node embeddings and d/2 channels for node attributes, where d is the number of principal components retained in each case.  Neural embeddings. We used the node2vec algorithm <ref type="bibr" target="#b12">[13]</ref> as our node embedding method for the real-world networks, as it was giving much better results than the basic spectral method. Node2vec applies the very fast Skip-Gram language model <ref type="bibr" target="#b19">[20]</ref> to truncated biased random walks performed on the graph. The algorithm scales linearly with the number of nodes in the network. We used the high performance, publicly available C++ implementation of the authors 6 .</p><p>Image resolution. We stuck to similar values as in our initial experiments involving spectral embeddings, i.e., 14 pixels per unit (14:1). We also tried 9 pixels per unit (9:1).</p><p>Image size. On a given dataset, image size is calculated as the range |max(coordinates) − min(coordinates)| × resolution, where coordinates are the flattened node embeddings. For instance, on COLLAB with a resolution of 9:1, image size is equal to 37 × 37, since |2.78 − (−1.33)| × 9 ≈ 37.</p><p>Number of channels. With the p and q parameters of node2vec held constant and equal to 1, we conducted a search on the coarse grid <ref type="bibr" target="#b13">(14,</ref><ref type="bibr" target="#b8">9)</ref>; <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b4">5)</ref> to get more insights about the impact of resolution and number of channels (respectively). When using 5 channels, the graphs with less than 10 nodes were removed, because for these graphs, we cannot get a 10-dimensional node embedding space (there cannot be more dimensions than data points). This represented only a few graphs overall. On the PROTEINS full dataset, we only experimented with 2 node embeddings channels and 2 node attribute channels. p,q,c, and d n2v node2vec parameters. With the best resolution and number of channels, we then tuned the return and in-out parameters p and q of node2vec. Those parameters respectively bias the random walks towards exploring larger areas of the graph or staying in local neighborhoods, allowing the embeddings to encode a similarity that interpolates between structural equivalence (two nodes acting as, e.g., flow controllers, are close to each other) and homophily (two nodes belonging to the same community are close to each other). Following the node2vec paper <ref type="bibr" target="#b12">[13]</ref>, we tried 5 combinations of values for (p, q):</p><p>(1, 1); (0.25, 4); (4, 0.25); (0.5, 2); (2, 0.5) . Note that p = q = 1 is equivalent to DeepWalk <ref type="bibr" target="#b26">[27]</ref>. Since the graphs in the COLLAB and the IMDB-B datasets are very dense (&gt; 50%, average diameter of 1.86), we also tried to set the context size c to smaller values of 1 and 2 (the default value is c = 10). The context size is used when generating the training examples (context,target) to pass to the skipgram model: in a given random walk, it determines how many nodes before and after the target node should be considered part of the context. Finally, d n2v is the dimension of the node embeddings learned by node2vec. The default value is 128. Since the graphs in COLLAB, IMDB-B, and PRO-TEINS full are small (20, 74, and 39 nodes on average), we experimented with lower values than the default one on this dataset, namely 12 and 4. Note that with d n2v = 4, we can get at most 2 channels. The final values of p, q, c, and d n2v for each dataset are summarized in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>REDDIT-B REDDIT-5K REDDIT-12K COLLAB IMDB-B PROTEINS full Res. 9:1 9:1 9:  Baselines. On the social network datasets (on which there are no continuous node attributes), we re-implemented two state-of-the-art graph kernels, the graphlet kernel <ref type="bibr" target="#b29">[30]</ref> (we sampled 2000 graphlets of size up to 6 from each graph) and the Weisfeiler-Lehman (WL) kernel framework <ref type="bibr" target="#b30">[31]</ref> with the subtree graph kernel <ref type="bibr" target="#b10">[11]</ref> (we used node degrees as labels).</p><p>We also report for comparison purposes the performance of multiple stateof-the-art baselines that all share with us the same experimental setting: Deep Graph Kernels (DGK) <ref type="bibr" target="#b37">[38]</ref>, the best graph CNN from <ref type="bibr" target="#b22">[23]</ref>, namely PSCN k = 10, and Deep Graph CNN (DGCNN) <ref type="bibr" target="#b38">[39]</ref>. Note that to be fair, we excluded DGK from the comparison on the bioinformatics dataset since it doesn't make use of node attributes.</p><p>On the PROTEINS full dataset, we compared with the following baselines, which all take into account node attribute vectors: Hash Graph Kernel (HGK-SP) with shortest-path base kernel <ref type="bibr" target="#b20">[21]</ref>, Hash Graph Kernel with Weisfeiler-Lehman subtree base kernel (HGK-WL) <ref type="bibr" target="#b20">[21]</ref>, the best performing variant of Graph invariant kernels (GIK) <ref type="bibr" target="#b24">[25]</ref>, GraphHopper <ref type="bibr" target="#b9">[10]</ref> and baselines within, propagation kernel with diffusion scheme (PROP-diff) <ref type="bibr" target="#b21">[22]</ref>, and propagation kernel with hashing-based label discretization and WL kernel update (PROP-WL) <ref type="bibr" target="#b21">[22]</ref>.</p><p>Configuration. Following best practice, we used 10-fold cross validation and repeated each fold 3 times in all our experiments. For the graphlet and WL kernels, we used a C-SVM classifier 7 <ref type="bibr" target="#b25">[26]</ref>. The C parameter of the SVM and the number of iterations in WL were jointly optimized on a 90-10 % partition of the training set of each fold by searching the grid (10 −4 , 10 4 , len = 10); (2, 7, step = 1) .</p><p>For our 2D CNN, we used Xavier initialization <ref type="bibr" target="#b11">[12]</ref>, a batch size of 32, and for regularization, a dropout rate of 0.3 and early stopping with a patience of 5 epochs (null delta). The categorical cross-entropy loss was optimized with Adam <ref type="bibr" target="#b14">[15]</ref> (default settings). We implemented our model in Keras </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Results are reported in <ref type="table" target="#tab_6">Table 3</ref> for the social network datasets and <ref type="table" target="#tab_7">Table 4</ref> for the bioinformatics dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>REDDIT-B (size=2,000;nclasses=2)</p><p>REDDIT-5K <ref type="bibr">(4,999;5)</ref> REDDIT-12K <ref type="bibr">(11,929;11)</ref> COLLAB (5,000;3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB-B</head><p>(   Our approach shows statistically significantly better than all baselines on the REDDIT-12K and REDDIT-B datasets, with large absolute improvements of 6.81 and 2.82 in accuracy over the best performing competitor, respectively. We also reach best performance on the REDDIT-5K and PROTEINS full datasets, with respective improvements in accuracy of 1.34 and 0.52 over the best performing baselines. In particular, the fact that we reach best performance on PROTEINS full shows that our approach is flexible enough to leverage not only the topology of the network, but also continuous node attributes, in a very simple and unified way (we simply concatenate node embeddings with node attribute vectors). Note that when not using node attributes (only the first 2 node embeddings channels), the performance of our model decreases from 77.12 to 73.43, showing that our approach is capable of using both topological and attribute information.</p><p>Finally, on the IMDB-B dataset, we get third place, very close (≤ 1.2) to the top performers (no statistically significant difference). The only dataset on which a baseline proved significantly better than our approach is actually COLLAB (WL graph kernel). On this dataset though, the WL kernel beats all models by a wide margin, and we are relatively close to the other Deep Learning approaches (≤ 2.43 difference). Runtimes. Even if not directly comparable, we report in <ref type="table" target="#tab_9">Table 5</ref>   Discussion. Replacing the raw counts by the empirical joint probability density function, either by normalizing the histograms, or with a Kernel Density Estimate, significantly deteriorated performance. This suggests that keeping the absolute counts is important, which makes sense, as some categories might be associated with larger or smaller graphs, on average. We also observed that increasing the number of channels to more than 5 does not yield better results. This was expected, as higher order channels contain less information than lower order channels. However, reducing the number of channels improves performance in some cases, probably because it plays a regularization role. Data augmentation. Generating more training examples by altering the input images is known to improve performance in image classification <ref type="bibr" target="#b16">[17]</ref>. However, since we had direct access to the underlying data that were used to generate the images of the graphs, which is typically not the case in computer vision, we thought it would be more sensible to implement a synthetic data generation scheme at the node embeddings level rather than at the image level. More precisely, we used a simple but powerful nonparametric technique known as the smoothed bootstrap with variance correction <ref type="bibr" target="#b31">[32]</ref>, detailed in Alg. 1. This generator is used in hydroclimatology to improve modeling of precipitation <ref type="bibr" target="#b17">[18]</ref> or streamflow <ref type="bibr" target="#b28">[29]</ref>. Unlike the traditional bootstrap <ref type="bibr" target="#b8">[9]</ref> which simply draws with replacement from the initial set of observations, the smoothed bootstrap can generate values outside of the original range while still being faithful to the structure of the underlying data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>, as shown in <ref type="figure">Fig. 5</ref>.  |V A | is the number of nodes in the graph whose node embeddings are contained in A. Note that µ A , σ 2 A , h A , in lines 3, 4, 5 and 9 are all d-dimensional vectors (i.e., the functions are applied column-wise). N and U in lines 7 and 8 are the normal and discrete uniform distributions. The function N (0, √ x) in line 9 is applied to each element of h A . It generates Gaussian noise in each direction. Line 5 computes the bandwidths of the Kernel Density Estimate (KDE) over each dimension. The bandwidth controls the degree of smoothing. While there are many different ways to evaluate the bandwidth, we used the classical and widespread Silverman's rule of thumb <ref type="bibr" target="#b31">[32]</ref>, as shown in Eq. 1 below for a sample X of N scalars: silverman X = 0.9min σ X , Q3(X)−Q1(X)</p><formula xml:id="formula_0">1.34 N 1 /5<label>(1)</label></formula><p>Where Q 3 and Q 1 represent respectively the third and first quartiles, σ X is the standard deviation of the sample, and N is the size of the sample.</p><p>Line 11 simulates a new embedding vector. Using the smoothed bootstrap scheme can be viewed as sampling from the KDE. This is consistent with our way of representing graphs as images, since a KDE is nothing more than a smoothed histogram.</p><p>In our experiments, using the smoothed bootstrap improved performance only on REDDIT-B (+0.33% in accuracy), for p b = 0.2 (i.e., augmenting the dataset with 20% of synthetic graphs). Other values of p b (0.05, 0.1, 0.5) were not successful. Further research is thus necessary to understand how to make the proposed data augmentation strategy more effective.</p><p>Limitations. Even though results are very good out-of-the-box in most cases, finding an embedding algorithm that works well, or the right combination of parameters for a given dataset, can require some efforts. For instance, on COLLAB and IMDB-B (the only two datasets on which we do not reach best performance), we hypothesize that our results are inferior because the default parameter values of node2vec may not be well-suited to very dense graphs such as the ones found in COLLAB and IMDB-B (diameter&lt; 2, density &gt; 50). Optimizing the node2vec parameters on these datasets probably requires more than a coarse grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Motivated by the outstanding performance recently reached by CNNs in computer vision, e.g. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b16">17]</ref>, much research has been devoted to generalizing CNNs to graphs. Solutions fall into two broad categories: spatial and spectral techniques <ref type="bibr" target="#b4">[5]</ref>. Spectral approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> invoke the convolution theorem from signal processing theory to perform graph convolutions as pointwise multiplications in the Fourier domain of the graph. The basis used to send the graph to the Fourier domain is given by the SVD decomposition of the Laplacian matrix of the graph, whose eigenvalues can be viewed as frequencies. By contrast, spatial methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref> operate directly on the graph structure. For instance, in <ref type="bibr" target="#b22">[23]</ref>, the algorithm first determines the sequence of nodes for which neighborhood graphs (of equal size) are created. To serve as receptive fields, the neighborhood graphs are then normalized, i.e., mapped to a vector space with a linear order, in which nodes with similar structural roles in the neighborhood graphs are close to each other. Normalization is the central step, and is performed via a labeling procedure. A 1D CNN architecture is finally applied to the receptive fields. While the aforementioned sophisticated frameworks have made great strides, we showed in this paper that graphs can also be processed by vanilla 2D CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We showed that CNN for images can be used for learning graphs in a completely off-the-shelf manner. Our approach is flexible and can take continuous node attributes into account. We reach better results than state-of-the-art graph kernels and graph CNN models on 4 real-world datasets out of 6. Furthermore, these good results were obtained with limited parameter tuning and by using a basic 2D CNN model. From a time complexity perspective, our approach is preferable to graph kernels too, allowing to process bigger datasets featuring larger graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our 3-step approach represents graphs as "images" suitable for vanilla 2D CNNs. Continuous node attribute vectors can be passed as extra channels. Steps 1 &amp; 2: graph node embeddings and compression with PCA. Step 3: computation and stacking of the 2D histograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Node embeddings and image representation of graph ID #10001 (577 nodes, 1320 edges) from the REDDIT-12K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>2D CNN architecture used in our experiments. The number within parentheses refer to the output dimensions of the tensors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Overlayed node embeddings in the space made of the first two eigenvectors of the adjacency matrices (first channel) and associated overlayed bivariate histograms for all graphs in the first four classes of the random dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>[6] version 1.2.2 8 with tensorflow [1] backend. The hardware used consisted in an NVidia Titan X Pascal GPU with an 8-thread Intel Xeon 2.40 GHz CPU and 16 GB of RAM. The graph kernel baselines were run on an 8-thread Intel i7 3.4 GHz CPU, with 16 GB of RAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 Fig. 5 :</head><label>25</label><figDesc>Example of data augmentation with the smoothed bootstrap generator, for graph ID #3 of IMDB-B dataset (35 nodes, 133 edges).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Statistics of the social network datasets (first 5 columns) and the bioinformatics dataset used in our experiments.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Final resolution, number of channels, and p, q, c, and dn2v node2vec parameters for each dataset. number of channels for node embeddings and continuous node attributes. -means default value(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>10-fold CV average test set classification accuracy of our proposed method compared to state-of-the-art graph kernels and graph CNNs, on the social network datasets. ± is standard deviation. Best performance per column in bold. indicates stat. sign. at the p &lt; 0.05 level (our 2D CNN vs. WL) using the Mann-Whitney U test.</figDesc><table><row><cell></cell><cell cols="9">2D CNN DGCNN PSCN k = 10 HGK-SP HGK-WL GIK GraphHopper PROP-diff PROP-WL</cell></row><row><cell></cell><cell cols="9">our method Zhang18 Niepert16 Morris16 Morris16 Orsini15 Feragen13 Neumann12 Neumann12</cell></row><row><cell>Acc.</cell><cell>77.12</cell><cell>75.54</cell><cell>75.00</cell><cell>75.14</cell><cell>74.88</cell><cell>76.6</cell><cell>74.1</cell><cell>73.3</cell><cell>73.1</cell></row><row><cell>Std. dev.</cell><cell>2.79</cell><cell>0.94</cell><cell>2.51</cell><cell>0.47</cell><cell>0.64</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>10-fold CV average test set classification accuracy of our proposed method compared to state-of-the-art graph kernels and graph CNNs on the bioinformatics dataset (PROTEINS full).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>kernel matrix computation time for the two graph kernel baselines, along with the time per epoch of our 2D CNN model.</figDesc><table><row><cell></cell><cell cols="4">REDDIT-B REDDIT-5K REDDIT-12K COLLAB</cell><cell cols="2">IMDB-B PROTEINS full</cell></row><row><cell cols="6">Size, average (# nodes, # edges) 2000, (430,498) 4999, (509,595) 11929, (391,457) 5000, (74,2458) 1000, (20,97)</cell><cell>1113, (39,73)</cell></row><row><cell>Input shapes (for our approach)</cell><cell>(5,62,62)</cell><cell>(2,65,65)</cell><cell>(5,73,73)</cell><cell>(5,36,36)</cell><cell>(5,37,37)</cell><cell>(4,70,70) (2,70,70)</cell></row><row><cell>Graphlet Shervashidze2009</cell><cell>551</cell><cell>5046</cell><cell>12208</cell><cell>3238</cell><cell>275</cell><cell>-</cell></row><row><cell>WL Shervashidze2011</cell><cell>645</cell><cell>5087</cell><cell>20392</cell><cell>1579</cell><cell>23</cell><cell>-</cell></row><row><cell>2D CNN (our approach)</cell><cell>6</cell><cell>16</cell><cell>52</cell><cell>5</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Runtimes in seconds, rounded to the nearest integer. without using node attributes</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Algorithm 1 :</head><label>1</label><figDesc>Smoothed bootstrap with variance correctionInput : list L of M arrays of original d-dimensional node embeddings, p b ∈ R + Output: list L of M = int p b × M arrays of synthetic node embeddings 1 L ← select M elements from L at random without replacement 2 for A ∈ R |V A |×d in L do for j ← 1 to int N |VA|, |V A |</figDesc><table><row><cell>3</cell><cell>µA ← apply(mean, A)</cell></row><row><cell>4</cell><cell>σ 2 A ← apply(var, A)</cell></row><row><cell>5</cell><cell cols="2">hA ← apply(compute kde bandwidth, A)</cell></row><row><cell>6</cell><cell>A ← empty array</cell></row><row><cell></cell><cell>5</cell><cell>do</cell></row><row><cell>8 9</cell><cell>sample i from U (1, |VA|) ← apply(N (0, √ x), hA)</cell></row><row><cell>10 11</cell><cell cols="2">for k ← 1 to d do A [j, k] ← µA[k] + (A[i,k]−µ A [k]+ [k]) / √ 1+h A [k]/σ 2 A [k]</cell></row><row><cell>12</cell><cell>end</cell></row><row><cell>13</cell><cell>end</cell></row><row><cell>14</cell><cell>store A in L</cell></row><row><cell cols="2">15 end</cell></row><row><cell cols="2">16 return L</cell></row></table><note>7</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">the concept of spatial dependence is well summarized by: "everything is related to everything else, but near things are more related than distant things"<ref type="bibr" target="#b34">[35]</ref>. For instance in images, close pixels are more related than distant pixels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">our representation is unrelated to the widespread color histogram encoding of images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/snap-stanford/snap/tree/master/examples/node2vec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html 8 https://faroit.github.io/keras-docs/1.2.2/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank the anonymous reviewers for their helpful comments. The GPU used in this project was donated by NVidia as part of their GPU grant program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Data Mining</title>
		<meeting>the 5th International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Support vector machine solvers. Large scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bootstrap methods: another look at the jackknife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="569" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A nonparametric wet/dry spell model for resampling daily precipitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Tarboton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water resources research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2803" to="2823" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster kernels for graphs with continuous attributes via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient graph kernels by randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="378" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
		<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fourth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-fourth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3756" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multivariate nonparametric resampling scheme for generation of daily weather variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Tarboton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Hydrology and Hydraulics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="93" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Streamflow simulation: A nonparametric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Tarboton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water resources research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="308" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Graphlet Kernels for Large Graph Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Construction safety risk modeling and simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J P</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Hallowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rajagopalan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Risk analysis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A computer movie simulating urban growth in the detroit region</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Tobler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic geography</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">sup1</biblScope>
			<biblScope unit="page" from="234" to="240" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generalizing the convolution operator to extend cnns to irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Vialatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01166</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
