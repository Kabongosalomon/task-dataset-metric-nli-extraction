<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent ODEs for Irregularly-Sampled Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
							<email>rubanova@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto and the Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
							<email>rtqichen@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto and the Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
							<email>duvenaud@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto and the Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent ODEs for Irregularly-Sampled Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Hidden state trajectories. Vertical lines show observation times. Lines show different dimensions of the hidden state. Standard RNNs have constant or undefined hidden states between observations. The RNN-Decay model has states which exponentially decay towards zero, and are updated at observations. States of Neural ODE follow a complex trajectory but are determined by the initial state. The ODE-RNN model has states which obey an ODE between observations, and are also updated at observations. Recurrent neural networks (RNNs) are the dominant model class for high-dimensional, regularly-sampled time series data, such as text or speech. However, they are an awkward fit for irregularly-sampled time series data, common in medical or business settings. A standard trick for applying RNNs to irregular time series is to divide the timeline into equally-sized intervals, and impute or aggregate observations using averages. Such preprocessing destroys information, particularly about the timing of measurements, which can be informative about latent variables <ref type="bibr" target="#b7">[Lipton et al., 2016</ref><ref type="bibr" target="#b2">, Che et al., 2018</ref>].</p><p>An approach which better matches reality is to construct a continuous-time model with a latent state defined at all times. Recently, steps have been taken in this direction, defining RNNs with continuous dynamics given by a simple exponential decay between observations <ref type="bibr" target="#b2">[Che et al., 2018</ref><ref type="bibr" target="#b1">, Cao et al., 2018</ref><ref type="bibr" target="#b13">, Rajkomar et al., 2018</ref><ref type="bibr" target="#b10">, Mei and Eisner, 2017</ref>.</p><p>We generalize state transitions in RNNs to continuoustime dynamics specified by a neural network, as in Neural ODEs . We call this model the ODE-RNN, and use it to contruct two distinct continuous-time models. First, we use it as a standalone autoregressive model. Second, we refine the Latent ODE model of Chen et al. <ref type="bibr">[2018]</ref> by using the ODE-RNN as a recognition network. Latent ODEs define a generative process over time series based on the deterministic evolution of an initial latent state, and can be trained as a variational autoencoder <ref type="bibr" target="#b6">[Kingma and Welling, 2013]</ref>. Both models naturally handle time gaps between observations, and remove the need to group observations into equally-timed bins. We compare ODE models to several RNN variants and find that ODE-RNNs can perform better when the data is sparse. Since the absence of observations itself can be informative, we further augment Latent ODEs to jointly model times of observations using a Poisson process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Recurrent neural networks A simple way to handle irregularly-timed samples is to include the time gap between observations ∆ t = t i − t i−1 into the update function of the RNN:</p><formula xml:id="formula_0">h i = RNNCell(h i−1 , ∆ t , x i )</formula><p>(1) However, this approach raises the question of how to define the hidden state h between observations. A simple alternative introduces an exponential decay of the hidden state towards zero when no observations are made <ref type="bibr" target="#b2">[Che et al., 2018</ref><ref type="bibr" target="#b1">, Cao et al., 2018</ref><ref type="bibr" target="#b13">, Rajkomar et al., 2018</ref><ref type="bibr" target="#b11">, Mozer et al., 2017</ref>:</p><formula xml:id="formula_1">h i = RNNCell(h i−1 · exp{−τ ∆ t }, x i )</formula><p>(2) where τ is a decay rate parameter. However, <ref type="bibr" target="#b11">Mozer et al. [2017]</ref> found that empirically, exponentialdecay dynamics did not improve predictive performance over standard RNN approaches.</p><p>Neural Ordinary Differential Equations Neural ODEs  are a family of continuous-time models which define a hidden state h(t) to be the solution to an ODE initial-value problem (IVP):</p><formula xml:id="formula_2">dh(t) dt = f θ (h(t), t) where h(t 0 ) = h 0<label>(3)</label></formula><p>in which the function f θ specifies the dynamics of the hidden state, using a neural network with parameters θ. The hidden state h(t) is defined at all times, and can be evaluated at any desired times using a numerical ODE solver: h 0 , . . . , h N = ODESolve(f θ , h 0 , (t 0 , . . . , t N )) (4) Chen et al. <ref type="bibr">[2018]</ref> showed how adjoint sensitivities can be used to compute memory-efficient gradients w.r.t. θ, allowing black-box ODE solvers to be used as a building block in larger models. They also conducted toy experiments in a time-series model in which the latent state follows a Neural ODE. Chen et al. <ref type="bibr">[2018]</ref> used time-invariant dynamics in their time-series model: dh(t) /dt = f θ (h(t)) , and we follow the same approach, but adding time-dependence would be straightforward if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we use neural ODEs to define two distinct families of continuous-time models: the autoregressive ODE-RNN, and the variational-autoencoder-based Latent ODE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing an ODE-RNN Hybrid</head><p>Following <ref type="bibr" target="#b11">Mozer et al. [2017]</ref>, we note that an RNN with exponentially-decayed hidden state implicitly obeys the following ODE dh(t) dt = −τ h with h(t 0 ) = h 0 , where τ is a parameter of the model. The solution to this ODE is the pre-update term h 0 · exp{−τ ∆ t } in (2). This differential equation is time-invariant, and assumes that the stationary point (i.e. zero-valued state) is special. We can generalize this approach and model the hidden state using a Neural ODE. The resulting algorithm is given in Algorithm 1. We define the state between observations to be the solution to an ODE:</p><formula xml:id="formula_3">h i = ODESolve(f θ , h i−1 , (t i−1 , t i )</formula><p>) and then at each observation, update the hidden state using a standard RNN update h i = RNNCell(h i , x i ). Our model does not explicitly depend on t or ∆ t when updating the hidden state, but does depend on time implicitly through the resulting dynamical system. Compared to RNNs with exponential decay, our approach allows a more flexible parameterization of the dynamics. A comparison between the state dynamics of these models is given in table 2.</p><p>Autoregressive Modeling with the ODE-RNN The ODE-RNN can straightforwardly be used to probabilistically model sequences. Consider a series of observations {x i } N i=0 at times {t i } N i=0 . Autoregressive models make a one-step-ahead prediction conditioned on the history of observations, i.e. they factor the joint density p(x) = i p θ (x i |x i−1 , . . . , x 0 ). As in standard RNNs, we can use an ODE-RNN to specify the conditional distributions p θ (x i |x i−1 ...x 0 ) (Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The ODE-RNN. The only difference, highlighted in blue, from standard RNNs is that the pre-activations h evolve according to an ODE between observations, instead of being fixed.</p><p>Input: Data points and their timestamps {(</p><formula xml:id="formula_4">x i , t i )} i=1..N h 0 = 0 for i in 1, 2, . . . , N do h i = ODESolve(f θ , h i−1 , (t i−1 , t i )) Solve ODE to get state at t i h i = RNNCell(h i , x i )</formula><p>Update hidden state given current observation</p><formula xml:id="formula_5">x i end for o i = OutputNN(h i ) for all i = 1..N Return: {o i } i=1..N ; h N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Latent ODEs: a Latent-variable Construction</head><p>Autoregressive models such as RNNs and the ODE-RNN presented above are easy to train and allow fast online predictions. However, autoregressive models can be hard to interpret, since their update function combines both their model of system dynamics, and of conditioning on new observations. Furthermore, their hidden state does not explicitly encode uncertainty about the state of the true system. In terms of predictive accuracy, autoregressive models are often sufficient for densely sampled data, but perform worse when observations are sparse.</p><p>An alternative to autoregressive models are latent-variable models. For example, Chen et al. <ref type="bibr">[2018]</ref> proposed a latent-variable time series model, where the generative model is defined by ODE whose initial latent state z 0 determines the entire trajectory:  We follow Chen et al. <ref type="bibr">[2018]</ref> in using a variational autoencoder framework for both training and prediction. This requires estimating the approximate posterior q(z 0 |{x i , t i } N i=0 ). Inference and prediction in this model is effectively an encoder-decoder or sequence-to-sequence architecture, in which a variable-length sequence is encoded into a fixed-dimensional embedding, which is then decoded into another variable-length sequence, as in <ref type="bibr" target="#b16">Sutskever et al. [2014]</ref>.</p><formula xml:id="formula_6">z 0 ∼ p(z 0 ) (5) z 0 , z 1 , . . . , z N = ODESolve(f θ , z 0 , (t 0 , t 1 , . . . , t N )) (6) each x i indep. ∼ p(x i |z i ) i = 0, 1, . . . , N<label>(7)</label></formula><p>Chen et al. <ref type="bibr">[2018]</ref> used an RNN as a recognition network to compute this approximate posterior. We conjecture that using an ODE-RNN as defined above for the recognition network would be a more effective parameterization when the datapoints are irregularly sampled. Thus, we propose using an ODE-RNN as the encoder for a latent ODE model, resulting in a fully ODE-based sequence-tosequence model. In our approach, the mean and standard deviation of the approximate posterior q(z 0 |{x i , t i } N i=0 ) are a function of the final hidden state of an ODE-RNN:</p><formula xml:id="formula_7">q(z 0 |{x i , t i } N i=0 ) = N (µ z0 , σ z0 ) where µ z0 , σ z0 = g(ODE-RNN φ ({x i , t i } N i=0 ))<label>(8)</label></formula><p>Where g is a neural network translating the final hidden state of the ODE-RNN encoder into the mean and variance of z 0 . To get the approximate posterior at time point t 0 , we run the ODE-RNN encoder backwards-in-time from t N to t 0 . We jointly train both the encoder and decoder by maximizing the  <ref type="figure">Figure 2</ref>: The Latent ODE model with an ODE-RNN encoder. To make predictions in this model, the ODE-RNN encoder is run backwards in time to produce an approximate posterior over the initial state: q(z 0 |{x i , t i } N i=0 ). Given a sample of z 0 , we can find the latent state at any point of interest by solving an ODE initial-value problem. <ref type="figure">Figure adapted</ref> from Chen et al. <ref type="bibr">[2018]</ref>. evidence lower bound (ELBO):</p><formula xml:id="formula_8">Standard RNN h ti−1 RNN-Decay h ti−1 e −τ ∆t GRU-D h ti−1 e −τ ∆t ODE-RNN ODESolve(f θ , h i−1 , (t i−1 , t)) µ t N t 1 t 0~t0 t 1 t N q(z 0 |x 0 ..x N ) z 0 z 1 z i z N ODE Solve(f, z 0 , (t 0 ..t N )) GRU GRU GRU GRU ODE ODE x N x 1 x 0 x ix ix N x 1 x 0</formula><formula xml:id="formula_9">ELBO(θ, φ) = E z0∼q φ (z0|{xi,ti} N i=0 ) [log p θ (x 0 , . . . , x N ))] − KL q φ (z 0 |{x i , t i } N i=0 )||p(z 0 ) (9)</formula><p>This latent variable framework comes with several benefits: First, it explicitly decouples the dynamics of the system (ODE), the likelihood of observations, and the recognition model, allowing each to be examined or specified on its own. Second, the posterior distribution over latent states provides an explicit measure of uncertainty, which is not available in standard RNNs and ODE-RNNs. Finally, it becomes easier to answer non-standard queries, such as making predictions backwards in time, or conditioning on a subset of observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Poisson process likelihoods</head><p>Diastolic arterial blood pressure The fact that a measurement was made at a particular time is often informative about the state of the system <ref type="bibr" target="#b2">[Che et al., 2018]</ref>. In the ODE framework, we can use the continuous latent state to parameterize the intensity of events using aninhomogeneous Poisson point process <ref type="bibr" target="#b12">[Palm, 1943]</ref> where the event rate λ(t) changes over time.</p><p>Poisson point processes have the following log-likelihood:</p><formula xml:id="formula_10">log p(t 1 , . . . , t N |t start , t end , λ(·)) = N i=1 log λ(t i ) − tend tstart λ(t)dt</formula><p>Where t start and t end are the times at which observations started and stopped being recorded.</p><p>We augment the Latent ODE framework with a Poisson process over the observation times, where we parameterize λ(t) as a function of z(t). This means that instead of specifying and maximizing the conditional marginal likelihood p(x 1 , . . . , x N |t 1 , . . . , t N , θ), we can instead specify and maximizing the joint marginal likelihood p(x 1 , . . . , x N , t 1 , . . . , t N , |θ). To compute the joint likelihood, we can evaluate the Poisson intensity λ(t), precisely estimate its integral, and the compute latent states at all required time points, using a single call to an ODE solver. <ref type="bibr" target="#b10">Mei and Eisner [2017]</ref> used a similar approach, but relied on a fixed time discretization to estimate the Poisson intensity. Chen et al.</p><p>[2018] showed a toy example of using Latent ODEs with a Poisson process likelihood to fit latent dynamics from observation times alone. In section 4.4, we incorporate a Poisson process likelihood into a latent ODE to model observation rates in medical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Batching and computational complexity</head><p>One computational difficulty that arises from irregularly-sampled data is that observation times can be different for each time series in a minibatch. In order to solve all ODEs in a minibatch in sync, we must we must output the solution of the combined ODE at the union of all time points in the batch.</p><p>Taking the union of time points does not substantially hurt the runtime of the ODE solver, as the adaptive time stepping in ODE solvers is not sensitive to the number of time points (t 1 ...t N ) at which the solver outputs the state. Instead, it depends on the length on the time interval [t 1 , t N ] and the complexity of the dynamics. (see suppl. figure 3). Thus, ODE-RNNs and Latent ODEs have a similar asymptotic time complexity to standard RNN models. However, as the ODE must be continuously solved even when no observations occur, the compute cost does not scale with the sparsity of the data, as it does in decay-RNNs. In our experiments, we found that the ODE-RNN takes 60% more time than the standard GRU to evaluate, and the Latent ODE required roughly twice the amount of time to evaluate than the ODE-RNN.</p><p>3.5 When should you use an ODE-based model over a standard RNN?</p><p>Standard RNNs are ignore the time gaps between points. As such, standard RNNs work well on regularly spaced data, with few missing values, or when the time intervals between points are short.</p><p>Models with continuous-time latent state, such as the ODE-RNN or RNN-Decay, can be evaluated at any desired time point, and therefore are suitable for interpolation tasks. In these models, the future hidden states depend on the time since the last observation, also making them better suited for sparse and/or irregular data than standard RNNs. RNN-Decay enforces that the hidden state converges monontically to a fixed point over time. In ODE-RNNs the form of the dynamics between the observations is learned rather than pre-defined. Thus, ODE-RNNs can be used on sparse and/or irregular data without making strong assumptions about the dynamics of the time series.</p><p>Latent variable models versus autoregressive models We refer to models which iteratively compute the joint distribution p(x) = i p θ (x i |x i−1 , . . . , x 0 ) as autoregressive models (e.g. RNNs and ODE-RNNs). We call models of the form p(x) = i p(x i |z 0 )p(z 0 )dz 0 latent-variable models (e.g. Latent ODEs and RNN-VAEs).</p><p>In autoregressive models, both the dynamics and the conditioning on data are encoded implicitly through the hidden state updates, which makes them hard to interpret. In contrast, encoder-decoder models (Latent ODE and RNN-VAE) represent state explicitly through a vector z t , and represent dynamics explicitly through a generative model. Latent states in these models can be used to compare different time series, for e.g. clustering or classification tasks, and their dynamics functions can be examined to identify the types of dynamics present in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy dataset</head><p>We tested our model on a toy dataset of 1,000 periodic trajectories with variable frequency and the same amplitude. We sampled the initial point from a standard Gaussian, and added Gaussian noise to the observations. Each trajectory has 100 irregularly-sampled time points. During training, we subsample a fixed number of points at random, and attempt to reconstruct the full set of 100 points. Conditioning on sparse data Latent ODEs can often reconstruct trajectories reasonably well given a small subset of points, and provide an estimate of uncertainty over both the latent trajectories and predicted observations. To demonstrate this, we trained a Latent ODE model to reconstruct the full trajectory (100 points) from a subset of 30 points. At test time, we conditioned this model on a subset of 10, 30 or 50 points. Conditioning on more points results in a better fit as well as smaller variance across the generated trajectories ( <ref type="figure" target="#fig_1">fig. 4</ref>). <ref type="figure" target="#fig_1">Figure 4</ref>(b) demonstrates that the trajectories sampled from the prior of the trained model are also periodic.</p><p>Extrapolation Next, we show that a time-invariant ODE can recover stationary periodic dynamics from data automatically. <ref type="figure" target="#fig_2">Figure 5</ref> shows a Latent ODE trained to condition on 20 points in the [0; 2.5] interval (red area) and predict points on [2.5; 5] interval (blue area). A Latent ODE with an ODE-RNN encoder was able to extrapolate the time series far beyond the training interval and maintain periodic dynamics. In contrast, a Latent ODE trained with RNN encoder as in Chen et al.</p><p>[2018] did not extrapolate the periodic dynamics well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>We evaluate the models quantitavely on two tasks: interpolation and extrapolation. On each dataset, we used 80% for training and 20% for test. See the supplement a detailed description.</p><p>Baselines In the class of autoregressive models, we compare ODE-RNNs to standard RNNs. We compared the following autoregressive models: (1) ODE-RNN (proposed) (2) A classic RNN where ∆ t is concatenated to the input (RNN-∆ t ) (3) An RNN with exponential decay on the hidden states h · e −τ ∆t (RNN-Decay) (4) An RNN with missing values imputed by a weighted average of previous value and empirical mean (RNN-Impute), and (5) GRU-D <ref type="bibr" target="#b2">[Che et al., 2018]</ref> which combines exponential decay and the above imputation strategy. Among encoder-decoder models, we compare the Latent ODE to a variational autoencoder in which both the encoder and decoder are recurrent neural nets (RNN-VAE). The ODE-RNN can use any hidden state update formula for the RNNCell function in Algorithm 1. Throughout our experiments, we use the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b4">[Cho et al., 2014]</ref>. See the supplement for the architecture details.</p><p>Interpolation The standard RNN and the ODE-RNN are straightforward to apply to the interpolation task. To perform interpolation with a Latent ODE, we encode the time series backwards in time, compute the approximate posterior q(z 0 |{x i , t i } N i=0 ) at the first time point t 0 , sample the initial state of ODE z 0 , and generate mean observations at each observation time.</p><p>Extrapolation In the extrapolation setting, we use the standard RNN or ODE-RNN trained on the interpolation task, and then extrapolate the sequence by re-feeding previous predictions. To encourage extrapolation, we used scheduled sampling <ref type="bibr" target="#b0">[Bengio et al., 2015]</ref>, feeding previous predictions instead of observed data with probability 0.5 during training. One might expect that directly optimizing for extrapolation would perform best at extrapolation. Such a model would resemble an encoder-decoder model, which we consider separately below (the RNN-VAE). For extrapolation in encoder-decoder models, including the Latent ODE, we split the timeline in half. We encode the observations in the first half forward in time and reconstruct the second half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MuJoCo Physics Simulation</head><p>Next, we demonstrated that ODE-based models can learn an approximation to simple Newtonian physics. To show this, we created a physical simulation using the "Hopper" model from the Deepmind Control Suite <ref type="bibr" target="#b17">[Tassa et al., 2018]</ref>. We randomly sampled the initial position of the hopper and initial velocities such that hopper rotates in the air and falls on the ground (figure 6). These trajectories are deterministic functions of their initial states, which matches the assumptions made by the Latent ODE. The dataset is 14-dimensional, and we model it with a 15-dimensional latent state. We generated 10,000 sequences of 100 regularly-sampled time points each.</p><p>We perform both interpolation and extrapolation tasks on the MuJoCo dataset. During training, we subsampled a small percentage of time points to simulate sparse observation times. For evaluation, we measured the mean squared error (MSE) on the full time series.  <ref type="table" target="#tab_4">Table 3</ref> shows mean squared error for models trained on different percentages of observed points. Latent ODEs outperformed standard RNN-VAEs on both interpolation and extrapolation. Our ODE-RNN model also outperforms standard RNNs on the interpolation task. The gap in performance between RNN and ODE-RNN increases with sparser data. Notably, the Latent ODE (an encoderdecoder model) shows better performance than the ODE-RNN (an autoregressive model).</p><p>All autoregressive models performed poorly at extrapolation. This is expected, as they were only trained for one-step-ahead prediction, although standard RNNs performed better than ODE-RNNs. Latent ODEs outperformed RNN-VAEs on the extrapolation task.</p><p>Interpretability of the latent state <ref type="figure" target="#fig_3">Figure 6</ref> shows how the norm of the latent state time-derivative f θ (z) changes with time for two reconstructed MuJoCo trajectories. When the hopper hits the ground, there is a spike in the norm of the ODE function. In contrast, when the hopper is lying on the ground, the norm of the dynamics is small. <ref type="figure">Figure 7</ref> shows the entropy of the approximate posterior q(z 0 |{x i , t i } N i=0 ) of a trained model conditioned on different numbers of observations. The average entropy (uncertainty) monotonically decreases as more points are observed. <ref type="figure">Figure 8</ref> shows the latent state z 0 projected to 2D using UMAP <ref type="bibr" target="#b9">[McInnes et al., 2018]</ref>. The latent state corresponds closely to the physical parameters of the true simulation that most strongly determine the future trajectory of the hopper: distance from the ground, initial velocity on z-axis, and relative position of the leg of the hopper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Physionet</head><p>We evaluated our model on the PhysioNet Challenge 2012 dataset <ref type="bibr" target="#b15">[Silva et al., 2012]</ref>, which contains 8000 time series, each containing measurements from the first 48 hours of a different patient's admission to ICU. Measurements were made at irregular times, and of varying sparse subsets of the 37 possible features. Most existing approaches to modeling this data use a coarse discretization of the aggregated measurements per hour <ref type="bibr" target="#b2">[Che et al., 2018]</ref>, which forces the model to train on only one-twentieth of measurements. In contrast, our approach, in principle, does not require any discretization or aggregation of measurements. To speed up training, we rounded the observation times to the nearest minute, reducing the number of measurements only 2-fold. Hence, there are still 2880 (60*48) possible measurement times per time series under our model's preprocessing, while the previous standard was to used only 48 possible measurement times. We used 20 latent dimensions in the latent ODE generative model. See supplement for more details on hyperparameters.</p><p>Tables 4 and 5 report mean squared error averaged over runs with different random seeds, and their standard deviations. We run one-sided t-test to establish a statistical significance. Best models are marked in bold. ODE-based models have smaller mean squared error than RNN baselines on this dataset.</p><p>Finally, we constructed binary classifiers based on each model type to predict in-hospital mortality. We passed the hidden state at the last measured time point into a two-layer binary classifier. Due to class imbalance (13.75% samples with positive label), we report test area under curve (AUC) instead of accuracy. <ref type="table" target="#tab_7">Table 6</ref> shows that the ODE-RNN, Latent ODE and GRU-D achieved the similar classification AUC. A possible explanation is that modelling dynamics between time points does not make a difference for binary classification of the full time series.</p><p>We also included a Poisson Process likelihood on observation times, jointly trained with the Latent ODE model. <ref type="figure">Figure 3</ref> shows the inferred measurement rate on a patient from the dataset. Although the Poisson process was able to model observation times reasonably well, including this likelihood term did not improve classification accuracy.    0.846 ± 0.013</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Activity dataset</head><p>We trained the same classifier models as above on the Human Activity dataset, which contains time series from five individuals performing various activities: walking, sitting, lying, etc. The data consists of 3d positions of tags attached to their belt, chest and ankles (12 features in total). After preprocessing, the dataset has 6554 sequences of 211 time points (details in supplement). The task is to classify each time point into one of seven types of activities (walking, sitting, etc.). We used a 15-dimensional latent state (more details in the supplement). <ref type="table" target="#tab_8">Table 7</ref> shows that the Latent ODE-based classifier had higher accuracy than the ODE-RNN classifier on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Standard RNNs treat observations as a sequence of tokens, not accounting for variable gaps between observations. One way to accommodate this is to discretize the timeline into equal intervals, impute missing data, and then run an RNN on the imputed inputs. To perform imputation, <ref type="bibr" target="#b2">Che et al. [2018]</ref> used a weighted average between the empirical mean and the previous observation. Others have used a separate interpolation network [Shukla and Marlin, 2019], Gaussian processes <ref type="bibr" target="#b5">[Futoma et al., 2017]</ref>, or generative adversarial networks <ref type="bibr" target="#b8">[Luo et al., 2018]</ref> to perform interpolation and imputation prior to running an RNN on time-discretized inputs. In contrast, Lipton et al.</p><p>[2016] used a binary mask to indicate the missing measurements and reported that RNNs performs better with zero-filling than with imputed values. They note that such methods can be sensitive to the discretization granularity.</p><p>Another approach is to directly incorporate the time gaps between observations into RNN. The simplest approach is to append the time gap ∆ t to the RNN input. However, <ref type="bibr" target="#b11">Mozer et al. [2017]</ref> suggested that appending ∆ t makes the model prone to overfitting, and found empirically that it did not improve predictive performance. Another solution is to introduce the hidden states that decay exponentially over time <ref type="bibr" target="#b2">[Che et al., 2018</ref><ref type="bibr" target="#b1">, Cao et al., 2018</ref><ref type="bibr" target="#b13">, Rajkomar et al., 2018</ref>. <ref type="bibr" target="#b10">Mei and Eisner [2017]</ref> used hidden states with exponential decay to parametrize neural Hawkes processes, and explicitly modeled observation intensities. Hawkes processes are self-exciting processes whose latent state changes at each observation event. This architecture is similar to our ODE-RNN. In contrast, the Latent ODE model assumes that observations do not affect the latent state, but only affect the model's posterior over latent states, and is more appropriate when observations (such as taking a patient's temperature) do not substantially alter their state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and conclusion</head><p>We introduced a family of time series models, ODE-RNNs, whose hidden state dynamics are specified by neural ordinary differential equations (Neural ODEs). We first investigated this model as a standalone refinement of RNNs. We also used this model to improve the recognition networks of a variational autoencoder model known as Latent ODEs. Latent ODEs provide relatively interpretable latent states, as well explicit uncertainty estimates about latent states. Neither model requires discretizing observation times, or imputing data as a preprocessing step, making them suitable for the irregularly-sampled time series data common in many applications. Finally, we demonstrate that continuous-time latent states can be combined with Poisson process likelihoods to model the rates at which observations are made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary tables and figures 1 Experiment setup</head><p>We test the ODE models for two tasks: interpolation and extrapolation.</p><p>Interpolation Consider time series with time points (t 0 ...t N ). In interpolation task we condition on the subset of points from (t 0 ...t N ) and reconstruct the full set of points in the same time interval. We subsample the points by setting the value to zero in the data tensor and in the mask. On MUJoCo dataset we perform experiments with different proportion of subsampled points (ranging from 10% to 50%, results shown in table 3 of main manuscript). In Physionet and Human Activity datasets, we do not perform subsampling since the data is already sparse.</p><p>Extrapolation In extrapolation task we split the time series into two parts: (t 0 ...t N2 ) and (t N2 ..t N ). We encode the first half of the time series and reconstruct the second half. Similarly to interpolation task, we randomly sample a subset of time points from the time series and run a recognition network on this subset of points. We evaluate the model by the ability to reconstruct the full time series (without subsampling). Autoencoder models are straightforward to use for extrapolation task. We run the encoder on the first half of the sequence and decode the second half. For autoregressive models, we train the model to perform interpolation first, and then perform extrapolation at test time by re-feeding previous predictions of the model. During training, we feed in either the previous observed value or predicted value with probability 0.5, a common regularization method <ref type="bibr">(Goodfellow et al., 2016)</ref>. For all experiments, we report the mean squared error (MSE) on a test set of held-out sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of first time point of ODESolve</head><p>The generative model has a special time point t 0 where we put a prior on the latent state. ODE can be solved both forward or backward in time, and we are free to choose the time point t 0 depending on the task. As such, for interpolation task, we choose t 0 to be the time point of the first observation, as shown in eq. (5-7), and run ODE-RNN backwards in time to obtain the approximate posterior. For extrapolation task, we choose the initial point to the the last observed time point t N . In this case, we run ODE-RNN encoder forward in time.  </p><formula xml:id="formula_11">→ t 0 q(z(t 0 )|x 0 ..x N ) Forward in time in [t 0 , t N ] interval Extrapolation Forward in time t 0 → t N/2 q(z(t N/2 )|x 0 ..x N/2 ) Forward in time in [t N/2 , t N ]</formula><formula xml:id="formula_12">z = σ(f z ([h prev ; x])) Update coefficient r = σ(f r ([h prev ; x])) Reset coefficient h = g([r * h prev ; x]) Proposed new state h = (1 − z) * h + z * h prev Return: new hidden state h 2.2 Latent ODE Algorithm 2 Latent ODE Input: Data points {x i } i=1..N and corresponding times {t i } i=1..N z 0 = ODE-RNN({x i } i=1..N ) µ z0 , σ z0 = g µ (z 0 ), g σ (z 0 ) g µ and g σ are feed-forward NN z 0 ∼ N (µ z0 , σ z0 ) {z i } = ODESolve(f, z 0 , (t 0 ...t N )) x i = OutputN N (z i ) for all i = 1..N Return: {x i } i=1..N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Modelling poisson process likelihood</head><p>To model poisson process on Physionet dataset, we augment generative ODE d dt z = f (z) by adding extra latent dimensions z λ . Intensity function λ is a function of latents z λ : λ = g λ (z λ ), where g λ is two-layer feed-forward neural net. Dimensionality of λ has to be equal to dimensionality of the data (37 in Physionet). We used 20 latent dimensions for z and 20 dimensions for z λ . We further augment the ODE with the integral over λ. Notice that the derivative t 0 λ(τ )dτ is λ(t). Thus, the augmented ODE is defined as follows:</p><formula xml:id="formula_13">d dt   z z λ t 0 λ(τ )dτ   =   f (z) f z λ λ   ; where λ = g(z λ )</formula><p>We set the initial value for t 0 λ(τ )dτ to zero (notice that 0 0 λ(τ )dτ = 0. Initial values of z and z λ are sampled from approximate posterior. The augmented ODE is solved using a call to ODESolve.</p><p>Generative model with Poisson Process The joint generative model is specified as p(z 0 )p(t 0 , . . . , t N |z 0 ) N i=0 p(x i |z 0 ), where the distributions are specified below. p(z 0 ) = Normal z 0 ; 0, I (1)</p><formula xml:id="formula_14">{z(t i )} N i=0 = ODESolve f θ , z 0 , (t 0 , . . . , t N ) (2) p(t 0 ...t N |z 0 ) = PoissonProcess t 1 ...t N ; λ(z(t)) (3) p(x i |z 0 ) = Normal x i ; µ(z(t i )), σ(z(t i ))<label>(4)</label></formula><p>The Latent ODE framework specifies a generative model for time series.</p><formula xml:id="formula_15">y 0 ∼ p(y 0 ) (5) λ(t) = ODESolve(y 0 , f λ , θ f λ , t) (6) t 1 ...t N ∼ PoissonProcess(λ(t))<label>(7)</label></formula><p>y 1 , ...y N = ODESolve(y 0 , f y , θ fy , t 1 ...t N ) (8)</p><formula xml:id="formula_16">x i ∼ p(x|y i , θ x ), ∀i ∈ {1..N }<label>(9)</label></formula><p>3 Data generation and preprocessing Toy dataset We generate 1000 one-dimentional trajectories with 100 time points in each on the [0, 5] interval. We use sinusoid with fixed amplitude of 1 and sample frequency from [0.5, 1] interval. We sample the starting point from N (µ = 1, σ = 0.1).</p><p>MuJoCo We generate 10,000 simulations of the "Hopper" model from Deep Mind Control Suite and MuJoCo simulator. We sample the position of the body in 2d space uniformly from [0, 0.5]. We sample the relative position of limbs from [−2, 2] and initial velocities from [−5, 5] interval. We generate trajectories with 200 time steps for extrapolation tasks (100 points to condition on and 100 for extrapolation). We use 100 time points to perform interpolation task.</p><p>Physionet PhysioNet contains the data from the first 48 hours of patients in ICU. We have excluded four time-invariant features: Age, Gender, Height and ICUType. Hence, each patients has a set of up to 37 features (most patients have only a subset of those features). We round up the time stamps to one minute. Therefore, each time series can contain up to 2880 (60*48) points. The original Physionet challenge has Train set (labeled) and Test set (unlabeled). We combine them into a single dataset (8000 patients, 37 features in each) and randomly split into 80% train and 20% test set, similarly to other datasets. We normalize each feature across all patients in the dataset to be in [0, 1] interval. Each feature in each patients was measured at different time points. In order to perform batching of the features and patients during the training, we take the union of all time points across all features in a batch. We use a mask in ODE-GRU and in the recognition model of ODE-VAE to annotate the features that are present at the particular time point in order to update the latent state. Note that generative part of ODE-VAE does not require any masking. If the training example does not have any observations at the particular time point, we don't update the its hidden state at this time point.</p><p>Human Activity The original dataset contains 25 sequences from five people (6600 points on average in each sequence). For the sake of reducing the size of union of time points, we round up the time stamps of the measurement to 100 ms -such discretization does not change the overall number of points. We join the data from four tags (belt, chest, ankles) into a single time series and then split the each sequence into partially overlapping intervals of 50 time points (with overlap of 25 points). We combine sequence from all individuals into a single dataset. After taking the union of all time points in the dataset, we get the dataset of 6554 sequences with 211 time points in each. We do not perform normalization on this dataset. The labels are provided for each observation and denote the type of activity that the person is performing, such as walking, sitting, lying, etc. Original dataset has 11 classes. Some classes correspond to very similar activities, which are hard to distinguish. We decided to combine the classes within the following groups: "lying", "lying down" , "sitting", "sitting down" , "standing up from lying", "standing up from sitting", "standing up from sitting on the ground" . The resulting set of classes describes seven activity types: "walking", "falling", "lying", "sitting", "standing up", "on all fours", "sitting on the ground". We run all experiments and report results using these seven classes.</p><p>General notes For all datasets, we take the union of all time points in the dataset and run all models on the union. We randomly split the dataset into 80% train and 20% test. In Physionet dataset we rescale each feature to be between 0 and 1. We do not normalize other datasets. We also rescale the timeline to be in [0, 1]. ODE Solver We used ODE Solvers from torchdiffeq python package. We used fifth-order "dopri5" solver with adaptive step for generative model of Latent ODE. We used relative tolerance of 1e-3 and absolute tolerance of 1e-4. Adjoint method described in  can be used to reduce the memory use, at a cost of a longer computation time.</p><p>Loss To compute data log-likelihood, We use negative gaussian log-likelihood with fixed variance as a reconstruction loss. We used fixed variance of 0.001 for MuJoCo and 0.01 for other datasets. We report MSE on the time series in the test set. For classification task, we use cross-entropy loss. To compute ELBO for encoder-decoder models, we used three samples from distribution N (µ z0 , σ z0 ). On Physionet dataset, we got best performance by training reconstruction loss and cross-entropy loss together, with multiplier 100 on cross-entropy loss. We train on the whole dataset of 8000 patients and compute the CE loss only on labeled samples (4000 patients).</p><p>On Human Activity dataset, we trained solely with cross-entropy loss. Note that on Physionet labels are provided per time series, and reconstruction loss prevents overfitting on classification task. On Human Activity dataset, the labels are provided per time point and training with CE loss only does not lead to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN Baselines</head><p>We follow the available implementation of RNN GRU-D: https://github.com/zhiyongc/ GRU-D/blob/master/GRUD.py. We use exponential decay between hidden states and imputation technique from GRU-D as separate baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hyperparameters</head><p>Choosing hyperparameters First, we choose the hyperparameters such that delivered the best performance for the RNN baselines in our experiments. Then we run corresponding ODE models with the same hyperparameters. In autoregressive models we use the same size of hidden state, number of layers and units in encoder and decoder networks, etc. in both ODE-RNN and RNN baselines.</p><p>Hyperparameters for Latent ODE model We empirically found that ODE of the same or slightly smaller dimensionality as the data works best for generative model in Latent ODE. We also found that recognition model has to have bigger dimensionality than generative model. Fitting Poisson process on Physionet dataset For fitting Poisson process along with the likelihood together with reconstruction likelihood, our generative model had the following dimensions: 20 for modelling z(t) (for reconstruction), 20 for modelling z λ (for poisson process) and 37 for λ t 0 (τ )dτ ( equals data dimensionality). Hence, 77 dimensions in total. Other hyperparameters are the same as above.</p><p>Classification on Physionet In classification task, the goal is to classify each patient with a binary label. We used "in-hospital mortality" label from the original dataset. To prevent overfitting, we train the model together with reconstruction loss with coefficient 100 on cross-entropy loss. We compute reconstruction loss on all 8000 patients, and cross-entropy loss on 4000 labeled patients. We used a 2-layer classifier with 300 units and ReLU activations. Autoregressive models We used 10-dimensional hidden states, batch size 50 and learning rate 0.01. For ODE function in ODE-RNN, we used a 3-layer neural net with 50 units and Tanh activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder-decoder models</head><p>We used 20-dimensional latent state in generative model, 40-dimensional hidden state in recognition model, batch size 50 and learning rate 0.01. For ODE function, we used a 3-layer neural net with 30 units and Tanh activation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary figures and tables</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 3: Visualization of the inferred Poisson rate λ(t) (green line) for two selected features of different patients from the Physionet dataset. Vertical lines mark observation times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>(a) A Latent ODE model conditioned on a small subset of points. This model, trained on exactly 30 observations per time series, still correctly extrapolates when more observations are provided. (b) Trajectories sampled from the prior p(z 0 ) ∼ Normal z 0 ; 0, I of the trained model, then decoded into observation space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>(a) Approximate posterior samples from a Latent ODE trained with an RNN recognition network, as in Chen et al. [2018]. (b) Approximate posterior samples from a Latent ODE trained with an ODE-RNN recognition network (ours). At training time, the Latent ODE conditions on points in red area, and reconstruct points in blue area. At test time, we condition the model on 20 points in red area, and solve the generative ODE on a larger time interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top row: True trajectories from MuJoCo dataset. Second row: Trajectories reconstructed by a latent ODE model. Third row: Norm of the dynamics function f θ in the latent space of the latent ODE model. Fourth row: Norm of the hidden state of a RNN trained on the same dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Entropy of the approximate posterior over z 0 versus number of observed time points. The line shows the mean; shaded area shows 10% and 90% percentiles estimated Nonlinear projection of latent space of z 0 from a Latent ODE model trained on the MuJoCo dataset). Each point is the encoding of one time series. The points are colored by the (a) initial height (distance from the ground) (b) initial velocity in z-axis (c) relative initial position of the hip of the hopper. The latent state corresponds closely to the physical parameters of the true simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Observation times of a subset of features for one patient in the Physionet dataset. Black lines indicate observation times, whose number and timing vary across patients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Latent ODE with ODE encoder (ours): conditioned on 80 points Latent ODE with RNN encoder (Chen et al. 2018): conditioned on 80 points (a) Number of evaluations of ODE function f does not depend on the number of time points (t 0 ..t N ) where we evaluate ODE solution (b) Instead, number of ODE function evaluations depends on the length of the time interval [t 0 ..t N ] where ODE is solved. For plot (a) we randomly subsampled time series in MuJoCo dataset by variable number of points and computed number of function evaluations required by ODESolve in each case. For plot (b) we truncated the time series to (t 0 ..t i ) and computed number of ODE function evaluations for different i. Each line shows a number of ODE func evals for a single time series from MuJoCo dataset. Reconstructions on toy dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :Figure 8 :</head><label>568</label><figDesc>(a) Reconstructions on toy dataset from Latent ODE. Observations are shown as points. Lines are reconstructions for different samples of z 0 in Latent ODE model. (b) Corresponding latent state in the recognition model (first dimension). The recognition model encodes the data backwards in time (from right to left). The lines show latent ODE path in-between the encoded data points. The discontinuities between the paths show the update of the latent state using the observation at that time point. The end of the end of the ODE path from the previous observation is shown as a small circle. The updated state (and the start a new ODE path) is shown as cross. The shaded area shows the predicted standard deviation for the initial latent state z 0 . Notice that the right-most point is similar for all four trajectories -only one data point was encoded, which does not contain much information about the trajectory. As the encoding progresses (from right to left), the latent updated generally become smaller. True posterior p(z 0 |x 1 ..x N ) of Latent ODE trained on Physionet dataset with five features and two-dimensional latent space with and without poisson process likelihood. We train the model with 2dimensional latent space on a subset of first five attributes. We compute the unnormalized density of the true posterior using the Bayes rule: p(z 0 |x 1 ..x N ) ∝ p(z 0 )p(x 1 ..x N |z 0 ). Similarly, we train the model with poisson process likelihood in the same manner. The posterior distribution is clearly more narrow if trained without Examples of the inferred Poisson rate λ(t) (green line) for two selected features of different patients from the Physionet dataset. Vertical lines mark observation times. HR: heart rate; PaO2: partial pressure of arterial O2; NIDiasABP: noninvasive diastolic arterial blood pressure; MAP: invasive mean arterial blood pressure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Different encoder-decoder architectures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Definition of hidden state h(t) between observation. additionally updated by another network at each the hidden state is defined by an ODE, and is not change between updates. In ODE-RNNs, models. In standard RNNs, the hidden state does observation times t i−1 and t i in autoregressive</cell><cell>Model</cell><cell>State h(t i ) between observations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test Mean Squared Error (MSE) (×10 −2 ) on the MuJoCo dataset.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Interpolation (% Observed Pts.) 10% 20% 30% 50%</cell><cell cols="4">Extrapolation (% Observed Pts.) 10% 20% 30% 50%</cell></row><row><cell>Autoreg</cell><cell>RNN ∆t RNN GRU-D ODE-RNN (Ours)</cell><cell cols="5">2.454 1.714 1.250 0.785 1.968 1.421 1.134 0.748 38.130 20.041 13.049 7.259 6.792 6.594 1.647 1.209 0.986 0.665 13.508 31.950 15.465 26.463 30.571 5.833</cell></row><row><cell>Enc-Dec</cell><cell cols="2">RNN-VAE Latent ODE (RNN enc.) Latent ODE (ODE enc, ours) 0.360 0.295 0.300 0.285 6.514 6.408 6.305 6.100 2.477 0.578 2.768 0.447</cell><cell>2.378 1.663 1.441</cell><cell>2.135 1.653 1.400</cell><cell>2.021 1.485 1.175</cell><cell>1.782 1.377 1.258</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test MSE (mean ± std) on PhysioNet. Autoregressive models.</figDesc><table><row><cell>Model</cell><cell>Interp (×10 −3 )</cell></row><row><cell cols="2">RNN ∆ t RNN-Impute RNN-Decay RNN GRU-D ODE-RNN (Ours) 2.361 ± 0.086 3.520 ± 0.276 3.243 ± 0.275 3.215 ± 0.276 3.384 ± 0.274</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Test MSE (mean ± std) on PhysioNet.</figDesc><table><row><cell></cell><cell>Encoder-decoder models.</cell></row><row><cell>Model</cell><cell>Interp (×10 −3 ) Extrap (×10 −3 )</cell></row><row><cell cols="2">RNN-VAE Latent ODE (RNN enc.) 3.907 ± 0.252 3.162 ± 0.052 5.930 ± 0.249 3.055 ± 0.145 Latent ODE (ODE enc) 2.118 ± 0.271 2.231 ± 0.029 Latent ODE + Poisson 2.789 ± 0.771 2.208 ± 0.050</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Per-sequence classification.</figDesc><table><row><cell>AUC on Physionet.</cell><cell></cell></row><row><cell>Method</cell><cell>AUC</cell></row><row><cell cols="2">RNN ∆ t RNN-Impute RNN-Decay RNN GRU-D RNN-VAE Latent ODE (RNN enc.) 0.781 ± 0.018 0.787 ± 0.014 0.764 ± 0.016 0.807 ± 0.003 0.818 ± 0.008 0.515 ± 0.040 ODE-RNN 0.833 ± 0.009 Latent ODE (ODE enc) 0.829 ± 0.004 Latent ODE + Poisson 0.826 ± 0.007</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Per-time-point classification.</figDesc><table><row><cell cols="2">Accuracy on Human Activity.</cell></row><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell cols="2">RNN ∆ t RNN-Impute RNN-Decay RNN GRU-D RNN-VAE Latent ODE (RNN enc.) 0.835 ± 0.010 0.797 ± 0.003 0.795 ± 0.008 0.800 ± 0.010 0.806 ± 0.007 0.343 ± 0.040 ODE-RNN 0.829 ± 0.016 Latent ODE (ODE enc)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Task</cell><cell>Encoder</cell><cell>Approx. posterior Decoder</cell></row><row><cell>Interpolation</cell><cell>Backwards in time t N</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>4 ArchitectureODE function We use a feed-forward neural net for ODE function f . See section 5 for sizes of the network in each experiment. We used Tanh activations in ODE function. Tanh activation constrains the output and prevents the ODE gradients from taking large values. If values of ODE gradient are too big, it might be hard to solve an ODE with the specified tolerance. For this reason, we do not recommend using ReLU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>As such, we used 15-dimensional latent state in generative model and 30-dim recognition model for 14-dimensional Mujoco dataset. For 37dimensional physionet dataset we used 20 latent dimensions in the generative model and 40 in recognition model. For Human activity classification task (12 data dimentions), we used 15 latent dimensions in generative model and 100 in recognition model. Toy dataset To model 1-dimensional toy dataset, we used 10 latent dimensions in the generative model and 20 in recognition model, batch size of 50. ODE function for both generative and recognition models consist of 1 hidden layer with 100 units.MuJoCoIn encoder-decoder models, we used 15 latent dimensions in generative model, 30 dimensions in recognition model, batch size of 50. ODE functions had 3 layers and 500 units. We used 15-dimensional hidden state in autoregressive models.Physionet (interpolation and extrapolation) In encoder-decoder models, we used 20 latent dimensions in the generative model, 40 dimensions in recognition model and batch size of 50. ODE function have 3 layers with 50 units. We used 20-dimensional hidden state in autoregressive models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 2 :</head><label>2</label><figDesc>Mean squared error on the toy dataset</figDesc><table><row><cell>Interpolation</cell><cell>Extrapolation</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Chun-Hao Chang, Chris Cremer, Quaid Morris, and Ladislav Rampasek for helpful discussions and feedback. We thank the Vector Institute for providing computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Brits: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.10572" />
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks for Multivariate Time Series with Missing Values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-24271-9</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1259" />
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to detect sepsis with a multitask Gaussian process RNN classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Heller</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Directly modeling missing data in sequences with rnns: Improved classification of clinical time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Machine Learning for Healthcare Conference</title>
		<editor>Finale Doshi-Velez, Jim Fackler, David Kale, Byron Wallace, and Jenna Wiens</editor>
		<meeting>the 1st Machine Learning for Healthcare Conference<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="18" to="19" />
		</imprint>
	</monogr>
	<note>Children&apos;s Hospital LA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multivariate time series imputation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xiaojie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1596" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The neural hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6754" to="6764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discrete event, continuous time rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kazakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsey</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">V</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.04110" />
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Intensitätsschwankungen im fernsprechverker. Ericsson Technics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conny</forename><surname>Palm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable and accurate deep learning for electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Rajkomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Hajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mimi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Litsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mossin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-018-0029-1</idno>
	</analytic>
	<monogr>
		<title level="j">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpolation-prediction networks for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Satya Narayan Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marlin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1efr3C9Ym" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting In-Hospital Mortality of ICU Patients: The PhysioNet/Computing in Cardiology Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikaro</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<idno>2325-8861</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pubmed/24678516https://www.ncbi.nlm.nih.gov/pmc/PMC3965265/" />
	</analytic>
	<monogr>
		<title level="j">Computing in cardiology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="245" to="248" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00690</idno>
		<title level="m">DeepMind Control Suite. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Classification on Human Activity On Human Activity dataset, the task is to classify each time point by the type of activity</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Autoregressive models We used 30-dimensional hidden state, batch size 100 and learning rate 0.01. For ODE function, we used a neural net with four hidden layers and 1000 units</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">In recognition model, we used 100-dimensional hidden state and 4-layer neural net with 500 units for ODE function. We used batch size of 100 and learning rate 0</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Encoder-decoder models In the generative model, we used 15-dimensional latent state</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The code for generating and pre-processing of the datasets is included with the submission. The model is implemented in PyTorch 1.0. We used the ODE solvers from torchdiffeq package</title>
		<ptr target="http://www.cs.toronto.edu/~rtqichen/datasets/HopperPhysics/training.pt" />
	</analytic>
	<monogr>
		<title level="j">Datasets: Human Activity</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<ptr target="https://github.com/deepmind/dm_control" />
		<title level="m">MuJoCo dataset was created using DeepMind Control Suite</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
