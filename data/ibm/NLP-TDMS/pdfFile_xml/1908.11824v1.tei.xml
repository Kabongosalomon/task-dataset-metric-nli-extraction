<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reflective Decoding Network for Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
							<email>wenjiecoder@outlook.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<email>yuwingtai@tencent.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reflective Decoding Network for Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art image captioning methods mostly focus on improving visual features, less attention has been paid to utilizing the inherent properties of language to boost captioning performance. In this paper, we show that vocabulary coherence between words and syntactic paradigm of sentences are also important to generate high-quality image caption. Following the conventional encoder-decoder framework, we propose the Reflective Decoding Network (RDN) for image captioning, which enhances both the longsequence dependency and position perception of words in a caption decoder. Our model learns to collaboratively attend on both visual and textual features and meanwhile perceive each word's relative position in the sentence to maximize the information delivered in the generated caption. We evaluate the effectiveness of our RDN on the COCO image captioning datasets and achieve superior performance over the previous methods. Further experiments reveal that our approach is particularly advantageous for hard cases with complex scenes to describe by captions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of image captioning is to automatically generate fluent and informative language description of an image for human understanding. As an interdisciplinary task connecting Computer Vision and Nature Language Processing, it explores towards the cutting edge techniques of scene understanding <ref type="bibr" target="#b22">[23]</ref> and it is drawing increasing interests in recent years.</p><p>To build a top captioning system, there are two crucial requirements. First, the captioning model needs to distill representative and meaningful visual representation from an image. Thanks to the success in image classification <ref type="bibr" target="#b19">[20]</ref> and object recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>, recent methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> have shown significant advancements which mostly benefited from the improved quality of extracted visual features. Second, and the relatively neglected requirement, is to make the generated captions coherent and intelligent. Similar to the human language system, it needs to <ref type="bibr" target="#b0">1</ref> This work was done while Lei Ke was an intern at Tencent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basis decoder:</head><p>A black and white photo of a clock tower in the background.</p><p>Ours: A view of a bridge with a clock tower over a river.</p><p>A view of a bridge with a clock tower over a river. inference and reason during the generation process based on what has been generated and watched. Typically, this process is achieved by RNN (specifically, LSTM <ref type="bibr" target="#b13">[14]</ref>) in storing the sequential information during caption decoding. The traditional LSTM model, however, tends to focus more on the relatively closer vocabulary while neglecting the farther one. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the word 'bridge' has an important hint on predicting the word 'river' (which is neglected by the basis decoder), but the two words are separated by 6 words. Current mainstream caption decoder is weak in handling this kind of long-term dependency in sequential sentence, especially when the visual content of an image is complex and hard to describe, which usually leads to a general and less accurate caption description.</p><p>In this paper, we propose the Reflective Decoding Network (RDN) for image captioning, which mitigates the drawback of traditional caption decoder by enhancing its long sequential modeling ability. Different from previous methods which boost captioning performance by improving the visual attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref>, or by improving the encoder to supply more meaningful intermediate representation for the decoder <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>, our RDN focuses directly on the target decoding side and jointly apply attention mechanism in both visual and textual domain.</p><p>Besides, we propose to model the positional information of each word within a caption in a supervised way to capture the syntactic structure of natural language. Another advantage in RDN is to visualize how the model inferences and makes word prediction based on the generated words. For instance, our RDN successfully decodes the word 'river' in <ref type="figure" target="#fig_0">Figure 1</ref> by referring to the previously generated words, especially the most relevant word 'bridge'.</p><p>The main contributions of this paper are four folds:</p><p>• We propose the RDN that effectively enhances the long sequential modeling ability of the traditional caption decoder for generating high-quality image captions. • By considering long-term textual attention, we explicitly explore the coherence between words and visualize the word decision making process in text domain to show how we can interpret the principle and result of the framework from a novel perspective. • We design a novel positional module to enable our RDN to perceive the relative position of each word in the whole caption and thereby better comprehend the syntactic paradigm of natural language. • Our RDN achieves state-of-the-art performance on COCO captioning dataset and is particularly superior over existing methods in hard cases with complex scenes to describe by captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image Captioning. State-of-the-art captioning methods are mostly driven by advancements in machine translation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>, where the encoder-decoder framework has demonstrated to generate much more novel and coherent sentences compared to the traditional template-based <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b45">46]</ref> or search-based <ref type="bibr" target="#b8">[9]</ref> methods. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>, the authors introduced a framework which utilizes a pre-trained CNN as an encoder to extract image features, followed by an RNN as a decoder to generate image descriptions. This model was further improved by incorporating high-level semantic attribute information <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49]</ref> or regularizing the RNN decoder <ref type="bibr" target="#b5">[6]</ref>. To distill the salient objects or important regions from an image, different kinds of attention mechanisms were integrated into the captioning framework to exam the relevant image regions when generating sentences <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>. Fusion learning of multiple encoders or decoders forms an essential part of boosting image captioning performance. In <ref type="bibr" target="#b16">[17]</ref>, the authors utilized multiple CNNs to extract complementary image features, which forms a more informative and integrated representation for decoder. Yao et al. <ref type="bibr" target="#b47">[48]</ref> proposed GCN-LSTM to build two kinds of graphs to incorporate both semantic and spatial relations into the framework. The outputs from two different separately trained decoders are linearly fused to produce the final prediction.</p><p>Similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>, our RDN also utilizes the attention mechanism and follows the encoder-decoder framework. However, we explicitly study the coherence between words, which remedies the drawback of current captioning framework in modeling long-term dependency in decoder. Language Attention in joint vision and language tasks. Learning language attention has attracted increasing attention in other joint vision and language problems, such as VQA and grounding referential expressions. In <ref type="bibr" target="#b26">[27]</ref>, the authors proposed a model to jointly reason both visual and language attentions for visual question answering. Yu et al. <ref type="bibr" target="#b50">[51]</ref> attentively parsed the expressions into three phrase embeddings to address the task of referring expression comprehension. Different from them, image captioning task is a sequential language generation process. The target description of an image is unknown during inference stage. So, our RDN explores the language attention based on the generated words in previous states. With more time steps, the attended language content will increase dynamically, which enables the word predicted later to capture more useful information for reference. Language Attention in NLP tasks. Our RDN shares some ideas of the self-attention mechanism in machine translation models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, abstract summarization model <ref type="bibr" target="#b31">[32]</ref> and dialogue system <ref type="bibr" target="#b28">[29]</ref>. A typical self-attention model such as Transformer <ref type="bibr" target="#b39">[40]</ref> aims to learn a latent representation for each position of a sequence by referring to the whole context. In contrast, the Reflective Attention Module (RAM) of our RDN is designed as an attachable module which is seamlessly integrated into the recurrent decoding framework. Thanks to our special two-layer recurrent structure, our RAM collaborates smoothly with the visual attention component of our RDN by sharing the same query value to optimize the captioning process jointly, which is beneficial to ensure our generated captions match with the visual content of an image. To our knowledge, this paper is the first work in jointly exploring both visual and language attention in image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reflective Decoding Network</head><p>The overall architecture of our framework is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Given an input image, our model first uses Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> as Encoder to obtain the visual features of objects in the image. The visual features are then fed to the our Reflective Decoding Network (RDN) to generate caption. Our RDN contains three components: (1) Attention-based Recurrent Module, which attends to the visual features from Encoder; (2) Reflective Attention Module, which employs textual attention to model the compatibility between current and past decoding hidden states, thus it is able to capture more historical and comprehensive information for word decision; (3) Reflective Position Module, which introduces relative position information for each word in the generated caption and helps the model to perceive the syntactic struc- ture of sentences. RDN is able to tackle the long-term dependency difficulty in caption decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object-Level Encoder</head><p>The encoder in encoder-decoder framework aims to extract meaningful semantic representation from an input image. We leverage object detection module (Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>) with pretrained ResNet-101 <ref type="bibr" target="#b12">[13]</ref> to produce the region-level representation. The set of extracted regional visual representation R I of an image I are denoted as</p><formula xml:id="formula_0">R I = {r i } k i=1 , r i ∈R D ,</formula><p>where k denotes the number of extracted regions, D denotes the feature dimension of each region, and r i is the mean pooled convolutional feature within the extracted region. Compared to the conventional uniform meshing method on CNN features, the object-level encoder focuses more on salient objects/regions in an image that is closely related to the perception mechanism in human visual system <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reflective Decoder</head><p>Given a set of regional image features R I produced by encoder, the goal for the decoder is to generate the caption S, where S = {s 1 , s 2 , ..., s n } consisting of n words. The generated caption should not only capture the content information from the image but also be meaningful and coherent. Specifically, in <ref type="figure" target="#fig_1">Figure 2</ref>, the Attention-based Recurrent Module is employed to selectively attend to the detected regional features and serves the basic function of a captioning decoder while Reflective Attention Module and Reflective Position Module are designed above it as assistants to further enhance captioning quality. Thus, the complete Reflective Decoder is able to take both historical coherence between words and syntactic structure information into consideration while generating image captions.</p><p>Attention-based Recurrent Module includes the first LSTM layer and visual attention layer Att vis , which is designed mainly for top-down visual attention calculation. Its input x 1 t at time step t contains three concatenated parts, the mean-pooled image featurer = 1 k k i=1 r i , the embedding vector W e O t for current input word O t and the previous output h 2 t−1 from the second LSTM layer, wherer represents the contextual information of the given image, W e ∈R E×Do is the embedding matrix for the one-hot vec-tor O t , D o is the size of the captioning vocabulary and E is the embedding size. The formula for updating the LSTM units in the first layer is defined as :</p><formula xml:id="formula_1">h 1 t = LSTM(x 1 t , h 1 t−1 ), x 1 t = [r, W e O t , h 2 t−1 ]. (1)</formula><p>For the visual attention layer Att vis , given the generated h 1 t and the set of k image features R I = {r i } k i=1 , we calculate the normalized attention weight α vis t distribution over all the proposed object-level region denotes as :</p><formula xml:id="formula_2">α vis i,t = W 1 v tanh(W 1 rv r i + W 1 hv h 1 t ),<label>(2)</label></formula><formula xml:id="formula_3">α vis t = softmax(a vis t ), a vis t = α vis i,t k i=1 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W 1 v ∈ R 1×Dv , W 1 rv ∈ R Dv×D R , W 1 hv ∈ R Dv×D h are learned embedding matrices, α vis</formula><p>t denotes the calculated attention probability for each regional feature r i at time step t. So the attended feature is the weighted combination of each subregion,r t = k i=1 α vis i,t r i based on the weight distribution parameter a vis t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Reflective Attention Module.</head><p>The Reflective Attention Module contains reflective attention layer Att ref , combined with the second layer of LSTM, which is designed to output language description. Its input vector is concatenated by the attended feature resultr t and the hidden state h 1 t . Thus the formula for updating the LSTM units in the second layer of LSTM is denoted as :</p><formula xml:id="formula_5">h 2 t = LSTM(x 2 t , h 2 t−1 ), x 2 t = [r t , h 1 t ].<label>(4)</label></formula><p>Based on the current hidden state h 2 t at the time step t and the past hidden states set {h 2 1 , h 2 2 , ..., h 2 t−1 }, the reflective attention layer Att ref calculates the normalized weight distribution α ref t above all the generated t hidden states as shown in the top right of <ref type="figure" target="#fig_1">Figure 2</ref>. The formula is defined as :</p><formula xml:id="formula_6">α ref i,t = W 2 h tanh(W 2 h2h h 2 i + W 2 h1h h 1 t ),<label>(5)</label></formula><formula xml:id="formula_7">α ref t = softmax(a ref t ), a ref t = α ref i,t t i=1 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">W 2 h ∈ R 1×D f , W 2 h2h ∈ R D f ×D h , W 2 h1h ∈ R D f ×D h are three trainable matrices parameters, α ref t</formula><p>denotes the generated attention probability set for each hidden state h i in the set {h 2 i } t i=1 at time step t and α ref i,t reflects the relevance between the past predicted word at i-th step and current prediction (at t-th step) by measuring the compatibility between their corresponding hidden states. So we can calculate the attended hidden state resultĥ 2</p><formula xml:id="formula_9">t = t i=1 α ref i,t h 2 i . The reflective decoding outputĥ 2</formula><p>t of the top attention layer Att ref is utilized to predict the word s t under the conditional probability distribution :</p><formula xml:id="formula_10">p(s t |s 1:t−1 ) = softmax(W sĥ 2 t + b s ),<label>(7)</label></formula><p>where W s ∈ R Do×D h are the trainable weights and b s ∈ R Do are the biases. By calculating s t in this way, all the generated hidden states {h i } t i=1 play a role in word precision and their extent of contributions can be clearly visualized, as will be demonstrated in section 4.3.2.</p><p>It should be noted that our proposed Reflective Attention Module models the dependencies between pairs of words at different time steps explicitly, taking into account the corresponding hidden states. In contrast, LSTM memorizes the historical sequence information by balancing the overall relevance of all time steps instead of modeling the dependency for each pair of words specifically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Reflective Position Module.</head><p>It is often the case that many of the words have relatively fixed positions in a sentence due to the syntactic structure in natural language. For example, the numeral and subject words, i.e. 'a man' or 'a woman', mostly appear at the beginning of the sentence while the predicates tend to occupy the middle position. So we propose the Reflective Position Module by injecting the word position information during training as a guidance for the sequence decoding model to perceive its relative position or progress in the whole sentence. When decoding the t-th word, its actual relative position I t r and the predicted relative position I t p are calculated as :</p><formula xml:id="formula_11">I t r = t n , I t p = σ(W lĥ 2 t ),<label>(8)</label></formula><p>where n is the length of the sentence, σ is the sigmoid function and W l ∈ R 1×D h is the trainable relative position embedding matrix, respectively. The reflective position module shown in top left of <ref type="figure" target="#fig_1">Figure 2</ref> aims to minimize the difference between I t r and I t p by designing a loss function, which refines the attended hidden stateĥ 2 t mentioned in 3.2.1 by enabling it to perceive more sequential information of its relative position.</p><p>It is different from the popular position embedding methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref>, which add the absolute position embedding to the corresponding input features in each dimension. Our Reflective Position Module models the relative position information individually in a supervised way. A key benefit of this design is that it can avoid the potential inter-pollution between the regular input feature and the position embedding, and equip our model with a strong perception of relative position for each word in the caption. Thus, the syntactic structure in natural language can be well preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>Training. Two kinds of losses are utilized for optimizing our RDN model. The first is the cross entropy loss in traditional captioning training, which is to minimize the negative log likelihood:</p><formula xml:id="formula_12">L xe = −logp(S * |I) = − n t=2 logp(s * t |s * 1:t−1 ),<label>(9)</label></formula><p>where I is the given image, S * is the ground truth caption, formula for calculating p(s * t |s * 1:t−1 ) is defined in equation 7 and s * 0 is the start of the sentence. The second loss is defined as the Position-Perceptive Loss L pos :</p><formula xml:id="formula_13">L pos = n t=1 I t r − I t p 2 ,<label>(10)</label></formula><p>where I t r and I t p are the actual relative position and predicted relative position defined in Equation <ref type="bibr" target="#b7">8</ref> and L pos is designed to minimize the gap between them.</p><p>The objective function for optimizing our RDN is defined as :</p><formula xml:id="formula_14">L = L xe + λL pos .<label>(11)</label></formula><p>The trade-off parameter λ balances the contribution between the traditional caption loss in encoder-decoder framework and the Position-Perceptive Loss.</p><p>Inference. During the inference stage, since the length of the whole predicted sentence is unknown, the relative position information is removed from the input. As the discrepancy problem <ref type="bibr" target="#b21">[22]</ref> between training and inference, which means the previous ground truth captioning token is not available for inference, we use the previously predicted word as input instead of ground truth word as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45]</ref>. This method is called teacher forcing algorithm <ref type="bibr" target="#b42">[43]</ref>. Also, we adopt the popular beam search strategy which iteratively selects the top-k best sentences at time step t as candidates to generate the new top-k sentence at time t + 1 in our experiment instead of greedy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experimental Settings</head><p>COCO Dataset. COCO captions dataset <ref type="bibr" target="#b3">[4]</ref> contains 82,783 images for training and 40,504 images for validation. Each image has five corresponding human-annotated captions. Also, we adopt the 'Karpathy' splits setting <ref type="bibr" target="#b17">[18]</ref>, which includes 113,287 training images, 5K validation images and 5K testing images for offline evaluation. For the online server evaluation, the entire images and captions in dataset is used for training. Following the text preprocessing in <ref type="bibr" target="#b1">[2]</ref>, we convert all the captions to lower case and remove the less frequent words which occur less than 5 times, obtaining a captioning vocabulary of 10,010 words. Visual Genome Dataset. Visual Genome <ref type="bibr" target="#b18">[19]</ref> is a large dataset for modeling the interactions and relations between objects within an image. The dataset consists of 108K images with densely annotated objects, attributes and pairwise relations. Compared to <ref type="bibr" target="#b47">[48]</ref>, we only utilize the annotated object and attribute data from the dataset to pretrain the object-level encoder and discard the pairwise relation data. We follow the same data split setting in <ref type="bibr" target="#b1">[2]</ref> to include 98K images for training, 5K images respectively for validation and testing. After cleaning these annotated object and attribute strings, we obtain a dataset including 400 attributes and 1,600 objects classes to train our Faster R-CNN model. Evaluation Metrics. To objectively evaluate the performance of our captioning model, we use five widely accepted automatic evaluation metrics, including CIDEr <ref type="bibr" target="#b40">[41]</ref>, SPICE <ref type="bibr" target="#b0">[1]</ref>, BLEU <ref type="bibr" target="#b30">[31]</ref>, METEOR <ref type="bibr" target="#b7">[8]</ref> and ROUGE-L <ref type="bibr" target="#b23">[24]</ref>. Implementation Details. We implement our RDN using Caffe <ref type="bibr" target="#b15">[16]</ref>. To train the object-level encoder, we use the Faster R-CNN with ResNet-101 pre-trained for image classification on ImageNet <ref type="bibr" target="#b34">[35]</ref> and further refine it on the Visual Genome dataset. For each image, we set the IoU thresholds for region proposal suppression and object prediction to 0.7 and 0.3 respectively. For the remaining image subregions, we set a filter threshold 0.2. We rank the leftover boxes by their confidence scores from high to low and choose no more than top 100 as the final feature representations. Each region with dimension number 2,048 is the global average pooling result of the layer Res5c. We set the word embedding size and the hidden size in each LSTM layer to 1,000. The dimensions for attention layers Att vis and Att ref are set to 512 respectively. During training, the initial learning rate is set to 0.01 and the polynomial decay strategy is adopted to decline the effective learning rate to zero by 70k iterations using a batch size 100. We tune the trade-off parameter λ on the 'Karpathy' validation split to obtain the best performance and finally set it to 0.02. For data augmentation, we adopt it only during the online test server submission to boost performance by flipping the original image and randomly cropping 90%. During decoding process, the beam search size is set to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study on Reflective Modules</head><p>To study the effects of Reflective Attention Module and Reflective Position Module in our model, an ablation experiment is designed to compare the performance with following combinations: <ref type="formula">(1)</ref>  In <ref type="table">Table 1</ref> RDN ref performs obviously better than baseline, which shows the importance of model's ability to capture longterm dependency between words. In particular, with a suitable combination of the two modules, RDN achieves the best result with CIDEr score 115.7, BLEU-4 score 37.0 and BLEU-3 score 47.9, yielding the improvement over our baseline model by 2.0%, 2.2% and 1.5% respectively, which is a considerable advancement over the benchmark. Compared the baseline model with total 1.15B parameters, RDN has only 0.84% more in model size, which is neglectable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance Comparison and Analysis</head><p>We compare our proposed RDN with other state-of-theart image captioning methods considering different aspects both in offline and online situation. Latest and representative works include: (1) Adaptive <ref type="bibr" target="#b25">[26]</ref> which proposes the adaptive attention through designing a visual sentinel gate for captioning model to decide whether to attend to the image feature or just rely on the sequential language model, (2) LSTM-A3 <ref type="bibr" target="#b48">[49]</ref> which incorporates the high level semantic attribute information to the encoder-decoder model, (3) Up-Down <ref type="bibr" target="#b1">[2]</ref> which introduces the bottom-up and topdown attention mechanism to enable attention calculated at the level of objects or salient subregions and (4) RFNet <ref type="bibr" target="#b16">[17]</ref> which uses multiple kinds of CNNs to extract complementary image feature and generate a more informative repre-sentation for the decoder.</p><p>For fair comparison, our model and the baseline use standard ResNet-101 as basic architecture for encoder and all the reported results on test portions of MSCOCO 'Karpathy' splits are trained without additional CIDEr optimization <ref type="bibr" target="#b33">[34]</ref>. GCN-LSTM <ref type="bibr" target="#b47">[48]</ref> is not included because it uses the additional densely annotated pair-wise relation data between objects to pretrain semantic relation detector and build convolutional graphs. We only adopt CIDEr optimization strategy for the online server submission, since directly optimizing the CIDEr metric has little effect on perceived caption quality during human evaluation <ref type="bibr" target="#b24">[25]</ref> and small difference in its optimization implementation would influence the caption performance a lot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Quantitative Analysis</head><p>Offline Evaluation. For offline evaluation, we compare performance of different models on 'Karparthy' split dataset both in single and ensemble model situations. In <ref type="table">Table 2</ref>, it can be observed that our single RDN achieves the best results among all existing captioning methods across the six evaluation metrics including all the BLEU entries, ROUGE-L and CIDEr, performs on par with RFNet in SPICE and is slightly inferior to it in METEOR. Different from the previous captioning models (Up-Down, RFNet, Review Net, Adaptive, etc.) that boost performance through   extracting more indicative and compact visual representation, the enhancement of our captioning model only attributes to a better reasoning and inference ability of the decoder directly on the target side. Moreover, from the <ref type="table">Table 3</ref>, we can see that our ensembled RDN outperforms other ensemble models in most of evaluation metrics, with the highest CIDEr score 117.3, and performs only inferiorly to RFNet in METEOR and SPICE entry. RDN is the ensembling result of 6 single models with different random seed initialization while RFNet is composed of 4 RFNets with a total of 20 groups of different image representations.</p><p>Online Evaluation on COCO Testing Server. We also compare our model with the published state-of-the-art captioning systems on COCO Testing Server with 5(c5) and 40(c40) reference sentences as shown in <ref type="table" target="#tab_1">Table 4</ref>. Using the ensemble of 9 CIDEr optimized models, our RDN achieves leading performance over all metrics while performing on par with RFNet <ref type="bibr" target="#b16">[17]</ref>. Surprisingly, RFNet has a much better performance in online evaluation compared to the of-fline case, in which it performs much poorer than our model (even poorer than Up-Down <ref type="bibr" target="#b1">[2]</ref> in some cases) shown in <ref type="table">Table 2</ref>. Since the code of RFNet is not released, it is hard to investigate the inconsistence. Nevertheless, our RDN achieves the superior performance in all the c40 entries. Compared to c5, c40 has far more reference sentences and existing evaluation experiments show it achieves higher correlation with human judgement <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. Moreover, our model is more simple and elegant with only one encoderdecoder in single model compared to RFNet, which utilizes multiple encoders (ResNet, DenseNet <ref type="bibr" target="#b14">[15]</ref>, Inception-V3 <ref type="bibr" target="#b37">[38]</ref> , Inception-V4, and Inception-ResNet-V2 <ref type="bibr" target="#b36">[37]</ref>) to extract 5 groups of features and includes time-consuming feature fusion steps to produce the final thought vectors. Besides, our RDN boosts captioning performance by optimizing the decoding stage while RFNet mainly focuses on improving the encoder. Thus, it is a promising extension to apply the encoding mechanisms of RFNet to our RDN. Compared to Up-Down <ref type="bibr" target="#b1">[2]</ref>, which uses traditional LSTM and object-level encoder, the CIDER-c40, CIDER-c5 and METEOR-c40 are improved by 3.9%, 2.7% and 2.9%. Evaluation on hard Image Captioning. We further investigate the effect of the average length of annotations (ground truth captions) on the captioning performance, since generally the images with averagely longer annotations contain more complex scenes and thus are harder for captioning. Specifically, we rank the whole 'Karparthy' testset (5000 images) according to their average length of annotations in descending order and extract four different size of subsets (all set, top-1000, top-500, top-300 respectively). Smaller subset corresponds to averagely longer annotations and implies harder image captioning. <ref type="figure" target="#fig_4">Figure 3</ref> shows the comparison between our RDN and Up-Down <ref type="bibr" target="#b1">[2]</ref> (main difference of the two models is that Up-Down uses traditional LSTM). It reveals that the performance of both models are decreasing with the increasing average length of annotations, which reflects that the captioning is getting harder. However, our model exhibits more superiority over Up-Down in harder cases, which in turn validates the ability of our RDN to capture the long-term dependencies within captions. We also provide one such comparison between our RDN and Att2in <ref type="bibr" target="#b33">[34]</ref> on hard captioning in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reflective weight visualization</head><p>Basis decoder: a train that is sitting on the tracks Ours: a train that is sitting on the tracks at a station Basis decoder: a group of boats parked next to each other Ours: a group of boats docked in front of trees and buildings in the water Basis decoder: a bedroom with a bed and a desk Ours: a bedroom with a bed and a desk with a lamp Basis decoder: a group of people are standing in the water Ours: a group of people on a beach with some surfboards a train that is sitting on the tracks at a station a group of boats docked in front of trees and buildings in the water a bedroom with a bed and a desk with a lamp a group of people on a beach with some surfboards </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Qualitative Analysis</head><p>To investigate the physical interpretability of RDN model's improvement, some qualitative comparisons of captioning results are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. Compared to basis decoder, our RDN is able to generate more detailed and discriminative descriptions for images. Take the first case in <ref type="figure" target="#fig_5">Figure 4</ref> as an example, the basis decoder can provide a general and reasonable caption for the image. However, it cannot recognize the word 'station' which actually exists in the image while our model successfully infers it based on the previously generated words, especially the closely related words 'train','tracks' and 'sitting'. For the reflective weight visualization, the generated words with the largest contribution to the predicting word are usually strong related in vocabulary coherence, such as the correlations between words "boat" and "water", "beach" and "surfboards". We show additional results in supplementary material.</p><p>In <ref type="figure">Figure 5</ref>, compared to other captioning models, our RDN is able to predict the word and its relative position in the sentence simultaneously during caption generation. The predicted relative position in blue for each word is highly close to its actual relative position value in sentence, which demonstrates a good position-perceptive ability of our model to capture the syntactic structure of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel architecture, Reflective Decoding Network (RDN), which explicitly explores the co-GT: an older woman is talking on a cell phone RDN: an older lady sitting in a chair talking on a cell phone 10% 15% 23% 32% 41% 48% 57% 65% 70% 78% 89% 98% an older lady sitting in a chair talking on a cell phone 8% 17% 25% 33% 42% 50% 58% 67% 75% 83% 92% 100% Predict:</p><p>Actual: <ref type="figure">Figure 5</ref>. Top: Ground-truth caption and the example caption generated by our RDN model. Bottom: The predicted relative position value (shown in blue) from the Reflective Position Module and the actual relative position for each word in the sentence. All value reported in integer percentage value (%).</p><p>herence between words in the captioning sentence and enhances the long-term sequence inference ability of LSTM. Particularly, the attention mechanism applied in both visual and textual domain and the proposed position-perceptive scheme are to maximize the reference information available for captioning model. We also show how the learned attention in textual domain can provide interpretability during the captioning generation process from a new perspective. Extensive experiments conducted on standard and hard COCO image captioning dataset with superior performance validate the effectiveness of our proposal. For future work, we are interested in extending our model to source code captioning and text summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Top: Example captions generated by the basis decoder (using traditional LSTM) and our Reflective Decoding Network model. Bottom: The reflective attention weight distribution over the past generated hidden states is shown when predicting the word 'river'. The thicker line indicates a relatively larger weight and the red line means the largest contribution to the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our framework. Att ref in RAM (Reflective Attention Module) is the attention layer used to selectively attend to the generated decoding hidden states, Attvis layer in Attention-based Recurrent Module determines the attention distribution over the detected image regions. I t p and I t r in RPM (Reflective Position Module) are respectively the t-th predicted and actual relative position in sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Baseline: the baseline denotes the RDN without Reflective Attention Module and Reflective Position Module; (2) RDN pos : the RDN with the Reflective Attention Module removed, with only position module reserved, the number of attention layers in decoder is reduced to one; (3) RDN ref : the RDN with the Reflective Position Module removed, cutting down the relative position information input; (4) RDN: the complete RDN implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Performance comparison between our RDN model and Up-Down<ref type="bibr" target="#b1">[2]</ref> on hard Image Captioning as a function of average length of annotations (ground truth captions). We rank the 'Karpathy' test set according to their average length of annotations in descending order and extract four different size of subsets. Smaller subset corresponds to averagely longer annotations and harder captioning. It reveals that our model exhibits more superiority over Up-Down in harder cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Examples of captions generated by our RDN compared to the basis decoder (using traditional LSTM) and their reflective attention weight distribution over the past generated hidden states when predicting the key words highlighted in green. The thicker line indicates a relatively larger weight and the red line means the largest contribution. More examples are provided in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .Table 3</head><label>123</label><figDesc>, it can be observed that both the Reflective Position Module and Reflective Attention Module are important for our model and RDN improves the caption performance over all the metrics compared to baseline. The fact that RDN pos outperforms baseline model validates the contribution of Reflective Position Module to enhance the quality of decoding hidden states during caption generation. Also, by injecting the Reflective Attention Module, Ablation study on COCO 'Karpathy' test split on single model. Our experiments show the contribution for reflective attention and position module, respectively. Results are obtained with beam size 5 without CIDEr optimization. All value reported in percentage (%). Performance comparison on MSCOCO 'Karpathy' test split on single model. All image captioning models trained without optimizing CIDEr metric. (−) indicates the metric is not provided.</figDesc><table><row><cell>Model</cell><cell cols="9">BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr SPICE</cell></row><row><cell>Baseline</cell><cell></cell><cell>77.0</cell><cell>61.3</cell><cell>47.2</cell><cell>36.1</cell><cell>26.8</cell><cell>56.1</cell><cell>113.2</cell><cell>20.1</cell></row><row><cell>RDNpos</cell><cell></cell><cell>77.4</cell><cell>61.6</cell><cell>47.5</cell><cell>36.3</cell><cell>27.0</cell><cell>56.5</cell><cell>114.3</cell><cell>20.4</cell></row><row><cell>RDN ref</cell><cell></cell><cell>77.6</cell><cell>61.6</cell><cell>47.4</cell><cell>36.3</cell><cell>27.1</cell><cell>56.7</cell><cell>115.0</cell><cell>20.5</cell></row><row><cell>RDN</cell><cell></cell><cell>77.5</cell><cell>61.8</cell><cell>47.9</cell><cell>36.8</cell><cell>27.2</cell><cell>56.8</cell><cell>115.3</cell><cell>20.5</cell></row><row><cell>Model</cell><cell></cell><cell cols="8">BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr SPICE</cell></row><row><cell cols="2">Review Net [47]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.0</cell><cell>23.7</cell><cell>-</cell><cell>88.6</cell><cell>-</cell></row><row><cell cols="2">LSTM-A3 [49]</cell><cell>73.5</cell><cell>56.6</cell><cell>42.9</cell><cell>32.4</cell><cell>25.5</cell><cell>53.9</cell><cell>99.8</cell><cell>18.5</cell></row><row><cell>Att2in [34]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.3</cell><cell>26.0</cell><cell>54.3</cell><cell>101.3</cell><cell>-</cell></row><row><cell>Adaptive [26]</cell><cell></cell><cell>74.2</cell><cell>58.0</cell><cell>43.9</cell><cell>33.2</cell><cell>26.6</cell><cell>-</cell><cell>108.5</cell><cell>-</cell></row><row><cell>Up-Down [2]</cell><cell></cell><cell>77.2</cell><cell>-</cell><cell>-</cell><cell>36.2</cell><cell>27.0</cell><cell>56.4</cell><cell>113.5</cell><cell>20.3</cell></row><row><cell>RFNet [17]</cell><cell></cell><cell>76.4</cell><cell>60.4</cell><cell>46.6</cell><cell>35.8</cell><cell>27.4</cell><cell>56.5</cell><cell>112.5</cell><cell>20.5</cell></row><row><cell>RDN</cell><cell></cell><cell>77.5</cell><cell>61.8</cell><cell>47.9</cell><cell>36.8</cell><cell>27.2</cell><cell>56.8</cell><cell>115.3</cell><cell>20.5</cell></row><row><cell>Model</cell><cell></cell><cell cols="8">BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr SPICE</cell></row><row><cell>NIC [42]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.1</cell><cell>25.7</cell><cell>-</cell><cell>99.8</cell><cell>-</cell></row><row><cell>Att2in [34]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.8</cell><cell>26.7</cell><cell>55.1</cell><cell>106.5</cell><cell>-</cell></row><row><cell cols="2">Review Net [47]</cell><cell>76.7</cell><cell>60.9</cell><cell>47.3</cell><cell>36.6</cell><cell>27.4</cell><cell>56.8</cell><cell>113.4</cell><cell>20.3</cell></row><row><cell>RFNet [17]</cell><cell></cell><cell>77.4</cell><cell>61.6</cell><cell>47.9</cell><cell>37.0</cell><cell>27.9</cell><cell>57.3</cell><cell>116.3</cell><cell>20.8</cell></row><row><cell>RDN</cell><cell></cell><cell>77.6</cell><cell>62.2</cell><cell>48.6</cell><cell>37.8</cell><cell>27.5</cell><cell>57.4</cell><cell>117.3</cell><cell>20.6</cell></row></table><note>. Performance comparison on MSCOCO 'Karpathy' test split on ensemble models trained with cross entropy loss. Our model is the ensembling result of 6 single models initialized with different random seeds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison of published image captioning models on COCO Leaderboard. RDN achieves superior performance when comparing to other state-of-the-art methods. Top-3 rankings are indicated by red footnote for each metric.</figDesc><table><row><cell>Model</cell><cell>BLEU-1 c5 c40</cell><cell>BLEU-4 c5 c40</cell><cell>METEOR c5 c40</cell><cell>ROUGE-L c5 c40</cell><cell>c5</cell><cell cols="2">CIDEr c40</cell><cell>SPICE c5 c40</cell></row><row><cell>NIC [42]</cell><cell cols="4">71.3 89.5 30.9 58.7 25.4 34.6 53.0 68.2</cell><cell cols="2">94.3</cell><cell cols="2">94.6 18.2 63.6</cell></row><row><cell cols="5">Review Net [47] 72.0 90.0 31.3 59.7 25.6 34.7 53.3 68.6</cell><cell cols="2">96.5</cell><cell cols="2">96.9 18.5 64.9</cell></row><row><cell>LSTM-A3 [49]</cell><cell cols="7">78.7 93.7 35.6 65.2 27.0 35.4 56.4 70.5 116.0 118.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Adaptive [26]</cell><cell cols="8">74.8 92.0 33.6 63.7 26.4 35.9 55.0 70.5 104.2 105.9 19.7 67.3</cell></row><row><cell>Att2all [34]</cell><cell cols="7">78.1 93.7 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Up-Down [2]</cell><cell cols="7">80.22 95.22 36.93 68.53 27.63 36.73 57.13 72.43 117.93 120.53</cell><cell>-</cell><cell>-</cell></row><row><cell>RFNet [17]</cell><cell cols="7">80.41 95.03 38.01 69.22 28.21 37.22 58.21 73.12 122.91 125.12</cell><cell>-</cell><cell>-</cell></row><row><cell>RDN</cell><cell cols="7">80.22 95.31 37.32 69.51 28.12 37.81 57.42 73.31 121.22 125.21</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularizing rnns for caption generation by reconstructing the past with the present</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01809</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent fusion network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alias</forename><surname>Parth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards total scene understanding: Classification, annotation and segmentation in an automatic framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Optimization of image description metrics using policy gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1612.00370</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coherent dialogue with attention-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-attentive residual decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich Werlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Recurrent memory networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01272</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Lik</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
