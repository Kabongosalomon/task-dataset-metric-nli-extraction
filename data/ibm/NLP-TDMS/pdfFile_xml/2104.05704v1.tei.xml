<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Escaping the Big Data Paradigm with Compact Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Escaping the Big Data Paradigm with Compact Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rise of Transformers as the standard for language processing, and their advancements in computer vision, along with their unprecedented size and amounts of training data, many have come to believe that they are not suitable for small sets of data. This trend leads to great concerns, including but not limited to: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we dispel the myth that transformers are "data hungry" and therefore can only be applied to large sets of data. We show for the first time that with the right size and tokenization, transformers can perform head-to-head with state-of-the-art CNNs on small datasets. Our model eliminates the requirement for class token and positional embeddings through a novel sequence pooling strategy and the use of convolutions. We show that compared to CNNs, our compact transformers have fewer parameters and MACs, while obtaining similar accuracies. Our method is flexible in terms of model size, and can have as little as 0.28 M parameters and achieve reasonable results. It can reach an accuracy of 94.72% when training from scratch on CIFAR-10, which is comparable with modern CNN based approaches, and a significant improvement over previous Transformer based models. Our simple and compact design democratizes transformers by making them accessible to those equipped with basic computing resources and/or dealing with important small datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers have rapidly been increasing in popularity and become a major focus of modern machine learning research. With the advent of Attention is All You Need <ref type="bibr" target="#b30">[31]</ref>, the community saw an explosion in transformer and attention based research. While this work originated in natural language processing, these models have been applied to other fields, such as computer vision. Vision Transformer * Equal contribution. Our code and pre-trained models will be made publicly available at https://github.com/SHI-Labs/Compact-Transformers Convolution · · · CCT can be quickly be trained from scratch on small datasets, while achieving high accuracy (in under 30 minutes one can get 90% on an NVIDIA 2080Ti GPU or 80% on an AMD 5900X CPU). With CCT the class token becomes unnecessary and positional embedding becomes optional, resulting in only slightly different accuracy.</p><p>(ViT) <ref type="bibr" target="#b6">[7]</ref> was the first major demonstration of a pure transformer backbone being applied to computer vision tasks. With this explosion in research, we've also seen an explosion in model sizes and datasets. It has become a common understanding that transformers are data hungry models and that they need sufficiently large datasets to perform as well or better than their convolutional counterparts. ViT authors argued that "Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.".</p><p>This "data hungry" paradigm has made transformers intractable for many types of pressing problems, where there are typically several orders of magnitude less data. It also limits major contributions in the research to those with vast computational resources. Reducing machine learning's dependence on large sums of data is important, as many do-mains, such as science and medicine, would hardly have datasets the size of ImageNet. This is because events are far more rare and it would be more difficult to properly assign labels, let alone create a set of data has low bias and is appropriate for conventional deep neural networks. In medical research, for instance, it may be difficult to compile positive samples of images for a rare disease without other correlating factors, such as medical equipment being attached to patients who are actively being treated. Additionally, for a sufficiently rare disease there may only be a few thousand images for positive samples, which is typically not enough to train a network with good statistical prediction unless it can sufficiently be pre-trained on data with similar attributes. This inability to handle smaller datasets has impacted the scientific community where they are much more limited in the models and tools that they are able to explore. Frequently, problems in scientific domains have little in common with domains of pre-trained models and when domains are sufficiently distinct pre-training can have little to no effect on the performance within a new domain <ref type="bibr" target="#b39">[40]</ref>. Additionally, it has been shown that performing strongly on ImageNet does not necessarily mean that the model will perform strongly in other domains, such as medicine <ref type="bibr" target="#b15">[16]</ref>. A non-"data hungry" transformer would allow these domains to utilize the advantages of long range interdependence that self-attention provides within their research and applications. This makes it important to build more efficient models that can be effective in less data intensive domains and allow for datasets that are orders of magnitude less than those conventionally seen in computer vision and NLP problems.</p><p>Additionally, the requisite of large data results in a requisite of large computational resources and this subsequently prevents many researchers from being able to provide insight. This not only limits the ability to apply models in different domains, but also limits reproducibility, making our field vulnerable to the reproducibility crisis. Verification of state of the art machine learning algorithms should not be limited to those with large infrastructures and computational resources. Throughout the history of research, we have learned that the speed of scientific advancement is directly proportional to the number of researchers, making it essential to enable researchers to participate in all areas of research.</p><p>On the other end of the spectrum, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b18">[19]</ref> became the standard for computer vision, since the success of AlexNet <ref type="bibr" target="#b17">[18]</ref>. AlexNet showed that convolutions are adept at vision based problems due to their invariance to spacial translations as well as having low relational inductive bias. He et al. <ref type="bibr" target="#b10">[11]</ref> extended this work by introducing residual connections, allowing for significantly deeper models to perform efficiently. Convolutions leverage three important concepts that lead to their ef-ficiency: sparse interaction, weight sharing, and equivariant representations <ref type="bibr" target="#b8">[9]</ref>. Translation equivariance and translational invariance are due to the convolutions and pooling layers, respectively <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. They allow CNNs to leverage natural image statistics and subsequently allow models to have higher sampling efficiency <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b24">25]</ref>. Conventionally, CNNs have commonly been used for works on smaller datasets because they have been shown to be the best performers in this regime. Additionally, CNNs are more efficient, both computationally and in terms of memory, when compared to transformers They require less time and data to train while also requiring a lower number of parameters to accurately fit data. The above properties lead them to be effective for these areas, but they come with limitations in long range interdependence that attention mechanisms provide.</p><p>Both Transformers and CNNs have highly desirable qualities for statistical inference and prediction, but each comes with their own costs. In this work, we try to bridge the gap between these two architectures and develop an architecture that can both attend to important features within images, while also being spatially invariant, where we have sparse interactions and weight sharing. This allows for a Transformer based model to be trained, from scratch, on small datasets like CIFAR-10, CIFAR-100, and MNIST while providing competitive results.</p><p>In this paper we introduce ViT-Lite, a smaller and more compact version of ViT, which can obtain over 90% accuracy with correct patch sizing. We expand on ViT-Lite by adding using our novel sequence pooling and forming the Compact Vision Transformer (CVT), and further iterate by adding convolutional blocks to the tokenization step and thus creating the Compact Convolutional Transformer (CCT). Both of these additions add to significant increases in performance, leading to a top-1%accuracy of 94.72% on CIFAR-10. This is only slightly behind the top performing CNN based algorithm, ResNet1001-v2, which obtains an accuracy of 95.38% but with 2.7× model size and 1.6× GMACs comparing to ours. Our model outperforms all previous transformer based models within this domain. Additionally, we show that our model can be lightweight, only needing 0.28 million parameters and still reach close to 90% top-1% accuracy on CIFAR-10.</p><p>The main contributions of this paper are:</p><p>• Dispelling the myth of "data hungry" transformers by introducing ViT-Lite, which can be trained efficiently with high accuracy on small datasets such as CIFAR-10.</p><p>• Introducing Compact Vision Transformer (CVT) with a novel sequence pooling stratgey, which removes the need of the conventional class token design in vision transformers and make it more accurate.</p><p>• Introducing Compact Convolutional Transformer (CCT) to increase performance and provide flexibility for input image sizes while also demonstrating that these variants do not depend as much on Positional Embedding compared to the rest.</p><p>In addition, we demonstrate that our CCT model is extremely fast, obtaining 90% accuracy on CIFAR-10 using a single NVIDIA 2080Ti GPU and 80% when trained on a CPU (AMD 5900X), both in under 30 minutes. Additionally, since our model has a relatively small number of parameters we can train on the majority of GPUs, even if researchers do not have access to top of the line hardware. Through these efforts, we aim to democratise transformer research and make it more accessible to the average person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Since the advent of deep neural networks, CNNs have predominantly been used for visual recognition. In NLP based research, attention mechanisms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23]</ref> gained popularity for their ability to weigh different features within sequential data. In the past, many researchers leveraged a combination of attention and convolutions in neural networks for visual tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. Ramachandran et al. <ref type="bibr" target="#b23">[24]</ref> introduced the first a vision model that relies purely on an attention mechanism. Following this success, transformers have been used in a wide variety of tasks, including: machine translation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>, visual question answering <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>, action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>, and the like. Dosovitskiy et al. <ref type="bibr" target="#b6">[7]</ref> introduced the first stand-alone transformer based model (ViT) for image classification. In the following subsections we briefly revisit ViT and several related works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision Transformer</head><p>Vision Transformers (ViT) were introduced as a way to compete with CNNs on image classification tasks and utilize the benefits of attention within these networks. The motivation was because attention has many desirable properties for a network, specifically its ability to make long range connections. Dosovitskiy et al. showed that large scale training triumphs over the advantage of inductive bias that CNNs have, allowing their model to be competitive with CNN based architectures given sufficiently large amount of training data. ViT is composed of several parts: Image Tokenization, Positional Embedding, Classification Token, the Transformer Encoder, and a Classification Head. These subjects are discussed in more detail below.</p><p>Image Tokenization: A standard transformer takes as input a sequence of vectors, called tokens. For traditional NLP based transformers the word ordering provides a natural order to sequence the data, but this is not so obvious for images. To tokenize an image, ViT subdivides an image into non-overlapping square patches and orders them from top left to bottom right. The sequence of patches, x p ∈ R H×(P 2 C) with patch size P , are flattened into 1D vectors and transformed into latent vectors of dimension d, using a linear layer to obtain the token embeddings. This simple patching method has a few limitations, in particular information is lost along the boundary regions.</p><p>Positional Embedding: Positional embedding is a method that adds spatial information into the network. Since the model does not actually know anything about the spatial relationship between tokens we must add some extra information. Typically this is either a learned embedding or tokens are given weights from two sine waves with high frequencies, which is sufficient for the model to learn that there exists a positional relationship between these tokens.</p><p>Classification Token: Vision transformers typically add an extra learnable [class] token to the sequence of the embedded patches, representing the class parameter of an entire image and its state after transformer encoder can be used for classification.</p><p>[class] token contains the latent information that is later used for classification. This [class] token design originated from BERT <ref type="bibr" target="#b5">[6]</ref>.</p><p>Transformer Encoder: A transformer encoder consists of a series of stacked encoding layers. Each encoder layer is comprised of two sub-layers: Multi-headed Self-Attention (MSA) and a Multi-Layer Perceptron (MLP) head. Each sub-layer is preceded by a layer normalization (LN), and followed by a residual connection to the next sub-layer. Considering an input sequence x, the output y, of a single encoder layer (ViT's notation) can be obtained as follows:</p><formula xml:id="formula_0">x = x −1 + MSA(LN(x −1 )) ∈ {1, 2, ..., L} (1) x = x + MLP(LN(x ))<label>(2)</label></formula><p>Classification: Similar to BERT <ref type="bibr" target="#b5">[6]</ref>, the position of the [class] token yields the location of the image representation. A linear layer is then applied to this representation to obtain a confidence score for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data-Efficient Transformers</head><p>In an effort to reduce the dependence upon data, Data-Efficient Image Transformers (DeiT) <ref type="bibr" target="#b29">[30]</ref> use knowledge distillation to improve the classification performance of the vision transformer. This work builds off of Xu et al. <ref type="bibr" target="#b33">[34]</ref>, which optimizes transformers for smaller linguistic datasets by introducing a novel initialization technique inspired by Huang et al. <ref type="bibr" target="#b14">[15]</ref>. DeiT's work focuses on reducing the data dependence of vision transformers, showing that this model could perform well on mid-sized dataset like Ima-geNet, as well as fine tune on small datasets like CIFAR-10. This work pushes forward accessibility of transformers in smaller dataset domains but makes an assumption that pretrained models are accessible and that the smaller datasets share enough attributes for fine tuning to work. However, if a small dataset happens to be sufficiently novel, then the pre-trained model will not help train on that domain and the model will not be appropriate for that dataset.</p><p>Yuan et al. <ref type="bibr" target="#b36">[37]</ref> proposed Tokens-to-token ViT (T2T-ViT), which adopts a layer-wise tokenization strategy in which overlapping image patches are flattened, sent through self-attention layers, and reshaped back into patches. This strategy allows their network to model local structures, including along the boundaries between patches. Additionally, this process can be repeated multiple times and the final outputs are flattened into a sequence of vectors. This repetition of attention allows for a feature rich and learnable tokenization that allows T2T-ViT to obtain high accuracy on ImageNet. T2T-ViT differs from our work in that it focuses on medium sized datasets like ImageNet, which are not only far too large for many research problems in science and medicine but also resource demanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Other Concurrent Works</head><p>Many concurrent works have been recently released on arXiv to improve vision transformers and eliminate the need to pretrain on super large datasets like JFT-300M <ref type="bibr" target="#b28">[29]</ref> that is not accessible by the large community. ConViT <ref type="bibr" target="#b4">[5]</ref>, make an effort to combine the strength of CNNs and transformer based models. They introduce a gated positional self-attention (GPSA) that allows for a "soft" convolutional inductive bias within their model. This GPSA allows their network to have more flexibility with respect to positional information. Since their GPSA is able to be initialized as a convolutional layer, this allows their network to sometimes have the properties of convolutions or alternatively having the properties of attention. Their gating parameter can be adjusted by the network allowing it to become more expressive and adapt to the needs of the dataset. Convolutionenhanced image Transformers (Ceit) <ref type="bibr" target="#b35">[36]</ref> uses a convolutions to extract low level features. They use an image-totoken module to extract this information, introduce a locally enhanced feed-forward layer that processes the spatial information form the extracted tokens, and utilize a layerwise cross attention mechanism. This allows them to create a network that is competitive with DeiT on ImageNet. Convolutional vision Transformer (CvT) <ref type="bibr" target="#b32">[33]</ref> also uses a convolutional layer for the tokenization process. Wu et al. uses these convolutional projections instead of the standard linear projections for their embeddings into the encoding layer of the transformer. This allows them to achieve competitive results to DeiT. All of these works report results when trained from scratch on ImageNet or larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Comparison</head><p>Our work differs from the aforementioned in several ways but our work focuses on answering the follow-ing question: Can vision transformers be trained from scratch on small datasets? Focusing on a small datasets we seek to create a model that can be trained, from scratch, on datasets that are orders of magnitude smaller than Im-ageNet. Having a model that is compact, small in size, and efficient allows greater accessibility, as training on Im-ageNet is still a difficult and data intensive task for many researchers. Thus our focus is on an accessible model, with few parameters, that can quickly and efficiently be trained on smaller platforms while still maintaining SOTA results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In order to dispel the myth that vision transformers are only applicable when dealing with large sets of data, we propose three different models: ViT-Lite, Compact Vision Transformers (CVT), and Compact Convolutional Transformers (CCT). ViT-Lite is near identical to the original ViT, but with more suitable smaller patch sizing for small datasets. In other words, an image is not always worth 16×16 words <ref type="bibr" target="#b6">[7]</ref> and a suitable size matters. CVT builds on this by introducing a sequential pooling method, called Se-qPool, that pools the sequential based information that results from the transformer encoder. SeqPool eliminates the need for the extra Classification Token(Sec 3.3). Lastly, we have CCT which introduces a convolutional based patching method, which preserves local information and is able to encode relationships between patches, unlike the original ViT. A detailed modular-level comparison of these models can be viewed in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Our convolution based model, CCT, consist of convolutions with small strides that allow for efficient tokenization and preserves local spatial relationships. Furthermore, we eliminate the need for the [class] token in CCT (also eliminated in CVT), allowing for greater flexibility within our model. While simple in design, we show that our network can be quite effective if composed in the correct manner. To further differentiate the subtle differences between CVT and CCT we provide <ref type="figure" target="#fig_3">Figure 3</ref> which outlines the similarities and differences of the networks. The components of our compact transformers are further discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutional Block</head><p>In order to introduce an inductive bias into the model, we replace the "image patching" and "embedding" layers in ViT with a simple convolutional block. Our convolutional block follows conventional design, which consists of a single convolution layer, ReLU activation, and a max pool operation. Given an image x ∈ R H×W ×C :</p><formula xml:id="formula_1">x 0 = MaxPool(ReLU(Conv2d(x)))<label>(3)</label></formula><p>where the Conv2d operation has d filters, same number as the embedding dimension of the transformer backbone. The  number of these blocks can be to more than 1. When iterating this process, we use 64 filters for the earlier convolution operations, and d filters for the final one. A single-block convolution block is presented in Algorithm 1. By using this convolutional block we gain some added flexibility over models like ViT. While ViT patches the image, we seek to use convolutions to embed the image into a latent representation that will be more efficient for the transformer. Changes in image size do not affect the number of parameters, but do affect the sequence length and subsequently the amount of computation needed. Although, ViT's patching requires that both the image height and width be divisible by the patch size. This allows us to take as input different size data without the need for cropping or padding. In other words, our CCT model does not have the requirement of evenly subdividing an image into patches, as ViT or CVT. Additionally, the convolution and max pool operations can be overlapping, which also increases the sequence length, but on the other hand, could increase performance by injecting inductive bias. Having these convolutional patches allows our model to maintain locally spatial information. This also gives us more flexibility toward removing the positional embedding in our model, as it manages to maintain a very good performance. This is further discussed in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer-based Backbone</head><p>In terms of model design, we follow the original Vision Transformer <ref type="bibr" target="#b6">[7]</ref>, and original Transformer <ref type="bibr" target="#b30">[31]</ref>, relatively closely. As mentioned in Section 2.1, the encoder consists of transformer blocks, each including a MSA layer and a MLP head. Simillar to ViT, we apply layer norm after the positional embeddings are added. The transformer encoder uses Layer Normalization, GELU activation, and dropout. The positional embeddings are learnable by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SeqPool</head><p>In order to map the sequential outputs to a singular class index, instead of using a class token (like BERT <ref type="bibr" target="#b5">[6]</ref>, ViT <ref type="bibr" target="#b6">[7]</ref>, and most other transformer-based classifiers), we propose Sequence Pooling. As the name suggests, we pool the outputs of the transformer backbone across the sequence. We pool over the entire sequence of data since it contains relevant information across different parts of the input image, making our model compact. We can think of this operation as the mapping transformation T : R b×n×d → R b×d . Given:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>· · ·</head><formula xml:id="formula_2">x L = f(x 0 ) ∈ R b×n×d where x L or f(x 0 )</formula><p>is the output of an L layer transformer encoder, b is the mini-batch size, n is the sequence length, and d is the embedding dimension, we feed x L to a linear layer g(x L ) ∈ R d×1 and apply softmax activation:</p><formula xml:id="formula_3">x L = softmax g(x L ) T ∈ R b×1×n</formula><p>Then, we simply compute: <ref type="formula">(4)</ref> and by pooling the second dimension we are left with z ∈ R b×d . Then this output can be sent through a linear classifier, like most other such networks. This SeqPool allows for our network to weigh the sequential embeddings of the latent space produced by the transformer encoder and better correlate data across the input data. This can be thought of this as attending to the sequential data, where we are assigning importance weights across the sequence of data. We have tested several variations of this pooling method, including learnable and static methods, and found that the learnable pooling performs the best. We believe that the learnable weighting is more efficient because each embedded patch does not contain the same amount of entropy. This allows our model to give higher weight to patches that contain more information relevant to the classifier. Additionally, the sequence pooling allows our model to better utilize information across spatially sparse data. We will further study the effects of this pooling in the ablation study (Sec 4.3).</p><formula xml:id="formula_4">z = x L x L = softmax g(x L ) T × x L ∈ R b×1×d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Small and Compact Models</head><p>We propose some smaller and more compact transformer models compared to ViT, and use them alongside the sizes that they've already proposed. In our notation, we use the number of layers to specify size, as well as the kernel size for our convolutional layers: for instance, CCT-12/7×2 means that the variant's transformer encoder has 12 layers, and uses 2 convolutional blocks with 7×7 convolutions. We follow the same format for other models: ViT-Lite, which is like ViT but with smaller patches and CVT which is like ViT but with SeqPool (making it compact). We further summarize these models in Appendix C, <ref type="table">Table 7</ref>, and those with "smaller" convolutions which are more suitable for smallresolution images are summarized in <ref type="table">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted image classification experiments using our method on the following datasets: CIFAR-10, CIFAR-100, MNIST, and Fashion-MNIST. These four datasets not only have a small number of training samples, but they are also small in resolution. Additionally, MNIST and Fashion-MNIST only contain a single channel, greatly reducing the information density.</p><p>In all experiments, we conducted a hyperparamter search for every different method and report the best results we were able to achieve. We provide a detailed report on the hyperparamter settings in Appendix A. All reported top-1% accuracy values are best out of 4 runs. Unless stated otherwise, all tests were run for 200 epochs with a batch size of 128, and the learning rate is reduced per epoch based on cosine annealing <ref type="bibr" target="#b20">[21]</ref>. Label smoothing with a probability of 0.1 was also used during training. All methods are warmed up for either 5 or 10 epochs (which will be speci-fied in Appendix A). In all experiments (unless stated otherwise), CIFAR-specific augmentations computed by Au-toAugment <ref type="bibr" target="#b3">[4]</ref> are used for CIFAR-10 and CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance Comparison</head><p>In order to argue that vision transformers can be as effective as convolutional neural networks even in settings with small sets of data, we compare our compact transformers to ResNets <ref type="bibr" target="#b10">[11]</ref>, which are still very useful CNNs for small to medium amounts of data, as well as to MobileNetV2 <ref type="bibr" target="#b25">[26]</ref>, which are very compact and small-sized CNNs. We also compare with results from <ref type="bibr" target="#b11">[12]</ref> where He et al. designed very deep (up to 1001 layers) CNNs specifically for CIFAR. The results are presented in <ref type="table" target="#tab_1">Table 1</ref>. Other than CNNs, we also compare our method to the original ViT <ref type="bibr" target="#b6">[7]</ref> in order to express the effectiveness of smaller sized backbones, convolutional layers, as well our pooling technique. It should be noted that only the first row represents ViT according to the original paper, and the rest of the rows are simply "smaller" ViT variants according to our own models. These smaller ViT variants are denoted as ViT-Lite. Following ViT's original notation, we denote ViT variants with their patch size, but use the number of layers instead of the nominal descriptions ("Base", "Large", "Huge"). For instance, ViT-12/8 means a ViT with a 12 layer transformer encoder and an 8 × 8 patch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Sizing</head><p>We also evaluate smaller variants of our model. These variants can have as little as 284 K parameters and still achieve a considerably good performance on these datasets. These results are also presented in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We extend our previous comparisons by doing an ablation study on our method. We do so by progressively transforming the original ViT in structure into CCT, with intermediate models called ViT-Lite and CVT, and comparing the accuracy between these models. In this particular study, we report the results on CIFAR-10 and CIFAR-100, we summarize the results (best out of 4 runs) in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>The first column in <ref type="table" target="#tab_2">Table 2</ref> represents the usage of the weighted Adam optimizer <ref type="bibr" target="#b38">[39]</ref>, as opposed to Adam <ref type="bibr" target="#b16">[17]</ref>, which was the primary optimizer used for training ViT from scratch. The "Model" column refers to the model sizes in <ref type="table">Tables 7 and 8</ref>. ViT-Base is equivalent to our model with size 12 in terms of the transformer encoder, and the number after the forward slash is the patch size (for ours it is the convolution kernel size). The column "Conv" specifies the number of convolutional blocks used, and "Conv Size" specifies the kenel size. "Aug" denotes the use of dataset-specific augmentations, which were not reported in ViT, and were therefore excluded from some of the experiments. "Tuning" specifies a minor change in dropout, attention dropout, and/or stochastic depth. ViT, by default, uses a dropout for the MLP heads only with a probability of 0.1, and no attention dropout. Conversely, we do not use MLP dropout and only use attention dropout (p = 0.1). We also use stochastic depth with a drop probability of 0.1.</p><p>The first row in <ref type="table" target="#tab_2">Table 2</ref> are essentially ViT with the same settings from the paper. In the second row, the optimizer has been changed to weighted Adam, which we found more useful for training vision transformers. The next three rows are modified variants of ViT, which are not proposed in the original paper. These variants are more compact and use smaller patch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Positional Embedding</head><p>To determine the effects of the convolution layers and SeqPool, we perform an ablation study just for the purpose of positional embedding, seen in <ref type="table" target="#tab_4">Table 3</ref>. In this study we look at ViT, ViT-Lite, CVT, and CCT, and investigate the effects of: a learnable positional embedding, the standard sinusoidal embedding, as well as no positional embedding. We finish the table with our best model, which also has augmented training and an optimal tuning (refer to Appendix A). In these experiments we find that positional encoding matters in all variants, but to varying degrees. In particular, CCT relies less on positional encoding and can be safely removed much impact in accuracy. We also tested our CCT model without SeqPool, in place using the standard class token, and found that there was little to no effect from having a positional encoder or not, depending on model size. This demonstrates that the convolutions are the feature that helps provide spatially sparse information to the transformer as well as helps our model overcome some of the previous lim- itations, allowing for more efficient use of data. We do find that SeqPool helps slightly in this respect, but overall has a larger effect on increasing total accuracy. Lastly, we find that with proper data augmentation and tuning that we can increase the overall performance and maintain a low depen-dence on positional information. This demonstrates that our model is more robust to sparse data and that users may not even be required to positionally embed their datasets at all. ViT-Lite-7/4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCT-7/3x2</head><p>Samples per class Top-1 validation accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance vs Dataset Size</head><p>In this experiment we reduce the size of CIFAR-10 to determine the relationship between our model's performance and the number of samples within a dataset. This experiment disassociates the dimensionality of the data from the size of the dataset, measuring which metric is the "data hungry" aspect of transformers. To do this we remove sam-ples, uniformly, from each class in CIFAR-10 and measure the performance of both ViT and CCT. In <ref type="figure" target="#fig_5">Figure 5</ref> we see the comparison of each model's accuracy vs the number of samples per class. We show how the model runs when we have 500, 1000, 2000, 3000, 4000, or 5000 (original) samples per class, meaning the total data set ranges from one tenth the size to full. We can see here that our model is more robust since it is able to obtain higher accuracy with a lower number of samples per class, especially in the low sample regime. This allows us to conclude that our model is not data hungry and can still effectively learn on very small datasets, obtaining over 84% accuracy with a dataset that has only 10k samples and a total of 10 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Performance vs Dimensionality</head><p>In order to determine if transformers need highly dimensional data, as opposed to the number of samples (explored in Section 4.5) we again use CIFAR-10 and downsample or upsample the images to determine this relationship. In <ref type="figure" target="#fig_6">Figure 6</ref>, we show the image dimensionality vs the performance of our networks, where we trained both CCT and ViT-Lite with images of sizes ranging from 16×16 to 64×64. We can see that CCT performs better on all image sizes, with a widening difference as the number of pixels increases. From this we can infer that our model is able to better utilize the information density of an image, where ViT does not see continued performance increases after the standard 32x32 size. We go into further detail about the effects  of positional embeddings in Appendix B, where it should be noted that the difference seen here is exaggerated once positional embeddings are removed. Additionally, we show the difference between training a model with these parameters and inferring on a model that was trained on 32×32 sized images, denoting the generalizability of the models. We note that the number of parameters for these competing models is roughly the same, see <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The myth of "data hungry" transformers has persisted since they were introduced. We have shown within this paper that this is not necessarily true, and that with proper configuration, a transformer can be just as efficient as stateof-the-art CNN image classifiers. We democratise the use of Transformers by providing a framework that allows for these SOTA results on small datasets and minimal hard-ware. Our method is flexible in size, and the smallest of our variants can be easily loaded on even a minimal GPU, or even a CPU. While the trend of research has been bigger and bigger transformers, we show that there is still much research to be done to make efficient networks that work on small datasets and less efficient hardware. This kind of research is extremely important for many scientific domains where data is far more limited that the conventional datasets we use for general research. Continuing research in this direction will help open research up to more people and domain, helping democratize machine learning research.  In table 4, we summarize the different hyperparamters used for different models in table 1. Methods using SGD also used momentum (0.9), and those using AdamW used the default beta values β 1 = 0.9 and β 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper Param</head><p>ViT-12 ViT-7 CCT Optimizer AdamW AdamW AdamW LR 1e-4 5e-4 5e-4 Weight Decay 0 3e-2 3e-2  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dimensionality Experiments</head><p>Within this appendix we extend the analysis from Section 4.6, showing the difference in performance when using different types of positional embedding. <ref type="figure" target="#fig_7">Figure 7</ref> shows the difference of the accuracy when models are being trained from scratch. On the other hand, <ref type="figure" target="#fig_8">Figure 8</ref> shows the performance difference when models are only used in inference and pre-trained on the 32×32 sized images. We note that in <ref type="figure" target="#fig_8">Figure 8</ref>(a) that we do not provide inference for image sizes greater than the pre-trained image because the learnable positional embeddings do not allow us to extend in this direction. We draw the reader's attention to <ref type="figure" target="#fig_7">Figure 7</ref>(c) and <ref type="figure" target="#fig_8">Figure 8</ref>(c) to denote the large difference between the models when positional embedding is not used. We can see that in training CCT has very little difference when positional embeddings are used. Additionally, it should be noted that when performing inference our non-positional embedding CCT model has much higher generalizability than its ViT-Lite counterpart.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CCT Variants</head><p>The different variants of our model are presented in tables 7 and 8.</p><p>As it can be noticed, with more convolutional blocks, the computational complexity (MACs) decreases, due to the smaller sequence length produced. However, the performance may drop as well, and the number of parameters slightly increases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of Compact Convolutional Transformer (CCT): the convolutional variant of our compact transformer models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparing ViT (top) to CVT (middle) and CCT (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>CCT ConvBlock(PyTorch-style)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Overview of CCT vs CVT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>CIFAR-10 accuracy vs model size for model with size &lt; 12M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>CIFAR-10 with Reduced Number of Samples per Class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Image Size vs Accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>CIFAR-10 resolution vs top-1% validation accuracy (training from scratch). Images are square.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>CIFAR-10 resolution vs top-1% validation accuracy (inference only). Images are square.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Model CIFAR-10 CIFAR-100 Fashion-MNIST MNIST # Params MACs Top-1 validation accuracy comparisons. The numbers reported are best out of 4 runs. Hyperparamters are mentioned in Appendix A. Variants with † used a batch size of 64 instead of the default 128.</figDesc><table><row><cell cols="3">Convolutional Networks (Designed for ImageNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet18</cell><cell>90.27%</cell><cell>63.41%</cell><cell>93.51%</cell><cell cols="3">99.18% 11.182 M 0.037 G</cell></row><row><cell>ResNet34</cell><cell>90.51%</cell><cell>64.52%</cell><cell>93.47%</cell><cell cols="3">99.24% 21.290 M 0.075 G</cell></row><row><cell>ResNet50</cell><cell>90.60%</cell><cell>61.68%</cell><cell>93.17%</cell><cell cols="3">99.18% 23.529 M 0.084 G</cell></row><row><cell>MobileNetV2/0.5</cell><cell>84.78%</cell><cell>56.32%</cell><cell>93.49%</cell><cell>99.08%</cell><cell>0.700 M</cell><cell>0.002 G</cell></row><row><cell>MobileNetV2/1.0</cell><cell>89.07%</cell><cell>63.69%</cell><cell>93.62%</cell><cell>99.28%</cell><cell>2.237 M</cell><cell>0.007 G</cell></row><row><cell>MobileNetV2/1.25</cell><cell>90.60%</cell><cell>65.24%</cell><cell>93.83%</cell><cell>99.25%</cell><cell>3.465 M</cell><cell>0.010 G</cell></row><row><cell>MobileNetV2/2.0</cell><cell>91.02%</cell><cell>67.44%</cell><cell>94.07%</cell><cell>99.38%</cell><cell>8.723 M</cell><cell>0.024 G</cell></row><row><cell cols="3">Convolutional Networks (Designed for CIFAR)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet164-v1[12] ResNet164-v2[12] ResNet1001-v1[12] ResNet1001-v2[12] ResNet1001-v2  † [12]</cell><cell>94.07% 94.54% 92.39% 95.08% 95.38%</cell><cell>74.84% 75.67% 72.18% 77.29% −</cell><cell>− − − − −</cell><cell>− − − − −</cell><cell cols="2">1.704 M 1.703 M 10.329 M 1.549 G 0.257 G 0.257 G 10.328 M 1.548 G 10.328 M 1.548 G</cell></row><row><cell>Vision Transformers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-12/16</cell><cell>76.42%</cell><cell>46.61%</cell><cell>83.88%</cell><cell cols="3">82.66% 85.631 M 0.427 G</cell></row><row><cell>ViT-Lite-7/16</cell><cell>76.52%</cell><cell>48.25%</cell><cell>82.53%</cell><cell>81.37%</cell><cell>3.886 M</cell><cell>0.019 G</cell></row><row><cell>ViT-Lite-6/16</cell><cell>76.47%</cell><cell>48.54%</cell><cell>82.06%</cell><cell>81.15%</cell><cell>3.359 M</cell><cell>0.017 G</cell></row><row><cell>ViT-Lite-7/8</cell><cell>87.49%</cell><cell>63.52%</cell><cell>90.73%</cell><cell>98.9%</cell><cell>3.741 M</cell><cell>0.063 G</cell></row><row><cell>ViT-Lite-6/8</cell><cell>87.50%</cell><cell>62.76%</cell><cell>90.44%</cell><cell>98.9%</cell><cell>3.215 M</cell><cell>0.054 G</cell></row><row><cell>ViT-Lite-7/4</cell><cell>91.38%</cell><cell>69.74%</cell><cell>94.39%</cell><cell>99.29%</cell><cell>3.717 M</cell><cell>0.239 G</cell></row><row><cell>ViT-Lite-6/4</cell><cell>90.94%</cell><cell>69.2%</cell><cell>94.35%</cell><cell>99.26%</cell><cell>3.191 M</cell><cell>0.205 G</cell></row><row><cell cols="2">Compact Vision Transformers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CVT-7/8</cell><cell>89.66%</cell><cell>66.02%</cell><cell>92.47%</cell><cell>99.05%</cell><cell>3.741 M</cell><cell>0.060 G</cell></row><row><cell>CVT-6/8</cell><cell>89.03%</cell><cell>65.71%</cell><cell>92.52%</cell><cell>99.11%</cell><cell>3.215 M</cell><cell>0.051 G</cell></row><row><cell>CVT-7/4</cell><cell>92.43%</cell><cell>73.01%</cell><cell>94.55%</cell><cell>99.32%</cell><cell>3.717 M</cell><cell>0.236 G</cell></row><row><cell>CVT-6/4</cell><cell>92.58%</cell><cell>72.25%</cell><cell>94.54%</cell><cell>99.27%</cell><cell>3.190 M</cell><cell>0.202 G</cell></row><row><cell cols="2">Compact Convolutional Transformers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CCT-2/3×2</cell><cell>89.17%</cell><cell>66.90%</cell><cell>93.90%</cell><cell>99.21%</cell><cell>0.284 M</cell><cell>0.033 G</cell></row><row><cell>CCT-4/3×2</cell><cell>91.45%</cell><cell>70.46%</cell><cell>93.99%</cell><cell>99.18%</cell><cell>0.482 M</cell><cell>0.046 G</cell></row><row><cell>CCT-6/3×2</cell><cell>93.56%</cell><cell>74.47%</cell><cell>94.23%</cell><cell>99.25%</cell><cell>3.327 M</cell><cell>0.241 G</cell></row><row><cell>CCT-7/3×2</cell><cell>93.65%</cell><cell>74.77%</cell><cell>93.95%</cell><cell>99.26%</cell><cell>3.853 M</cell><cell>0.275 G</cell></row><row><cell>CCT-7/7×1</cell><cell>92.57%</cell><cell>73.01%</cell><cell>93.53%</cell><cell>99.19%</cell><cell>3.742 M</cell><cell>0.245 G</cell></row><row><cell>CCT-7/3×1</cell><cell>94.47%</cell><cell>75.59%</cell><cell>94.40%</cell><cell>99.24%</cell><cell>3.760 M</cell><cell>0.947 G</cell></row><row><cell>CCT-7/3×1  †</cell><cell>94.72%</cell><cell>76.67%</cell><cell>94.48%</cell><cell>99.21%</cell><cell>3.760 M</cell><cell>0.947 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top-1 validation accuracy of CIFAR-10 and CIFAR-100 when transforming ViT into CCT step by step. The numbers reported are best out of 4 runs. Hyperparamters are mentioned in Appendix A.</figDesc><table><row><cell>W Model</cell><cell cols="6">Pool # Conv Conv Size Aug Tuning CIFAR-10 CIFAR-100 # Params</cell><cell>MACs</cell></row><row><cell>ViT-12/16</cell><cell>CT</cell><cell></cell><cell></cell><cell>69.49%</cell><cell>40.52%</cell><cell cols="2">85.631 M 0.427 G</cell></row><row><cell>ViT-12/16</cell><cell>CT</cell><cell></cell><cell></cell><cell>69.82%</cell><cell>40.57%</cell><cell cols="2">85.631 M 0.427 G</cell></row><row><cell cols="2">ViT-Lite-7/16 CT</cell><cell></cell><cell></cell><cell>71.78%</cell><cell>41.59%</cell><cell cols="2">3.886 M 0.019 G</cell></row><row><cell>ViT-Lite-7/8</cell><cell>CT</cell><cell></cell><cell></cell><cell>83.38%</cell><cell>55.69%</cell><cell cols="2">3.741 M 0.063 G</cell></row><row><cell>ViT-Lite-7/4</cell><cell>CT</cell><cell></cell><cell></cell><cell>83.59%</cell><cell>58.43%</cell><cell cols="2">3.717 M 0.239 G</cell></row><row><cell>CVT-7/16</cell><cell>SP</cell><cell></cell><cell></cell><cell>72.26%</cell><cell>42.37%</cell><cell cols="2">3.886 M 0.015 G</cell></row><row><cell>CVT-7/8</cell><cell>SP</cell><cell></cell><cell></cell><cell>84.24%</cell><cell>55.49%</cell><cell cols="2">3.741 M 0.060 G</cell></row><row><cell>CVT-7/8</cell><cell>SP</cell><cell></cell><cell></cell><cell>87.15%</cell><cell>63.14%</cell><cell cols="2">3.741 M 0.060 G</cell></row><row><cell>CVT-7/4</cell><cell>SP</cell><cell></cell><cell></cell><cell>88.06%</cell><cell>62.06%</cell><cell cols="2">3.717 M 0.236 G</cell></row><row><cell>CVT-7/4</cell><cell>SP</cell><cell></cell><cell></cell><cell>91.72%</cell><cell>69.59%</cell><cell cols="2">3.717 M 0.236 G</cell></row><row><cell>CVT-7/4</cell><cell>SP</cell><cell></cell><cell></cell><cell>92.43%</cell><cell>73.01%</cell><cell cols="2">3.717 M 0.236 G</cell></row><row><cell>CVT-7/2</cell><cell>SP</cell><cell></cell><cell></cell><cell>84.8%</cell><cell>57.98%</cell><cell cols="2">3.757 M 0.940 G</cell></row><row><cell>CCT-7/7×1 CCT-7/7×1 CCT-7/7×1 CCT-7/3×2 CCT-7/3×1</cell><cell>SP SP SP SP SP</cell><cell>1 1 1 2 1</cell><cell>7 × 7 7 × 7 7 × 7 3 × 3 3 × 3</cell><cell>87.81% 91.85% 92.29% 93.65% 94.47%</cell><cell>62.83% 69.43% 72.46% 74.77% 75.59%</cell><cell cols="2">3.742 M 0.245 G 3.742 M 0.245 G 3.742 M 0.245 G 3.853 M 0.275 G 3.760 M 0.947 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Top-1 validation accuracy comparison when changing the positional embedding method. The numbers reported are best out of 4 runs with random initializations. † denotes model trained with extra augmentation and hyperparameter tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Hyperparamters from Table 1 experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>ViT and CCT hyperparamters.</figDesc><table><row><cell>Hyper Param</cell><cell cols="2">Not Tuned Tuned</cell></row><row><cell>MLP Dropout</cell><cell>0.1</cell><cell>0</cell></row><row><cell>MSA Dropout</cell><cell>0</cell><cell>0.1</cell></row><row><cell>Stochastic Depth</cell><cell>0</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Difference between tuned and not tuned runs.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameter tuning</head><p>We tuned the hyperparamters per experiment and arrived at the following for each table in Sec. 4. It should be noted that we directly report the numbers from the paper "Identity mappings in deep residual networks" <ref type="bibr" target="#b11">[12]</ref> for ResNet164 and ResNet1001.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Stéphane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697,2021.4</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving transformer optimization through better initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Shi Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4475" to="4483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chextransfer: performance and parameter efficiency of imagenet models for chest x-ray interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oishi</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning</title>
		<meeting>the Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistics of natural images: Scaling in the woods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">814</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808,2021.4</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15355</idno>
		<title level="m">Optimizing deeper transformers on small datasets: An application on text-to-sql semantic parsing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>PMLR, 2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adam revisited: a weighted past gradients perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A comprehensive survey on transfer learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Model # Convs Input size Transformer Backbone Computational cost</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
