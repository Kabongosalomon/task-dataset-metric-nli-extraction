<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KERMIT: Generative Insertion-Based Modeling for Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-04">4 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
							<email>williamchan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
							<email>kitaev@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
							<email>kguu@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">AI Language Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
							<email>mitchell@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KERMIT: Generative Insertion-Based Modeling for Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-04">4 Jun 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present KERMIT, a simple insertion-based approach to generative modeling for sequences and sequence pairs. KERMIT models the joint distribution and its decompositions (i.e., marginals and conditionals) using a single neural network and, unlike much prior work, does not rely on a prespecified factorization of the data distribution. During training, one can feed KERMIT paired data (x, y) to learn the joint distribution p(x, y), and optionally mix in unpaired data x or y to refine the marginals p(x) or p(y). During inference, we have access to the conditionals p(x | y) and p(y | x) in both directions. We can also sample from the joint distribution or the marginals. The model supports both serial fully autoregressive decoding and parallel partially autoregressive decoding, with the latter exhibiting an empirically logarithmic runtime. We demonstrate through experiments in machine translation, representation learning, and zero-shot cloze question answering that our unified approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems across a wide range of tasks without the need for problem-specific architectural adaptation. * Equal contribution. WC initiated the KERMIT project for machine translation, implemented the corresponding code and experiments (Section 4.1) and advised the project. NK proposed using the same model for text generation, implemented and evaluated different monolingual pre-training approaches, and conducted all representation learning experiments (Section 4.2). KG proposed using KERMIT as a zero-shot QA model and conducted all associated experiments (Section 4.3); he also co-developed KERMIT's training and inference infrastructure. MS developed the mathematical formalism for the model (Section 3) and assisted in the implementation of KERMIT for translation. JU helped conceive the initial idea and advised the project.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural sequence models <ref type="bibr" target="#b23">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Cho et al., 2014)</ref> have been successfully applied to many conditional generation applications, including machine translation <ref type="bibr" target="#b4">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b14">Luong et al., 2015)</ref>, speech recognition <ref type="bibr" target="#b6">(Chan et al., 2016;</ref><ref type="bibr" target="#b5">Bahdanau et al., 2016)</ref>, speech synthesis <ref type="bibr" target="#b15">(Oord et al., 2016;</ref><ref type="bibr" target="#b28">Wang et al., 2017)</ref> and image captioning <ref type="bibr" target="#b29">Xu et al., 2015)</ref>. Much of the prior work in this area follows the seq2seq encoder-decoder paradigm, where an encoder builds a representation of an observed sequence x, and a decoder gives the conditional output distribution p(y | x) according to a predetermined factorization, usually left-to-right.</p><p>While effective for straightforward conditional generation, such an approach is inflexible and cannot readily be applied to other inference tasks such as non-left-to-right generation or infilling. In this work, we present a more general approach called Kontextuell Encoder Representations Made by Insertion Transformations, or KERMIT for short. KERMIT is a simple architecture that directly models the joint distribution p(x, y) and its decompositions (such as the marginals p(x) and p(y)  <ref type="figure">Figure 1</ref>: An example of the KERMIT insertion objective for the English ↔ German translation pair "The courses proved quite popular" ↔ "Die Kurse waren sehr beliebt". The model is trained to predict the set of words that need to be inserted at each location. By incurring a loss on both sides, our system learns a fully generative model of the joint distribution over (x, y) pairs, and can accommodate arbitrary generation orders.  <ref type="bibr" target="#b9">Devlin et al. (2019)</ref> and the conditionals p(y | x) and p(x | y)) in a unified manner. In contrast with traditional seq2seq models, KERMIT does not rely on a prespecified factorization, but is instead able to condition on whatever information is available and infer what remains.</p><p>During training, we present KERMIT with paired data (x, y) to learn the joint, and can optionally mix in unpaired data x or y to refine the marginals in a semi-supervised setting. At test time, a single KERMIT model can be used for conditional inference in either direction by restricting the output distribution to p(x | y) or p(y | x) as required. We can also generate paired samples from the joint distribution (x, y) ∼ p(x, y), or unpaired samples from the marginals x ∼ p(x) or y ∼ p(y).</p><p>KERMIT uses a simple architecture and is easy to implement. It does not have a separate encoder and decoder, nor does it require causality masks. In our implementation, KERMIT consists of a single Transformer decoder stack <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>. The model is trained to insert the missing tokens into any partially-complete sequence, as shown in <ref type="figure">Figure 1</ref>. We describe the implementation in more detail in Section 3.</p><p>We apply KERMIT to a diverse set of tasks, finding that our unified approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems without the need for problemspecific components. We first apply KERMIT to machine translation, where the inputs and outputs are parallel sentence pairs. Then, like its friends ELMo <ref type="bibr" target="#b16">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, and ERNIE <ref type="bibr" target="#b22">(Sun et al., 2019)</ref>, we can also use KERMIT for self-supervised representation learning for use in downstream NLP tasks. Finally, we apply KERMIT to a zero-shot cloze questionanswering task demonstrating the infilling capabilities of the model. <ref type="table" target="#tab_1">Table 1</ref> summarizes our results on all three tasks compared to other highly tuned models: Transformer, BERT, GPT and GPT-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we define some notation and give a brief review of existing sequence models, including autoregressive left-to-right models <ref type="bibr" target="#b23">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Cho et al., 2014)</ref> and masked language models <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Autoregressive Left-to-Right Models</head><p>Let X and Y be the set of all input and output sequences, respectively. In a standard sequence-tosequence task, we are presented with training data consisting of sequence pairs (x, y) ∈ X × Y, e.g. parallel translations, and we aim to learn the conditional distribution p(y | x). Traditional autoregressive models <ref type="bibr" target="#b23">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Cho et al., 2014)</ref> use a left-to-right factorization, decomposing the distribution as a chain of predictions conditioning on the input x and prefixes y &lt;t :</p><formula xml:id="formula_0">p(y | x) = t p(y t | x, y &lt;t ).<label>(1)</label></formula><p>This structure is also used for unconditional sequence tasks such as language modeling where the goal is to learn an unconditional output distribution on its own. A left-to-right factorization is convenient because it allows for exact log-likelihood computation, thereby permitting efficient maximum likelihood estimation. It also leads to simple approximate inference algorithms such as greedy decodingŷ</p><formula xml:id="formula_1">t = argmax y p(y | x,ŷ &lt;t )<label>(2)</label></formula><p>or beam search over sets of multiple hypotheses.</p><p>However, there are some drawbacks to the autoregressive approach. First, in the case of conditional generation, it cannot handle situations where the input x is only partially observed. Second, since it utilizes a fixed left-to-right factorization, it cannot be used for other inference tasks like infilling where generation is not monotonic. Moreover, standard inference algorithms require n generation steps to generate n tokens, which could be a bottleneck in end-use applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Masked Language Models</head><p>Masked Language Models (MLMs) <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> comprise another class of models targeting the unconditional setting. For MLMs, a partial canvas x s ⊆ x is observed where some of the tokens in x have been masked out, and the objective is to recover x from x s . For example, for a ground truth canvas x * = (A, B, C, D, E) and a partial canvas x * s = (A, _, C, D, _), the model should learn to replace the second blank with B and the last blank with E. The model outputs an independent prediction at each position, and its objective is to maximize p(x | x s ).</p><p>Because the exact locations of the slots are known in x s , the model does not need to predict where the missing items are located, but only what they should be. Consequently, the model is not immediately suitable for generation, as the canvas size needs to be fixed during inference and cannot change over time (i.e., |x s | = |x|). MLMs have been successfully applied in self-supervised representation learning settings, leading to strong results on downstream language tasks <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KERMIT</head><p>In this section we propose KERMIT, a novel insertion-based generative model. Unlike the prior work mentioned in Section 2, KERMIT does not have the rigid construction of modeling the target sequence given some fully observed source sequence, nor does it assume a left-to-right factorization (and generation order) of the output sequence. To motivate and arrive at our model, we formalize then extend a recent insertion-based conditional modeling framework proposed by <ref type="bibr" target="#b20">Stern et al. (2019)</ref>.</p><p>We begin with the unconditional setting. In order to model sequences without requiring a fixed factorization or imposing constraints on the order of generation, we make use of a framework in which sequences are constructed via insertion operations. Given a sequence x = (x 1 , . . . , x n ) and a generation order z represented as a permutation of the indices {1, . . . , n}, we define the corresponding sequence ((c z 1 , l z 1 ), . . . , (c z n , l z n )) of insertion operations which produces x according to order z. Here, c z i ∈ C is an element of the vocabulary and 1 ≤ l z i ≤ i is an insertion location relative to the current hypothesis. For example, if constructing the sequence</p><formula xml:id="formula_2">(A, B, C) as () → (C) → (A, C) → (A, B, C), we would have z = (3, 1, 2) with (c z 1 , l z 1 ) = (C, 1), (c z 2 , l z 2 ) = (A, 1), (c z 3 , l z 3 ) = (B, 2). Next let (x z,i 1 , .</formula><p>. . , x z,i i ) denote the subsequence of x corresponding to the (ordered) extraction of the elements at indices {z 1 , . . . , z i }. This is the partial output at iteration i. Note that this will be the same for all permutations z with the same unordered set of indices in the first i positions. For the example above for instance, we have (x z,2 1 , x z,2 2 ) = (A, C). Armed with these definitions, we can now write out p(x) as a marginalization over all possible orders z ∈ S n for sequence length n, where S n denotes the set of all permutations on n elements:</p><formula xml:id="formula_3">p(x) = z∈Sn p(x, z) (3) = z∈Sn p(z)p(x | z) (4) = z∈Sn p(z) n i=1 p((c z i , l z i ) | (c z 1 , l z 1 ), . . . , (c z i−1 , l z i−1 )) (5) = z∈Sn p(z) n i=1 p((c z i , l z i ) | x z,i−1 1:i−1 ),<label>(6)</label></formula><p>where the last line encodes the Markov assumption that the order of insertions leading to a given canvas is not important, just the result. Typically we will use a uniform prior over permutations for p(z), though other options are available, such as the balanced binary tree prior described by <ref type="bibr" target="#b20">Stern et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning</head><p>Although exact computation of the log-likelihood is intractable due to the marginalization over the generation order z, we can lower bound the log-likelihood using Jensen's inequality via</p><formula xml:id="formula_4">log p(x) = log z∈Sn p(z)p(x | z) (7) ≥ z∈Sn p(z) log p(x | z) =: L(x).<label>(8)</label></formula><p>Substituting in our expression for p(x | z) from above, we have</p><formula xml:id="formula_5">L(x) = z∈Sn p(z) log n i=1 p((c z i , l z i ) | x z,i−1 1:i−1 ) (9) = z∈Sn p(z) n i=1 log p((c z i , l z i ) | x z,i−1 1:i−1 ).<label>(10)</label></formula><p>Next we interchange the summations and break the permutation z down into (z 1 , . . . , z i−1 ) corresponding to previous insertions, z i corresponding to the next insertion, and (z i+1 , . . . , z n ) corresponding to future insertions, giving</p><formula xml:id="formula_6">L(x) = n i=1 z∈Sn p(z) log p((c z i , l z i ) | x z,i−1 1:i−1 ) (11) = n i=1 z1:i−1 zi zi+1:n p(z) log p((c z i , l z i ) | x z,i−1 1:i−1 ) (12) = n i=1 z1:i−1 p(z 1:i−1 ) zi p(z i | z 1:i−1 ) log p((c z i , l z i ) | x z,i−1 1:i−1 ) zi+1:n p(z i+1:n | z 1:i ) (13) = n i=1 z1:i−1 p(z 1:i−1 ) zi p(z i | z 1:i−1 ) log p((c z i , l z i ) | x z,i−1 1:i−1 ),<label>(14)</label></formula><p>where the simplification in the last line follows from the fact that zi+1:n p(z i+1:n | z 1:i ) = 1.</p><p>From here, we can multiply and divide the outer sum by n to turn it into a mean, then arrive at the following simple sampling procedure to compute an unbiased estimate of our lower bound L(x) on the log-likelihood for a single example:</p><p>1. Sample a generation step i ∼ Uniform([1, n]).</p><p>2. Sample a partial permutation z 1:i−1 ∼ p(z 1:i−1 ) for the first i − 1 insertions. 3. Compute a weighted sum over the next-step losses log p((c z i , l z i ) | x z,i−1 1:i−1 ) scaled by the weighting distribution p(z i | z 1:i−1 ) and the sequence length n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax</head><p>Casual Self-Attention + Cross-Attention Embedding Embedding Self-Attention </p><formula xml:id="formula_7">s ′ A ′ B ′ C ′ D ′ A B C D E F G s E ′ (a) Transformer Softmax Self-Attention Embedding A B C _ _ F s _ B ′ C ′ D ′ _ _ s ′ D E A ′ E ′ F ′ (b) BERT Softmax Self-Attention + Cross-Attention Embedding Embedding Self-Attention A B C D E F G s B ′ C ′ D ′ s ′ A ′ {E ′ , F ′ } (c) Insertion Transformer Softmax Self-Attention Embedding A B C F s B ′ C ′ D ′ s ′ A ′ {E ′ , F ′ } {D, E} (d) KERMIT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>Using this model, inference can be autoregressive via greedy decoding</p><formula xml:id="formula_8">(ĉ,l) = argmax c,l p(c, l|x t )<label>(15)</label></formula><p>or partially autoregressive via parallel decodinĝ</p><formula xml:id="formula_9">c l = argmax c p(c | l,x t ).<label>(16)</label></formula><p>In the case of parallel decoding, we perform simultaneous insertions at all non-finished slots. If we use a balanced binary tree prior for p(z) <ref type="bibr" target="#b20">(Stern et al., 2019)</ref>, we can even achieve an empirical runtime of ≈ log 2 n iterations to generate n tokens. One key advantage of insertion-based models over MLMs is that the output canvas can dynamically grow in size, meaning the length does not need to be chosen before the start of generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pairs of Sequences</head><p>Thus far, we have discussed KERMIT for single sequences. We can easily extend KERMIT to pairs of sequences by directly modeling (x, y) as a concatenation of two sequences, (x, y) = (x 1 , . . . , x n , y 1 , . . . , y m ). For example, let our first sequence be x = (A, B, C, EOS ) and our second sequence be y</p><formula xml:id="formula_10">= (A ′ , B ′ , C ′ , D ′ , E ′ , EOS ). The concatenated sequence would then be (x, y) = (A, B, C, EOS , A ′ , B ′ , C ′ , D ′ , E ′ , EOS ).</formula><p>With this approach, we can model pairs of sequences as if they were single sequences. Moreover, unlike seq2seq, our model is symmetric with regards to its treatment of the source and target, making it a strong candidate for extensions to multimodal data settings in future work.</p><p>By keeping our architecture order-agnostic and marginalizing over all possible orders in our training objective, KERMIT is able to learn the joint distribution and all its decompositions, including the marginals p(x) and p(y) and conditionals p(y | x) and p(x | y). We can also perform targeted training. More explicitly, if the model is provided with a canvas that fully contains x or y, then it will learn a conditional distribution. If the model is provided with an example where x or y is empty, then it will learn the opposing marginal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model</head><p>We implement KERMIT as a single Transformer decoder stack <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>, without any form of causal masking. The full self-attention mechanism allows the model to capture any rela-  tionships between the input canvas and the predicted insertion operations with a constant number of operations. We follow <ref type="bibr" target="#b20">Stern et al. (2019)</ref> and model the (content, location) distribution p(c, l) as a factorized distribution p(c, l) = p(c | l)p(l), where p(c | l) is the standard Transformer softmax over the vocabulary, and a p(l) is a softmax over the locations. <ref type="figure" target="#fig_0">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform experiments with KERMIT on the tasks of machine translation, self-supervised representation learning, and zero-shot cloze question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Machine Translation</head><p>We first apply KERMIT on the competitive WMT 2014 English ↔ German translation task. We follow the hyperparameter settings of the base Transformer <ref type="bibr" target="#b24">(Vaswani et al., 2018)</ref>. However, since KERMIT does not have an encoder, we simply double the decoder width. We perform no additional hyperparameter tuning. We also follow prior work <ref type="bibr" target="#b10">(Gu et al., 2018;</ref><ref type="bibr" target="#b21">Stern et al., 2018</ref><ref type="bibr" target="#b20">Stern et al., , 2019</ref><ref type="bibr" target="#b13">Lee et al., 2018)</ref> in using distillation <ref type="bibr" target="#b11">(Hinton et al., 2015;</ref><ref type="bibr" target="#b12">Kim and Rush, 2016)</ref> to train our models. We follow <ref type="bibr" target="#b20">Stern et al. (2019)</ref> in using a balanced binary tree loss, and we similarly observe an empirically logarithmic number of generation steps in sequence length when using parallel decoding. However, unlike <ref type="bibr" target="#b20">Stern et al. (2019)</ref> we did not need to tune an EOS penalty, but simply set it to zero for all experiments.</p><p>We train several different KERMIT models for translation. First we train two unidirectional models, where the model observes a full source sentence (i.e., English or German) and is asked to generate the corresponding target sentence (i.e., German or English  <ref type="figure">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</ref>   <ref type="figure">Figure 3</ref>: Example parallel decodes using KERMIT for English → German and German → English translation. In each row, the blue underlined tokens are those being inserted, and the gray tokens are those from the final output that have not yet been generated. Empirically, KERMIT requires only ≈ log 2 n steps to generate a sequence of length n when trained with a balanced binary tree prior.</p><p>additionally be used for sampling or completing partial inputs. We use the same hyperparameter set as before. Since the model is now faced with a much more challenging task, it does slightly worse when limited to the same model size, but still reaches a respectable 25.6/27.4 BLEU. Unlike the previous models, however, we can incorporate monolingual data into the joint model's training setup to supplement its knowledge of the marginals p(x) and p(y). We accordingly train a joint model with all our paired data and 1M additional samples of English and German monolingual data randomly selected from the WMT 2014 monolingual corpus. Without altering model capacity, we find that refining the marginals gives us a 1.2 BLEU improvement on German → English. Finally, we take the model which was trained on the full joint distribution with marginal refinement, and further finetune it on both the unidirectional and bidirectional settings. We find a small improvement in BLEU over the original models in both settings. <ref type="table" target="#tab_3">Table 2</ref> summarizes our results. We emphasize that virtually all of our models outperform prior non-fully-autoregressive approaches in terms of BLEU. We also note that the observed number of iterations required to generate n tokens is roughly log 2 n due to the use of a balanced binary tree loss and parallel decoding, which is substantially lower than autoregressive models which require n steps. Some examples of parallel decodes are shown in <ref type="figure">Figure 3</ref>. Our models require an average of 5.5-6.5 decoding iterations for the sentences in the test set, outperforming the constant-time models of <ref type="bibr" target="#b13">Lee et al. (2018)</ref> which require 10 iterations in both BLEU and empirical decoding complexity.</p><p>We also draw samples from the model to highlight its infilling and generation capabilities. <ref type="figure" target="#fig_2">Figure 4</ref> captures some examples. We first show unconditional sampling of an (English, German) sentence pair. We also take a translation example from the newstest2013 dev set and split it in half, sampling completions after seeding the English side with the first half and the German side with the second half. We find the model is capable of generating a very diverse set of coherent samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Representation Learning</head><p>Like its close friend BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, KERMIT can also be used for self-supervised representation learning and applied to various language understanding tasks. We follow the same training procedure and hyperparameter setup as BERT LARGE . However, instead of masking 15% of the tokens and replacing them with blank tokens like in BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, KERMIT simply drops them out completely from the sequence.   <ref type="table">Table 3</ref>: GLUE benchmark scores (as computed by the GLUE evaluation server). Of these models, only GPT and KERMIT admit a straightforward generation process.</p><p>Prior to BERT, the best representation learning approach was to use a language model such as GPT <ref type="bibr" target="#b17">(Radford et al., 2018)</ref>. BERT outperforms GPT in large part because of its deeply bi-directional architecture, but in the process BERT sacrifices the ability to perform straightforward generation. While we find KERMIT to perform slightly behind BERT, KERMIT maintains the ability to generate text while obtaining results that are much closer to BERT rather than GPT. The GLUE benchmark  results are summarized in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero-Shot Cloze Question Answering</head><p>Finally, we also investigate the infilling abilities of KERMIT and related approaches by evaluating their performance on zero-shot cloze question answering. In particular, we aim to understand how effective these models are for fill-in-the-blank-style question answering after being trained only on language modeling data without any task-specific fine-tuning.</p><p>For this experiment, we use the human-annotated QA2D dataset assembled by <ref type="bibr" target="#b8">Demszky et al. (2018)</ref>, which consists of examples from the SQuAD dataset <ref type="bibr" target="#b19">(Rajpurkar et al., 2016)</ref> in which the answer has been extended from a single phrase into a full declarative sentence. These can be transformed into cloze instances by removing the answer phrase from the declarative output. For example, given the question "When was Madonna born?" and the answer "August 16, 1958", the full declarative answer would be "Madonna was born on August 16, 1958.", and the associated cloze instance would be "Madonna was born on ."</p><p>Plymouth has a post-war shopping area in the city centre with substantial pedestrianisation. At the west end of the zone inside a grade II listed building is the Pannier Market that was completed in 1959 -pannier meaning "basket" from French, so it translates as "basket market". In terms of retail floorspace, Plymouth is ranked in the top five in the South West, and 29th nationally. Plymouth was one of the first ten British cities to trial the new Business Improvement District initiative. The Tinside Pool is situated at the foot of the Hoe and became a grade II listed building in 1998 before being restored to its 1930s look for £3.4 million. What notable location was named a grade II listed building in 1998? ___ was named a grade II listed building in 1998 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Answer</head><p>GPT-2 → "Plymouth" + Oracle Length → "A listed building" BERT → "plymouth" + Oracle Length → ": the pool"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KERMIT</head><p>→ "the tinside pool"</p><p>Correct → "Tinside Pool" . . <ref type="figure">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</ref> . . . . . We take the KERMIT model trained from Section 4.2 and two powerful language models (BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and the largest public version of GPT-2 2 <ref type="bibr" target="#b18">(Radford et al., 2019)</ref>), and evaluate their ability to fill in the blank of each cloze instance, each without specifically being trained on data of this form. We employ different decoding strategies as required for each model, detailed below. where cloze(left) and cloze(right) are the portions of the declarative answer before and after the gap. Since KERMIT can natively perform insertions, we simply perform a parallel decode constrained to take place within the gap and extract the output as our answer. Here we include explicit <ref type="bibr">[MASK]</ref> tokens, running separate decodes with n = 1, 2, . . . up to 4 or the oracle answer length, whichever is greater. We then choose the one with the highest score under the model and extract the outputs at the masked positions as the answer. Each decode consists of a beam search in which one <ref type="bibr">[MASK]</ref> is filled at a time. For each element on the beam, we choose the remaining <ref type="bibr">[MASK]</ref> position with the highest confidence (lowest entropy) as the next position to fill. We found that this beam-search did substantially better than left-to-right decoding or parallel decoding.</p><p>GPT-2 For GPT-2, a left-to-right language model, we cannot directly condition on both the left and right context. Instead, we first present the model with the prefix passage question cloze <ref type="bibr">(left)</ref> and sample continuations of varying lengths. For each continuation, we then append cloze(right) and compute the score of the full sequence under the model. We select the best-scoring sequence and extract the portion in the gap as the answer. To efficiently obtain continuations of varying lengths, we generate 20 extended continuations from the model, then treat all prefixes of those continuations as candidate values to go in the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Exact Match F1  We evaluate on 50,000 cloze-formulated questions from SQuAD, using the standard SQuAD evaluation script to compute accuracy in terms of exact match and token-level F1. Results are presented in <ref type="table" target="#tab_8">Table 4</ref>. KERMIT performs significantly better on this zero-shot cloze task than the other two approaches thanks to its infilling capabilities learned through its insertion-oriented objective, achieving 30.3 F1 and 20.9% exact match. BERT's performance falls short of KERMIT, as it often prefers shorter completions since it is not required to handle length modeling during training. GPT-2 lags further behind the others due to its inability to condition on the context on both sides of the gap during inference. Even when the oracle length (i.e., the ground-truth length of the answer) is provided to BERT and GPT-2, KERMIT still substantially outperforms all other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present KERMIT, an insertion-based framework for sequences that can model the joint data distribution and its decompositions (i.e., marginals and conditionals). KERMIT can generate text in an arbitrary order -including bidirectional machine translation and cloze-style infilling -and empirically can generate sequences in logarithmic time. It uses a simple neural architecture that can additionally produce contextualized vector representations of words and sentences. We find KERMIT is capable of matching or exceeding state-of-the-art performance on three diverse tasks: machine translation, representation learning, and zero shot cloze question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Diagram of various models. The Transformer (a) model predicts the next right token given the left context. The BERT (b) model predicts what is missing in the blank slots given the context. The Insertion Transformer (c) model predicts where and what is missing given the context. The KERMIT (d) model is an generalization of (c) where the context is over multiple sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>visualizes the differences between a standard Transformer<ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>, BERT<ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, Insertion Transformer<ref type="bibr" target="#b20">(Stern et al., 2019)</ref> and KERMIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Paired translation samples drawn from KERMIT, with and without seeding the initial canvas with text. In the bottom portion of the figure, the seed text is shown in gray, and different continuations sampled from the model are shown in black. We emphasize the diversity of generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Example of KERMIT, BERT and GPT-2 performing zero-shot cloze question answering on SQuAD. The question and cloze question are bolded. Note that BERT and GPT-2 prefer a shorter, incorrect answer, unless given the oracle answer length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>KERMIT</head><label></label><figDesc>We split the passage in half and present KERMIT with examples of the form [CLS] passage(1/2) [SEP] passage(2/2) question cloze(left) cloze(right) [SEP]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>BERT</head><label></label><figDesc>We split the passage in half and present BERT with examples of the form [CLS] passage(1/2) [SEP] passage(2/2) question cloze(left) [MASK]*n cloze(right) [SEP]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The KERMIT architecture works well for three categories of tasks: machine translation, representation learning, and zero-shot cloze question answering. a Vaswani et al. (2017) b Radford et al.</figDesc><table><row><cell></cell><cell cols="3">Machine Translation Representation Learning Cloze Question Answering</cell></row><row><cell></cell><cell>En → De (BLEU)</cell><cell>GLUE</cell><cell>Zero-shot SQuAD (F1)</cell></row><row><cell>Autoregressive (Transformer, GPT, GPT-2)</cell><cell>27.3 a</cell><cell>72.8 b</cell><cell>16.6</cell></row><row><cell>Masking (BERT)</cell><cell>N/A</cell><cell>80.5 c</cell><cell>18.9</cell></row><row><cell>Insertion (KERMIT -Our Work)</cell><cell>27.8</cell><cell>79.8</cell><cell>30.3</cell></row></table><note>(2018) c</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Model↔ En → De De → En Iterations</figDesc><table><row><cell>Autoregressive</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer (Vaswani et al., 2017)</cell><cell>✗</cell><cell>27.3</cell><cell></cell><cell>n</cell></row><row><cell>Transformer (Our Implementation)</cell><cell>✗</cell><cell>27.8</cell><cell>31.2</cell><cell>n</cell></row><row><cell>Non-Autoregressive</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAT (Gu et al., 2018)</cell><cell>✗</cell><cell>17.7</cell><cell>21.5</cell><cell>1</cell></row><row><cell>Iterative Refinement (Lee et al., 2018)</cell><cell>✗</cell><cell>21.6</cell><cell>25.5</cell><cell>10</cell></row><row><cell>Blockwise Parallel (Stern et al., 2018)</cell><cell>✗</cell><cell>27.4</cell><cell></cell><cell>≈ n/5</cell></row><row><cell>Insertion Transformer (Stern et al., 2019)</cell><cell>✗</cell><cell>27.4</cell><cell></cell><cell>≈ log 2 n ≪ 10</cell></row><row><cell>KERMIT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Unidirectional (p(y | x) or p(x | y)) Bidirectional (p(y | x) and p(x | y)) Joint (p(x, y)) + Marginal Refining (p(x) and p(y)) ֒→ Unidirectional Finetuning ֒→ Bidirectional Finetuning</cell><cell>✗ ✓ ✓ ✓ ✗ ✓</cell><cell>27.8 27.2 25.6 25.8 28.7 28.1</cell><cell>30.7 27.6 27.4 28.6 31.4 28.6</cell><cell>≈ log 2 n ≪ 10 ≈ log 2 n ≪ 10 ≈ log 2 n ≪ 10 ≈ log 2 n ≪ 10 ≈ log 2 n ≪ 10 ≈ log 2 n ≪ 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>WMT English ↔ German newstest2014 BLEU. Models capable of translating in both directions are marked with ↔.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>In order to develop such a concept, the town is reliant on the cooperation of its citizens.Predicted: Um ein solches Konzept zu entwickeln, ist die Stadt auf die Zusammenarbeit ihrer Bürger angewiesen. Um_ ein_ solches_ Konzept_ zu_ entwickeln_ , _ ist_ die_ Stadt_ auf_ die_ Zusammenarbeit_ ihrer_ Bürger_ angewiesen_ ._ Um_ ein_ solches_ Konzept_ zu_ entwickeln_ , _ ist_ die_ Stadt_ auf_ die_ Zusammenarbeit_ ihrer_ Bürger_ angewiesen_ ._ Um_ ein_ solches_ Konzept_ zu_ entwickeln_ , _ ist_ die_ Stadt_ auf_ die_ Zusammenarbeit_ ihrer_ Bürger_ angewiesen_ ._ Um_ ein_ solches_ Konzept_ zu_ entwickeln_ , _ ist_ die_ Stadt_ auf_ die_ Zusammenarbeit_ ihrer_ Bürger_ angewiesen_ ._ Um_ ein_ solches_ Konzept_ zu_ entwickeln_ , _ ist_ die_ Stadt_ auf_ die_ Zusammenarbeit_ ihrer_ Bürger_ angewiesen_ .</figDesc><table><row><cell>Parallel decode:</cell></row><row><cell>). These separately learn the conditional</cell></row><row><cell>distributions p(y | x) and p(x | y), mimicking the traditional conditional generation setup. On the</cell></row><row><cell>WMT 2014 test set, we achieve 27.8/30.7 BLEU with this approach, roughly matching our base</cell></row><row><cell>Transformer baseline of 27.8/31.2 BLEU. We also train a bidirectional model on the union of the</cell></row><row><cell>two unidirectional training sets, yielding a single model that captures both conditional distributions</cell></row></table><note>p(y | x) and p(x | y). We do not change any hyperparameters when training this model (i.e., we do not increase model capacity). The combined approach obtains 27.2/27.6 BLEU, nearly matching the baseline for English → German but falling slightly behind in the reverse direction. We also train a full joint model that captures the full joint distribution p(x, y) and factorizations thereof. Like the bidirectional model, the joint model can translate in either direction, but it canInput:_ . . .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. . . . .Input:Frühere Gespräche zwischen den Parteien haben nur wenig zur Beilegung der Spannungen beigetragen, die durch eine Reihe von Zusammenstößen in diesem Jahr befeuert wurden.Predicted: Previous talks between the parties have done little to resolve the tensions fueled by a series of clashes this year.Prev ious_ talks_ between_ the_ parties_ have_ done_ little_ to_ resolve_ the_ tensions_ fueled_ by_ a_ series_ of_ cla she s_ this_ year_ ._ Prev ious_ talks_ between_ the_ parties_ have_ done_ little_ to_ resolve_ the_ tensions_ fueled_ by_ a_ series_ of_ cla she s_ this_ year_ ._ Prev ious_ talks_ between_ the_ parties_ have_ done_ little_ to_ resolve_ the_ tensions_ fueled_ by_ a_ series_ of_ cla she s_ this_ year_ ._ Prev ious_ talks_ between_ the_ parties_ have_ done_ little_ to_ resolve_ the_ tensions_ fueled_ by_ a_ series_ of_ cla she s_ this_ year_ ._ Prev ious_ talks_ between_ the_ parties_ have_ done_ little_ to_ resolve_ the_ tensions_ fueled_ by_ a_ series_ of_ cla she s_ this_ year_ ._. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc><table><row><cell>Parallel decode:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>SQuAD zero-shot cloze question answering.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The 345M parameter "medium size" model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We give thanks to Samy Bengio, Zhifeng Chen, Jamie Kiros, Luheng He, Geoffrey Hinton, Quoc Le, Lala Li, Mohammad Norouzi, Yu Zhang, and the Google Brain team for useful discussions and technical assistance. Special thanks to Jamie Kiros for brainstorming the name KERMIT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">German: Dennoch sind wir mit Wut der Ansicht, dass die 500 Millionen keinen Widerspruch in einem gemeinsamen Produktivitätsansatz aufweisen</title>
	</analytic>
	<monogr>
		<title level="m">English: Nonetheless, we feel</title>
		<imprint/>
	</monogr>
	<note>with fury</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">English</forename><surname>Groundtruth</surname></persName>
		</author>
		<title level="m">Please tell us, in simple terms, about the work your research group does. German Groundtruth: -Beschreiben Sie bitte kurz, welche Forschungen Ihre Gruppe betreibt</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">English Seed: -Please tell us</title>
		<imprint/>
	</monogr>
	<note>in simple terms German Seed: welche Forschungen Ihre Gruppe betreibt</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">English: -Please tell us what research, in simple terms, what your group actually runs. German: -Bitte sagen Sie uns ganz einfach, welche Forschungen Ihre Gruppe eigentlich betreibt. English: -Please, tell us what research your group is doing, in more simple terms. German: -Bitte sagen Sie uns, welche Forschungen Ihre Gruppe einfacher betreibt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>English</surname></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Please tell us what your group will be conducting public research on, in simple terms. German: -Bitte teilen Sie uns einfach mit, welche Forschungen Ihre Gruppe betreibt</title>
		<imprint/>
	</monogr>
	<note>English: -Please tell us, what sort of research your group is undertaking in simple terms. German: -Bitte sagen Sie uns, welche Forschungen Ihre Gruppe betreibt</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-End Attentionbased Large Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transforming Question Answering Datasets Into Natural Language Inference Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-Autoregressive Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence-Level Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">WaveNet: A Generative Model for Raw Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<title level="m">Improving Language Understanding by Generative Pre-Training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Insertion Transformer: Flexible Sequence Generation via Insertion Operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blockwise Parallel Decoding for Deep Autoregressive Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ERNIE: Enhanced Representation through Knowledge Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tensor2Tensor for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMTA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tacotron: Towards End-to-End Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
