<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face Alignment in the Wild (3DFAW) Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian.bulat@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>yorgos.tzimiropoulos@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face Alignment in the Wild (3DFAW) Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D face alignment</term>
					<term>Convolutional Neural Networks</term>
					<term>Con- volutional Part Heatmap Regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our submission to the 1st 3D Face Alignment in the Wild (3DFAW) Challenge. Our method builds upon the idea of convolutional part heatmap regression [1], extending it for 3D face alignment. Our method decomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z (depth) estimation. At the first stage, our method estimates the X,Y coordinates of the facial landmarks by producing a set of 2D heatmaps, one for each landmark, using convolutional part heatmap regression. Then, these heatmaps, alongside the input RGB image, are used as input to a very deep subnetwork trained via residual learning for regressing the Z coordinate. Our method ranked 1st in the 3DFAW Challenge, surpassing the second best result by more than 22%. Code can be found at http://www.cs.nott.ac.uk/~psxab5/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face alignment is the problem of localizing a set of facial landmarks in 2D images. It is a well-studied problem in Computer Vision research, yet most of prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and challenges <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> have focused on frontal images. However, under a totally unconstrained scenario, faces might be in arbitrary poses. To address this limitation of prior work, recently, a few methods have been proposed for large pose face alignment <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. 3D face alignment goes one step further by treating the face as a full 3D object and attempting to localize the facial landmarks in 3D space. To boost research in 3D face alignment, the 1st Workshop on 3D Face Alignment in the Wild (3DFAW) &amp; Challenge is organized in conjunction with ECCV 2016 <ref type="bibr">[11]</ref>. In this paper, we describe a Convolutional Neural Network (CNN) architecture for 3D face alignment, that ranked 1st in the 3DFAW Challenge, surpassing the second best result by more than 22%.</p><p>Our method is a CNN cascade consisting of three connected subnetworks, all learned via residual learning <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13]</ref>. See <ref type="figure" target="#fig_0">Fig. 1</ref>. The first two subnetworks perform residual part heatmap regression <ref type="bibr" target="#b0">[1]</ref> for estimating the X,Y coordinates of the facial landmarks. As in <ref type="bibr" target="#b0">[1]</ref>, the first subnetwork is a part detection network trained to detect the individual facial landmarks using a per-pixel softmax loss. The output of this subnetwork is a set of N landmark detection heatmaps. The second subnetwork is a regression subnetwork that jointly regresses the landmark detection heatmaps stacked with image features to confidence maps representing the location of the landmarks. Then, on top of the first two subnetworks, we added a third very deep subnetwork that estimates the Z coordinate of each fiducial point. The newly introduced network is guided by the heatmaps produced by the 2D regression subnetwork and subsequently learns where to "look" by explicitly exploiting information about the 2D location of the landmarks. We show that the proposed method produces remarkable fitting results for both X,Y and Z coordinates, securing the first place on the 3DFAW Challenge.</p><p>This paper is organized as follows: section 2 describes our system in detail. Section 3 describes the experiments performed and our results on the 3DFAW dataset. Finally, section 4 summarizes our contributions and concludes the paper. The system submitted to the 3DFAW Challenge. The part detection and regression subnetworks implement convolutional part heatmap regression, as described in <ref type="bibr" target="#b0">[1]</ref>, and produce a series of N heatmaps, one for the X,Y location of each landmark. They are both very deep networks trained via residual learning <ref type="bibr" target="#b11">[13]</ref>. The produced heatmaps are then stacked alongside the input RGB image, and used as input to the Z regressor which regresses the depth of each point. The architecture for the Z regressor is based on ResNet <ref type="bibr" target="#b11">[13]</ref>, as described in section 2.2 .</p><p>The proposed method adopts a divide et impera technique, splitting the 3D facial landmark estimation problem into two tasks as follows: The first task estimates the X,Y coordinates of the facial landmarks and produces a series of N regression heatmaps, one for each landmark, using the network described in section 2.1. The second task, described in section 2.2, predicts the Z coordinate (i.e. the depth of each landmark), using as input the stacked heatmaps produced by the first task, alongside the input RGB image. The overall architecture is illustrated in <ref type="figure" target="#fig_0">Fig.1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">2D (X,Y) landmark heatmap regression</head><p>The first part of our system estimates the X,Y coordinates of each facial landmark using part heatmap regression <ref type="bibr" target="#b0">[1]</ref>. The network consists of two connected subnetworks, the first of which performs landmark detection while the second one refines the initial estimation of the landmarks' location via regression. Both networks are very deep been trained via residual learning <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13]</ref>. In the following, we briefly describe the two subnetworks; the exact network architecture and layer specification for each of them are described in detail in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Landmark detection subnetwork. The architecture of the landmark detection network is based on a ResNet-152 model <ref type="bibr" target="#b10">[12]</ref>. The network was adapted for landmark localization by: (1) removing the fully connected layer alongside the preceding average pooling layer, (2) changing the stride of the 5th block from 2 to 1 pixels, and (3) adding at the end a deconvolution <ref type="bibr" target="#b12">[14]</ref> followed by a fully convolutional layer. These changes convert the model to a fully convolutional network, recovering to some extent the lost spatial resolution. The ground truth was encoded as a set of N binary maps (one for each landmark), where the values located within the radius of the provided ground truth landmark are set to 1, while the rest to 0. The radius defining the "correct location" was empirically set to 7 pixels, for a face with a bounding box height equal to approximately 220 pixels. The network was trained using the pixel wise sigmoid cross entropy loss function.</p><p>Landmark regression subnetwork. As in <ref type="bibr" target="#b0">[1]</ref>, the regression subnetwork plays the role of a graphical model aiming to refine the initial prediction of the landmark detection network. It is based on a modified version of the "hourglass network" <ref type="bibr" target="#b13">[15]</ref>. The hourglass network starts from the idea presented in <ref type="bibr" target="#b14">[16]</ref>, improving a few important concepts: (1) it updates the model using residual learning <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13]</ref>, and <ref type="formula" target="#formula_1">(2)</ref> introduces an efficient way to analyze and recombine features at different resolutions. Finally, as in <ref type="bibr" target="#b0">[1]</ref>, we replaced the original nearest neighbour upsampling of <ref type="bibr" target="#b13">[15]</ref> by learnable deconvolutional layers, and added another deconvolutional layer in the end that brings the output to the input resolution. The network was then trained using a pixel wise L2 loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Z regression</head><p>In this section, we introduce a third subnetwork for estimating the Z coordinate i.e. the depth of each landmark. As with the X,Y coordinates, the estimation of the Z coordinate is performed jointly for all landmarks. The input to the Z regressor subnetwork is the stacked regression heatmaps produced by the regression subnetwork alongside the input RGB image. The use of the stacked heatmaps is a key feature of the subnetwork as they provide pose-related information (encoded by the X,Y location of all the landmarks) and guide the network where to "look", explicitly showing where the depth should be estimated.  <ref type="table">Table 1</ref>. See also text.</p><p>. <ref type="table">Table 1</ref>: Block specification for the Z regression network. Torch notations (channels, kernel, stride) and (kernel, stride) are used to define the conv and pooling layers. The bottleneck modules are defined as in <ref type="bibr" target="#b11">[13]</ref>. We encode each landmark as a heatmap using a 2D Gaussian with std=6 pixels centered at the X,Y coordinates of that landmark. The proposed Z regression network is based on the latest ResNet-200 network with preactivation modules <ref type="bibr" target="#b11">[13]</ref> modified as follows: in order to adapt the model for 1D regression, we replaced the last fully connected layer (used for classification) with one that has N output channels, one for each landmark. Additionally, the first convolutional layer of the network was modified to accommodate 3+N input channels. The network is described in detail in <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="table">Table 1</ref>. All newly introduced filters were initialized randomly using a Gaussian distribution. Finally, the network was trained using the L2 loss:</p><formula xml:id="formula_0">l 2 = 1 N N n=1 ( z n − z n ) 2 ,<label>(1)</label></formula><p>where z n and z n are the predicted and ground truth Z values (in pixels) for the nth landmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>For training, all images were cropped around an extended (by 20-25%) bounding box, and then resized so that the final cropped image had a resolution of 384x384 pixels. While batch normalization is known to prevent overfitting to some extent, we additionally augmented the data with a set of image transformations applied randomly: flipping, in-plane rotation (between −35 o and 35 o ), scaling (between 0.85 and 1.15) and colour jittering. While the entire system, shown in <ref type="figure" target="#fig_0">Fig.1</ref>, can be trained jointly from the beginning, in order to accelerate convergence we trained each task independently. For 2D (X,Y) landmark heatmap regression, the network was fine-tuned from a pretrained model on the large ImageNet <ref type="bibr" target="#b15">[17]</ref> dataset, with the newly introduced layers initialized with zeros. The detection component was then trained for 30 epochs with a learning rate progressively decreasing from 1e − 3 to 2.5e − 5. The regression subnetwork, based on the "hourglass" architecture was trained for 30 epochs using a learning rate that varied from 1e − 4 to 2.5e − 5. During this, the learning rate for the detection subnetwork was frozen. All the newly introduced deconvolutional layers were initialised using the bilinear upsampling filters. Finally, the subnetworks were trained jointly for 30 more epochs.</p><p>For Z regression, we again fine-tuned from a model previously trained on Im-ageNet <ref type="bibr" target="#b15">[17]</ref>; this time we used a ResNet-200 network <ref type="bibr" target="#b11">[13]</ref>. The newly introduced filters in the first convolutional layer were initialized from a Gaussian distribution with std=0.01. The same applied for the fully connected layer added at the end of the network. During training, we used as input to the Z regression network both the heatmaps generated from the ground truth landmark locations and the ones estimated by the first task. We trained the subnetwork for about 100 epochs with a learning rate varying from 1e − 2 to 2.5e − 4.</p><p>The network was implemented and trained using Torch7 <ref type="bibr" target="#b16">[18]</ref> on two Titan X 12Gb GPUs using a batch of 8 and 16 images for X,Y landmark heatmap regression and Z regression, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>In this section, we present the performance of our system on the 3DFAW dataset.</p><p>Dataset. We trained and tested our model on the 3D Face Alignment in the Wild (3DFAW) dataset. <ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b18">[20]</ref><ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref> The dataset contains images from a wide range of conditions, captured in both controlled and in-the-wild settings. The dataset includes images from MultiPIE <ref type="bibr" target="#b17">[19]</ref> and BP4D <ref type="bibr" target="#b19">[21]</ref> as well as images collected from the Internet. All images were annotated in a consistent way with 66 3D fiducial points. The final model was trained on the training set (13672 images) and the validation set (4725 images), and tested on the test set containing 4912 images. We also report results on the validation set with a model trained on the training set, only.</p><p>Metrics. Evaluation was performed using two different metrics: Ground Truth Error(GTE) and Cross View Ground Truth Consistency Error(CVGTCE).</p><p>GTE measures the average point-to-point Euclidean error normalized by the inter-ocular distance, as in <ref type="bibr" target="#b4">[5]</ref>. GTE is calculated as follows:</p><formula xml:id="formula_1">E(X, Y) = 1 N N n=1 x n − y n 2 d i ,<label>(2)</label></formula><p>where X is the predicted set of points, Y is their corresponding ground truth and d i denotes the interocular distance for the ith image. CVGTCE evaluates the cross-view consistency of the predicted landmarks of the same subject and is defined as follows:</p><formula xml:id="formula_2">E vc (X, Y, T ) = 1 N N n=1 sRx n − y n 2 d i ,<label>(3)</label></formula><p>where P = {s, R, t} denotes scale, rotation and translation, respectively. CVGTCE is computed as follows:</p><formula xml:id="formula_3">{s, R, t} = argmin s,R,t N n=1 y k − (sRx k + t) 2 2 .<label>(4)</label></formula><p>Results. <ref type="table" target="#tab_1">Table 2</ref> shows the performance of our system on the 3DFAW test set, as provided by the 3DFAW Challenge team. As it can be observed, our system outperforms the second best method (the result is taken from the 3DFAW Challenge website) in terms of GTE by more than 22%. In order to better understand the performance of our system, we also report results on the validation set using a model trained on the training set, only. To measure performance, we used the Ground Truth Error measured on (X,Y), (X,Y,Z), X alone, Y alone and Z alone, and report the cumulative curve corresponding to the fraction of images for which the error was less than a specific value. Results are reported in <ref type="figure" target="#fig_3">Fig.3</ref> and <ref type="table" target="#tab_2">Table 3</ref>. It can be observed that our system performs better on predicting the X and Y coordinates (compared to Z), but this difference is quite small and, to some extent, expected as Z is estimated at a later stage of the cascade. Finally, <ref type="figure">Fig.4</ref> shows a few fitting results produced by our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we proposed a two-stage CNN cascade for 3D face alignment. The method is based on the idea of splitting the 3D alignment task into two separate subtasks: 2D landmark estimation and 1D (depth) estimation, where the first one guides the second. Our system secured the first place in the 1st 3D Face Alignment in the Wild (3DFAW) Challenge.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgment</head><p>This work was supported in part by the EPSRC project EP/M02153X/1 Facial Deformable Models of Animals. <ref type="figure">Fig. 4</ref>: Fitting results produced by our system on the 3DFAW test set. Observe that our method copes well with a large variety of poses and facial expressions on both controlled and in-the-wild images. Best viewed in colour.</p><p>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The system submitted to the 3DFAW Challenge. The part detection and regression subnetworks implement convolutional part heatmap regression, as described in [1], and produce a series of N heatmaps, one for the X,Y location of each landmark. They are both very deep networks trained via residual learning [13]. The produced heatmaps are then stacked alongside the input RGB image, and used as input to the Z regressor which regresses the depth of each point. The architecture for the Z regressor is based on ResNet [13], as described in section 2.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The architecture of the Z regression subnetwork. The network is based on ResNet-200 (with preactivation) and its composing blocks. The blocks B1-B6 are defined in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>GTE vs fraction of test images on the 3DFAW validation set, on (X,Y), (X,Y,Z), X alone, Y alone and Z alone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance on the 3DFAW test set.</figDesc><table><row><cell>Method</cell><cell>GTE (%)</cell><cell>CVGTCE</cell></row><row><cell></cell><cell></cell><cell>(%)</cell></row><row><cell>Ours</cell><cell>4.5623</cell><cell>3.4767</cell></row><row><cell cols="2">Second best 5.8835</cell><cell>3.9700</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance on the 3DFAW validation set, on (X,Y), (X,Y,Z), X alone, Y alone and Z alone.</figDesc><table><row><cell>Axes</cell><cell>GTE (%)</cell></row><row><cell>xy</cell><cell>3.6263</cell></row><row><cell>xyz</cell><cell>4.9408</cell></row><row><cell>x</cell><cell>2.12</cell></row><row><cell>y</cell><cell>2.48</cell></row><row><cell>z</cell><cell>2.77</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Method z x,y x,y,z</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A semi-automatic methodology for facial landmark annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<editor>CVPR-W.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<editor>ICCV-W.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large-pose face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<ptr target="http://mhug.disi.unitn.it/workshop/3dfaw" />
	</analytic>
	<monogr>
		<title level="m">BMVC. (2016) 11. : 1st Workshop on 3D Face Alignment in the Wild (3DFAW) &amp; Challenge</title>
		<imprint>
			<biblScope unit="page" from="2016" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV, IEEE</publisher>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06937</idno>
		<title level="m">Stacked hourglass networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W. Number EPFL-CONF-192376</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A high-resolution 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2008. FG&apos;08. 8th IEEE International Conference On</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense 3d face alignment from 2d video for real-time use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
