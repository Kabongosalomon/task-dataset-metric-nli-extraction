<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
							<email>yourenchun@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
							<email>guozhiyao45@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
							<email>longxiang@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
							<email>baoyingze@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
							<email>wenshilei@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baidu</forename><surname>Vis</surname></persName>
						</author>
						<title level="a" type="main">Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-label image and video classification are fundamental yet challenging tasks in computer vision. The main challenges lie in capturing spatial or temporal dependencies between labels and discovering the locations of discriminative features for each class. In order to overcome these challenges, we propose to use cross-modality attention with semantic graph embedding for multi-label classification. Based on the constructed label graph, we propose an adjacency-based similarity graph embedding method to learn semantic label embeddings, which explicitly exploit label relationships. Then our novel cross-modality attention maps are generated with the guidance of learned label embeddings. Experiments on two multi-label image classification datasets (MS-COCO and NUS-WIDE) show our method outperforms other existing state-of-the-arts. In addition, we validate our method on a large multi-label video classification dataset (YouTube-8M Segments) and the evaluation results demonstrate the generalization capability of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-label image classification (MLIC) and multi-label video classification (MLVC) are important tasks in computer vision, where the goal is to predict a set of categories present in an image or a video. Compared with single-label classification (e.g. assigns one label to an image or video), multilabel classification is more useful in many applications such as internet search, security surveillance, robotics, etc. Since MLIC and MLVC are very similar tasks, in the following technical discussion we will mainly focus on MLIC, whose conclusions can be migrated to MLVC naturally.</p><p>Recently, single-label image classification has achieved great success thanks to the evolution of deep Convolutional Neural Networks (CNN) <ref type="bibr" target="#b10">(He et al. 2016;</ref><ref type="bibr" target="#b20">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b20">Szegedy et al. 2016</ref>). Single-label image classification can be naively extended to MLIC tasks by treating the problem as a series of singlelabel classification tasks. However, such naive extension usually provides poor performance, since the semantic dependencies among multiple labels are ignored, which are especially important for multi-label classification. Therefore, a number of prior works aim to capture the label relations by Recurrent Neural Networks (RNN). However, these methods do not model the explicit relationships between semantic labels and image regions, thus they lack the capacity of sufficient exploitation of the spatial dependency in images.</p><p>An alternative solution for MLIC is to introduce object detection techniques. Some methods <ref type="bibr" target="#b21">(Wei et al. 2014;</ref><ref type="bibr" target="#b23">Zhang et al. 2018;</ref><ref type="bibr" target="#b9">Hao et al. 2016</ref>) extract region proposals using extra bounding box annotations, which are much more expensive to label than simple image level annotations. Many other methods <ref type="bibr" target="#b23">Zhu et al. 2017)</ref> apply attention mechanism to automatically focus on the regions of interest. However, the attentional regions are learned only with image-level supervision, which lacks explicit semantic guidance.</p><p>To address above issues, we argue that an effective model for multi-label classification should reach two capacities: (1) capturing semantic dependencies among multiple labels in terms of spatial context; (2) locating regions of interest with more semantic guidance.</p><p>In this paper, we propose a novel cross-modality attention network associated with graph embedding, so as to simultaneously search for discriminative regions and label spatial semantic dependencies. Firstly, we introduce a novel Adjacency-based Similarity Graph Embedding (ASGE) method which captures the rich semantic relations between labels. Secondly, the learned label embedding will guide the generation of attentional regions in terms of crossmodality guidance, which is referred to as Cross-modality Attention (CMA) in this paper. Compared with traditional self-attention methods, our attention explicitly introduces the rich label semantic relations. Benefiting from the CMA mechanism, our attentional regions are more meaningful and discriminative. Therefore they capture more useful information while suppressing the noise or background information for classification. Furthermore, the spatial context dependencies of labels will be captured, which further improve the performance in MLIC.</p><p>The major contributions of this paper are briefly summa- rized as follows:</p><p>• We propose an ASGE method to learn semantic label embedding and exploit label correlations explicitly.</p><p>• We propose a novel attention paradigm, namely crossmodality attention, where the attention maps are generated by leveraging more prior semantic information, resulting in more meaningful attention maps.</p><p>• A general framework combining CMA and ASGE module, as shown in <ref type="figure" target="#fig_0">Fig.1 and Fig.2</ref>, is proposed for multilabel classification, which can capture dependencies between spatial and semantic space and discover the location of discriminative feature effectively. We evaluate our framework on MS-COCO dataset and NUS-WIDE dataset for MLIC task, and new state-of-the-art performances are achieved on both of them. We also evaluate our proposed method on YouTube-8M dataset for MLVC, which also achieves remarkable performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The task of MLIC has attracted an increasing interest recently. The easiest way to address this problem is to treat each category independently, then the task can be directly converted into a series of binary classification tasks <ref type="bibr" target="#b5">(Chen et al. 2019)</ref>. However, such techniques are limited by without considering the relationships between labels. Several approaches have been applied to model the correlations between labels. <ref type="bibr" target="#b19">Read et al. (2011)</ref> extends the multilabel classification by training the chain binary-classifiers and introducing the correlations between labels by inputting the previously predicted labels. Some other works <ref type="bibr" target="#b16">(Li, Zhao, and Guo 2014;</ref> formulate the task as a structural inference problem based on probabilistic graphical models. Besides, the latest work <ref type="bibr" target="#b5">(Chen et al. 2019</ref>) explores the label dependencies by graph convolutional network. However, none of aforementioned methods consider the associations between semantic labels and image contents, and the spatial contexts of images have not been sufficiently exploited.</p><p>In MLIC task, visual concepts are highly related with local image regions. To explore information in local regions better, some works <ref type="bibr" target="#b21">(Wei et al. 2014;</ref><ref type="bibr" target="#b9">Hao et al. 2016)</ref> introduce region proposal techniques to focus on informative regions. <ref type="bibr" target="#b21">Wei et al. (2014)</ref> extracts an arbitrary number of object hypotheses, then inputs them into the shared CNN and aggregates the output with max pooling to obtain the ultimate multi-label predictions. <ref type="bibr" target="#b9">Hao et al. (2016)</ref> introduces local information provided by generated proposals to boost the discriminative power of feature extraction. Although above methods have used region proposals to enhance feature representation, they are still limited by requiring extra objectlevel annotations and without considering the dependencies between objects.</p><p>Alternatively,  discovers the attentional regions corresponding to multiple semantic labels by spatial transformer network and captures the spatial dependencies of the regions by Long Short-Term Memory (LSTM). Analogously, <ref type="bibr" target="#b23">Zhu et al. (2017)</ref> proposes the spatial regularization network to generate label-related attention maps and capture the latent relationships by attention maps implicitly. The advantage of above attention approaches is that no additional step of obtaining region proposal is needed. Nevertheless, the attentional regions are learned only with image-level supervision, which lacks of explicit semantic guidance. While in this paper, the semantic guidance is introduced to the generation of attention maps by leveraging label semantic embeddings, which improves the prediction performance significantly.</p><p>In this paper, the label semantic embeddings are learned by graph embedding, which is a technique aiming to learn representation of graph-structured data. The approaches of graph embedding mainly contain matrix factorization-based <ref type="bibr" target="#b3">(Cao, Lu, and Xu 2015)</ref>, random walk-based <ref type="bibr" target="#b19">(Perozzi, Al-Rfou, and Skiena 2014)</ref> and neural network-based methods <ref type="bibr" target="#b23">Zhu et al. 2018)</ref>. A main assumption of these approaches is the embeddings of adjacent nodes on the graph are similar, while in our task, we also require embeddings of non-adjacent nodes are mutually ex-clusive from each other. Therefore, we propose an ASGE method, which can further separate the embeddings of nonadjacent nodes.</p><p>MLVC is similar to MLIC, but it involves additional temporal relationships <ref type="bibr" target="#b7">(Gan et al. 2015;</ref><ref type="bibr" target="#b18">Long et al. 2018a;</ref><ref type="bibr" target="#b2">Campos et al. 2017;</ref><ref type="bibr" target="#b0">Arandjelovic et al. 2016;</ref><ref type="bibr" target="#b22">Wu, Ma, and Hu 2017;</ref><ref type="bibr" target="#b18">Long et al. 2018b</ref>). It has been applied to many applications such as emotion recognition , human activity understanding <ref type="bibr" target="#b1">(Caba Heilbron et al. 2015)</ref>, and event detection <ref type="bibr" target="#b23">(Xu, Yang, and Hauptmann 2015)</ref>. In this paper, we validate our proposed method both in MLIC and MLVC task, and achieve remarkable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The overall frameworks of our approach for MLIC and MLVC are shown in <ref type="figure" target="#fig_0">Fig.1 and Fig.2</ref> respectively. The pipeline includes several stages: Firstly, the label graph is taken as the input of ASGE module to learn label embeddings which encode the semantic relationships between labels. Secondly, the learned label embeddings and visual features will be fed together into the CMA module to obtain category-wise attention maps. Finally, the category-wise attention maps are used to weightedly average the visual features for each category. We will describe our two key components ASGE and CMA in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adjacency-based Similarity GE</head><p>The relationships between labels play a crucial role in multilabel classification task as discussed in section 1. However, how to express such relationships is an open issue to be solved. Our intuition is that the co-occurrence properties between labels can be described as joint probability, which is suitable for modeling the label relationships. Nevertheless, the joint probability is easy to suffer from the influence of class imbalance. Instead, we utilize the conditional probability between labels to solve this issue, which is obtained by normalizing the joint probability through dividing by marginal probability. Based on this, it is possible to construct a label graph where the labels are nodes and the conditional probability between the labels is edge weight. Inspired by the popular applications of graph embedding method in natural language processing (NLP) tasks, where the learned label embeddings are entered into the network as additional information, we propose a novel ASGE method to encode the label relationships.</p><p>We formally define the graph as</p><formula xml:id="formula_0">G = (V, C), where V = {v 1 , v 2 , ..., v N } represents the set of N nodes and C represents the edges. The adjacency matrix A = {A ij } N i,j=1</formula><p>of graph G contains non-negative weights associated with each edge. Specifically, V is the set of labels and C is the set of connections between any two labels, and the adjacency matrix A is the conditional probability matrix by setting</p><formula xml:id="formula_1">A ij = P (v i /v j ), where P is calculated through training set. Since P (v i |v j ) = P (v j |v i ), namely A ij = A ji ,</formula><p>in order to facilitate a better optimization, we symmetrize A by</p><formula xml:id="formula_2">A = 1 2 (A + A ) .<label>(1)</label></formula><p>To capture the label correlations defined by the graph structure, we apply a neural network to map the one-hot embedding of each label o i to semantic embedding space and produce the label embedding</p><formula xml:id="formula_3">e i = Φ(o i ),<label>(2)</label></formula><p>where Φ denotes the neural network which consists of three fully-connected layers followed by Batch Normalization(BN) and ReLU activation. Our goal is to achieve the optimal label embedding set E = {e i } N i=0 , where e i ∈ R Ce . Such that cos(e i , e j ) is close to A ij for all i, j, where cos(e i , e j ) denotes the cosine similarity between e i and e j . Thereby, the objective function is defined as follows:</p><formula xml:id="formula_4">L ge = N i=1 N j=1 e i e j e i e j − A ij 2 ,<label>(3)</label></formula><p>where L ge denotes the loss of our graph embedding.</p><p>Optimization Relaxation. In order to optimize Eq.3, the cosine similarity cos(e i , e j ) are required to be close to the corresponding edge weight A ij for all i, j. However, it is hard to satisfy this strict constraint, especially when the graph is large and sparse. In order to address this problem, a hyperparameter α is introduced to the Eq.3 to relax the optimization. The new objective function is as follows:</p><formula xml:id="formula_5">L ge = N i=1 N j=1 σ ij · e i e j e i e j − A ij 2 ,<label>(4)</label></formula><p>where σ ij is an indicator function:</p><formula xml:id="formula_6">σ ij = 0, A ij &lt; α and e i ej ei ej &lt; α 1, otherwise .<label>(5)</label></formula><p>By adding this relaxation, it only needs to make the embedding pairs (e i , e j ) be away instead of strictly enforcing cos(e i , e j ) to be A ij when A ij &lt; α, thus focusing more on the strong relationships between labels and reducing the difficulty of the optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CMA for Multi-label Classification</head><p>We formally define the multi-label classification task as a mapping function F : x → y, where x denotes a input image or video, y = [y 1 , y 2 , ..., y N ] denotes corresponding labels, N is the total number of categories and y n ∈ {0, 1} denotes whether the label is assigned to the image or video.</p><p>For multi-label classification, we propose an novel attention mechanism, named cross-modality attention, which uses semantic embeddings to guide spatial or temporal integration of visual features. The semantic embeddings here are the label embedding set E = {e i } N i=0 achieved by ASGE and the visual features I = ψ(x) are extracted by backbone neural networks ψ. Note that for different tasks, we only need to apply different backbones to extract visual features, and the rest part of the framework is completely generic for both tasks.</p><p>Backbone. In the MLIC task, we apply ResNet-101 network to extract the last convolutional feature map as the visual features. Additionally, we use an 1 × 1 convolution to reduce the dimension, and obtain final visual feature map I ∈ R H×W ×C , where H × W is the spatial resolution of the last feature map and C is the number of channels.</p><p>In the MLVC task, frame level features x are pre-extracted by an Inception network and then processed by PCA with whitening. Considering the meaningful and discriminative information of video is derived from some pivotal frames while others may be redundant, we apply a Squeezing Network (SNet) to squeeze the temporal dimensions. The SNet is built on 4 successive 1D convolution and pooling layers. We can obtain the final visual feature I = f SN et (x), where I ∈ R T ×C and T is the temporal resolution of the final feature map and C is the number of channels.</p><p>Cross-Modality Attention. The learned label embeddings by ASGE compose a semantic embedding space, while the extracted features from CNN Backbone define a visual feature space. Our goal is to let semantic embeddings guide the generation of attention maps. However, semantic embedding space and visual feature space exist a semantic gap because of modality difference. In order to measure the compatibility between different modalities, we first learn a mapping function from the visual feature space to the semantic embedding space, then the compatibility can be measured by a cosine similarity between projected visual feature and semantic embedding, namely cross-modality attention. Formal definition is introduced as follows.</p><p>Firstly, we project the visual feature to semantic space by a Cross-Modality Transformer (CMT) module, which is built with several 1 × 1 convolution layers followed by a BN and a ReLU activation.</p><p>I s = f cmt (I) (6) where I s ∈ R M ×Ce (M = W × H for image and M = T for video), f cmt denotes the map function of the CMT module. The category-specific cross-modality attention map z i k is yielded by calculating the cosine similarity between label embedding e k and projected visual feature vector I i s at location i of I s :</p><formula xml:id="formula_7">z i k = ReLU I i s e k I i s e k .<label>(7)</label></formula><p>The category-specific attention map z i k is then normalized to:</p><formula xml:id="formula_8">a i k = z i k M i=1 z i k .<label>(8)</label></formula><p>For each location i, if the CMA mechanism generates a high positive value, it can be interpreted as the location i is highly semantic related to label embedding k or relative more important than other locations, thus the model needs to focus on location i when considering category k . Then the category-specific cross-modality attention map is used to weightedly average the visual feature vectors for each category: where h k is the final feature vector for label k. Then h k is fed into the fully-connected layers for estimating probability of category k:</p><formula xml:id="formula_9">h k = M i=1 α i k I i ,<label>(9)</label></formula><formula xml:id="formula_10">y * k = σ(w k h k + b),<label>(10)</label></formula><p>where w k ∈ R C and b are learnable parameters. y * k is the predicted probability for label k. For convenience, we denote the calculation of whole CMA module as y * k = f cma (I, E).</p><p>Compared with general single attention map method, where the attention map is shared by all categories, our CMA module benefits in two ways: Firstly, our categorywise attention map is related to image regions corresponding to category k, thus better learn category-related regions. Secondly, with the guidance of label semantic embeddings, the discovered attentional regions can be better match with the annotated semantic labels.</p><p>The potential advantage of our framework is to capture the latent spatial dependency, which is helpful for visual ambiguous labels. As shown in <ref type="figure" target="#fig_1">Fig.3</ref>, we consider frisbee as an example to explain the spatial dependency. Firstly, The ASGE module learns label embeddings through the label graph, which encodes the label relationships. Since the dog and frisbee are often co-exist while glasses not, therefore the label embeddings of dog and frisbee are close to each other and far away from glasses's, namely e d ≈ e f = e g . The optimization procedure during training will enforce the cosine similarity between visual feature and the corresponding label embedding become high, in other words, the cos(e d , v d ), cos(e g , v g ) and cos(e f , v f ) will be large. Since e d ≈ e f = e g , cos(e f , v d ) will also be large, while cos(e f , v g ) will be small. And the final feature representation of frisbee is h</p><formula xml:id="formula_11">f = β 1 v g + β 2 v f + β 3 v d ,where β 1 = cos(e f , v g ), β 2 = cos(e f , v f ), β 3 = cos(e f , v d ).</formula><p>Thus, the recognition of frisbee is depending on the semantic related label dog and not related to label glasses, indicating that our model is capable of capturing spatial dependencies. Specially, considering that the frisbee is a hard case to be recognized, β 2 will be small. Fortunately, the β 3 may still be large, so the visual information of dog will be a helpful context to aid in the recognition of label frisbee.</p><p>Multi-Scale CMA. Single-scale feature representation may not be sufficient for multiple objects from different scales. It is noteworthy that the calculation of attention involves the label embedding slides densely over all locations of the feature map, in other words, the spatial resolution of feature map may effect on attention result. Our intuition is that the low-resolution feature maps have more representational capacity for small objects while high-resolution is opposite. The design of CMA mechanism makes it can be naturally applied to multi-scale feature maps via a score fusion strategy. Specially, we extract a set of feature maps {I 1 , I 2 , ..., I L } and the final predicted probability of multiscale CMA is</p><formula xml:id="formula_12">y * k = 1 L L l=1 f cma (I l , E).<label>(11)</label></formula><p>Training Loss. Finally we define our object function for multi-label classification as follows</p><formula xml:id="formula_13">L(θ) = − N k=1 w k [y k · log(y * k ) + (1 − y k ) · (1 − log(y * k ))] w k = y k · e β(1−p k ) + (1 − y k ) · e βp k ,<label>(12)</label></formula><p>Where w k is used to alleviate the class imbalance, β is a hyperparameter and p k is the ratio of label k in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To assess our model, we perform experiments on two benchmark multi-label image recognition datasets (MS-COCO ) and NUS-WIDE <ref type="bibr" target="#b6">(Chua et al. 2009)</ref>) . We also validate the effectiveness of our model on one multilabel video recognition dataset (YouTube-8M Segments) , and the results demonstrate the extensibility of our method. In this section, we will introduce the results on MLIC and MLVC respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-label Image Classification</head><p>Implementation Details. In ASGE module, the dimensions of the three hidden layers and label embeddings are all set as 256. The optimization relaxation is not applied here since the label graph is relatively small. The optimizer is Stochastic Gradient Descent (SGD) with momentum 0.9 and the initial learning rate is 0.01. In the classification part, the input image is randomly cropped and resized to 448 × 448 with random horizontal flip for augmentation. The batch size is set as 64. The optimizer is SGD with momentum 0.9. Weight decay is 10 −5 . The initial learning rate is 0.01 and decays by a factor 10 every 30 epochs. And the hyperparameter β in the Eq.12 is 0. in MS-COCO dataset and 0.4 in NUS-WIDE dataset. Based on this setup, we implement two models: CMA and Multi-Scale CMA(MS-CMA). The MS-CMA model uses three scale features I 1 ∈ R 28×28×1024 , I 2 ∈ R 14×14×2048 from ResNet-101 backbone and I 3 ∈ R 7×7×512 obtained by applying a residual block on I 2 . While the CMA model only uses I 2 .</p><p>Evaluation Metrics. We use the same evaluation metrics as other works , which are the percategory and overall metrics: precision (CP and OP), recall (CR and OR) and F1 (CF1 and OF1). In addition, we also calculate the mean average precision (mAP), which is relatively more important than other metrics, and we mainly focus on the performance of mAP.</p><p>Results on MS-COCO Dataset. The MS-COCO dataset is widely used in MLIC task. It contains 122,218 images with 80 labels and almost 2.9 labels per image. We divide the dataset into two parts: 82,081 images for training and 40,137 images for testing, according to the officially provided division criteria.</p><p>We compare with the currently published state-of-the-art methods, including CNN-RNN , RNN-Attention , Order-Free RNN , ML-ZSL , <ref type="bibr">SRN (Zhu et al. 2017)</ref> and Multi-Evidence <ref type="bibr" target="#b8">(Ge, Yang, and Yu 2018)</ref>. Besides, we run the source code released by ML-GCN <ref type="bibr" target="#b5">(Chen et al. 2019)</ref> to train and get the results for comparison. The quantitative results of CMA and MS-CMA model are shown in <ref type="table">Table 1</ref>. Our two models both perform better than the state-of-the-art methods over almost all metrics. Specially, our MS-CMA model achieves better performance than the CMA model, demonstrating the multi-scale attentions yield performance improvement.</p><p>Results on NUS-WIDE Dataset. The NUS-WIDE is a web dataset including 269,648 images and 5018 labels from the Flickr. After removing the noise and the rare labels, there are 1000 categories left. The images are further manually annotated into 81 concepts with 2.4 concepts per image on average. We follow the split used in <ref type="bibr" target="#b18">(Liu et al. 2018)</ref>, i.e. 150,000 images for training and 59,347 for testing after removing the images without any labels.</p><p>In this dataset, we compare with the current state-of-theart models, including CNN-RNN  The quantitative results are shown in the <ref type="table" target="#tab_2">Table 2</ref>. The comparison results are similar to MS-COCO's. Our CMA and MS-CMA perform better than state-of-the-art methods on most metrics. The mAP metric of our MS-CMA, which is mostly concerned, exceeds the previous state-of-the-art result by 1.3%. We observe that the average edge weight per label of NUS-WIDE is 3.3, while that of MS-COCO is 3.9. This shows that the label graph of MS-COCO is denser than NUS-WIDE. And the performance gain of NUS-WIDE is slightly less obvious than that of MS-COCO. These observations indicate that richer label relationships may bring performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All</head><p>Top  Ablation Study. In this section, we expect to answer the following questions:</p><p>• Comparing with the backbone (ResNet-101) model, does our CMA model improve significantly?</p><p>• Does our proposed CMA mechanism have an advantage over the general self-attention methods?</p><p>• Can the CMA extend to multi-scale and bring performance improvement?</p><p>• Is our ASGE more advantageous than other embedding methods, e.g. Word2vec?</p><p>To answer these questions, we conduct some ablation studies on the MS-COCO dataset, as shown in table 3. Firstly, we investigate how CMA contributes to mAP. It is obvious to see that the vanilla ResNet-101 achieves 79.9% mAP, while increases to 83.4% when CMA module is added. This result shows the significant effectiveness of the CMA mechanism. Secondly, we implement a general selfattention method by replacing Eq.7 with z i = σ(f conv (I i )), where f conv denotes the map function of 1 × 1 convolution layer. Our CMA mechanism performs better than general self-attention mechanism by achieving 2.3% mAP improvement, which indicates that the label semantic embeddings Methods mAP ResNet-101 (2016) 79.9</p><p>Self-attention 81.1 W2V-MS-CMA 82.5 CMA 83.4 MS-CMA 83.8 <ref type="table">Table 3</ref>: Comparison of mAP with several models on the MS-COCO dataset.</p><p>guided attention mechanism is superior to the general selfattention due to introducing much more prior information. Thirdly, expanding our CMA mechanism to multiple scales could obtain about 0.4% improvement. This result demonstrates that our attention mechanism is well adapted for multi-scale features. Finally, we compared our ASGE with other embedding methods, in this paper, we take Word2vec as an exmaple, which is a group of related models used to produce word embeddings. Specially, we view the label set in each image as a single sentence, and the window size in Word2vec is set as the length of longest sentence to eliminate the influence of label order. The experiment results show our ASGE based MS-CMA performs better than Word2vec based MS-CMA (represented as W2V-MS-CMA) by 1.3% improvement. In our ASGE, label relationships are explicitly represented by adjacency matrix which is treated as a direct optimization target. Instead, Word2vec implicitly encodes label relationships in a data-driven manner without directly optimizing label relationships. Therefore, our ASGE will capture label relationships much better.</p><p>Visualization and Analysis. In this section, we visualize the learned attention maps to illustrate the ability of exploiting discriminative or meaningful regions and capturing the spatial semantic dependencies. We show the attention visualization examples in <ref type="figure" target="#fig_3">Fig.4</ref>. The three rows show the category-wise attention maps generated by CMA model and general self-attention respectively. It is observed that the CMA model concentrates more on semantic regions and has stronger response than general self-attention, thus it is capable of exploiting more discriminative and meaningful information. Besides, our CMA mechanism has the ability of capturing the spatial semantic dependencies, especially for the indiscernible or small objects occur in the image, e.g. attention of sports ball also pays attention to tennis racket due to their semantic similarity. It's quite helpful because these objects need richer contextual cues to help recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-label Video Classification</head><p>Implementation Details. For the training of ASGE, we apply optimization relaxation and set α = 0.1. Other settings are same as described in MLIC task. For the training of classification, the initial learning rate is 0.0002 and decay each 2 × 10 6 samples with momentum 0.8 . The hyperparameter β in the Eq.12 is 0. The optimizer is SGD with momentum 0.9. The batch size is 256. In this task, we only implemented a single scale CMA model. Results on YouTube-8M Segments Dataset. In the MLVC task, we verify the effectiveness of our model on the YouTube-8M Segments dataset, which is an extension of the YouTube-8M dataset(Abu-El-Haija et al. 2016).</p><p>In our experiment, we only use frame-level image features, while the state-of-the-art methods use additional audio features and most are built on model ensemble, which is unfair to compare with. For this reason, we compare our CMA   <ref type="figure">Figure 5</ref>: The attention scores for each frame. The label is skiing.</p><p>model with general self-attention model to validate the effectiveness in MLVC task. Besides, in order to explore the impact of backbone network on CMA mechanism, we implement the SNet-based and FC-based (two fully connected layers) models. The quantitative results are shown in the Table 4. It can be found that all of our metrics are better than the self-attention model. It seems that the improvements compared with self-attention model are not so significant as that of MLIC, but considering the input of our model is fixed pre-extracted features and the models only differ in attention mechanism, the performance gains are quite remarkable.</p><p>Visualization and Analysis. We also present the visualization results of attention scores for a video in <ref type="figure">Fig.5</ref>. The label of the video is skiing, and our CMA model pays more attention to skiing-related frames while partly ignores the redundant frames, suggesting that our attention mechanism is capable of locating attentional frames and demonstrating the effectiveness of our model more intuitively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel cross-modality attention mechanism with semantic graph embedding for both MLIC and MLVC task. The proposed method can effectively discover semantic location with rich discriminative features and capture the spatial or temporal dependencies between labels. The extensive evaluations on two MLIC datasets MS-COCO and NUS-WIDE show our method outperforms state-ofthe-arts. In addition, we conducted expriments on MLVC datasset YouTube-8M Segments and achieve excellent performance, which validate the strong generalization of our method.</p><p>6 Acknowledgement</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overall framework of the model for MLVC task. The input is pre-extracted features instead of raw video, and the pre-extracted features are processed through SNet to extract visual features. Other parts are quite similar to that of MLIC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of latent spatial dependency. The different colors indicate different categories. The solid arrows represent the learned label embeddings, expressed as e, while the dotted arrows represent the projected visual features through CMT module, expressed as v . The angles between label embeddings and projected visual features, namely α, β and γ, represent the category-wise attention scores. For a detailed discussion, refer to section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) , CNN-SREL-RNN (Liu et al. 2017) , CNN-LSEP (Li, Song, and Luo 2017), Order-Free RNN(Chen et al. 2018), ML-ZSL (Lee et al. 2018), S-CLs(Liu et al. 2018), Attention transfer(Zagoruyko and Komodakis 2016) and FitsNet(Romero et al. 2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The visualization of attention maps. The first column: original image, second and fourth column: attention maps of MS-CMA and self-attention respectively, third and fifth column: attention maps projected on the original image of MS-CMA and self-attention respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparisons with state-of-the-art methods on the MS-COCO dataset. We report two our proposed model: CMA and MS-CMA. The bold numbers indicate the best results in different metrics, while the underlined numbers indicate the suboptimal results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-3</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">mAP CP</cell><cell cols="3">CR CF1 OP</cell><cell>OR</cell><cell cols="2">OF1 CP</cell><cell cols="3">CR CF1 OP</cell><cell>OR</cell><cell>OF1</cell></row><row><cell>CNN-RNN (2016)</cell><cell></cell><cell>61.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">66.0 55.6 60.4 69.2 66.4 67.8</cell></row><row><cell>CNN-LSEP (2017)</cell><cell></cell><cell>-</cell><cell cols="6">73.5 56.4 62.9 76.3 61.8 68.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CNN-SREL-RNN (2017)</cell><cell>-</cell><cell cols="6">67.4 59.8 63.4 76.6 68.7 72.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">RNN-Attention (2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">79.1 58.7 67.4 84.0 63.0 72.0</cell></row><row><cell cols="2">Order-Free RNN (2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">71.6 54.8 62.1 74.2 62.2 67.7</cell></row><row><cell>ML-ZSL (2018)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">74.1 64.5 69.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SRN (2017)</cell><cell></cell><cell cols="13">77.1 81.6 65.4 71.2 82.7 69.9 75.8 85.2 58.8 67.4 87.4 62.5 72.9</cell></row><row><cell>S-CLs (2018)</cell><cell></cell><cell>74.6</cell><cell>-</cell><cell>-</cell><cell>69.2</cell><cell>-</cell><cell>-</cell><cell>74.0</cell><cell>-</cell><cell>-</cell><cell>66.8</cell><cell>-</cell><cell>-</cell><cell>72.7</cell></row><row><cell cols="2">Multi-Evidence (2018)</cell><cell>-</cell><cell cols="12">80.4 70.2 74.9 85.2 72.5 78.4 84.5 62.2 70.6 89.1 64.3 74.7</cell></row><row><cell>ML-GCN (2019)</cell><cell></cell><cell cols="13">82.4 82.1 73.1 77.3 83.7 76.3 79.9 87.2 64.6 74.2 89.1 66.7 76.3</cell></row><row><cell>CMA</cell><cell></cell><cell cols="13">83.4 83.4 72.9 77.8 86.8 76.3 80.9 86.7 64.9 74.3 90.9 67.2 77.2</cell></row><row><cell>MS-CMA</cell><cell></cell><cell cols="13">83.8 82.9 74.4 78.4 84.4 77.9 81.0 88.2 65.0 74.9 90.2 67.4 77.1</cell></row><row><cell>Table 1: Methods</cell><cell cols="5">All mAP CF1 OF1 CF1 OF1 Top-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN-RNN (2016)</cell><cell>56.1</cell><cell>-</cell><cell>-</cell><cell cols="2">34.7 55.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN-LSEP (2017)</cell><cell>-</cell><cell cols="2">52.9 70.8</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN-SREL-RNN (2017)</cell><cell>-</cell><cell cols="2">52.7 70.9</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Order-Free RNN (2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">54.7 70.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ML-ZSL (2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Attention transfer (2016) 57.6 55.2 70.3 51.7 68.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FitsNet (2014)</cell><cell cols="5">57.4 54.9 70.4 51.4 68.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S-CLs (2018)</cell><cell cols="5">60.1 58.7 73.7 53.8 71.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CMA</cell><cell cols="5">60.8 60.4 73.7 55.5 70.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS-CMA</cell><cell cols="5">61.4 60.5 73.8 55.7 69.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with state-of-the-art methods on the NUS-WIDE dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Evaluation Metrics. In the MLVC task, we use several metrics to evaluate our model, including Global Average Precision (GAP)(Shin et al. 2018), Average Hit Rate (Avg Hit@1), Precision at Equal Recall Rate (PERR) and Mean Average Precision (mAP)(Abu-El-Haija et al. 2016).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The comparison between self-attention model and ours CMA model on the YouTube-8M Segments dataset.</figDesc><table><row><cell>Attention Score</cell><cell>0.05 0.10 0.15 0.20 0.25</cell></row><row><cell></cell><cell>0.00</cell></row><row><cell></cell><cell>Frames</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China(61671397). We thank all anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abu-El-Haija</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Youtube-8m: A large-scale video classification benchmark</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A largescale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>[caba Heilbron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Campos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06834</idno>
		<title level="m">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu ; Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<title level="m">Grarep: Learning graph representations with global structural information. In CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Order-free rnn with visual attention for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIVR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu ; Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emonets: Multimodal deep learning approaches for emotion recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kahou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional graphical lasso for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving pairwise ranking for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-label image classification with a probabilistic label enhancement model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic regularisation for recurrent image annotation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
	</analytic>
	<monogr>
		<title level="m">MM. ACM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>KDD. ACM</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Approach for video classification with multi-label on youtube-8m dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Romero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<idno>arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Multi-label image recognition by recurrently discovering attentional regions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
	</analytic>
	<monogr>
		<title level="m">Cnn: Single-label to multi-label</title>
		<editor>KDD. ACM. [Wei et al.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional neural networks for camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<editor>ICRA. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<idno>Zhang et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
