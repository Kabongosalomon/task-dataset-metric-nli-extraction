<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continuous Conditional Generative Adversarial Networks for Image Generation: Novel Losses and Label Input Mechanisms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Xin</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yongwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuheng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Continuous Conditional Generative Adversarial Networks for Image Generation: Novel Losses and Label Input Mechanisms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms CcGAN</term>
					<term>conditional generative adversarial networks</term>
					<term>continuous and scalar conditions</term>
					<term>image generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes the concept of a continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (e.g., class labels). Conditioning on regression labels is mathematically distinct and raises two fundamental problems: (P1) since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice; and (P2) since regression labels are scalar and infinitely many, conventional label input mechanisms (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. We solve these problems by: (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) mechanism and an improved label input (ILI) mechanism to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. Hence, we propose four versions of CcGAN employing different proposed losses and label input mechanisms. The error bounds of the discriminator trained with HVDL and SVDL respectively, are derived under mild assumptions. To evaluate the performance of CcGANs, two new benchmark datasets  are created. A novel evaluation metric (Sliding Fréchet Inception Distance) is also proposed to replace Intra-FID when Intra-FID is not applicable. Our extensive experiments on several benchmark datasets (i.e., the Circular 2-D Gaussians, RC-49, UTKFace, Cell-200, and Steering Angle datasets) support the following findings: the proposed CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label; and CcGAN substantially outperforms cGAN both visually and quantitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conditional generative adversarial networks (cGANs), first proposed in <ref type="bibr" target="#b0">[1]</ref>, aim to estimate the distribution of images conditioning on some auxiliary information, especially class labels. Subsequent studies <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> confirm the feasibility of generating diverse, high-quality (even photo-realistic), and class-label consistent fake images from class-conditional GANs. Unfortunately, these cGANs do not work well for image generation with continuous, scalar conditions, termed regression labels, due to two problems: (P1) cGANs are often trained to minimize the empirical versions of their losses (a.k.a. empirical cGAN losses) on some training data, a principle also known as empirical risk minimization (ERM) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The success of ERM relies on a large sample size for each distinct condition. Unfortunately, we usually have only a few real images for some regression labels. Moreover, since regression labels are continuous, some values may not even appear in the training set. Consequently, a cGAN cannot accurately estimate the image distribution conditional on such missing labels. (P2) In class-conditional image generation, class labels are often encoded by one-hot vectors or label embedding and then fed into the generator and discriminator by hidden concatenation <ref type="bibr" target="#b0">[1]</ref>, an auxiliary classifier <ref type="bibr" target="#b1">[2]</ref> or label projection <ref type="bibr" target="#b2">[3]</ref>. A precondition for such label encoding is that the number of distinct labels (e.g., the number of classes) is finite and known. Unfortunately, in the continuous scenario, we may have infinitely many distinct regression labels.</p><p>A naive approach to solve (P1)-(P2) is to "bin" the regression labels into a series of disjoint intervals and still train a cGAN in the class-conditional manner (these interval are treated as independent classes) <ref type="bibr" target="#b8">[9]</ref>. However, this approach has four shortcomings: <ref type="bibr" target="#b0">(1)</ref> our experiments in Section 5 show that this approach often makes cGANs collapse; <ref type="bibr" target="#b1">(2)</ref> we can only estimate the image distribution conditional on membership in an interval and not on the target label; (3) a large interval width leads to high label inconsistency; <ref type="bibr" target="#b3">(4)</ref> inter-class correlation is not considered (images in successive intervals have similar distributions).</p><p>In machine learning, vicinal risk minimization (VRM) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref> is an alternative rule to ERM. VRM assumes that a sample point shares the same label with other samples in its vicinity. Motivated by VRM, in generative modeling conditional on regression labels where we estimate a conditional distribution p(x|y) (x is an image and y is a regression label), it is natural to assume that a small perturbation to y results in a negligible change to p(x|y). This assumption is consistent with our perception of the world. For example, the image distribution of facial features for a population of <ref type="bibr">15-</ref>year-old teenagers should be close to that of 16-year olds.</p><p>We therefore introduce the continuous conditional GAN (CcGAN) to tackle (P1) and (P2). To our best knowledge, this is the first generative model for image generation conditional on regression labels. It is noted that <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> train GANs in an unsupervised manner and synthesize unlabeled fake images for a subsequent image regression task. <ref type="bibr" target="#b12">[13]</ref> proposes a semi-supervised GAN for dense crowd counting. CcGAN is fundamentally different from these works since they do not estimate the conditional image distribution. Moreover, some recent works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> propose several novel schemes to train GANs when training data are limited, which seems to be relevant to (P1). However, they are also fundamentally different from CcGAN, since they are designed for the unconditional and class-conditional scenarios rather than the continuous scenario. Our contributions can be summarized as follows:</p><p>• We propose in Section 2.1 a solution to address (P1), which consists of two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL), and a novel empirical generator loss. We take the vanilla cGAN loss as an example to show how to derive HVDL, SVDL, and the novel empirical generator loss by reformulating existing empirical cGAN losses. • In Section 2.2, we propose two novel label input mechanisms, consisting of a naive label input (NLI) mechanism and an improved label input (ILI) mechanism, as solutions to address (P2). <ref type="bibr">•</ref> We derive in Section 3 the error bounds of a discriminator trained with HVDL and SVDL. <ref type="bibr">•</ref> We propose in Section 4 a novel evaluation metric, termed Sliding Fréchet Inception Distance (SFID), to evaluate the generative image modeling conditional on regression labels when there are insufficient real images to compute Intra-FID <ref type="bibr" target="#b2">[3]</ref>. • In Section 5, we propose two new benchmark datasets, RC-49 and Cell-200, for generative image modeling conditional on regression labels, since very few benchmark datasets are suitable for the studied continuous scenario. We conduct experiments on the new datasets and three others to demonstrate that CcGAN not only generates diverse, high-quality, and label consistent images, but also substantially outperforms cGAN both visually and quantitatively. The effectiveness of SFID is also studied on the RC-49 dataset at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FROM CGAN TO CCGAN</head><p>In this section, we introduce the continuous conditional GAN (CcGAN), consisting of solutions to (P1) and (P2). The combinations of two vicinal discriminator losses (HVDL and SVDL) proposed in Section 2.1 and two novel label input mechanisms (NLI and ILI) proposed in Section 2.2 result in four CcGAN methods denoted by HVDL+NLI, SVDL+NLI, HVDL+ILI, and SVDL+ILI respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Solution to (P1): Reformulated Empirical Losses</head><p>Theoretically cGAN losses (e.g., the vanilla cGAN loss <ref type="bibr" target="#b0">[1]</ref>, the Wasserstein loss <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and the hinge loss <ref type="bibr" target="#b19">[20]</ref>) are suitable for both class labels and regression labels; however, their empirical versions fail in the continuous scenario (i.e., (P1)). Our first solution (S1) focuses on reformulating these empirical cGAN losses to fit into the continuous scenario. Without loss of generality, we only take the vanilla cGAN loss as an example to show such reformulation (the empirical versions of the Wasserstein loss and the hinge loss can be reformulated similarly). The vanilla discriminator loss and generator loss <ref type="bibr" target="#b0">[1]</ref> are defined as: = − log(D(G(z, y), y))q(z)p g (y)dzdy,</p><p>where x ∈ X is an image of size d × d, y ∈ Y is a label, p r (y) and p g (y) are respectively the true and fake label marginal distributions, p r (x|y) and p g (x|y) are respectively the true and fake image distributions conditional on y, p r (x, y) and p g (x, y) are respectively the true and fake joint distributions of x and y, and q(z) is the probability density function of N (0, I).</p><p>Since the distributions in the losses of Eqs. <ref type="bibr" target="#b0">(1)</ref> and <ref type="bibr" target="#b1">(2)</ref> are unknown, for class-conditional image generation, <ref type="bibr" target="#b0">[1]</ref> follows ERM and minimizes the empirical losses: </p><p>where C is the number of classes, N r and N g are respectively the number of real and fake images, N r c and N g c are respectively the number of real and fake images with label c, x r c,j and x g c,j are respectively the j-th real image and the j-th fake image with label c, and the z c,j are independently and identically sampled from q(z). Eq. (3) implies we estimate p r (x, y) and p g (x, y) by their empirical probability density functions as follows: </p><p>where δ(·) is a Dirac delta function (Appendix A of <ref type="bibr" target="#b20">[21]</ref>) centered at 0. However,p δ r (x, y) andp δ g (x, y) in Eq. <ref type="bibr" target="#b4">(5)</ref> are not good estimates in the continuous scenario because of (P1).</p><p>To overcome (P1), we propose a novel estimate for each of p r (x, y) and p g (x, y), termed the hard vicinal estimate (HVE). We also provide an intuitive alternative to HVE, named the soft vicinal estimate (SVE). The HVEs of p r (x, y) and p g (x, y) are:</p><formula xml:id="formula_3">p HVE r (x, y) =C 1 ·   1 N r N r j=1 exp − (y − y r j ) 2 2σ 2   · 1 N r y,κ N r i=1 1 {|y−y r i |≤κ} δ(x − x r i ) , p HVE g (x, y) =C 2 ·   1 N g N g j=1 exp − (y − y g j ) 2 2σ 2   · 1 N g y,κ N g i=1 1 {|y−y g i |≤κ} δ(x − x g i ) ,<label>(6)</label></formula><p>where x r i and x g i are respectively real image i and fake image i, y r i and y g i are respectively the labels of x r i and x g i , κ and σ are two positive hyper-parameters, C 1 and C 2 are two constants making these two estimates valid probability density functions, N r y,κ is the number of the y r i satisfying |y − y r i | ≤ κ, N g y,κ is the number of the y g i satisfying |y − y g i | ≤ κ, and 1</p><p>is an indicator function with support in the subscript. The terms in the first square brackets ofp HVE r andp HVE g imply we estimate the marginal label distributions p r (y) and p g (y) by kernel density estimates (KDEs) <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The terms in the second square brackets are designed based on the assumption that a small perturbation to y results in negligible changes to p r (x|y) and p g (x|y). If this assumption holds, we can use images with labels in a small vicinity of y to estimate p r (x|y) and p g (x|y). The SVEs of p r (x, y) and p g (x, y) are:</p><formula xml:id="formula_4">p SVE r (x, y) =C 3 ·   1 N r N r j=1 exp − (y − y r j ) 2 2σ 2   · N r i=1 w r (y r i , y)δ(x − x r i ) N r i=1 w r (y r i , y) , p SVE g (x, y) =C 4 ·   1 N g N g j=1 exp − (y − y g j ) 2 2σ 2   · N g i=1 w g (y g i , y)δ(x − x g i ) N g i=1 w g (y g i , y) ,<label>(7)</label></formula><p>where C 3 and C 4 are two constants making these two estimates valid probability density functions,</p><formula xml:id="formula_5">w r (y r i , y) = e −ν(y r i −y) 2 and w g (y g i , y) = e −ν(y g i −y) 2 ,<label>(8)</label></formula><p>and the hyper-parameter ν &gt; 0. In Eq. <ref type="bibr" target="#b6">(7)</ref>, similar to the HVEs, we estimate p r (y) and p g (y) by KDEs. Instead of using samples in a hard vicinity, the SVEs use all respective samples to estimate p r (x|y) and p g (x|y) but each sample is assigned with a weight based on the distance of its label from y. Two diagrams in <ref type="figure" target="#fig_3">Fig. 1</ref> visualize the process of using hard/soft vicinal samples to estimate p(x|y), i.e., a univariate Gaussian distribution conditional on y. By plugging Eq. (6) and <ref type="formula" target="#formula_4">(7)</ref> into Eq. (1), we derive the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) as follows:  <ref type="formula" target="#formula_3">6)</ref>) and SVE (Eq. <ref type="formula" target="#formula_4">(7)</ref>) estimate p(x|y) (a univariate Gaussian conditional on y) using two samples in hard and soft vicinities, respectively, of y. To estimate p(x|y) (the red Gaussian curve) only from samples drawn from p(x|y1) and p(x|y2) (the blue Gaussian curves), estimation is based on the samples (red dots) in a hard vicinity (defined by y ± κ) or a soft vicinity (defined by the weight decay curve) around y. The histograms in blue are samples in the hard or soft vicinity. The labels y1, y, and y2 on the x-axis denote the means of x conditional on y1, y, and y2, respectively.</p><formula xml:id="formula_6">L HVDL (D) = − C 5 N r N r j=1 N r i=1 E r ∼N (0,σ 2 ) W 1 log(D(x r i , y r j + r )) − C 6 N g N g j=1 N g i=1 E g ∼N (0,σ 2 ) W 2 log(1 − D(x g i , y g j + g )) ,<label>(9)</label></formula><formula xml:id="formula_7">L SVDL (D) = − C 7 N r N r j=1 N r i=1 E r ∼N (0,σ 2 ) W 3 log(D(x r i , y r j + r )) − C 8 N g N g j=1 N g i=1 E g ∼N (0,σ 2 ) W 4 log(1 − D(x g i , y g j + g )) ,<label>(10)</label></formula><p>where r y − y r j , g y − y g j ,</p><formula xml:id="formula_8">W 1 = 1 {|y r j + r −y r i |≤κ} N r y r j + r ,κ , W 2 = 1 {|y g j + g −y g i |≤κ} N g y g j + g ,κ W 3 = w r (y r i , y r j + r ) N r i=1 w r (y r i , y r j + r ) , W 4 = w g (y g i , y g j + g ) N g i=1 w g (y g i , y g j + g )</formula><p>, and C 5 , C 6 , C 7 , and C 8 are some constants.</p><p>Generator training: The generator of CcGAN is trained by minimizing Eq. <ref type="formula" target="#formula_9">(11)</ref>,</p><formula xml:id="formula_9">L (G) = − 1 N g N g i=1 E g ∼N (0,σ 2 ) log(D(G(z i , y g i + g ), y g i + g )).<label>(11)</label></formula><p>How do HVDL, SVDL, and Eq. (11) overcome (P1)? (i) Given a label y as the condition, we use images in a hard/soft vicinity of y to train the discriminator instead of just using images with label y. It enables us to estimate p r (x|y) when there are not enough real images with label y. (ii) From Eqs. <ref type="bibr" target="#b8">(9)</ref> and <ref type="formula" target="#formula_7">(10)</ref>, we can see that the KDEs in Eqs. (6) and <ref type="bibr" target="#b6">(7)</ref> are adjusted by adding Gaussian noise to the labels. Moreover, in Eq. (11), we add Gaussian noise to seen labels (assume y g i 's are seen) to train the generator to generate images at unseen labels. This enables estimation of p r (x|y ) when y is not in the training set. Remark 1. Based on the kernel density estimation <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> and the property of the Dirac delta function (Appendix A of <ref type="bibr" target="#b20">[21]</ref>), δ(x − x r i )dx = δ(x − x g i )dx = 1 and C 1 = C 2 = C 3 = C 4 = 1/σ. Therefore, C 5 = C 6 and C 7 = C 8 , which implies these constants C 1 , ...C 8 can be ignored when minimizing L HVDL (D) and L SVDL (D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2.</head><p>An algorithm is proposed in Supp. S.8 for training CcGAN in practice. Moreover, CcGAN does not require any specific network architecture, so it can use the modern GAN architectures in practice such as SNGAN <ref type="bibr" target="#b19">[20]</ref>, SAGAN <ref type="bibr" target="#b25">[26]</ref> and BigGAN <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Solutions to (P2): Two Novel Label Input Mechanisms</head><p>In this section, we propose two solutions, consisting of a naive and an improved label input mechanisms, to solve (P2). A naive label input (NLI) mechanism: We first propose a naive approach to incorporate the regression labels into the cGANs. For G, we add the label y element-wisely to the output of its first linear layer. For D, the label y is first projected to the latent space learned by an extra linear layer. Then, we incorporate the embedded label into the discriminator by label projection <ref type="bibr" target="#b2">[3]</ref>. Figs. 2 and 3 visualize the naive label input mechanism. An improved label input (ILI) mechanism: Empirical studies in Section 5 show that CcGAN with the naive label input mechanism already outperforms much over cGAN. Nevertheless, it still suffers from severe label inconsistency on some datasets (e.g., Cell-200 and Steering Angle). To improve the label consistency of CcGAN, we propose an improved label input (ILI) mechanism. The ILI approach consists of a pre-trained CNN and a label embedding network. The pre-trained CNN, as shown in <ref type="figure" target="#fig_6">Fig. 4</ref>, includes two subnetworks, T 1 and T 2 , where T 1 maps an image x to a feature space and T 2 maps the extracted feature h to a regression label y. The dimension of the feature space is set 128 in our experiments. The label  embedding network T 3 , as shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, is a multilayer perceptron (MLP) <ref type="bibr" target="#b24">[25]</ref> mapping a regression label y back to its hidden representation h in the feature space defined by T 1 . Assume that there are m distinct regression labels in the training set, i.e., y u 1 , y u 2 , . . . , y u m , then the label embedding network T 3 is trained by:</p><formula xml:id="formula_10">min T3 1 m E γ∼N (0,σ 2 γ ) [T 2 (T 3 (y u i + γ)) − (y u i + γ)] ,<label>(12)</label></formula><p>where σ γ is often a small value and it is set 0.2 in this paper. Then, given a regression label y, we can evaluate T 3 (y) to get the hidden representation of y which will be incorporated into CcGAN as the condition (visualized in <ref type="figure" target="#fig_9">Fig. 6</ref> and <ref type="figure" target="#fig_10">Fig. 7)</ref>. Specifically, for G, we input the embedded label by using the conditional batch normalization <ref type="bibr" target="#b26">[27]</ref>. For D, similar to the naive approach, we input the embedded label into D by the label projection <ref type="bibr" target="#b2">[3]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ERROR BOUNDS</head><p>In this section, we derive the error bounds of a discriminator trained with L HVDL and L SVDL under the theoretical loss L. First, without loss of generality, we assume y ∈ , W r (y) w r (y , y)p r (y )dy and W g (y) w g (y , y)p g (y )dy . Denote by D * the optimal discriminator <ref type="bibr" target="#b27">[28]</ref> which minimizes L but may not be in D. Let D arg min D∈D L(D). Let D HVDL arg min D∈D L HVDL (D); similarly, we define D SVDL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1. (Hölder Class) Define the Hölder class of functions as:</head><formula xml:id="formula_11">Σ(L) p : ∀t 1 , t 2 ∈ Y, ∃L &gt; 0, s.t. |p (t 1 ) − p (t 2 )| |t 1 − t 2 | ≤ L .<label>(13)</label></formula><p>Please see Supp. S.10.1 for more details of these notations. Moreover, we will also work with the following assumptions: (A1). All   </p><formula xml:id="formula_12">&gt; 0, s.t. |p g (x|y ) − p g (x|y)| ≤ g g (x)|y − y| with g g (x)dx = M g . (A4). p r (y) ∈ Σ(L r ) and p g (y) ∈ Σ(L g ).</formula><p>With these definitions and assumptions, we derive four lemmas based on which we derive the error bounds of a discriminator trained by using HVDL and SVDL in Theorems 1 and 2.</p><p>Lemma 1. Suppose that (A1)-(A2) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_13">sup D∈D 1 N r y,κ N r i=1 1 {|y−y r i |≤κ} [− log D(x r i , y)] − E x∼pr(x|y) [− log D(x, y)] ≤ U 1 2N r y,κ log 2 δ + κU M r 2 ,<label>(14)</label></formula><p>for a given y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.</head><p>Suppose that (A1), (A3) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_14">sup D∈D 1 N g y,κ N g i=1 1 {|y−y g i |≤κ} [− log(1 − D(x i , y))] − E x∼pg(x|y) [− log(1 − D(x, y))] ≤ U 1 2N g y,κ log 2 δ + κU M g 2 ,<label>(15)</label></formula><p>for a given y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.</head><p>Suppose that (A1), (A2) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_15">sup D∈D 1 N r N r i=1 w r (y r i , y) [− log D(x r i , y)] 1 N r N r i=1 w r (y r i , y) − E x∼pr(x|y) [− log D(x, y)] ≤ U W r (y) 1 2N r log 4 δ + U M r 2 E y ∼p r w (y |y) [|y − y|] ,<label>(16)</label></formula><p>for a given y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.</head><p>Suppose that (A1), (A3) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_16">sup D∈D 1 N g N g i=1 w g (y g i , y) [− log(1 − D(x g i , y))] 1 N g N g i=1 w g (y g i , y) − E x∼pg(x|y) [− log(1 − D(x, y))] ≤ U W g (y) 1 2N g log 4 δ + U M g 2 E y ∼p g w (y |y) [|y − y|] ,<label>(17)</label></formula><p>for a given y.</p><p>Theorem 1. Assume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_17">L( D HVDL ) − L(D * ) ≤2U   C KDE 1,δ log N r N r σ + L r σ 2   + 2U   C KDE 2,δ log N g N g σ + L g σ 2   + κU (M r + M g ) + 2U 1 2 log 8 δ E y∼p KDE r (y) 1 N r y,κ + E y∼p KDE g (y) 1 N g y,κ + L( D) − L(D * ),<label>(18)</label></formula><p>for some constants C KDE 1,δ , C KDE 2,δ depending on δ. Theorem 2. Assume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_18">L( D SVDL ) − L(D * ) ≤2U   C KDE 1,δ log N r N r σ + L r σ 2   + 2U   C KDE 2,δ log N g N g σ + L g σ 2   + 2U 1 2 log 16 δ 1 √ N r E y∼p KDE r (y) 1 W r (y) + 1 √ N g E y∼p KDE g (y) 1 W g (y) + U M r E y∼p KDE r (y) E y ∼p r w (y |y) |y − y| + M g E y∼p KDE g (y) E y ∼p g w (y |y) |y − y| + L( D) − L(D * ),<label>(19)</label></formula><p>for some constant C KDE 1,δ , C KDE 2,δ depending on δ. Remark 3. Please see Supp. S.10.2 for the proofs to these lemmas and theorems. Remark 4. The error bounds in both theorems reflect the distance of D HVDL and D SVDL from D * . Enlightened by the two upper bounds, when implementing CcGAN, we should (1) avoid letting D output extreme values (close to 0 or 1) so that U is kept at a moderate level; (2) avoid using a too small or a too large κ or ν to keep the third and fourth terms moderate in Eqs. <ref type="bibr" target="#b17">(18)</ref> and <ref type="bibr" target="#b18">(19)</ref>. Please see Supp. S.10.2.5 for a more detailed interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SLIDING FRÉCHET INCEPTION DISTANCE</head><p>A conditional GAN (no matter what type of the condition is) needs to be evaluated from three perspectives <ref type="bibr" target="#b28">[29]</ref>: (1) the visual quality, (2) the intra-label diversity, and (3) the label consistency (whether assigned labels of fake images are consistent with their true labels). Measuring the performance of cGANs from these three perspectives is often conducted by using a popular overall metric, termed the Intra-FID <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Intra-FID computes the Fréchet inception distance (FID) <ref type="bibr" target="#b29">[30]</ref> separately at each of the distinct labels and reports the average FID score. Intra-FID is also used in our experiments on RC-49, UTKFace, and Cell-200 in Section 5; however, Intra-FID is not reliable or even inapplicable when we have very few (even zero) real images for some distinct regression labels, e.g., the experiment on the Steering Angle dataset in Section 5.5. We therefore propose a novel metric, termed the Sliding Fréchet Inception Distance (SFID), to replace Intra-FID in this scenario. SFID computes FID within an interval sliding on the range of the regression label y, and then reports the average of these FIDs. Specifically, we first preset finite points c SFID 's (termed SFID centers) on the range of y evenly and a constant r SFID (termed the SFID radius). Then, based on these c SFID 's and r SFID , we can define many joint intervals (termed SFID intervals) in the form of [c SFID − r SFID , c SFID + r SFID ]. For each SFID interval, we compute FID between real and fake images with labels within this interval. Finally, SFID reports the average of these FIDs. Usually, we also report the standard deviation of these FIDs. We visualize the procedure for computing SFID in <ref type="figure">Fig. 8</ref>. A pseudo code for computing SFID is shown in Alg. 1. Similar to Intra-FID, a small SFID is preferred. <ref type="figure">Fig. 8</ref>: Sliding Fréchet Inception Distance (SFID). We preset finite centers (blue dots) on y's range evenly and a radius r SFID . Given an interval [c SFID − r SFID , c SFID + r SFID ], we compute FID between fake and real images with labels in this interval. SFID is the average of these FIDs.</p><p>Algorithm 1: An algorithm to compute the Sliding Fréchet Inception Distance (SFID).</p><p>Data: Pretrained G, real data real label set {y r (1) , y r (2) , · · · , y r (N r ) }, corresponding real data observation set {X r (1) , X r (2) , · · · , X r (N r ) }; preset evaluation fake label set {y f (1) , y f (2) , · · · , y f (N f ) }, preset SFID center set {y c (1) , y c (2) , · · · , y c (N c ) }; preset number of fake images N f in an evaluation label, preset window radius r SF ID . Result: SF ID. 1 Initialize sliding-window data observation set Ω r (l) = φ, Ω f (l) = φ, interval FIDs F ID(l) = ∞, l = 1, 2, · · · , N c ; 2 Initialize fake data evaluation set X f (k) = φ, k = 1, 2, · · · , N f ; 3 for k = 1 to N f do <ref type="bibr" target="#b3">4</ref> Sample {z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> , · · · , z (N f ) } from noise prior N (0, I);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Create fake data evaluation set X f (k) = {G(z <ref type="bibr" target="#b0">(1)</ref> , y f (k) ), G(z <ref type="bibr" target="#b1">(2)</ref> , y f (k) ), · · · , G(z (N f ) , y f (k) )}; 6 end 7 for l = 1 to N c do <ref type="bibr" target="#b7">8</ref> Create real data observation set around y c (l) : Ω r l = |y r (j) −y c (l) |≤r SF ID X r (j) , j = 1, 2, · · · , N r ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9</head><p>Create fake data observation set around y c (l) :</p><formula xml:id="formula_19">Ω f l = |y f (i) −y c (l) |≤r SF ID X f (i) , i = 1, 2, · · · , N f ; 10 Compute current interval FID: F ID(l) = F ID(Ω r (l) , Ω f (l) ) ; 11 end 12 Compute SF ID = 1 N c N c l=1 F ID(l).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we study the effectiveness of CcGAN on five datasets where cGAN <ref type="bibr" target="#b0">[1]</ref> cannot generate realistic samples. For a fair comparison, cGAN and CcGAN use the same network architecture (a customized fully-connected architecture for Circular 2-D Gaussians, a customized DCGAN <ref type="bibr" target="#b30">[31]</ref> architecture for Cell-200 and the SNGAN <ref type="bibr" target="#b19">[20]</ref> architecture for the rest datasets) except for the label input modules. The four CcGAN methods (i.e., HVDL+NLI, SVDL+NLI, HVDL+ILI, and SVDL+ILI) are tested in all our experiments below except Circular 2-D Gaussians. Please note that since Circular 2-D Gaussians is a simple, low-dimensional simulation, we use a hidden concatenation like scheme to input the regression label into CcGAN instead of NLI and ILI. For stability, regression labels in RC-49, UTKFace, Cell-200, and Steering Angle are normalized to [0, 1] during training.</p><p>The proposed SFID (see Section 4) is only used in the experiment conducted on the Steering Angle dataset in Section 5.5 when we don't have enough real images to compute Intra-FID. The effectiveness of SFID is studied on the RC-49 dataset in Section 5.6 where we can control the sample size of real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Circular 2-D Gaussians</head><p>We first test on the synthetic data generated from 120 2-D Gaussians with different means. Experimental setup: The means of the 120 Gaussians are evenly arranged on a unit circle centered at the origin O of a 2-D space. The Gaussians share a common covariance matrixσ 2 I 2×2 , whereσ = 0.02. We generate 10 samples from each Gaussian for training. <ref type="figure" target="#fig_11">Fig. 9</ref>(a) shows 1,200 training samples (blue dots) from these Gaussians with their means (red dots) on a unit circle. The unit circle can be seen as a clock where we take the mean at 12 o'clock (point A) as the baseline point. Given another mean on the circle (point B), the label y for samples generated from the Gaussian with mean B is defined as the clockwise angle (in radians) between line segments OA and OB. E.g., the label for samples from the Gaussian at A is 0. Both cGAN and our proposed CcGAN are trained on this training set. When implementing cGAN, angles are treated as class labels (each Gaussian is treated as a class) and input into cGAN by the hidden concatenation <ref type="bibr" target="#b0">[1]</ref>; while when implementing CcGAN, angles are treated as real numbers and incorporated into CcGAN by a hidden concatenation like scheme. The two novel label input mechanisms (NLI and ILI) are not implemented in this simple, low-dimensional scenario. Please see Supp. S.11.1 for the net architectures of cGAN and CcGAN and more details of the label input schemes. Both cGAN and CcGAN are trained for 6,000 iterations. We use the rule of thumb formulae in Supp. S.9 to select the hyper-parameters of HVDL and SVDL, i.e., σ ≈ 0.074, κ ≈ 0.017 and ν = 3600 (see Supp. S.11.2 for details).</p><p>For testing, we choose 360 points evenly distributed on the unit circle as the means of 360 Gaussians. For each Gaussian, we generate 100 samples, yielding a test set with 36,000 samples. It should be noted that, among these 360 Gaussians, at least 240 are not used at the training. In other words, there are at least 240 labels in the testing set which do not appear in the training set. For each test angle, we generate 100 fake samples from each trained GAN, yielding 36,000 fake samples from each GAN in total. The quality of these fake samples is evaluated. We repeat the whole experiment three times and report in <ref type="table" target="#tab_1">Table 1</ref> the average quality over three repetitions. Evaluation metrics and quantitative results: In the label-conditional scenario, each fake sample x with label y is compared with the mean (sin(y), cos(y)) of a Gaussian on the unit circle with label y. A fake sample is defined as "high-quality" if its Euclidean distance from x to (sin(y), cos(y)) is smaller than 4σ = 0.08. A mode (i.e., a Gaussian) is said to be recovered if at least one high-quality sample is assigned to it. We also measure the quality of fake samples with label y by computing the 2-Wasserstein Distance (W 2 ) [32] between p r (x|y) = N ([sin(y), cos(y)] ,σI) and p g (x|y) = N (µ g y , Σ g y ), where we assume p g (x|y) is Gaussian and its mean and covariance are estimated by the sample mean and sample covariance of 100 fake samples with label y. In <ref type="table" target="#tab_1">Table 1</ref>, we report the average percentage of high-quality fake samples and the average percentage of recovered modes over 3 repetitions. We also report the average W 2 over 360 testing angles. We can see CcGAN substantially outperforms cGAN. Visual results: We select 12 angles which do not appear in the training set. We then use cGAN and CcGAN to generate 100 samples for each unobserved angle. <ref type="figure" target="#fig_11">Fig. 9</ref> visually confirms the obervation from the numerical metrics: the fake samples from the two CcGAN methods are more realistic.  where green and blue dots stand for fake and real samples respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RC-49</head><p>Since most benchmark datasets in the GAN literature do not have continuous, scalar regression labels, we propose a new benchmark dataset-RC-49, a synthetic dataset created by rendering 49 3-D chair models at different yaw angles. Each of 49 chair models is rendered at 899 yaw angles ranging from 0.1 to 89.9 with step size 0. i.e., σ ≈ 0.047, κ ≈ 0.004 and ν = 50625. The two novel label input mechanisms for CcGAN (NLI and ILI) are implemented in this experiment. For ILI, we pre-train a modified ResNet-34 <ref type="bibr" target="#b32">[33]</ref> with 3 linear layers after the average pooling layer and we only keep the last linear layer for label embedding (i.e., the T 1 + T 2 in <ref type="figure" target="#fig_6">Fig. 4</ref>). We use a five-layer MLP with 128 nodes in each layer to convert an angle into its hidden representation (i.e., the T 3 in <ref type="figure" target="#fig_7">Fig. 5</ref>). Both cGAN and CcGAN are trained for 30,000 iterations with batch size 256. Afterwards, we evaluate the trained GANs on all 899 angles by generating 200 fake images for each angle. Please see Supp. S.12 for the network architectures and more details about the training/testing setup. Quantitative and visual results: To evaluate (1) the visual quality, (2) the intra-label diversity, and (3) the label consistency of fake images, we study an overall metric and three separate metrics here. (i) Intra-FID <ref type="bibr" target="#b2">[3]</ref> is utilized as the overall metric. It computes the FID <ref type="bibr" target="#b29">[30]</ref> separately at each of the 899 evaluation angles and reports the average FID score along with the standard deviation of these 899 FIDs. (ii) Naturalness Image Quality Evaluator (NIQE) <ref type="bibr" target="#b33">[34]</ref> measures the visual quality only. (iii) Diversity is the average entropy of predicted chair types of fake images over evaluation angles. (iv) Label Score is the average absolute error between assigned angles and predicted angles. Please see Supp. S.12.5 for details of these metrics.</p><p>We report in <ref type="table" target="#tab_3">Table 2</ref> the performances of each GAN. The example fake images in <ref type="figure" target="#fig_3">Fig. 10</ref> and line graphs in <ref type="figure" target="#fig_3">Fig. 11</ref> support the quantitative results. cGAN often generates unrealistic, identical images for a target angle (i.e., low visual quality and low intra-label diversity). "Binning" [0.1, 89.9] into other number of classes (e.g., 90 classes and 210 classes) is also tried but does not improve cGAN's performance. In contrast, strikingly better visual quality and higher intra-label diversity of the four CcGAN methods are visually evident. Moreover, both two ILI-based CcGANs outperform two NLI-based CcGANs in terms of all four metrics especially the label score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">UTKFace</head><p>In this section, we compare CcGAN and cGAN on UTKFace <ref type="bibr" target="#b34">[35]</ref>, a dataset consisting of RGB images of human faces which are labeled by age. Experimental setup: In this experiment, we only use images with age in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">60]</ref>. Some images with bad visual quality and watermarks are also discarded. After the preprocessing, 14,760 images are left. The number of images for each age ranges from 50 to 1051. We resize all selected images to 64 × 64. Some example UTKFace images are shown in the first image array in <ref type="figure" target="#fig_3">Fig. 13</ref>.  <ref type="figure">Fig. (d)</ref> shows that two ILI-based CcGANs outperform cGAN at most angles and the graph of cGAN is very unstable. All graphs of CcGANs appear much smoother than those of cGAN because of HVDL and SVDL. Moreover, in most graphs, we can clearly see ILI-based CcGANs perform better than NLI-based CcGANs. When implementing cGAN, each age is treated as a class. For CcGAN we use the rule of thumb formulae in Supp. S.9 to select the three hyper-parameters of HVDL and SVDL, i.e., σ ≈ 0.041, κ ≈ 0.017 and ν = 3600. Similar to the RC-49 experiment, we use NLI and ILI to incorporate ages into CcGAN. Both cGAN and CcGAN are trained for 40,000 iterations with batch size 512. In testing, we generate 1,000 fake images from each trained GAN for each age. Please see Supp. S.13 for more details of the data preprocessing, network architectures and training/testing setup. Quantitative and visual results: Similar to the RC-49 experiment, we evaluate the quality of fake images by Intra-FID, NIQE, Diversity (entropy of predicted races), and Label Score. We report in <ref type="table" target="#tab_4">Table 3</ref> the average quality of 60,000 fake images. From this table, we can see the four CcGAN methods substantially outperform cGAN and ILI performs better than NLI. We also show in <ref type="figure" target="#fig_3">Fig. 13</ref> some example fake images from cGAN and CcGAN and line graphs of FID/NIQE/Diversity/Lable Score versus Age in <ref type="figure" target="#fig_3">Fig. 14.</ref> Analogous to the quantitative comparisons, we can see that CcGAN performs much better than cGAN. ,000 to 20,000 and slightly increase m κ in Supp. S.9 from 1 to 2 (we therefore use a wider hard/soft vicinity). We visualize the line graphs of Intra-FID versus the maximum sample size for each age of cGAN and CcGAN in <ref type="figure" target="#fig_3">Fig. 15</ref>. From the figure, we can clearly see that a smaller sample size worsens the performance of both cGAN and CcGAN. Moreover, the Intra-FID scores of cGAN always stay at a high level and are much larger than those of four CcGAN methods. The ILI-based CcGANs are also better than the NLI-based CcGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cell-200</head><p>In addition to RC-49, we propose another benchmark dataset-Cell-200, a dataset of synthetic fluorescence microscopy images with cell populations generated by SIMCEP <ref type="bibr" target="#b35">[36]</ref>. Please see Supp. S.14.1 for more details about the data generation. Some example images are shown in <ref type="figure" target="#fig_3">Fig. 16</ref>.  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">200]</ref> into 100 equal intervals where each interval is treated as a class. We use the rule of thumb formulae in Supp. S.9 to select the three hyperparameters of HVDL and SVDL, i.e., σ ≈ 0.077, κ ≈ 0.020 and ν = 2500. Both cGAN and CcGAN are trained for 5,000 iterations. Afterwards, we evaluate the trained GANs on all 200 cell All graphs of CcGANs also appear much smoother than those of cGAN because of HVDL and SVDL. <ref type="figure">Fig. (d)</ref> shows that the ILI-based CcGANs have higher label consistency than the NLI-based CcGANs. <ref type="figure" target="#fig_3">Fig. 15</ref>: Line graphs of Intra-FID versus the maximum sample size for each distinct angle in the training set of UTKFace. <ref type="figure" target="#fig_3">Fig. 15</ref> shows that four CcGAN methods perform much better than cGAN and ILI is better than NLI. Moreover, a smaller sample size deteriorates the performance of both cGAN and CcGAN.</p><p>counts by generating 1,000 fake images for each count. Please see Supp. S.14 for the network architectures and more details about the training/testing setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative and visual results:</head><p>We evaluate the quality of fake images by Intra-FID, NIQE, and Label Score. Please note that the Diversity score is not available in this experiment because there is no class label in Cell-200. We report in <ref type="table" target="#tab_6">Table 4</ref> the average quality of 200,000 fake images from cGAN and CcGAN. We also show in <ref type="figure" target="#fig_3">Fig. 16</ref> some example fake images from cGAN and CcGAN and line graphs of FID/NIQE/Label Score versus Cell Count in <ref type="figure" target="#fig_3">Fig. 17</ref>. Different from the experimental results on RC-49 and UTKFace, although NLI-based CcGANs outperforms cGAN in terms of Intra-FID and NIQE, their label scores are very high, which implies low label consistency. Fortunately, two ILI-based CcGANs still perform very well and substantially outperform cGAN and two NLI-based CcGANs.  <ref type="figure" target="#fig_3">Fig. 16</ref>: Three Cell-200 images for each of 6 cell counts absent in the training data: real images and example fake images from cGAN and four proposed CcGANs, respectively. cGAN has severe mode collapse problem in this experiment. Two NLI-based CcGANs do not perform well enough but two ILI-based CcGANs produce images with higher visual quality, more diversity, and higher label consistency.  <ref type="figure">Figs. (a)</ref> to (c) show that, although the NLI-based CcGANs do not perform well, the ILI-based CcGANs outperform cGAN across most cell counts. All graphs of CcGANs also appear much smoother than those of cGAN because of HVDL and SVDL. Moreover, in all figures, we can see ILI-based CcGANs perform better than NLI-based CcGANs especially in <ref type="figure">Fig. (c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Steering Angle</head><p>In this section, we demonstrate the effectiveness of the proposed CcGAN on the Steering Angle dataset, a subset of an autonomous driving dataset <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr">[38]</ref>. The complete autonomous driving dataset consists of 109,231 RGB images. Each image is taken by using a dash camera mounted on a car and, at the same moment, the angle of the steering wheel rotation of the same car (i.e., steering angle) is recorded by a device attached to the steering wheel. Thus, each image in this autonomous driving dataset is paired with a steering angle ranging from −338.82 • to 501.78 • . Experimental setup: To make the training and evaluation easier, we remove many images in this autonomous driving dataset where an image is removed due to at least one of the following reasons:</p><p>• This image is incorrectly labeled (e.g., some images show that the car was turning left/right but the corresponding steering angles are zero). • This image has very bad visual quality due to overexposure or underexposure. • There is no reference object (e.g., double amber lines or side roads) in the image to let a human visually determines whether the car was turning left/right. • The corresponding steering angle is outside [−80 • , 80 • ]. Eventually, there are 12,271 images left with 1,904 distinct steering angles in [−80 • , 80 • ]. These images are then resized to 64 × 64 and they form a subset of the autonomous driving dataset <ref type="bibr" target="#b36">[37]</ref>, termed Steering Angle in this paper. Please note that the Steering Angle dataset is highly imbalanced and a histogram of steering angles is shown in <ref type="figure" target="#fig_3">Fig. S.15</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative and visual results:</head><p>To evaluate the quality of fake images, we use the proposed Sliding Fréchet Inception Distance (SFID) as the overall metric instead of Intra-FID, since we have very few real images for many angles (e.g., angles close to the two end points of [−80 • , 80 • ]). We preset 1,000 SFID centers in [−80 • , 80 • ] and let the SFID radius be 2 • . NIQE, Diversity (entropy of predicted types of scenes), and Label Score are also reported. Please see Supp. S.15.5 for more details of these performance measures.</p><p>We report in <ref type="table" target="#tab_8">Table 5</ref> the average quality of 100,000 fake images from each candidate method. Some example fake images are also shown in <ref type="figure" target="#fig_3">Fig. 18</ref>. We also compute FID, NIQE, Diversity, and Label Score in each SFID interval and plot the line graphs of FID/NIQE/Diversity/Label Score versus SFID Center in <ref type="figure" target="#fig_3">Fig. 19</ref>. Based on these quantitative and visual results, we can conclude:</p><p>• Two ILI-based CcGAN methods are better than cGAN in terms of all four metrics; however, two NLI-based CcGAN methods have lower label consistency than cGAN. • Although the NIQE score and Label Score of cGAN are not too bad, cGAN has a very low Diversity score and <ref type="figure" target="#fig_3">Fig. 19(c)</ref> shows that the Diversity scores are almost zeros at some angles. Example fake images in <ref type="figure" target="#fig_3">Fig. 18</ref> also show that cGAN has the mode collapse problem <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b38">[40]</ref> (i.e., always generating the same image for some angles). • Line graphs in <ref type="figure" target="#fig_3">Fig. 19</ref> shows that the performance of cGAN is very unstable across all SFID intervals. • Two ILI-based CcGANs perform better than two NLI-based CcGANs in terms of all metrics except NIQE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Effectiveness of SFID</head><p>In this section, we study the effectiveness of SFID on RC-49. Since RC-49 has a large enough sample size of real images, we can get a reliable Intra-FID. At the same time, we can also deliberately reduce the sample size of real images to mimic the scenario where a reliable Intra-FID is not applicable but SFID still works well. The experiment in this section can be also conducted on Cell-200 but is omitted in this paper. We study 11 combinations of the SFID radius (r SFID ) and the number of SFID centers (# c SFID ) in this experiment where we use SFID to evaluate cGAN and two CcGAN methods (i.e., HVDL+NLI and HVDL+ILI) pre-trained in Section 5.2. In Setting 1, we let r SFID = 0 so SFID degenerates to Intra-FID. In the same setting, we evaluate the three GANs on all 899 distinct angles and all real images in RC-49 are used to compute Intra-FID, so Setting 1 is taken as the oracle setting in this experiment. In Setting 2, we also let r SFID = 0 so SFID degenerates to Intra-FID again. Differently, in Setting 2, we only evaluate GANs on the 450 distinct angles which are seen in the training set of the experiment in Section 5.2. Moreover, to simulate the scenario where we have very few real images to compute Intra-FID, we deliberately reduce the number of real images at each distinct angle from 49 to 10. Therefore, in Setting 2, there are 10 real images for each angle seen in the training set and 0 real image for each angle unseen in the training set. Setting 2 is treated <ref type="figure" target="#fig_3">Fig. 18</ref>: Three Steering Angle images for each of 6 angles: real images and example fake images from cGAN and four proposed CcGANs, respectively. cGAN has severe mode collapse problem in this experiment. Two NLI-based CcGANs do not work well but two ILI-baesd CcGANs produce images with higher visual quality and more diversity. <ref type="figure" target="#fig_3">Fig. 19</ref>: Line graphs of FID/NIQE/Diversity/Lable Score versus SFID Center on the Steering Angle dataset. To plot these line graphs, we compute these metrics within each SFID interval defined by the corresponding SFID center. Figs. (a) to (d) show that, although the NLI-based CcGANs do not have good label consistency, the ILI-based CcGANs substantially outperform cGAN in most SFID intervals. All graphs of CcGANs also appear much smoother than those of cGAN because of HVDL and SVDL.</p><p>as the baseline in this experiment. Settings 3 to 11 are designed to show the effectiveness of SFID so we let r SFID &gt; 0. Similar to Setting 2, from Settings 3 to 11, real images are available only for those 450 distinct angles seen in the training set and only 10 real images are available for each angle. We consider three values for r SFID (i.e., 0.5, 1, and 2) and three values for the number of c SFID 's (i.e., 400, 600, and 800). For all settings, we compute one FID in each SFID interval (in Settings 1 and 2, the SFID interval degenerates to an angle) and report in <ref type="table" target="#tab_9">Table 6</ref> the mean of these FIDs along with their standard deviation after the "±" symbol. Setting 1 is the oracle setting whose evaluation results can be seen as the ground truth, and we hope the evaluation results of SFID are close to Setting 1. In Setting 2, when we have very few real images (even zero) for each angle, Intra-FID overestimates the average FID of each GAN (e.g., from 1.7201 to 1.9664 for cGAN) and underestimates the quantitative difference between cGAN and CcGANs (e.g., from (1.7201 − 0.6119)/1.7201 ≈ 64.4% to (1.9664 − 1.2102)/1.9664 ≈ 38.5% for cGAN and HVDL+NLI). However, the performance of our proposed SFID in Settings 3 to 11 is very close to the oracle setting. If we do the comparison within Settings 3 to 11, we can see r SFID is inversely proportional to the SFID score while # c SF ID does not have obvious influence on SFID. From <ref type="table" target="#tab_9">Table 6</ref>, we may conclude that as long as r SFID is set at a moderate level, SFID is a valid proxy to the oracle Intra-FID when we don't have enough real images to compute Intra-FID. Evaluation results of SFID on RC-49 under different setups of r SFID and number of c SFID 's. In the first two settings, SFID degenerates to Intra-FID since r SFID = 0. In Setting 1, we evaluate GANs on all 899 distinct angles and all real images are used to compute Intra-FID, so Setting 1 is the oracle setting. In Setting 2, we evaluate GANs on the 450 angles seen in the training set and, for each angle, 10 real images are used to compute Intra-FID, so Setting 2 is treated as the baseline. The performance of our proposed SFID (Settings 3 to 11) is close to the oracle setting while Intra-FID (Setting 2) tends to overestimate the average FID and underestimate the quantitative difference between cGAN and CcGANs.</p><p>Settings r SFID # c SFID cGAN HVDL+NLI HVDL+ILI </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>As the first generative model, we propose the CcGAN in this paper for generative image modeling conditional on regression labels. In CcGAN, two novel empirical discriminator losses (HVDL and SVDL), a novel empirical generator loss and two novel label input mechanisms (NLI and ILI) are proposed to overcome the two problems of existing cGANs. The error bounds of a discriminator trained under HVDL and SVDL are studied in this work. Two new benchmark datasets, RC-49 and Cell-200, are also proposed for the continuous scenario. A new evaluation metric, termed SFID, is also proposed to replace Intra-FID when we don't have enough real images. Train G; <ref type="bibr" target="#b11">12</ref> Draw m g labels Y g with replacement from Υ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head><p>Create another set of target labels Y g, = {y i + |y i ∈ Y g , ∈ N (0, σ 2 ), i = 1, . . . , m g } (G training is conditional on these labels) ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14</head><p>Generate m g fake images conditional on Y g, and put these image-label pairs in Ω f g ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15</head><p>Update G with samples in Ω f g via gradient-based optimizers based on Eq. Train G; <ref type="bibr" target="#b13">14</ref> Draw m g labels Y g with replacement from Υ; <ref type="bibr" target="#b14">15</ref> Create another set of target labels Y g, = {y i + |y i ∈ Y g , ∈ N (0, σ 2 ), i = 1, . . . , m g } (G training is conditional on these labels) ; <ref type="bibr" target="#b15">16</ref> Generate m g fake images conditional on Y g, and put these image-label pairs in Ω f g ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Update G with samples in Ω f g via gradient-based optimizers based on Eq.(11) ; 18 end Remark S.5. If should be noted that, for computational efficiency, the normalizing constants N r y r j + r ,κ , N g y g j + g ,κ , N r i=1 w r (y r i , y r j + r ), and N g i=1 w g (y g i , y g j + g ) in Eq. <ref type="formula" target="#formula_6">(9)</ref> and <ref type="formula" target="#formula_7">(10)</ref> are excluded from the training and only used for theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.9 A RULE OF THUMB FOR HYPER-PARAMETER SELECTION</head><p>In our experiments, we normalize labels to real numbers in [0, 1] and the hyper-parameter selection is conducted based on the normalized labels. To be more specific, the hyper-parameter σ is computed based on a rule-of-thumb formula for the bandwidth selection of KDE <ref type="bibr" target="#b23">[24]</ref>, i.e., σ = 4σ 5 y r /3N r 1/5 , whereσ y r is the sample standard deviation of normalized labels in the training set. Let κ base = max y r <ref type="bibr" target="#b1">[2]</ref>  , where y r [l] is the l-th smallest normalized distinct real label and N r uy is the number of normalized distinct labels in the training set. The κ is set as a multiple of κ base (i.e., κ = m κ κ base ) where the multiplier m κ stands for 50% of the minimum number of neighboring labels used for estimating p r (x|y) given a label y. For example, m κ = 1 implies using 2 neighboring labels (one on the left while the other one on the right). In our experiments, m κ is generally set as 1 or 2. In some extreme case when many distinct labels have too few real samples, we may consider increasing m κ . We also found ν = 1/κ 2 works well in our experiments. We then bound the two terms of the RHS separately as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.10 MORE DETAILS</head><p>1) Real images with labels in [y − κ, k + κ] can be seen as independent samples from p y,κ r (x). Then the first term can be bounded by applying Hoeffding's inequality as follows: ∀δ ∈ (0, 1), with at least probability 1 − δ, Similarly, we apply identical proof strategy to the fake images x g and generator distribution p g (x|y).</p><formula xml:id="formula_20">sup D∈D 1 N r y,κ N r i=1 1 {|y−y r i |≤κ} U − log D(x r i , y) U − E x∼p y,κ r (x) U − log D(x, y) U ≤ U 1 2N r y,κ</formula><p>Lemma S.6. (Restatement of Lemma 2) Suppose that (A1), (A3) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_21">sup D∈D 1 N g y,κ N g i=1 1 {|y−y g i |≤κ} [− log(1 − D(x i , y))] − E x∼pg(x|y) [− log(1 − D(x, y))] ≤ U 1 2N g y,κ log 2 δ + κU M g 2 , (S.25)</formula><p>for a given y.</p><p>Proof. This proof is omitted because it is almost identical to the one for Lemma 1.</p><p>The following two lemmas provide the bounds for SVDL.</p><p>Lemma S.7. (Restatement of Lemma 3) Suppose that (A1), (A2) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ, for a given y.</p><formula xml:id="formula_22">sup D∈D 1 N r N r i=1 w r (y r i , y) [− log D(x r i , y)] 1 N r N r i=1 w r (y r i , y) − E x∼pr(x|y) [− log D(x,</formula><p>Proof. For brevity, denote by f (x, y) = − log D(x, y) and F = − log D. Then,</p><formula xml:id="formula_23">sup D∈D 1 N r N r i=1 w r (y r i , y) [− log D(x r i , y)] 1 N r N r i=1 w r (y r i , y) − E x∼pr(x|y) [− log D(x, y)] = sup f ∈F 1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − E x∼pr(x|y) [f (x, y)] ≤ sup f ∈F 1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − E x∼p y,w r r (x) [f (x, y)] + sup f ∈F E x∼p y,w r r (x) [f (x, y)] − E x∼pr(x|y) [f (x, y)] (S.27)</formula><p>where the inequality is by triangular inequality. We then derive bounds for both two terms of the last line. 1) For the first term, we can further split it into two parts,</p><formula xml:id="formula_24">1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − E x∼p y,w r r (x) [f (x, y)] ≤ 1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − 1 N r N r i=1 w r (y r i , y)f (x r i , y) W r (y) + 1 N r N r i=1 w r (y r i , y)f (x r i , y) W r (y) − E x∼p y,w r r (x) [f (x, y)] (S.28)</formula><p>Focusing on the first part of RHS of Eq.(S.28). By (A1),</p><formula xml:id="formula_25">1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − 1 N r N r i=1 w r (y r i , y)f (x r i , y) W r (y) ≤ U 1 N r N r i=1 w r (y r i , y) − W r (y) W r (y)</formula><p>Note that ∀y, y , w r (y , y) = e −ν|y−y | 2 ≤ 1 and hence given y, w r (y , y) is a random variable bounded by 1. Apply Hoeffding's inequality to the numerator of above, yielding that with probability at least 1 − δ , dy . Thus,</p><formula xml:id="formula_26">1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − 1 N r N r i=1 w r (y r i , y)f (x r i , y) W r (y) ≤ U W r (</formula><formula xml:id="formula_27">1 N r N r i=1 w r (y r i , y)f (x r i , y) W r (y) − E x∼p y,w r r (x) [f (x, y)] = 1 W r (y) 1 N r N r i=1 w r (y r i , y)f (x r i , y) − E (x,y )∼pr(x,y ) [w r (y , y)f (x r i , y)] ,</formula><p>where p r (x, y ) = p r (x|y )p r (y ) denotes the joint distribution of real image and its label. Again, since w r (y , y)f (x r i , y) is uniformly bounded by U under (A1), we can apply Hoeffding's inequality. This implies that with probability at least 1 − δ , the above can be upper bounded by</p><formula xml:id="formula_28">U W r (y) 1 2N r log 2 δ . (S.30)</formula><p>Combining Eq. (S.29) and (S.30) and by setting δ = δ 2 , we have with probability at least 1 − δ,</p><formula xml:id="formula_29">1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − E x∼p y,w r r (x) [f (x, y)] ≤ U W r (y) 1 2N r log 4 δ .</formula><p>Since this holds for ∀f ∈ F, taking supremum over f , we have </p><formula xml:id="formula_30">sup f ∈F 1 N r N r i=1 w r (y r i , y)f (x r i , y) 1 N r N r i=1 w r (y r i , y) − E x∼p y,</formula><formula xml:id="formula_31">sup D∈D 1 N r N r i=1 w r (y r i , y) [− log D(x r i , y)] 1 N r N r i=1 w r (y r i , y) − E x∼pr(x|y) [− log D(x, y)] ≤ U W r (y) 1 2N r log 4 δ + U M r 2 E y ∼p r w (y |y) [|y − y|] .</formula><p>This finishes the proof.</p><p>Lemma S.8. (Restatement of Lemma 4) Suppose that (A1), (A3) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_32">sup D∈D 1 N g N g i=1 w g (y g i , y) [− log(1 − D(x g i , y))] 1 N g N g i=1 w g (y g i , y) − E x∼pg(x|y) [− log(1 − D(x, y))] ≤ U W g (y) 1 2N g log 4 δ + U M g 2 E y ∼p g w (y |y) [|y − y|] , (S.33)</formula><p>for a given y.</p><p>Proof. This proof is omitted because it is almost identical to the one for Lemma 3.</p><p>As introduced in Section 2, we use KDE for the marginal label distribution with Gaussian kernel. The next theorem characterizes the difference between a p r (y), p g (y) and their KDE using n i.i.d. samples.  for some constants C KDE 1,δ , C KDE 2,δ depending on δ. Proof. By ( <ref type="bibr" target="#b39">[41]</ref>; P.12), for any p(t) ∈ Σ(L) (the Hölder Class, see Definition 1), with probability at least 1 − δ, S.10.2.2 Error bounds for HVDL and SVDL Based on the lemmas and theorems in Supp. S.10.2.1, we derive the error bounds of HVDL and SVDL, which will be used in the proofs of Theorems 1 and 2.</p><p>Theorem S.4. Assume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_33">sup D∈D L HVDL (D) − L(D) ≤ U   C KDE 1,δ log N r N r σ + L r σ 2   + U   C KDE 2,δ log N g N g σ + L g σ 2   + κU (M r + M g ) 2 + U 1 2 log 8 δ E y∼p KDE r (y) 1 N r y,κ + E y∼p KDE g (y) 1 N g y,κ , (S.37)</formula><p>for some constants C KDE 1,δ , C KDE 2,δ depending on δ.</p><p>Proof. We first decompose sup D∈D L HVDL (D) − L(D) as follows</p><formula xml:id="formula_34">sup D∈D L HVDL (D) − L(D) ≤ sup D∈D [− log D(x, y)] p r (x|y)dx (p r (y) −p KDE r (y))dy + sup D∈D [− log(1 − D(x, y))] p g (x|y)dx (p g (y) −p KDE g (y))dy + sup D∈D 1 N r y,κ N r i=1 1 {|y−y r i |≤κ} [− log D(x r i , y)] − E x∼pr(x|y) [− log D(x, y)] p KDE r (y)dy + sup D∈D 1 N g y,κ N r i=1 1 {|y−y g i |≤κ} [− log(1 − D(x g i , y))] − E x∼pg(x|y) [− log(1 − D(x, y))] p KDE g (y)dy .</formula><p>These four terms in the RHS can be bounded separately as follows 1) The first term can be bounded by using Theorem S. <ref type="bibr" target="#b2">3</ref>  for some constants C KDE 1,δ1 depending on δ 1 . 2) The second term can be bounded by using Theorem S.3 and the boundness of D and y ∈ [0, 1]. For the first term, ∀δ 2 ∈ (0, 1), with at least probability 1 − δ 2 ,</p><formula xml:id="formula_35">sup D∈D [− log(1 − D(x, y))] p g (x|y)dx (p g (y) −p KDE g (y))dy ≤U   C KDE 2,δ2 log N g N r σ + L g σ 2   , (S.39)</formula><p>for some constants C KDE 2,δ2 depending on δ 2 . 3) The third term can be bounded by using Lemma 1 and S.6. For the third term, ∀δ 3 ∈ (0, 1), with at least probability 1 − δ 3 ,   <ref type="figure" target="#fig_3">(0, 1)</ref>, with at least probability 1 − δ 4 ,</p><formula xml:id="formula_36">sup D∈D 1 N g y,κ N r i=1 1 {|y−y g i |≤κ} [− log(1 − D(x g i , y))] − E x∼pg(x|y) [− log(1 − D(x, y))] dx p KDE g (y)dy ≤ κU M g 2 + U 1 2 log 2 δ 4 E y∼p KDE g (y) 1 N g y,κ</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(S.41)</head><p>With δ 1 = δ 2 = δ 3 = δ 4 = δ 4 , combining Eq. (S.38) -(S.41) leads to the upper bound in Theorem S.4. Theorem S.5. Assume that (A1)-(A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ,</p><formula xml:id="formula_37">sup D∈D L SVDL (D) − L(D) ≤ U   C KDE 1,δ log N r N r σ + L r σ 2   + U   C KDE 2,δ log N g N g σ + L g σ 2   + U 1 2 log 16 δ 1 √ N r E y∼p KDE r (y) 1 W r (y) + 1 √ N g E y∼p KDE g (y) 1 W g (y) + U 2 M r E y∼p KDE r (y) E y ∼p r w (y |y) |y − y| + M g E y∼p KDE g (y) E y ∼p g w (y |y) |y − y| (S.42)</formula><p>for some constant C KDE 1,δ , C KDE 2,δ depending on δ.</p><p>Proof. Similar to the decomposition for Theorem S.4, we can decompose sup D∈D L SVDL (D) − L(D) into four terms which can be bounded by using Theorem S.3, the boundness of D, Lemma 3, and Lemma 4. The detail is omitted because it is almost identical to the one of Theorem S.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.10.2.3 Proof of Theorem 1</head><p>Based on Theorem S.4, we derive Theorem 1.</p><p>Proof. We first decompose L( D HVDL ) − L(D * ) as follows  S.10.2.5 Interpretation of Theorems 1 and 2 Both theorems imply HVDL and SVDL perform well if the output of D is not too close to 0 or 1 (i.e., favor small U ). The first two terms in both upper bounds control the quality of KDE, which implies KDE works better if we have larger N r and N g and a smaller σ. The rest terms of the two bounds are different. In the HVDL case, we favor smaller κ, M r , and M g . However, we should avoid setting κ for a too small value because we prefer larger N r y,κ and N g y,κ . In the SVDL case, we prefer small M r and M g but large W r (y) and W g (y). Large W r (y) and W g (y) imply that the weight function decays slowly (i.e., small ν; similar to large N r y,κ and N g y,κ in Eq.(S.37)). However, we should avoid setting ν too small because a small ν leads to large E y ∼p r w (y |y) |y − y| and E y ∼p g w (y |y) |y − y| (i.e., y 's which are far away from y have large weights). In our experiments, we use some rule-of-thumb formulae to select κ and ν. As a future work, a refined hyper-parameter selection method should be proposed. <ref type="figure" target="#fig_7">SIMULATION IN SECTION 5</ref> Please note that when input labels into CcGAN, we first convert y to a point on the circle (i.e., (sin(y), cos(y))) and concatenate the 2-D coordinate of this point with z or x. When input labels into cGAN, we use the hidden concatenation scheme <ref type="bibr" target="#b0">[1]</ref>.  S.11.2: Network architectures for the generator and discriminator of our proposed CcGAN in the simulation. The label y is treated as a real scalar so its dimension is 1. We do not directly input y into the generator and discriminator. We first convert each y into the coordinate of the mean represented by this y, i.e., (sin(y), cos(y)). Then we insert this coordinate into the networks.</p><formula xml:id="formula_38">L( D HVDL ) − L(D * ) =L( D HVDL ) − L( D HVDL ) + L( D HVDL ) − L( D) + L( D) − L( D) + L( D) − L(D * ) (by L( D HVDL ) − L( D) ≤ 0) ≤2 sup D∈D L HVDL (D) − L(D) + L( D) − L(D * ) (by Theorem S.4) ≤2U   C KDE 1,δ log N r N r σ + L r σ 2   + 2U   C KDE 2,δ log N g N g σ + L g σ 2   + κU (M r + M g ) + 2U 1 2 log 8 δ E y∼p KDE r (y)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.11 MORE DETAILS OF THE</head><p>(a) Generator </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.11.2 Training setups</head><p>The cGAN and CcGAN are trained for 6000 iterations on the training set with the Adam <ref type="bibr" target="#b41">[43]</ref> optimizer (with β 1 = 0.5 and β 2 = 0.999), a constant learning rate 5 × 10 −5 and batch size 128. The rule of thumb formulae in Section S.9 are used to select the hyper-parameters for HVDL and SVDL, where we let m κ = 2. Thus, the three hyper-parameters in this experiments are set as follows: σ = 0.074, κ = 0.017, ν = 3600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.11.3 Testing setups</head><p>When evaluating the trained cGAN, if a test label y is unseen in the training set, we first find its closest, seen label y. Then, we generate samples from the trained cGAN at y instead of at y . On the contrary, generating samples from CcGAN at unseen labels is well-defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.11.4 Extra Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.11.4.1 Varying Number of Gaussians for Training Data Generation</head><p>In this section, we study the influence of the number of Gaussians used for training data generation on the performance of cGAN and CcGAN. We vary the number of Gaussians from 120 to 10 with step size 10 but keep other settings in Section 5.1 unchanged and plot the line graphs of 2-Wasserstein Distance (log scale) versus the number of Gaussians in <ref type="figure" target="#fig_3">Fig. S.11.1</ref>. Reducing the number of Gaussians for training implies a larger gap between any two consecutive distinct angles in the training set. As the number of Gaussians decreases, the continuous scenario gradually degenerates to the categorical scenario, therefore the assumption that a small perturbation to y results in a negligible change to p(x|y) is no longer satisfied. Consequently, the 2-Wasserstein distances of the proposed two CcGAN methods gradually increase and eventually surpass the 2-Wasserstein distance of cGAN when the number of Gaussians is small (e.g., less than 40). Note that reducing the number of Gaussians in the training data generation will not improve the performance of cGAN in the testing because many angles seen in the testing stage (we evaluate each method on 360 angles) do not appear in the training set. As the number of Gaussians decreases, the continuous scenario gradually degenerates to the categorical scenario, therefore the assumption that a small perturbation to y results in a negligible change to p(x|y) is no longer satisfied. Consequently, the 2-Wasserstein distances of two CcGAN methods gradually increase and eventually surpass the 2-Wasserstein distance of cGAN when the number of Gaussians is small (e.g., less than 40).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.12 MORE DETAILS OF THE EXPERIMENT ON RC-49 IN SECTION 5.2 S.12.1 Description of RC-49</head><p>To generate RC-49, firstly we randomly select 49 3-D chair object models from the "Chair" category provided by ShapeNet <ref type="bibr" target="#b42">[44]</ref>. Then we use Blender v2.79 1 to render these 3-D models. Specifically, during the rendering, we rotate each chair model along with the yaw axis for a degree between 0.1 • and 89.9 • (angle resolution as 0.1 • ) where we use the scene image mode to compose our dataset. The rendered images are converted from the RGBA to RGB color model. In total, the RC-49 dataset consists of 44051 images of image size 64×64 in the PNG format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.12.2 Network architectures</head><p>The RC-49 dataset is a more sophisticated dataset compared with the simulation, thus it requires networks with deeper layers. We employ the SNGAN architecture <ref type="bibr" target="#b19">[20]</ref> in both cGAN and CcGAN consisting of residual blocks for the generator and the discriminator. Moreover, for the generator in cGAN, the regression labels are input into the network by the label embedding <ref type="bibr" target="#b40">[42]</ref> and the conditional batch normalization <ref type="bibr" target="#b26">[27]</ref>. For the discriminator in cGAN, the regression labels are fed into the network by the label embedding and the label projection <ref type="bibr" target="#b2">[3]</ref>. For CcGAN, the regression labels are fed into networks by the two proposed label input methods (NLI and ILI) in Section 2.2. The pre-trained CNN T 1 + T 2 for ILI is a modified ResNet-34 with two extra linear layers before the final linear layer. The label embedding network T 3 is a 5-layer MLP with 128 nodes in each layer. The dimension of the noise z is 128 for NLI-based CcGANs and 256 for ILI-based CcGANs. Please refer to our codes for more details about the network specifications of cGAN and CcGAN.   <ref type="figure" target="#fig_3">Fig. S.12.4</ref>. Since, at each angle, the degenerated CcGAN only uses the images at this angle, it leads to the mode collapse problem (e.g, the row in the yellow rectangle) and bad visual quality (e.g., images in the red rectangle) at some angles. Note that the degenerated CcGAN is still different from cGAN, since we still treat y as a continuous scalar instead of a class label here and we use the proposed label input method (e.g., NLI) to incorporate y into the generator and the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.12.7.3 cGAN: different number of classes</head><p>In this experiment, we show that cGAN still fails even though we bin [0.1, 89.9] into other number of classes. We experimented with three different bin setting -grouping labels into 90, 150, and 210 classes, respectively. Experimental results are shown in <ref type="figure" target="#fig_3">Fig. S.12.5</ref> and we observe all three cGANs completely fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.13 MORE DETAILS OF THE EXPERIMENT ON THE UTKFACE DATASET IN SECTION 5.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.13.1 Description of the UTKFace dataset</head><p>The UTKFace dataset is an age regression dataset <ref type="bibr" target="#b34">[35]</ref>, with human face images collected in the wild. We use a preprocessed version (cropped and aligned), with ages spanning from 1 to 60. After the data cleaning (i.e., removing images of very low quality or with clearly wrong labels), the number of images left is 14760. These images are then resized to 64 × 64. The histogram of the UTKFace dataset after data cleaning is shown in S.13.6. From <ref type="figure" target="#fig_3">Fig. S.13</ref>.6, we can see UTKFace dataset is very imbalanced so the samples from the minority age groups are unlikely to be chosen at each iteration during the GAN training. Consequently, cGAN and CcGAN may not be well-trained at these minority age groups. To increase the chance of drawing these minority samples during training, we randomly replicate samples in the minority age groups to ensure that the sample size of each age is more than 200. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.13.2 Network architectures</head><p>The network architectures used in this experiment is similar to those in the RC-49 experiment. Please refer to our codes for more details about the network specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.13.3 Training setups</head><p>The cGAN and CcGAN are trained for 40,000 iterations on the training set with the Adam <ref type="bibr" target="#b41">[43]</ref> optimizer (with β 1 = 0.5 and β 2 = 0.999), a constant learning rate 10 −4 and batch size 512. The rule of thumb formulae in Section S.9 are used to select the hyper-parameters for HVDL and SVDL, where we let m κ = 1.</p><p>Please see our codes for more details of the training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.13.4 Performance measures</head><p>Similar to the RC-49 experiment, we evaluate the quality of fake images by Intra-FID, NIQE, Diversity, and Label Score. We also train an AE (bottleneck dimension is 512), a classification-oriented ResNet-34, and a regression-oriented ResNet-34 on the UTKFace dataset. Please note that, the UTKFace dataset consists of face images from 5 races based on which we train the classification-oriented ResNet-3. The AE and both two ResNets are trained for 200 epochs with a batch size 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.13.5 More example UTKFace images</head><p>More example UTKFace images are shown in <ref type="figure" target="#fig_3">Fig. S.13</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.13.6.2 Degenerated CcGAN</head><p>We consider the extreme cases of the proposed CcGANs on the UTKFace dataset. As shown in <ref type="figure" target="#fig_3">Fig. S.13</ref>.9, the degenerated NLI-based CcGANs fails to generate facial images at some ages (e.g., 51 and 57) because of too small sample sizes.</p><p>S.13.6.3 cGAN: different number of classes In the last experiment, we bin samples into different number of classes based on ground-truth labels, in order to increase the number of training samples at each class. Then we train cGAN using samples from the binned classes. We experimented with two different bin setting, i.e., binning image samples into 60 classes and 40 classes, respectively. The results are reported in <ref type="figure" target="#fig_3">Fig.S.13.10</ref>. The results demonstrate cGANs consistently fail to generate diverse synthetic images with labels aligned with their conditional information. Moreover, the image quality is much worse than those from the proposed CcGANs. In conclusion, compared with existing cGANs, our proposed CcGANs have substantially better performance in terms of the image quality and diversity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14 MORE DETAILS OF THE EXPERIMENT ON THE CELL-200 DATASET IN SECTION 5.4 S.14.1 Description of Cell-200</head><p>Cell-200 is a synthetic image dataset, emulating the colonies of bacterial cells in the view of fluorescence microscope. This dataset contains cell populations with overall number varying between 1 and 200, generated with <ref type="bibr" target="#b43">[45]</ref>. For each cell population (e.g., 1 to 200), we generate 1000 different synthetic fluorescence microscopic images, with diverse cell variations (e.g., shapes, locations, overlaps and blurring effects). As in <ref type="bibr" target="#b44">[46]</ref>, we set nucleus radius as 5, and image size as 256 × 256. To alleviate computational burden, images in the Cell-200 dataset are then resized to 64 × 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.2 Network architectures</head><p>The network architectures for cGAN and CcGAN in this experiment are adapted from the famous DCGAN <ref type="bibr" target="#b30">[31]</ref> architecture. The dimension of the noise z is 128 for NLI-based CcGANs and 256 for ILI-based CcGANs. Please refer to our codes for more details about the network specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.3 Training setups</head><p>The cGAN and CcGAN are trained for 5000 iterations on the training set with the Adam <ref type="bibr" target="#b41">[43]</ref> optimizer (with β 1 = 0.5 and β 2 = 0.999) and a constant learning rate 10 −4 . The rule of thumb formalue in Section S.9 are used to select the hyperparameters for HVDL and SVDL, where we let m κ = 2. For cGAN training, the cell count range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">200]</ref> is split into 100 disjoint intervals, i.e., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3)</ref>, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5)</ref>, . . . , <ref type="bibr">[197,</ref><ref type="bibr">199)</ref>, <ref type="bibr">[199,</ref><ref type="bibr">200]</ref>. In this case, cGAN estimates image distribution conditional on these intervals. In Supp. S.14.7.2, we also compare the performance of cGAN under different splitting schemes. Please note that we use different batch sizes for cGAN and CcGAN in this experiment. The batch size for cGAN is 512. Differently, for all CcGAN methods in this experiment, the batch size is 512 for the generator but 32 for the discriminator. The reason that we use different batch sizes for the generator and discriminator in CcGAN is based on some observations we got during training. In this experiment, if the generator and the discriminator in CcGAN have the same batch size, the discriminator loss often decreases to almost zero very quickly while the generator loss still maintains at a high level.</p><p>Consequently, at each iteration, the discriminator can easily distinguish the real and fake images while the generator cannot fool the discriminator and won't improve in the next iteration, which implies a high imbalance between the generator update and the discriminator update. To balance the training of the generator and the discriminator, we deliberately decrease the number of images seen by the discriminator at each iteration to slow down the update of the discriminator so that the generator can catch up.</p><p>Please see our codes for more details of the training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.4 Testing setups</head><p>We evaluate the trained cGAN and four CcGAN methods on all 200 cell counts (half of them are blinded during training). Each method generates 1,000 images for each cell count, so there are 200,000 fake images from each method. When evaluating the trained cGAN, if a test label y is unseen in the training set, we just need to find which interval (recall we split <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">200]</ref> is split into 100 disjoint intervals) covers this label. Then, we generate samples from the trained cGAN conditional on this interval instead of y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.5 Performance measures</head><p>Similar to the previous two experiments, we evaluate the quality of fake images by Intra-FID, NIQE, and Label Score but Diversity. The Diversity score is not available here because we don't have any class label in Cell-200. An AE with a bottleneck dimension of 512 and a regression-oriented ResNet-34 are pre-trained on the complete Cell-200 dataset (i.e., 200,000 images) to compute the Intra-FID and Label Score respectively. The possibility of lacking class labels in regression-oriented datasets is another reason that we propose to use an AE to compute Intra-FID instead of a classification-oriented CNN. The AE is trained for 50 epochs with a batch size of 256. The regression-oriented ResNet-34 is trained for 200 epochs with a batch size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.6 More example UTKFace images</head><p>More example Cell-200 images are shown in <ref type="figure" target="#fig_3">Fig. S.14.11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.7 Extra experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.7.1 Interpolation</head><p>To perform the label interpolation, we keep the noise vector z fixed and vary label from 10 to 200 for the four CcGANs. The interpolation results are illustrated in S.14.12. As cell count y increases, we observe the cells in images become more crowded. This observation convincingly shows that all four CcGANs do not simply memorize or overfit to the training set. Indeed, our CcGANs demonstrate continuous control over synthetic images with respect to cell counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.14.7.2 cGAN: different number of classes</head><p>In this experiment, we experimented with two different bin setting -grouping labels into 100 classes and 50 classes, respectively. Experimental results are shown in <ref type="figure" target="#fig_3">Fig. S.14.13</ref>. We observe both cGANs fail in this experiment. First of all, cGANs still suffer from the mode collapse problems. Besides, cell counts of generated images do not match those of their given labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.15 MORE DETAILS OF THE EXPERIMENT ON THE STEERING ANGLE DATASET IN SECTION 5.5 S.15.1 Description of Steering Angle</head><p>The Steering Angle dataset is a subset of an autonomous driving dataset <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr">[38]</ref>. Steering Angle consists of 12,271 RGB images with 1,904 distinct steering angles ranging from −80 • to 80 • . We resize all images to 64 × 64. The histogram of the steering angles in this dataset is show in <ref type="figure" target="#fig_3">Fig. S.15</ref>.14. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.15.2 Network architectures</head><p>The network architectures used in this experiment is similar to those in the RC-49 and UTKFace experiments. Please refer to our codes for more details about the network specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.15.3 Training setups</head><p>The cGAN and CcGAN are trained for 20,000 iterations on the training set with the Adam <ref type="bibr" target="#b41">[43]</ref> optimizer (with β 1 = 0.5 and β 2 = 0.999) and a constant learning rate 10 −4 . The rule of thumb formalue in Section S.9 are used to select the hyperparameters for HVDL and SVDL, where we let m κ = 5. Please note that, similar to the Cell-200 experiment, we use different batch sizes for the generator and discriminator in four CcGAN methods. The batch size is set to 64 and 512 respectively for the discriminator and generator in CcGAN. Please refer to Supp. S.14.3 for the reason.</p><p>Please see our codes for more details of the training setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.15.4 Testing setups</head><p>At the testing stage, we first set 2,000 evenly spaced evaluation labels in [−80 • , 80 • ] and we ask each GAN model to generate 50 images conditional on each evaluation label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.15.5 Performance measures</head><p>The quality of generated images from each GAN is evaluated by SFID, NIQE, Diversity, and Label Score.</p><p>• SFID: To computing SFID, we preset 1,000 SFID centers [−80 • , 80 • ] and let r SFID = 2 • . These SFID centers and r SFID = 2 define 1,000 joint SFID intervals. We compute one FID between real and fake images with labels in this interval. We report the mean (i.e., SFID) and standard deviation of these FIDs for each GAN. • NIQE <ref type="bibr" target="#b33">[34]</ref>: Different from previous three experiments, we train only one NIQE model by using all real images in the Steering Angle dataset since this dataset is highly imbalanced. In the evaluation, we compute one NIQE score for each SFID interval. The reported NIQE score in <ref type="table" target="#tab_8">Table 5</ref> is the mean of these NIQE scores. • Diversity: The original autonomous driving dataset <ref type="bibr" target="#b36">[37]</ref>, [38] does not have class labels. To compute Diversity, we manually group the images in Steering Angle into five categories according to their background objects or the types of the road in the images. The five groups are labeled respectively by tree, tree+barrier, bush, bush+barrier, and winding mountain road. Some example images for these five groups are show in <ref type="figure" target="#fig_3">Fig. S.15.15</ref>. Images in the tree group all have trees in the background. Images in the tree+barrier group all have trees and barriers in the background. Images in the bush group all have bushes in the background. Images in the bush+barrier group all have bushes and barriers in the background. Images in the winding mountain road group all correspond to the scenes of winding mountain roads. A classification-oriented ResNet-34 is trained to classify images from these five groups, and then the Diversity score can be computed based on the entropies of predicted scenes in each SFID interval. <ref type="figure" target="#fig_3">Fig. S.15</ref>.15: Example Steering Angle images from 5 scenes, i.e., tree, tree+barrier, bush, bush+barrier, and winding mountain road (from left to right).</p><p>• Label Score: Similar to previous experiments, we pre-train a regression-oriented ResNet-34 to predict the angle for each fake image, and then computes Label Score. Please note that, when plotting the line graph of Label Score versus SFID Center in <ref type="figure">Fig ??</ref>, one Label Score is computed for each SFID interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.15.6 More example Steering Angle images</head><p>More example Steering Angle images are shown in <ref type="figure" target="#fig_3">Fig. S.15</ref>.16. <ref type="figure" target="#fig_3">Fig. S.15</ref>.16: Three Steering Angle images for each of 10 angles: real images and example fake images from cGAN and four proposed CcGANs, respectively. cGAN has severe mode collapse problem in this experiment. Two NLI-based CcGANs do not work well but two ILI-baesd CcGANs produce images with higher visual quality and more diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.15.7 Extra experiments</head><p>S.15.7.1 Interpolation In this section, for each CcGAN method, we fix the noise vector z but vary the regression label y from −71.8 • to 72 • . We can see the road in the image gradually changes from a left turn to a right turn.</p><p>S.15.7.2 cGAN: different number of classes In this experiment, we experimented with three different bin setting -grouping labels into 90 classes, 150 classes, and 210 classes, respectively. Experimental results are shown in <ref type="figure" target="#fig_3">Fig. S.15</ref>.18. We observe that different bin settings cannot improve cGAN's performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>L</head><label></label><figDesc>(D) = −E y∼pr(y) E x∼pr(x|y) [log (D(x, y))] − E y∼pg(y) E x∼pg(x|y) [log (1 − D(x, y))] = − log(D(x, y))p r (x, y)dxdy − log(1 − D(x, y))p g (x, y)dxdy,(1)L(G) = −E y∼pg(y) E z∼q(z) [log (D(G(z, y), y))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(G(z c,j , c), c)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>− x g c,j )δ(y − c),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 :</head><label>1</label><figDesc>HVE (Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>The naive label input mechanism (NLI) for G of CcGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>The naive label input mechanism (NLI) for D of CcGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>The pre-trained CNN T1 + T2 for label embedding. The first subnetwork T1 consists of some convolutional layers (Conv.) and some linear layers. The second subnetwork T2 includes one linear layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>The label embedding network is a multilayer perceptron (MLP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>D's in D are measurable and uniformly bounded. Let U max{sup D∈D [− log D] , sup D∈D [− log(1 − D)]} and U &lt; ∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :</head><label>6</label><figDesc>The improved label input mechanism (ILI) for G of CcGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 :</head><label>7</label><figDesc>The improved label input mechanism (ILI) for D of CcGAN.(A2). For ∀x ∈ X and y, y ∈ Y, ∃g r (x) &gt; 0 and M r &gt; 0, s.t. |p r (x|y ) − p r (x|y)| ≤ g r (x)|y − y| with g r (x)dx = M r . (A3). For ∀x ∈ X and y, y ∈ Y, ∃g g (x) &gt; 0 and M g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 :</head><label>9</label><figDesc>Visual results for the Circular 2-D Gaussians simulation. (a) shows 1,200 training samples from 120 Gaussians, with 10 samples per Gaussian. In (b) to (d), each GAN generates 100 fake samples at each of 12 means not appearing in the training set,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Line graphs of FID/NIQE/Diversity/Lable Score versus yaw angle on RC-49. Figs. (a) to (c) show that four CcGAN methods consistently outperform cGAN across all angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 :</head><label>12</label><figDesc>Line graphs of Intra-FID versus the sample size for each distinct training angle of RC-49. The grey vertical dashed line stands for the sample size used in the main study of the RC-49 experiment. Four CcGAN methods substantially outperform cGAN and ILI performs better than NLI no matter what the sample size for each distinct angle in the training set. The overall trend in this figure shows that a smaller sample size deteriorates the performance of both cGAN and CcGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 :</head><label>14</label><figDesc>Line graphs of FID/NIQE/Diversity/Lable Score versus Age on UTKFace. The four CcGAN methods significantly outperform cGAN in Figs. (a) to (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 :</head><label>17</label><figDesc>Line graphs of FID/NIQE/Lable Score versus Cell Count on Cell-200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Algorithm 2 : 2 Train D; 3 4 5 Initialize Ω r d = φ, Ω f d = φ; 6 for i = 1 to m d do 7 8 9 end 10 Update</head><label>22345678910</label><figDesc>Finally we demonstrate the superiority of the proposed CcGAN to cGAN on the Circular 2-D Gaussians, RC-49, UTKFace, Cell-200, and Steering Angle datasets. Please find the codes for this paper at https://github.com/UBCDingXin/improved CcGAN S.8 ALGORITHMS FOR CCGAN TRAINING An algorithm for CcGAN training with the proposed HVDL.Data: N r real image-label pairs Ω r = {(x r 1 , y r 1 ), . . . , (x r N r , y r N r )}, N r uy ordered distinct labels Υ = {y r [1] , . . . , y r [N ruy ] } in the dataset, preset σ and κ, number of iterations K, the discriminator batch size m d , and the generator batch size m g . Result: Trained generator G.1 for k = 1 to K do Draw m d labels Y d with replacement from Υ;Create a set of target labels Y d, = {y i + |y i ∈ Y d , ∈ N (0, σ 2 ), i = 1, . . . , m d } (D training is conditional on these labels) ;Randomly choose an image-label pair (x, y) ∈ Ω r satisfying |y − y i − | ≤ κ where y i + ∈ Y d, and let Ω r d = Ω r d ∪ (x, y i + ). ;Randomly draw a label y from U (y i + − κ, y i + + κ) and generate a fake image x by evaluating G(z, y ), where z ∼ N (0, I).LetΩ f d = Ω f d ∪ (x , y i + ). ; D with samples in set Ω r d and Ω f d via gradient-based optimizers based on Eq.(6); 11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>3 : 2 Train D; 3 4 6 for i = 1 to m d do 7 8 9 3 ν, y i + + − log 10 − 3 ν) 11 end 12 Update</head><label>32346789331112</label><figDesc>An algorithm for CcGAN training with the proposed SVDL. Data: N r real image-label pairs Ω r = {(x r 1 , y r 1 ), . . . , (x r N r , y r N r )}, N r uy ordered distinct labels Υ = {y r [1] , . . . , y r [N ruy ] } in the dataset, preset σ and ν, number of iterations K, the discriminator batch size m d , and the generator batch size m g . Result: Trained generator G.1 for k = 1 to K do Draw m d labels Y d with replacement from Υ;Create a set of target labels Y d, = {y i + |y i ∈ Y d , ∈ N (0, σ 2 ), i = 1, . . . , m d } (D training is conditional on these labels) ;5 Initialize Ω r d = φ, Ω f d = φ;Randomly choose an image-label pair (x, y) ∈ Ω r satisfying e −ν(y−y i − ) 2 &gt; 10 −3 where y i + ∈ Y d, and let Ω r d = Ω r d ∪ (x, y i + ). This step is used to exclude real images with too small weights. ;Compute w r i (y, y i + ) = e −ν(y i + −y) 2 ;Randomly draw a label y from U (y i + − − log 10 −and generate a fake image x by evaluatingG(z, y ), where z ∼ N (0, I). Let Ω f d = Ω f d ∪ (x , y i + ). ; 10Compute w g i (y , y i + ) = e −ν(y i + −y ) 2 ; D with samples in set Ω r d and Ω f d via gradient-based optimizers based on Eq.(7);<ref type="bibr" target="#b12">13</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>OF THEOREMS 1 AND 2 S. 10 . 1 S. 10 . 2 Proofs of Theorems 1 and 2 S. 10 . 2 . 1 111</head><label>210110221021</label><figDesc>Some necessary definitions and notations • The hypothesis space D is a set of functions that can be represented by D (a neural network with determined architecture). • In the HVDL case, denote by p y,κ r (x) y+κ y−κ p r (x|y )p r (y )dy the marginal distribution of real images with labels in [y − κ, y + κ] and similarly to p y,κ g (x) of fake images. • In the SVDL case, given y and weight functions (E.q. (8)), if the number of real and fake images are infinite, the empirical density converges to p y,w r r (x) p r (x|y ) w r (y ,y)pr(y ) W r (y) dy and p y,w g g(x) p g (x|y ) w g (y ,y)pg(y) W g (y)dy respectively, where W r (y) w r (y , y)p r (y )dy and W g (y) w g (y , y)p g (y )dy .• Let p r w (y |y) w r (y ,y)p r (y ) W r (y)and p g w (y |y) w g (y ,y)p g (y )W g (y). • The Hölder Class defined in Definition 1 is a set of functions with bounded second derivatives, which controls the variation of the function when parameter changes. (A4) implies the two probability density functions p r (y) and p g (y) are assumed in the Hölder Class. • Given a G, the optimal discriminator which minimizes L is in the form ofD * (x, y) = p r (x, y) p r (x, y) + p g (x, y) . (S.20) However, D * may not be covered by the hypothesis space D. The D is the minimizer of L in the hypothesis space D. Thus, L( D) − L(D * ) should be a non-negative constant. In CcGAN, we minimize L HVDL (D) or L HVDL (D) with respect to D ∈ D, so we are more interested in the distance of D HVDL and D SVDL from D * , i.e., L( D HVDL ) − L(D * ) and L( D SVDL ) − L(D * ). Technical lemmas Before we move to the proofs of Theorems 1 and 2, we provide several technical lemmas used in the later proof. Recall notations and assumptions in Sections 3 and S.10.1, then we derive the following lemmas. Lemma S.5. (Restatement of Lemma 1) Suppose that (A1)-(A2) and (A4) hold, then ∀δ ∈ (0, 1), with probability at least 1 − δ, {|y−y r i |≤κ} [− log D(x r i , y)] − E x∼pr(x|y) [− log D(x, y)] {|y−y r i |≤κ} [− log D(x r i , y)] − E x∼pr(x|y) [− log D(x, y)] {|y−y r i |≤κ} [− log D(x r i , y)] − E x∼p y,κ r (x) [− log D(x, y)] + sup D∈D E x∼p y,κ r (x) [− log D(x, y)] − E x∼pr(x|y) [− log D(x, y)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>2 )</head><label>2</label><figDesc>For the second term, by the definition of p y,κ r (x) and defining p κ (y ) = 1 {|y −y|≤κ} p(y ) 1 {|y −y|≤κ} p(y )dy , we have sup D∈D E x∼p y,κ r (x) [− log D(x, y)] − E x∼pr(x|y) [− log D(x, y)] (by the definition of total variation and the boundness of − log D) ≤ U 2 |p y,κ r (x) − p r (x|y)| dx. (S.23) Then, focusing on |p y,κ r (x) − p r (x|y)|, |p y,κ r (x) − p r (x|y)| = p(x|y )p κ (y )dy − p(x|y) ≤ |p(x|y ) − p(x|y)| p κ (y )dy (by (A2)) ≤ g r (x)|y − y|p κ (y )dy ≤ κg r (x). Thus, Eq. (S.23) is upper bounded as follows, sup D∈D E x∼p y,κ r (x) [− log D(x, y)] − E x∼pr(x|y) [− log D(x, y)] ≤ κg r (x)dx (by (A2)) = κM r . (S.24) By combining Eq. (S.22) and (S.24), we can get Eq. (S.21), which finishes the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>2 )</head><label>2</label><figDesc>w r r (x) [f (x, y)] ≤ U W r (y) For the second term on the RHS of Eq.(S.27). By (A1) that |f | &lt; U , sup f ∈F E x∼p y,w r r (x) [f (x, y)] − E x∼pr(x|y) [f (x, y)] ≤ U p y,w r r (x) − p r (x|y) T V = U 2 |p y,w r r (x) − p r (x|y)|dx. Note that by the definition of p y,w r r (x) p r (x|y ) w r (y ,y)pr(y ) W r (y) dy and p r w (y |y) w r (y ,y)p r (y ) W r (y) , we have |p y,w r r (x) − p r (x|y)| = p r (x|y )p r w (y |y) dy − p r (x|y) ≤ |p r (x|y ) − p r (x|y)| p r w (y |y) dy . By (A.2) and y ∈ [0, 1], the above is upper bounded by g r (x)E y ∼p r w (y |y) [|y − y |]. Thus, sup f ∈F E x∼p y,w r r (x) [f (x, y)] − E x∼pr(x|y) [f (x, y)] ≤ U 2 g r (x)E y ∼p r w (y |y) [|y − y|] dx = U M r 2 E y ∼p r w (y |y) [|y − y|] . (S.32) Therefore, combining both Eq.(S.31) and (S.32), with probability at least 1 − δ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Theorem S. 3 .</head><label>3</label><figDesc>Letp KDE r (y) andp KDE g (y) stand for the KDE of p r (y) and p g (y) respectively. Under condition (A4), if the KDEs are based on n i.i.d. samples from p r /p g and a bandwidth σ, for all δ ∈ (0, 1), with probability at least 1 − δ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>sup t p KDE (t) − p(t) ≤ C KDE δ log n nσ + cσ 2 , (S.36)for some constants C KDE δ and c, where C depends on δ and c = L K(s)|s| 2 ds. Since in this work, K is chosen as Gaussian kernel, c = L K(s)|s| 2 ds = L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>and the boundness of D and y ∈ [0, 1]. For the first term, ∀δ 1 ∈ (0, 1), with at least probability 1 − δ 1 , sup D∈D [− log D(x, y)] p r (x|y)dx (p r (y) −p KDE r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>11</head><label></label><figDesc>{|y−y r i |≤κ} [− log D(x r i , y)] − E x∼pr(x|y) [− log D(x, y)] p KDE r {|y−y r i |≤κ} [− log D(x r i , y)] − E x∼pr(x|y) [− log D(x, y)] p KDE r Note that N r y,κ = N r i=1 1 {|y−y r i |} , which is a random variable of y i s. The above can be expressed as sup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>+</head><label></label><figDesc>L( D) − L(D * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>(S. 43 )</head><label>43</label><figDesc>S.10.2.4 Proof of Theorem 2Based on Theorem S.5, we derive Theorem 2.Proof. The detail is omitted because it is almost identical to the one of Theorem 1 in Supp. S.10.2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. S. 11 . 1 :</head><label>111</label><figDesc>Line graphs of 2-Wasserstein Distance (log scale) versus the number of Gaussians for training data generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. S. 12 . 2 :</head><label>122</label><figDesc>Three RC-49 example images for each of 10 angles: real images and example fake images from cGAN and four proposed CcGANs, respectively. CcGANs produce chair images with higher visual quality and more diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Fig. S. 12 . 3 :</head><label>123</label><figDesc>Some example RC-49 fake images from the four CcGAN methods. We fix the noise z but vary the label y.S.12.7.2 Degenerated CcGANIn this experiment, we consider the extreme case of the proposed CcGAN (degenerated CcGAN), i.e., σ → 0 and κ → 0 or ν → +∞. Some examples from a degenerated NLI-based CcGAN are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Fig. S. 12 . 4 :</head><label>124</label><figDesc>Some example RC-49 fake images from a degenerated NLI-based CcGAN.Fig. S.12.5: Example RC-49 fake images from cGAN when we bin the yaw angle range into different number of classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Fig. S. 13 . 6 :</head><label>136</label><figDesc>The histogram of the UTKFace dataset with ages varying from 1 to 60.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Fig. S. 13 . 8 :</head><label>138</label><figDesc>Some examples of generated UTKFace images from the four CcGAN methods. We fix the noise z but vary the label y from 3 to 57.Fig. S.13.9: Some example UTKFace fake images from a degenerated NLI-based CcGAN. Fig. S.13.10: Example UTKFace fake images from cGAN when we bin the age range into different number of classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Fig</head><label></label><figDesc>. S.14.11: Three Cell-200 images for each of 10 cell counts absent in the training data: real images and example fake images from cGAN and four proposed CcGANs, respectively. cGAN has severe mode collapse problem in this experiment. Two NLI-based CcGANs do not perform well enough but two ILI-based CcGANs produce images with higher visual quality, more diversity, and higher label consistency.Fig. S.14.12: Some examples of generated Cell-200 images from the four CcGAN methods. We fix the noise z but vary the label y from 10 to 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Fig. S. 14 . 13 :</head><label>1413</label><figDesc>Example Cell-200 fake images from cGAN when we bin the range of cell count into different number of classes.Fig. S.15.14: The histogram of the Steering Angle dataset with steering angles varying from −80 • to 80 • . At many angles, we only have 1 or 2 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Fig. S. 15 . 17 :</head><label>1517</label><figDesc>Some examples of generated Steering Angle images from the four CcGAN methods. We fix the noise z but vary the label y from −71.8 • to 72 • .Fig. S.15.18: Example Steering Angle fake images from cGAN when we bin the range of steering angles into different number of classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Average quality of 36,000 fake samples from cGAN and CcGAN over three repetitions with standard deviations after the "±" symbol. "↓" ("↑") indicates lower (higher) values are preferred. Two novel label input mechanisms are not tested in this simple, low-dimensional simulation.</figDesc><table><row><cell>Method</cell><cell cols="2">% High Quality ↑ % Recovered Modes ↑</cell><cell>2-Wasserstein Dist. ↓</cell></row><row><cell>cGAN (120 classes)</cell><cell>68.8 ± 4.8</cell><cell>81.8 ± 3.9</cell><cell>3.32 × 10 −2 ± 3.13 × 10 −2</cell></row><row><cell>CcGAN (HVDL)</cell><cell>99.3 ± 0.4</cell><cell>100.0 ± 0.0</cell><cell>3.03 × 10 −4 ± 5.05 × 10 −5</cell></row><row><cell>CcGAN (SVDL)</cell><cell>99.6 ± 0.1</cell><cell>100.0 ± 0.0</cell><cell>2.56 × 10 −4 ± 8.95 × 10 −6</cell></row></table><note>(a) 1200 training samples and 120 means (red dots).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :visual quality and more diversity. Extra experimental results:</head><label>2</label><figDesc>Average quality of 179,800 fake RC-49 images from cGAN and CcGAN with standard deviations after the "±" symbol. ± 0.081 1.805 ± 0.179 2.814 ± 0.052 1.816 ± 1.481 CcGAN (SVDL+ILI) 0.389 ± 0.095 1.783 ± 0.173 2.949 ± 0.069 1.940 ± 1.489Fig. 10: Three RC-49 example images for each of 6 angles: real images and example fake images from cGAN and four proposed CcGANs, respectively. CcGANs produce chair images with higher To test cGAN and CcGAN under more challenging scenarios, we vary the sample size for each distinct angle in the training set from 45 to 5. We visualize the line graphs of Intra-FID versus the sample size for each distinct training angle inFig. 12. From this figure,we can see the four CcGAN methods substantially outperform cGAN and ILI performs better than NLI no matter what is the sample size for each distinct angle in the training set. The overall trend in this figure also shows that smaller sample size reduces the performance of both cGAN and CcGAN.</figDesc><table><row><cell>Method</cell><cell>Intra-FID ↓</cell><cell>NIQE ↓</cell><cell>Diversity ↑</cell><cell>Label Score ↓</cell></row><row><cell>cGAN (150 classes)</cell><cell>1.720 ± 0.384</cell><cell>2.731 ± 0.162</cell><cell>0.779 ± 0.199</cell><cell>4.815 ± 5.152</cell></row><row><cell>CcGAN (HVDL+NLI)</cell><cell>0.612 ± 0.145</cell><cell>1.869 ± 0.181</cell><cell>2.353 ± 0.121</cell><cell>5.617 ± 4.452</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>0.515 ± 0.181</cell><cell>1.853 ± 0.159</cell><cell>2.610 ± 0.113</cell><cell>4.982 ± 4.439</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell>0.424</cell><cell></cell><cell></cell><cell></cell></row></table><note>"↓" ("↑") indicates lower (higher) values are preferred.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :higher visual quality and more diversity. Extra experimental results:</head><label>3</label><figDesc>Average quality of 60,000 fake UTKFace images from cGAN and CcGAN with standard deviations after the "±" symbol. "↓" ("↑") indicates lower (higher) values are preferred. ± 0.157 1.725 ± 0.171 1.298 ± 0.176 7.452 ± 6.022Fig. 13: Three UTKFace example images for each of 6 ages: real images and example fake images from cGAN and four proposed CcGANs, respectively. CcGANs produce face images with The histogram in Fig. S.13.6shows that the UTKFace dataset is highly imbalanced. To balance the training data and also test the performance of cGAN and CcGAN under smaller sample sizes, we vary the maximum sample size for each distinct age in the training from 200 to 50. Note that, we do not restrict the maximum sample size in the main study. Since we have a much smaller sample size, we reduce the number of iterations for the GAN training from 40</figDesc><table><row><cell>Method</cell><cell>Intra-FID ↓</cell><cell>NIQE ↓</cell><cell>Diversity ↑</cell><cell>Label Score ↓</cell></row><row><cell>cGAN (60 classes)</cell><cell>4.516 ± 0.965</cell><cell>2.315 ± 0.306</cell><cell>0.254 ± 0.353</cell><cell>11.087 ± 8.119</cell></row><row><cell>CcGAN (HVDL+NLI)</cell><cell>0.572 ± 0.167</cell><cell>1.739 ± 0.145</cell><cell>1.338 ± 0.178</cell><cell>9.782 ± 7.166</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>0.547 ± 0.181</cell><cell>1.753 ± 0.196</cell><cell>1.326 ± 0.198</cell><cell>10.739 ± 8.340</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell cols="4">0.480 ± 0.145 1.709 ± 0.169 1.280 ± 0.203 7.505 ± 5.857</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell>0.425</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Average quality of 200,000 fake Cell-200 images from cGAN and CcGAN with standard deviations after the "±" symbol. "↓" ("↑") indicates lower (higher) values are preferred.</figDesc><table><row><cell>Method</cell><cell>Intra-FID ↓</cell><cell>NIQE ↓</cell><cell>Label Score ↓</cell></row><row><cell>cGAN (100 classes)</cell><cell>90.255 ± 64.595</cell><cell>2.130 ± 2.440</cell><cell>66.748 ± 51.711</cell></row><row><cell cols="2">CcGAN (HVDL+NLI) 50.052 ± 20.584</cell><cell>1.488 ± 0.153</cell><cell>72.599 ± 37.425</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell cols="3">56.078 ± 19.334 1.8289 ± 0.3856 83.367 ± 49.577</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell>8.759 ± 6.652</cell><cell>1.283 ± 0.534</cell><cell>5.861 ± 4.900</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell>7.266 ± 2.305</cell><cell>1.220 ± 0.515</cell><cell>5.905 ± 5.020</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.14. When training cGAN, we divide [−80 • , 80 • ] into 210 equal intervals where each interval is taken as a class. When implementing CcGAN, the three hyper-parameters of HVDL and SVDL are selected by the rule of thumb formulae in Supp. S.9, i.e., σ ≈ 0.029, κ ≈ 0.032 and ν ≈ 1000.438. Both cGAN and CcGAN are trained for 20,000 iterations. To evaluate the candidate models, we choose 2,000 evenly spaced angles in [−80 • , 80 • ] and generate 50 images from each candidate GAN model for each of these angles. Please see Supp. S.15 for the network architectures and more details about the training/testing setup.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Average quality of 100,000 fake Steering Angle images from cGAN and CcGAN with standard deviations after the "±"</figDesc><table><row><cell>Method</cell><cell>SFID ↓</cell><cell>NIQE ↓</cell><cell>Diversity ↑</cell><cell>Label Score ↓</cell></row><row><cell>cGAN (210 classes)</cell><cell>3.285 ± 0.647</cell><cell>1.296 ± 0.095</cell><cell>0.603 ± 0.396</cell><cell>14.596 ± 15.402</cell></row><row><cell>CcGAN (HVDL+NLI)</cell><cell>1.969 ± 0.676</cell><cell>1.093 ± 0.024</cell><cell>0.991 ± 0.361</cell><cell>22.322 ± 18.758</cell></row><row><cell>CcGAN (SVDL+NLI)</cell><cell>1.866 ± 0.649</cell><cell>1.098 ± 0.038</cell><cell>1.007 ± 0.248</cell><cell>19.678 ± 18.281</cell></row><row><cell>CcGAN (HVDL+ILI)</cell><cell>1.635 ± 0.699</cell><cell>1.152 ± 0.047</cell><cell cols="2">1.153 ± 0.153 10.868 ± 9.644</cell></row><row><cell>CcGAN (SVDL+ILI)</cell><cell>1.546 ± 0.626</cell><cell>1.130 ± 0.078</cell><cell cols="2">1.156 ± 0.189 10.933 ± 8.978</cell></row></table><note>symbol. "↓" ("↑") indicates lower (higher) values are preferred.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>.1 S.11.1 Network architectures</head><label></label><figDesc>Please refer toTable S.11.1 and Table S.11.2 for the network architectures we adopted for cGAN and CcGAN in our Simulation experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE S .</head><label>S</label><figDesc>11.1: Network architectures for the generator and discriminator of cGAN in the simulation. "fc" denotes a fully-connected layer. "BN" stands for batch normalization. The label y is treated as a class label and encoded by label-embedding<ref type="bibr" target="#b40">[42]</ref> so its dimension equals to the number of distinct angles in the training set (i.e., y ∈ R 120 ). ReLU concat(output of previous layer, y) ∈ R 220 , where y ∈ R 120 is the label of x.</figDesc><table><row><cell>(a) Generator</cell><cell>(b) Discriminator</cell></row><row><cell>z ∈ R 2 ∼ N (0, I); y ∈ R 120 concat(z, y) ∈ R 122</cell><cell>A sample x ∈ R 2 fc→ 100; ReLU</cell></row><row><cell>fc→ 100; BN; ReLU</cell><cell>fc→ 100; ReLU</cell></row><row><cell>fc→ 100; BN; ReLU</cell><cell>fc→ 100; ReLU</cell></row><row><cell>fc→ 100; BN; ReLU fc→ 100; BN; ReLU fc→ 100; BN; ReLU fc→ 100; BN; ReLU</cell><cell>fc→ 100; fc→ 100; ReLU</cell></row><row><cell>fc→ 2</cell><cell>fc→ 1; Sigmoid</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>higher visual quality and more diversity. S.13.6 Extra experiments</head><label></label><figDesc>.7. Fig. S.13.7: Three UTKFace example images for each of 10 ages: real images and example fake images from cGAN and four proposed CcGANs, respectively. CcGANs produce face images with S.13.6.1 Interpolation To perform label interpolation experiments, we keep the noise vector z fixed and vary label from age 3 to age 57 for the four CcGANs. The interpolation results are illustrated in S.13.8. As age y increases, we observe the synthetic face gradually becomes older in appearance. This observation convincingly shows that all four CcGANs do not simply memorize or overfit to the training set. Indeed, our CcGANs demonstrate continuous control over synthetic images with respect to ages.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. https://www.blender.org/download/releases/2-79/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding machine learning : from theory to algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised regression with generative adversarial networks using minimal labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Olmschenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vicinal risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised regression with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>uS Patent App. 15/789,518</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reg-GAN: Semi-supervised learning based on generative adversarial networks for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rezagholiradeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Haidar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2806" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense crowd counting convolutional neural networks with minimal data using semi-supervised dual-goal generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Olmschenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition: Learning with Imperfect Data Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On data augmentation for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Image augmentations for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Quantum Physics of Light and Matter: A Modern Introduction to Photons, Atoms and Many-Body Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salasnich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Remarks on some nonparametric estimates of a density function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Lii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Politis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected Works of Murray Rosenblatt</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the evaluation of conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08175</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computational optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computational framework for simulating fluorescence microscope images with cell populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehmussola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selinummi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yli-Harja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1016" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How a high school junior made a self-driving car? @ONLINE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/how-a-high-school-junior-made-a-self-driving-car-705fa9b6e860" />
	</analytic>
	<monogr>
		<title level="m">The Steering Angle dataset @ONLINE</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">VEEGAN: Reducing mode collapse in GANs using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3308" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Escaping from collapsing modes in a constrained space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Density estimation @ONLINE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<ptr target="http://www.stat.cmu.edu/∼larry/=sml/densityestimation.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Computational framework for simulating fluorescence microscope images with cell populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehmussola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selinummi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yli-Harja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1016" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">rule of thumb formulae in Section S.9 are used to select the hyper-parameters for HVDL and SVDL, where we let m κ = 2. Thus, the three hyper-parameters in this experiments are set as follows: σ = 0.0473, κ = 0.004, ν = 50625. The modified ResNet-34 (i.e., the T 1 + T 2 in Fig. 4) for ILI is trained for 200 epochs with the SGD optimizer, initial learning rate 0.1 (decay at epoch 60, 120, and 160 with factor 0.2), weight decay 10 −4 , and batch size 256. The 5-layer MLP for the label embedding in ILI is trained for 500 epochs with the SGD optimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
	<note>Learning to count objects in images. initial learning rate 0.1 (decay at epoch 100, 200, and 400 with factor 0.2), weight decay 10 −4 , and batch size 256. Please see our codes for more details of the training setups</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">One overall metric (Intra-FID) and three separate metrics (NIQE, Diversity, and Label Score) are used. Their in terms of the four metrics, we first train an autoencoder (AE) , a regression-oriented ResNet-34 [33] and a classification-oriented ResNet-34 [33] on all real images of RC-49. The bottleneck dimension of the AE is 512 and the AE is trained to reconstruct the real images in RC-49 with the MSE loss. The regression-oriented</title>
	</analytic>
	<monogr>
		<title level="m">The RC-49 dataset consists of 899 distinct yaw angles and at each angle there are 49 images (corresponding to 49 types of chairs)</title>
		<imprint/>
	</monogr>
	<note>Please note that, among these 899 yaw angles, only 450 of them are seen at the training stage so real images at the rest 449 angles are not used in the training. We evaluate the quality of the fake images from three perspectives, i.e., visual quality, intra-label diversity, and label consistency. ResNet-34 is trained to predict the yaw angle of a given image. The classification-oriented ResNet-34</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">At each evaluation angle, we compute the FID [30] between 49 real images and 200 fake images in terms of the bottleneck feature of the pre-trained AE. The Intra-FID score is the average FID over all 899 evaluation angles. Please note that we also try to use the classification-oriented ResNet-34 to compute the Intra-FID but the Intra-FID scores vary in a very wide range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Intra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Fid</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>We take Intra-FID as the overall score to evaluate the quality of fake images and we prefer the small Intra-FID score. and sometimes obviously contradict with the three separate metrics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">We train one NIQE model with the 49 real images at each of the 899 angles so we have 899 NIQE models. During evaluation, an average NIQE score is computed for each evaluation angle based on the NIQE model at that angle. Finally, we report the average and standard deviations of the 899 average NIQE scores over the 899 yaw angels (i.e., &quot;the mean/standard deviation of 899 means&quot;). Note that the NIQE is implemented by the NIQE library in MATLAB. The block size and the sharpness threshold are</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Niqe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIQE is used to evaluate the visual quality of fake images with the real images as the reference and we prefer the small NIQE score</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
	<note>set to 8 and 0.1 respectively in this and rest experiments</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">At each evaluation angle, we ask a pre-trained classification -oriented ResNet-34 to predict the chair types of the 200 fake images and an entropy is computed based on these predicted chair types. The diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Diversity</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Diversity is used to evaluate the intra-label diversity and the larger the better. In RC-49, there are 49 chair types. reported in Table 2 is the average of the 899 entropies over all evaluation angles</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">We ask the pre-trained regressionoriented ResNet-34 to predict the yaw angles of all fake images and the predicted angles are then compared with the assigned angles. The Label Score is defined as the average absolute distance between the predicted angles and assigned angles over all fake images</title>
	</analytic>
	<monogr>
		<title level="m">• Label Score: Label Score is used to evaluate the label consistency and the smaller the better</title>
		<imprint/>
	</monogr>
	<note>which is equivalent to the Mean Absolute Error (MAP). Note that, to plot the line graphs, we compute Label Score at each of the 899 evaluation angles</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">For an input pair (z, y), we fix the noise z but perform label-wise interpolations, i.e., varying label y from 4.5 to 85.5. Clearly, all generated images are visually realistic and we can see the chair distribution smoothly changes over continuous angles. Please note that, Fig. S.12.3 is meant to show the smooth change of the chair distribution instead of one single chair so the chair type may change over angles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Hvdl+nli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hvdl+ili</forename><surname>Svdl+nli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svdl+ili)</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Interpolation In Fig. S.12.3, we present some interpolation results of the four CcGAN methods. This confirms CcGAN is capable of capturing the underlying conditional image distribution rather than simply memorizing training data</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
