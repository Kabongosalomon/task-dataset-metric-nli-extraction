<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoying</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Oriented object detection in aerial images is a challenging task as the objects in aerial images are displayed in arbitrary directions and are usually densely packed. Current oriented object detection methods mainly rely on twostage anchor-based detectors. However, the anchor-based detectors typically suffer from a severe imbalance issue between the positive and negative anchor boxes. To address this issue, in this work we extend the horizontal keypointbased object detector to the oriented object detection task. In particular, we first detect the center keypoints of the objects, based on which we then regress the box boundaryaware vectors (BBAVectors) to capture the oriented bounding boxes. The box boundary-aware vectors are distributed in the four quadrants of a Cartesian coordinate system for all arbitrarily oriented objects. To relieve the difficulty of learning the vectors in the corner cases, we further classify the oriented bounding boxes into horizontal and rotational bounding boxes. In the experiment, we show that learning the box boundary-aware vectors is superior to directly predicting the width, height, and angle of an oriented bounding box, as adopted in the baseline method. Besides, the proposed method competes favorably with state-of-the-art methods. Code is available at https:// github.com/ yijingru/ BBAVectors-Oriented-Object-Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection in aerial images serves as an essential step for numerous applications such as urban planning, traffic surveillance, port management, and maritime rescue <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. The aerial images are taken from the bird's-eye view. Detecting objects in aerial images is a challenging task as the objects typically have different scales and textures, and the background is complex. Moreover, the objects are usually densely packed and displayed in arbitrary directions. Consequently, applying the horizontal bounding boxes to oriented object detection would lead to misalign- ment between the detected bounding boxes and the objects <ref type="bibr" target="#b1">[2]</ref>. To deal with this problem, oriented bounding boxes are preferred for capturing objects in aerial images. Current oriented object detection methods are mainly derived from the two-stage anchor-based detectors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>. Generally, in the first stage, those detectors spread anchor boxes on the feature maps densely and then regress the offsets between the target box and the anchor box parameters in order to provide region proposals. In the second stage, the region-of-interest (ROI) features are pooled to refine the box parameters and classify the object categories. Notably, they use the center, width, height, and angle as the descriptions of an oriented bounding box. The angle is learned either in the first stage or in the second stage. For instance, R 2 CNN <ref type="bibr" target="#b6">[7]</ref>, Yang et al. <ref type="bibr" target="#b24">[25]</ref>, and ROI Transformer <ref type="bibr" target="#b1">[2]</ref> regress the angle parameters from the pooled horizontal region proposal features in the second stage; similarly, R 2 PN <ref type="bibr" target="#b27">[28]</ref>, R-DFPN <ref type="bibr" target="#b23">[24]</ref> and ICN <ref type="bibr" target="#b0">[1]</ref> generate oriented region proposals in the first stage. These oriented object detection methods share the same drawbacks with the anchor-based detectors. For example, the design of the anchor boxes is complicated; and the choices of aspect-ratios and the size of the anchor boxes need to be tuned carefully. Besides, the extreme imbalance between the positive and negative anchor boxes would induce slow training and sub-optimal performance <ref type="bibr" target="#b2">[3]</ref>. Moreover, the crop-and-regress strategies in the second stage are computationally expensive <ref type="bibr" target="#b29">[30]</ref>. Recently, the keypoint-based object detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3]</ref> have been developed to overcome the disadvantages of anchorbased solutions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref> in the horizontal object detection task. In particular, these methods detect the corner points of the bounding boxes and then group these points by comparing embedding distances or center distances of the points. Such strategies have demonstrated improved performance, yet with one weakness that the grouping process is time-consuming. To address this issue, Zhou's Center-Net <ref type="bibr" target="#b28">[29]</ref> suggests detecting the object center and regressing the width (w) and height (h) of the bounding box directly, which achieves faster speed at comparable accuracy. Intuitively, Zhou's CenterNet can be extended to the oriented object detection task by learning an additional angle Î¸ together with w and h (see <ref type="figure" target="#fig_0">Fig. 1a</ref>). However, as the parameters w and h are measured in different rotating coordinate systems for each arbitrarily oriented object, jointly learning those parameters may be challenging for the model.</p><p>In this paper, we extend Zhou's CenterNet to the oriented object detection task. However, instead of regressing the w, h and Î¸ at the center points, we learn the box boundary-aware vectors (BBAVectors, <ref type="figure" target="#fig_0">Fig. 1b</ref>) to capture the rotational bounding boxes of the objects. The BBAVectors are distributed in the four quadrants of the Cartesian coordinate system. Empirically we show that this design is superior to directly predicting the spatial parameters of bounding boxes. In practice, we observe that in the corner cases, where the vectors are very close to the boundary of the quadrants (i.e., xy-axes in <ref type="figure" target="#fig_0">Fig. 1c</ref>), it would be difficult for the network to differentiate the vector types. To deal with this problem, we group the oriented bounding box (OBB) into two categories and handle them separately. Specifically, we have two types of boxes: horizontal bounding box (HBB) and rotational bounding box (RBB), where RBB refers to all oriented bounding boxes except the horizontal ones. We summarize our contributions as follows:</p><p>â¢ We propose the box boundary-aware vectors (BBAVectors) to describe the OBB. This strategy is simple yet effective. The BBAVectors are measured in the same Cartesian coordinate system for all the arbitrarily oriented objects. Compared to the baseline method that learns the width, height and angle of the OBBs, the BBAVectors achieve better performance.</p><p>â¢ We extend the center keypoint-based object detector to the oriented object detection task. Our model is singlestage and anchor box free, which is fast and accurate. It achieves state-of-the-art performances on the DOTA and HRSC2016 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Oriented Object Detection</head><p>The horizontal object detectors, such as R-CNN <ref type="bibr" target="#b13">[14]</ref>, fast R-CNN <ref type="bibr" target="#b3">[4]</ref>, faster R-CNN <ref type="bibr" target="#b18">[19]</ref>, SSD <ref type="bibr" target="#b11">[12]</ref>, YOLO <ref type="bibr" target="#b16">[17]</ref>, are designed for horizontal objects detection. These methods generally use the horizontal bounding boxes (HBB) to capture the objects in natural images. Different from the horizontal object detection task, oriented object detection relies on oriented bounding boxes (OBB) to capture the arbitrarily oriented objects. Current oriented object detection methods are generally extended from the horizontal object detectors. For example, R 2 CNN [7] uses the region proposal network (RPN) to produce the HBB of the text and combines different scales of pooled ROI features to regress the parameters of OBB. R 2 PN <ref type="bibr" target="#b27">[28]</ref> incorporates the box orientation parameter into the RPN network and develops a rotated RPN network. R 2 PN also utilizes a rotated ROI pooling to refine the box parameters. R-DFPN <ref type="bibr" target="#b23">[24]</ref> employs the Feature Pyramid Network (FPN) <ref type="bibr" target="#b10">[11]</ref> to combine multiscale features and boost the detection performance. Based on the DFPN backbone, Yang et al. <ref type="bibr" target="#b24">[25]</ref> further propose an adaptive ROI Align method for the second-stage box regression. RoI Transformer <ref type="bibr" target="#b1">[2]</ref> learns the spatial transformation from the HBBs to OBBs. ICN <ref type="bibr" target="#b0">[1]</ref> develops an Image Cascade Network that enhances the semantic features before adopting R-DFPN. RRD <ref type="bibr" target="#b9">[10]</ref> uses active rotation filters to encode the rotation information. Gliding Vertex <ref type="bibr" target="#b22">[23]</ref> glide the vertex of the horizontal bounding boxes to capture the oriented bounding boxes. All these methods are based on anchor boxes. Overall, the anchor-based detectors first spread a large amount of anchor boxes on the feature maps densely, and then regress the offsets between the target boxes and the anchor boxes. Such anchor-based strategies suffer from the imbalance issue between positive and negative anchor boxes. The issue would lead to slow training and sub-optimal detection performances <ref type="bibr" target="#b8">[9]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Keypoint-Based Object Detection</head><p>The keypoint-based object detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref> capture the objects by detecting the keypoints and therefore provide anchor-free solutions. Keypoint detection is extensively employed in the face landmark detection <ref type="bibr" target="#b14">[15]</ref> and pose estimation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. In the horizontal object detection task, the keypoint-based detection methods propose to detect the corner points or the center points of the objects and extract the box size information from these points. Cornernet <ref type="bibr" target="#b8">[9]</ref> is one of the pioneers. It captures the top-left and bottom-right corner points of the HBB using heatmaps. The corner points are grouped for each object by comparing the embedding distances of the points. Duan's Center-Net <ref type="bibr" target="#b2">[3]</ref> detects both corner points and center points. Ex-tremeNet <ref type="bibr" target="#b29">[30]</ref> locates the extreme and center points of the boxes. These two methods both use the center information to group the box points. However, the post-grouping process in these methods is time-consuming. To address this problem, Zhou's CenterNet <ref type="bibr" target="#b28">[29]</ref> proposes to regress the width and height of the bounding box at the center point without a post-grouping process, which makes the prediction faster. The keypoint-based object detectors show advantages over the anchor-based ones in terms of speed and accuracy, yet the keypoint-based detectors are barely applied to oriented object detection task.</p><p>Baseline method. In this paper, we extend Zhou's Cen-terNet to the oriented object detection task. In particular, we first build a baseline method that directly regresses the width w and height h as well as the orientation angle Î¸ of the bounding boxes. We term this baseline method as Center+wh + Î¸ (see <ref type="figure" target="#fig_0">Fig. 1a</ref>). We compare the proposed method with Center+wh + Î¸ to demonstrate the advantages of box boundary-aware vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first describe the overall architecture of the proposed method, and then explain the output maps in detail. The output maps are gathered and decoded to generate the oriented bounding boxes of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>The proposed network (see <ref type="figure" target="#fig_2">Fig. 2</ref>) is built on a U-shaped architecture <ref type="bibr" target="#b19">[20]</ref>. We use the ResNet101 Conv1-5 <ref type="bibr" target="#b5">[6]</ref> as the backbone. At the top of the backbone network, we upsample the feature maps and output a feature map that is 4 times smaller (scale s = 4) than the input image. In the upsampling process, we combine a deep layer with a shallow layer through skip connections to share both the high-level semantic information and low-level finer details. In particular, we first up-sample a deep layer to the same size of the shallow layer through bilinear interpolation. The upsampled features map is refined through a 3 Ã 3 convolutional layer. The refined feature map is then concatenated with the shallow layer, followed by a 1 Ã 1 convolutional layer to refine the channel-wise features. Batch normalization and ReLU activation are used in the latent layers. Suppose an input RGB image is I â R 3ÃHÃW , where H and W are the height and width of the image. The output feature</p><formula xml:id="formula_0">map X â R CÃ H s Ã W s (C = 256 in this paper) is then trans- formed into four branches: heatmap (P â R KÃ H s Ã W s ), off- set (O â R 2Ã H s Ã W s ), box parameter (B â R 10Ã H s Ã W s ), and the orientation map (Î± â R 1Ã H s Ã W s ),</formula><p>where K is the number of dataset categories and s = 4 refers to the scale. The transformation is implemented with two convolutional layers with 3 Ã 3 kernels and 256 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Heatmap</head><p>Heatmap is generally utilized to localize particular keypoints in the input image, such as the joints of humans and the facial landmarks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. In this work, we use the heatmap to detect the center points of arbitrarily oriented objects in the aerial images. Specifically, the heatmap P â R KÃ H s Ã W s used in this work has K channels, with each corresponding to one object category. The map at each channel is passed through a sigmoid function. The predicted heatmap value at a particular center point is regarded as the confidence of the object detection.</p><p>Groundtruth Suppose c = (c x , c y ) is the center point of an oriented bounding box, we place a 2D Gaussian exp(â (pxâcx) 2 +(pyâcy) 2 2Ï 2</p><p>) (see <ref type="figure" target="#fig_2">Fig. 2</ref>) around each c to form the groundtruth heatmapP â R KÃ H s Ã W s , where Ï is a box size-adaptive standard deviation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9]</ref>, point p = (p x , p y ) indexes the pixel points onP .</p><p>Training Loss When training the heatmaps, only the center points c are positive. All the other points including the points in the Gaussian bumps are negative. Directly learning the positive center points would be difficult due to the imbalance issue. To handle this problem, following the work of <ref type="bibr" target="#b8">[9]</ref>, we decrease the penalty for the points inside the Gaussian bumps and use the variant focal loss to train the heatmap:</p><formula xml:id="formula_1">L h = â 1 N i (1 â p i ) Î± log(p i ) ifp i = 1 (1 âp i ) Î² p Î± i log(1 â p i ) otherwise,<label>(1)</label></formula><p>wherep and p refer to the ground-truth and the predicted heatmap values, i indexes the pixel locations on the feature map, N is the number of objects, Î± and Î² are the hyperparameters that control the contribution of each point. We choose Î± = 2 and Î² = 4 empirically as in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Offset</head><p>In the inference stage, the peak points are extracted from the predicted heatmap P as the center point locations of the objects. These center points c are integers. However, down-scaling a point from the input image to the output heatmap generates a floating-point number. To compensate for the difference between the quantified floating center point and the integer center point, we predict an offset map O â R 2Ã H s Ã W s . Given a ground-truth center point c = (c x ,c y ) on the input image, the offset between the scaled floating center point and the quantified center point is:</p><formula xml:id="formula_2">o = (c x s â c x s ,c y s â c y s ),<label>(2)</label></formula><p>The offset is optimized with a smooth L 1 loss <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_3">L o = 1 N N k=1 Smooth L1 (o k âÃ´ k ),<label>(3)</label></formula><p>where N is the total number of objects,Ã´ refers to the ground-truth offsets, k indexes the objects. The smooth L 1 loss can be expressed as:</p><formula xml:id="formula_4">Smooth L1 (x) = 0.5x 2 if |x| &lt; 1 |x| â 0.5 otherwise.<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Box Parameters</head><p>To capture the oriented bounding boxes, one natural and straightforward way is to detect the width w, and height h, and angle Î¸ of an OBB from the center point. We term this baseline method as Center+wh + Î¸ (see <ref type="figure" target="#fig_0">Fig. 1a</ref>). This method has several disadvantages. First, a small angle variation has marginal influence on the total loss in training, but it may induce a large IOU difference between the predicted box and the ground-truth box. Second, for each object, the w and h of its OBB are measured in an individual rotating coordinate system that has an angle Î¸ with respect to the y-axis. Therefore, it is challenging for the network to jointly learn the box parameters for all the objects. In this paper, we propose to use the box boundary-aware vectors (BBAVectors, see <ref type="figure" target="#fig_0">Fig. 1b</ref>) to describe the OBB. The BBAVectors contain the top t, right r, bottom b and left l vectors from the center points of the objects. In our design, the four types of vectors are distributed in four quadrants of the Cartesian coordinate system. All the arbitrarily oriented objects share the same coordinate system, which would facilitate the transmission of mutual information and therefore improve the generalization ability of the model. We intentionally design the four vectors instead of two (i.e., t and b, or r and l) to enable more mutual information to be shared when some local features are obscure and weak.</p><p>The box parameters are defined as b = [t, r, b, l, w e , h e ], where t, r, b, l are the BBAVectors, w e and h e are the external horizontal box size of an OBB, as described in <ref type="figure" target="#fig_2">Fig. 2</ref>. The details of w e and h e are explained in Section 3.5. Totally, the box parameter map B â R 10Ã H s Ã W s has 10 channels with 2 Ã 4 vectors and 2 external size parameters. We also use a smooth L 1 loss to regress the box parameters at the center point:</p><formula xml:id="formula_5">L b = 1 N N k=1 Smooth L1 (b k âb k ),<label>(5)</label></formula><p>where b andb are the predicted and ground-truth box parameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Orientation</head><p>In practice, we observe that the detection would fail in situations where the objects nearly align with xy-axes (see <ref type="figure" target="#fig_7">Fig. 4b</ref>). The reason would be that at the boundary of the quadrant, the types of the vectors are difficult to be differentiated. We term this problem as corner cases (see <ref type="figure" target="#fig_0">Fig. 1c</ref>). To address this issue, in this work we group OBBs into two categories and process them separately. In particular, the two types of boxes are HBB and RBB, where RBB involves all the rotation bounding boxes except the horizontal ones. The benefit of such a classification strategy is that we transform the corner cases into the horizontal ones, which can be dealt with easily. When the network encounters the corner cases, the orientation category and the external size (w e and h e in <ref type="figure" target="#fig_2">Fig. 2</ref>) can help the network to capture the accurate OBB. The additional external size parameters also enrich the descriptions of an OBB.</p><p>We define the orientation map as Î± â R 1Ã H s Ã W s . The output map is finally processed by a sigmoid function. To create the ground-truth of the orientation classÎ±, we define:</p><formula xml:id="formula_6">Î± = 1 (RBB) IOU(OBB, HBB) &lt; 0.95 0 (HBB) otherwise,<label>(6)</label></formula><p>where IOU is the intersection-over-union between the oriented bounding box (OBB) and the horizontal bounding box (HBB). The orientation class is trained with the binary cross-entropy loss:</p><formula xml:id="formula_7">L Î± = â 1 N N i (Î± i log(Î± i ) + (1 âÎ± i ) log(1 â Î± i )),<label>(7)</label></formula><p>where Î± andÎ± are the predicted and the ground-truth orientation classes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our method on two public aerial image datasets: DOTA <ref type="bibr" target="#b21">[22]</ref> and HRSC2016 <ref type="bibr" target="#b12">[13]</ref>.</p><p>DOTA. We use DOTA-v1.0 <ref type="bibr" target="#b21">[22]</ref> dataset for the oriented object detection. It contains 2,806 aerial images with a wide variety of scales, orientations, and shapes of objects. These images are collected from different sensors and platforms. The image resolution ranges from 800Ã800 to 4000Ã4000. and Helicopter (HC). The DOTA images involve the crowd and small objects in a large image. For accurate detection, we use the same algorithm as ROI Transformer <ref type="bibr" target="#b1">[2]</ref> to crop the original images into patches. In particular, the images are cropped into 600Ã600 patches with a stride of 100. The input images have two scales 0.5 and 1. The trainval set and testing set contain 69,337 and 35,777 images after the cropping, respectively. The trainval set refers to both training and validation sets <ref type="bibr" target="#b1">[2]</ref>. Following the previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, we train the network on the trainval set and test on the testing set. The detection results of cropped images are merged as the final results. Non-maximum-suppression (NMS) with a 0.1 IOU threshold is applied to the final detection results to discard repetitive detection. The testing dataset is evaluated through the online server.</p><p>HRSC2016. Ship detection in aerial images is important for port management, cargo transportation, and maritime rescue <ref type="bibr" target="#b23">[24]</ref>. The HRSC2016 <ref type="bibr" target="#b12">[13]</ref> is a ship dataset collected from Google Earth, which contains 1,061 images with ships in various appearances. The image sizes range from 300 Ã 300 to 1500 Ã 900. The dataset has 436 training images, 181 validation images, and 444 testing images. We train the network on the training set and use the validation set to stop the training when the loss on the validation set no longer decreases. The detection performance of the proposed method is reported on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We resize the input images to 608 Ã 608 in the training and testing stage, giving an output resolution of 152 Ã 152. We implement our method with PyTorch. The backbone weights are pre-trained on the ImageNet dataset. The other weights are initialized under the default settings of PyTorch. We adopt the standard data augmentations to the images in the training process, which involve random flipping and random cropping within scale range [0.9, 1.1]. We use Adam <ref type="bibr" target="#b7">[8]</ref> with an initial learning rate of 1.25 Ã 10 â4 to optimize the total loss L = L h + L o + L b + L Î± . The network is trained with a batch size of 20 on four NVIDIA GTX 1080 Ti GPUs. We train the network for about 40 epochs on the DOTA dataset and 100 epochs on the HRSC2016 dataset. We additionally report an experiment with a larger batch size 48 on 4 NVIDIA Quadro RTX 6000 GPUs, we mark the results with symbol * in <ref type="table" target="#tab_0">Table 1</ref>. The speed of the proposed network is measured on a single NVIDIA TITAN X GPU on the HRSC2016 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Testing Details</head><p>To extract the center points, we apply an NMS on the output heatmaps through a 3Ã3 max-pooling layer. We pick the top 500 center points from the heatmaps and take out the offsets (o), box parameters (b), and orientation class (Î±) at each center point (c). The heatmap value is used as the detection confidence score. We first adjust the center points by adding the offsetsc = c + o. Next, we rescale the obtained integer center points on the heatmaps byc = sc, s = 4. We obtain the RBB when Î± &gt; 0.5, and we get the HBB otherwise. We use the top-left (tl) </p><p>We gather the RBBs and HBBs as the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the State-of-the-arts</head><p>We compare the performance of the proposed method with the state-of-the-art algorithms on the DOTA and HRSC2016 datasets. To study the impact of orientation classification, we define two versions of the the proposed method: BBAVectors+r and BBAVectors+rh. BBAVectors+r only learns the box boundary-aware vectors to detect OBB, which contains box parameters b = [t, r, b, l]. BBAVectors+rh additionally learns the orientation class Î± and the external size parameters (w e and w h ).</p><p>DOTA. The detection results on the DOTA dataset are illustrated in <ref type="table" target="#tab_0">Table 1</ref>. YOLOv2 <ref type="bibr" target="#b17">[18]</ref> and FR-O <ref type="bibr" target="#b21">[22]</ref> are trained on HBB <ref type="bibr" target="#b21">[22]</ref> and their performances are comparably lower than the other methods. Notably, although the one-stage detector YOLOv2 runs faster, its accuracy is lower than the two-stage anchor-based detectors. R-DFPN <ref type="bibr" target="#b23">[24]</ref> learns the angle parameter from faster R-CNN <ref type="bibr" target="#b18">[19]</ref> and improves performance from 54.13% to 57.94%. R 2 CNN <ref type="bibr" target="#b6">[7]</ref> pools multiple sizes of region proposals at the output of RPN and improves the accuracy from 57.94% to 60.67%. Yang et al. <ref type="bibr" target="#b24">[25]</ref> use the adaptive ROI align to extract objects and achieve 1.62% improvement over 60.67%. ICN <ref type="bibr" target="#b0">[1]</ref> adopts the Image Cascaded Network to enrich features before R-DFPN and boost the performance from 62.29% to 68.16%. ROI Transformer <ref type="bibr" target="#b1">[2]</ref> transfers the horizontal ROIs to oriented ROIs by learning the spatial transformation, raising the accuracy from 68.16% to 69.56%. Different from these methods, the proposed method offers a new concept of oriented object detection, a keypoint-based detection method with box boundaryaware vectors. As shown in <ref type="table" target="#tab_0">Table 1</ref>  cation and additional external OBB size parameters (w e and h e ), the proposed BBAVectors+rh achieves 72.32% mAP, which exceeds ROI Transformer+FPN by 2.76%. Besides, BBAVectors+rh runs faster than ROI Transformer (see <ref type="table">Table 2</ref>). With a larger training batch size, the BBAVectors+rh * achieves about 3 points higher than BBAVectors+rh. The visualization of the detection results of BBAVectors+rh on DOTA dataset is illustrated in <ref type="figure" target="#fig_5">Fig. 3</ref>.</p><p>The background in the aerial images is complicated and the objects are arbitrarily oriented with different sizes and scales. However, the proposed method is robust to capture the objects even for the tiny and crowd small vehicles.</p><p>HRSC2016. The performance comparison results between the proposed method and the state-of-the-arts on HRSC2016 dataset is illustrated in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>We compare the performances of the proposed BBAVectors+r and BBAVectors+rh to study the impact of orientation classification. As we mentioned before, BBAVectors+r refers to box parameters b = [t, r, b, l]. BBAVectors+rh corresponds to b = [t, r, b, l, w e , h e , Î±]. BBAVectors+r has 274.01MB parameters with 25.82Ã10 9 FLOPs, while BBAVectors+rh has 276.28MB parameters and 27.68 Ã 10 9 FLOPs.</p><p>As can be seen in <ref type="figure" target="#fig_7">Fig. 4</ref>, the BBAVectors+r can hardly capture the bounding boxes that nearly align with the xyaxes. These are the corner cases as discussed above. The reason for the failure detection would be that it is difficult for the network to differentiate the type of vectors near the quadrant boundary (i.e., classification boundary). To address this problem, we separate the OBB into RBB and  <ref type="table">Table 3</ref>. Comparison between baseline method Center+wh+Î¸ and the proposed method BAVectors+r.</p><p>HBB by learning an orientation class Î± and we use the external parameters (w e and w h ) to describe HBB. As illustrated in <ref type="figure" target="#fig_7">Fig. 4</ref>, the BBAVectors+rh excels in capturing the oriented bounding box at the corner cases. On the DOTA dataset (see <ref type="table" target="#tab_0">Table 1</ref>), the BBAVectors+rh improves 0.71% over BBAVectors+r. On the HRSC2016 dataset (see <ref type="table">Table 2</ref>), BBAVectors+rh achieves 0.4% improvement over BBAVectors+r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison with Baseline</head><p>To explore the advantage of the box boundary-aware vectors, we also compare our method with the baseline Center+wh+Î¸ (see <ref type="figure" target="#fig_0">Fig. 1a</ref>), which employs the width (w), height (h) and angle (Î¸ â (â90 â¢ , â0 â¢ ]) as the descriptions of OBB. Note that the baseline method shares the same architecture as the proposed method except for the output box parameter and orientation map. The training procedure is the same as the proposed method. Here we do not explicitly handle the corner cases for a fair comparison. From Table 3, we can see that the proposed method performs 4.82% and 2.74% better than Center+wh+Î¸ on HRSC2016 and DOTA datasets, respectively. The results suggest that the box boundary-aware vectors are better for oriented object detection than learning the w, h, Î¸ of OBB directly. The reason would be that the box boundary-aware vectors are learned in the same Cartesian coordinate systems, while in the baseline method, the size parameters (w and h) of an OBB are measured in different rotating coordinate systems that have an angle Î¸ with respect to the y-axis. Jointly learning those parameters would be difficult for the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a new oriented object detection method based on box boundary-aware vectors and center points detection. The proposed method is single-stage and is free of anchor boxes. The proposed box boundary-aware vectors perform better in capturing the oriented bounding boxes than the baseline method that directly learns the width, height, and angle of the oriented bounding box. The results on the HRSC2016 and DOTA datasets demonstrate the superiority of the proposed method over the state-of-thearts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Oriented bounding box (OBB) descriptions for (a) baseline method, termed Center+wh+Î¸, where w, h, Î¸ are the width, height and angle of an OBB. Note that w and h of the OBBs are measured in different rotating coordinate systems for each object; (b) the proposed method, where t, r, b, l are the top, right, bottom and left box boundary-aware vectors. The box boundary-aware vectors are defined in four quadrants of the Cartesian coordinate system for all the arbitrarily oriented objects; (c) illustrates the corner cases where the vectors are very close to the xy-axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Ã</head><label></label><figDesc>Box Param : [ , , , , ! , â ! ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture and the oriented bounding box (OBB) descriptions of the proposed method. The input image is resized to 608 Ã 608 before being fed to the network. The architecture is built on a U-shaped network. Skip connections are adopted to combine feature maps in the up-sampling process. The output of the architecture involves four maps: the heatmap P , offset map O, box parameter map B, and orientation map Î±. The locations of the center points are inferred from the heatmap and offset map. At the center points, the box boundary-aware vectors (BBAVectors) are learned. The resolution of the output maps is 152 Ã 152. HBBs refer to the horizontal bounding boxes. RBBs indicate all oriented bounding boxes except the HBBs. The symbols t, r, b, l refer to the top, right, bottom and left vectors of BBAVectors, we and he are the external width and height of an OBB. The decoded OBBs are shown in red bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The fully-annotated images contain 188,282 instances. The DOTA-V1.0 has 15 categories: Plane, Baseball Diamond (BD), Bridge, Ground Track Field (GTF), Small Vehicle (SV), Large Vehicle (LV), Ship, Tennis Court (TC), Basketball Court (BC), Storage Tank (ST), Soccer-Ball Field (SBF), Roundabout (RA), Harbor, Swimming Pool (SP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>8 )</head><label>8</label><figDesc>, top-right (tr), bottom-right (br) and bottom-left (bl) points of the bounding box as the final decoded points. Specifically, for a center pointc, the decoded RBB points are obtained from: tl = (t + l) +c, tr = (t + r) +c br = (b + r) +c, bl = (b + l) +c. (For a HBB, the points are: tl = (c x â w e /2,c y â h e /2), tr = (c x + w e /2,c y â h e /2) br = (c x + w e /2,c y + h e /2), bl = (c x â w e /2,c y + h e /2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the detection results of BBAVectors+rh on DOTA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of BBAVectors+r and BBAVectors+rh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>51.79 43.98 31.35 22.3 36.68 14.61 22.55 11.89 FR-O [22] 54.13 79.42 77.13 17.7 64.05 35.3 38.02 37.16 89.41 69.64 59.28 50.3 52.91 47.89 47.4 46.3 R-DFPN [24] 57.94 80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 R 2 CNN [7] 60.67 80.94 65.75 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 Yang et al. [25] 62.29 81.25 71.41 36.53 67.44 61.16 50.91 56.60 90.67 68.09 72.39 55.06 55.60 62.44 53.35 51.47 ICN [1] 68.16 81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23 ROI Trans. [2] 67.74 88.53 77.91 37.63 74.08 66.53 62.97 66.57 90.5 79.46 76.75 59.04 56.73 62.54 61.29 55.56 ROI Trans.+FPN [2] 69.56 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 BBAVectors+r 71.61 88.54 76.72 49.67 65.22 75.58 80.28 87.18 90.62 84.94 84.89 47.17 60.59 65.31 63.91 53.52 BBAVectors+rh 72.32 88.35 79.96 50.69 62.18 78.43 78.98 87.94 90.85 83.58 84.35 54.13 60.24 65.22 64.28 55.70 BBAVectors+rh * 75.36 88.63 84.06 52.13 69.56 78.26 80.40 88.06 90.87 87.23 86.39 56.11 65.62 67.10 72.08 63.96 Detection results on the testing set of DOTA-v1.0. The performances are evaluated through the online server. Symbol * shows the result with a larger training batch size (i.e., 48 on 4 Quadro RTX 6000 GPUs). Red and Blue colors label the best and second best detection results in each column.</figDesc><table><row><cell>Method</cell><cell>mAP Plane</cell><cell>BD</cell><cell cols="2">Bridge GTF</cell><cell>SV</cell><cell>LV</cell><cell>Ship</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>Harbor</cell><cell>SP</cell><cell>HC</cell></row><row><cell>YOLOv2 [18]</cell><cell cols="2">25.49 52.75 24.24</cell><cell>10.6</cell><cell cols="3">35.5 14.36 2.41</cell><cell>7.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 2 .</head><label>22</label><figDesc>The R 2 PN<ref type="bibr" target="#b27">[28]</ref> BBAVectors+r ResNet101 608 Ã 608 TITAN X 12.88 88.2 BBAVectors+rh ResNet101 608 Ã 608 TITAN X 11.69 88.<ref type="bibr" target="#b5">6</ref> Detection results on the testing dataset of HRSC2016. The speed of the proposed method is measured on a single NVIDIA TITAN X GPU. learns the rotated region proposals based on VGG16 backbone, achieving 79.6% AP. RRD<ref type="bibr" target="#b9">[10]</ref> adopts activate rotating filters and improves the accuracy from 79.6% to 84.3%. ROI Transformer<ref type="bibr" target="#b1">[2]</ref> without FPN produces 86.2%, while BBAVectors+r achieves 88.2%. BBAVectors+rh performs slightly higher (0.4% over 88.2%) than BBAVectors+r. In the inference stage, the proposed method achieves 12.88 FPS on a single NVIDIA TITAN X GPU, which is 2.18x faster than ROI Transformer.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone Image Size</cell><cell>GPU</cell><cell>FPS</cell><cell>AP</cell></row><row><cell>R 2 PN [28]</cell><cell>VGG16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.6</cell></row><row><cell>RRD [10]</cell><cell>VGG16</cell><cell>384 Ã 384</cell><cell>-</cell><cell>-</cell><cell>84.3</cell></row><row><cell>ROI Trans. [2]</cell><cell cols="3">ResNet101 512 Ã 800 TITAN X</cell><cell>5.9</cell><cell>86.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multiclass object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>KÃ¶rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">R2cnn: Rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the 6th International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via a fullyconvolutional local-global context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multioriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Position detection and direction prediction for arbitrary-oriented ships via multitask rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="50839" to="50849" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-scale cell instance segmentation with keypoint graph based bounding boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Hoeppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Assd: Attentive single shot multibox detector. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page">102827</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
