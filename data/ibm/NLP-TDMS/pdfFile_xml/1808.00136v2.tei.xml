<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Cycle-consistent Generalized Zero-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Felix</surname></persName>
							<email>rafael.felixalves@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar B G</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<email>ian.reid@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal Cycle-consistent Generalized Zero-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>generalized zero-shot learning</term>
					<term>generative adversarial net- works</term>
					<term>cycle consistency loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In generalized zero shot learning (GZSL), the set of classes are split into seen and unseen classes, where training relies on the semantic features of the seen and unseen classes and the visual representations of only the seen classes, while testing uses the visual representations of the seen and unseen classes. Current methods address GZSL by learning a transformation from the visual to the semantic space, exploring the assumption that the distribution of classes in the semantic and visual spaces is relatively similar. Such methods tend to transform unseen testing visual representations into one of the seen classes' semantic features instead of the semantic features of the correct unseen class, resulting in low accuracy GZSL classification. Recently, generative adversarial networks (GAN) have been explored to synthesize visual representations of the unseen classes from their semantic features -the synthesized representations of the seen and unseen classes are then used to train the GZSL classifier. This approach has been shown to boost GZSL classification accuracy, but there is one important missing constraint: there is no guarantee that synthetic visual representations can generate back their semantic feature in a multi-modal cycle-consistent manner. This missing constraint can result in synthetic visual representations that do not represent well their semantic features, which means that the use of this constraint can improve GAN-based approaches. In this paper, we propose the use of such constraint based on a new regularization for the GAN training that forces the generated visual features to reconstruct their original semantic features. Once our model is trained with this multi-modal cycle-consistent semantic compatibility, we can then synthesize more representative visual representations for the seen and, more importantly, for the unseen classes. Our proposed approach shows the best GZSL classification results in the field in several publicly available datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Overview of the proposed multi-modal cycle-consistent GZSL approach. Our approach extends the idea of synthesizing visual representations of seen and unseen classes in order to train a classifier for the GZSL problem <ref type="bibr" target="#b0">[1]</ref>. The main contribution of the paper is the use of a new multi-modal cycle consistency loss in the training of the visual feature generator that minimizes the reconstruction error between the semantic feature a, which was used to synthesize the visual feature x, and the reconstructed semantic feature a mapped from x. This loss is shown to constrain the optimization problem more effectively in order to produce useful synthesized visual features for training the GZSL classifier.</p><p>Generalized Zero-shot Learning (GZSL) separates the classes of interest into a sub-set of seen classes and another sub-set of unseen classes. The training process uses the semantic features of both sub-sets and the visual representations of only the seen classes; while the testing process aims to classify the visual representations of both sub-sets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The semantic features available for both the training and testing classes are typically acquired from other domains, such as visual features <ref type="bibr" target="#b3">[4]</ref>, text <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref>, or learned classifiers <ref type="bibr" target="#b6">[7]</ref>. The traditional approach to address this challenge <ref type="bibr" target="#b1">[2]</ref> involves the learning of a transformation from the visual to the semantic space of the seen classes. Testing is then performed by transforming the visual representation of the seen and unseen classes into this semantic space, where classification is typically achieved with a nearest neighbor classifier that selects the closest class in the semantic space. In contrast to Zero-shot Learning (ZSL), which uses only the unseen domain for testing, GZSL approaches tend to be biased towards the seen classes, producing poor classification results, particularly for the unseen testing classes <ref type="bibr" target="#b0">[1]</ref>.</p><p>These traditional approaches rely on the assumption that the distributions observed in the semantic and visual spaces are relatively similar. Recently, this assumption has been relaxed to allow the semantic space to be optimized together with the transformation from the visual to the semantic space <ref type="bibr" target="#b7">[8]</ref> -this alleviates the classification bias mentioned above to a certain degree. More recent approaches consist of building a generative adversarial network (GAN) that synthesizes visual representations of the seen and unseen classes directly from their semantic representation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>. These synthesized features are then used to train a multi-class classifier of seen and unseen classes. This approach has been shown to improve the GZSL classification accuracy, but an obvious weakness is that the unconstrained nature of the generation process may let the approach generate unrepresentative synthetic visual representations, particularly of the unseen classes (i.e., representations that are far from possible visual representations of the test classes).</p><p>The main contribution of this paper is a new regularization of the generation of synthetic visual representations in the training of GANbased methods that address the GZSL classification problem. This regularization is based on a multi-modal cycle consistency loss term that enforces good reconstruction from the synthetic visual representations back to their original semantic features (see <ref type="figure">Fig. 1</ref>). This regularization is motivated by the cycle consistency loss applied in training GANs <ref type="bibr" target="#b9">[10]</ref> that forces the generative training approach to produce more constrained visual representations. We argue that this constraint preserves the semantic compatibility between visual features and semantic features. Once our model is trained with this multi-modal cycle consistency loss term, we can then synthesize visual representations for unseen classes in order to train a GZSL classifier <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Using the experimental setup described by Xian et al. <ref type="bibr" target="#b0">[1]</ref>, we show that our proposed regularization provides significant improvements not only in terms of GZSL classification accuracy, but also ZSL on the following datasets: Caltech-UCSD-Birds 200-2011 (CUB) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2]</ref>, Oxford-Flowers (FLO) <ref type="bibr" target="#b12">[13]</ref>, Scene Categorization Benchmark (SUN) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>, Animals with features (AWA) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref>, and Ima-geNet <ref type="bibr" target="#b14">[15]</ref> . In fact, the experiments show that our proposed approach holds the current best ZSL and GZSL classification results in the field for these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head><p>The starting point for our literature review is the work by Xian et al. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, who proposed new benchmarks using commonly accepted evaluation protocols on publicly available datasets. These benchmarks allow a fair comparison among recently proposed ZSL and GZSL approaches, and for this reason we explore those benchmarks to compare our results with the ones obtained from the current state of the art in the field. We provide a general summary of the methods presented in <ref type="bibr" target="#b1">[2]</ref>, and encourage the reader to study that paper in order to obtain more details on previous works. The majority of the ZSL and GZSL methods tend to compensate the lack of visual representation of the unseen classes with the learning of a mapping between visual and semantic spaces <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. For instance, a fairly successful approach is based on a bi-linear compatibility function that associates visual representation and semantic features. Examples of such approaches are ALE <ref type="bibr" target="#b17">[18]</ref>, DEVISE <ref type="bibr" target="#b18">[19]</ref>, SJE <ref type="bibr" target="#b19">[20]</ref>, ESZSL <ref type="bibr" target="#b20">[21]</ref>, and SAE <ref type="bibr" target="#b21">[22]</ref>. Despite their simplicity, these methods tend to produce the current state-of-the-art results on benchmark datasets <ref type="bibr" target="#b1">[2]</ref>. A straightforward extension of the methods above is the exploration of a non-linear compatibility function between visual and semantic spaces. These approaches, exemplified by LATEM <ref type="bibr" target="#b22">[23]</ref> and CMT <ref type="bibr" target="#b5">[6]</ref>, tend not to be as competitive as their bi-linear counterpart, probably because the more complex models need larger training sets to generalize more effectively. Seminal ZSL and GZSL methods were based on models relying on learning intermediate feature classifiers, which are combined to predict image classes (e.g., DAP and IAP) <ref type="bibr" target="#b3">[4]</ref> -these models tend to present relatively poor classification results. Finally, hybrid models, such as SSE <ref type="bibr" target="#b2">[3]</ref>, CONSE <ref type="bibr" target="#b23">[24]</ref>, SYNC <ref type="bibr" target="#b24">[25]</ref>, rely on a mixture model of seen classes to represent images and semantic embeddings. These methods tend to be competitive for classifying the seen classes, but not for the unseen classes.</p><p>The main disadvantage of the methods above is that the lack of visual training data for the unseen classes biases the mapping between visual and semantic spaces towards the semantic features of seen classes, particularly for unseen test images. This is an issue for GZSL because it has a negative effect in the classification accuracy of the unseen classes. Recent research address this issue using GAN models that are trained to synthesize visual representations for the seen and unseen classes, which can then be used to train a classifier for both the seen and unseen classes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. However, the unconstrained generation of synthetic visual representations for the unseen classes allows the production of synthetic samples that may be too far from the actual distribution of visual representations, particularly for the unseen classes. In GAN literature, this problem is known as unpaired training <ref type="bibr" target="#b9">[10]</ref>, where not all source samples (e.g., semantic features) have corresponding target samples (e.g., visual features) for training. This creates a highly unconstrained optimization problem that has been solved by Zhu et al. <ref type="bibr" target="#b9">[10]</ref> with a cycle consistency loss to push the representation from the target domain back to the source domain, which helped constraining the optimization problem. In this paper, we explore this idea for GZSL, which is a novelty compared to previous GAN-based methods proposed in GZSL and ZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-modal Cycle-consistent Generalized Zero Shot Learning</head><p>In GZSL and ZSL <ref type="bibr" target="#b1">[2]</ref>, the dataset is denoted by D = {(x, a, y) i } |D| i=1 with x ∈ X ⊆ R K representing visual representation (e.g., image features from deep residual nets <ref type="bibr" target="#b25">[26]</ref>), a ∈ A ⊆ R L denoting L-dimensional semantic feature (e.g., set of binary attributes <ref type="bibr" target="#b3">[4]</ref> or a dense word2vec representation <ref type="bibr" target="#b26">[27]</ref>), y ∈ Y = {1, ..., C} denoting the image class, and |.| representing set cardinality. The set Y is split into seen and unseen subsets, where the seen subset is denoted by Y S and the unseen subset by</p><formula xml:id="formula_0">Y U , with Y = Y S ∪ Y U and Y S ∩ Y U = ∅.</formula><p>The dataset D is also divided into mutually exclusive training and testing subsets: D T r and D T e , respectively. Furthermore, the training and testing sets can also be divided in terms of the seen and unseen classes, so this means that D T r S denotes the training samples of the seen classes, while D T r U represents the training samples of the unseen classes (similarly for D T e S and D T e U for the testing set). During training, samples in D T r S contain the visual representation x i , semantic feature a i and class label y i ; while the samples in D T r U comprise only the semantic feature and class label. During ZSL testing, only the samples from D T e U are used; while in GZSL testing, all samples from D T e are used. Note that for ZSL and GZSL problems, only the visual representation of the testing samples is used to predict the class label.</p><p>Below, we first explain the f-CLSWGAN model <ref type="bibr" target="#b0">[1]</ref>, which is the baseline for the implementation of the main contribution of this paper: the multi-modal cycle consistency loss used in the training for the feature generator in GZSL models based on GANs. The loss, feature generator, learning and testing procedures are explained subsequently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">f-CLSWGAN</head><p>Our approach is an extension of the feature generation method proposed by Xian et al. <ref type="bibr" target="#b0">[1]</ref>, which consists of a classification regularized generative adversarial network (f-CLSWGAN). This network is composed of a generative model G : A × Z → X (parameterized by θ G ) that produces a visual representation x given its semantic feature a and a noise vector z ∼ N (0, I) sampled from a multidimensional centered Gaussian, and a discriminative model D : X × A → [0, 1] (parameterized by θ D ) that tries to distinguish whether the input x and its semantic representation a represent a true or generated visual representation and respective semantic feature. Note that while the method developed by Yan et al. <ref type="bibr" target="#b27">[28]</ref> concerns the generation of realistic images, our proposed approach, similarly to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, aims to generate visual representations, such as the features from a deep residual network <ref type="bibr" target="#b25">[26]</ref> -the strategy based on visual representation has shown to produce more accurate GZSL classification results compared to the use of realistic images. The training algorithm for estimating θ G and θ D follows a minimax game, where G(.) generates synthetic visual representations that are supposed to fool the discriminator, which in turn tries to distinguish the real from the synthetic visual representations. We rely on one of the most stable training methods for GANs, called Wasserstein GAN, which uses the following loss function <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_1">θ * G , θ * D = arg min θ G max θ D W GAN (θ G , θ D ),<label>(1)</label></formula><p>with</p><formula xml:id="formula_2">W GAN (θ G , θ D ) = E (x,a)∼P x,a [D(x, a; θ D )] − E ( x,a)∼P x,a G [D( x, a; θ D )] − λE (x,a)∼P x,a α [(||∇xD(x, a; θ D )|| 2 − 1) 2 ],<label>(2)</label></formula><p>where E[.] represents the expected value operator, P x,a S is the joint distribution of visual and semantic features from the seen classes (in practice, samples from that distribution are the ones in D T r S ), P x,a G represents the joint distribution of semantic features and the visual features produced by the generative model G(.), λ denotes the penalty coefficient, and P x,a α is the joint distribution of the semantic features and the visual features produced byx ∼ αx</p><formula xml:id="formula_3">+ (1 − α) x with α ∼ U(0, 1) (i.e., uniform distribution).</formula><p>Finally, the f-CLSWGAN is trained with the following objective function:</p><formula xml:id="formula_4">θ * G , θ * C , θ * D = arg min θ G ,θ C max θ D W GAN (θ G , θ D ) + β CLS (θ C , θ G ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">CLS (θ C , θ G ) = −E ( x,y)∼P x,y G [log P (y| x, θ C )], with P (y| x, θ C ) = exp((θ C (y)) T x) c∈Y exp((θ C (c)) T x)<label>(4)</label></formula><p>representing the probability that the sample x has been predicted with its true label y, and β is a hyper-parameter that weights the contribution of the loss function. This regularization with the classification loss was found by Xian et al. <ref type="bibr" target="#b0">[1]</ref> to enforce G(.) to generate discriminative visual representations. The model obtained from the optimization in <ref type="formula" target="#formula_4">(3)</ref> is referred to as baseline in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-modal Cycle Consistency Loss</head><p>The main issue present in previously proposed GZSL approaches based on generative models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> is that the unconstrained nature of the generation process (from semantic to visual features) may produce image representations that are too far from the real distribution present in the training set, resulting in an ineffective multi-class classifier training, particularly for the unseen classes. The approach we propose to alleviate this problem consists of constraining the synthetic visual representations to generate back their original semantic features -this regularization has been inspired by the cycle consistency loss <ref type="bibr" target="#b9">[10]</ref>. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of our proposal. This approach, representing the main contribution of this paper, is represented by the following loss:</p><formula xml:id="formula_6">CY C (θ R , θ G ) = E a∼P a S ,z∼N (0,I) a − R(G(a, z; θ G ); θ R ) 2 2 + E a∼P a U ,z∼N (0,I) a − R(G(a, z; θ G ); θ R ) 2 2 ,<label>(5)</label></formula><p>where P a S and P a U denote the distributions of semantic features of the seen and unseen classes, respectively, and R : X → A represents a regressor that estimates the original semantic features from the visual representation generated by G(.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Generation</head><p>Using the losses proposed in Sections 3.1 and 3.2, we can propose several feature generators. First, we pre-train the regressor R(.) defined below in <ref type="formula" target="#formula_7">(6)</ref>, by minimizing a loss function computed only from the seen classes, as follows:</p><formula xml:id="formula_7">REG (θ R ) = E (a,x)∼P a,x S a − R(x; θ R ) 2 2 ,<label>(6)</label></formula><p>where P a,x S represents the real joint distribution of image and semantic features present in the seen classes. In practice, this regressor is defined by a multilayer perceptron, whose output activation function depends on the format of the semantic vector.</p><p>Our first strategy to build a feature generator consists of pre-training a regressor (using samples from seen classes) optimized by minimizing REG in <ref type="formula" target="#formula_7">(6)</ref>, which produces θ * R and training the generator and discriminator of the WGAN using the following optimization function:</p><formula xml:id="formula_8">θ * G , θ * D = arg min θ G max θ D W GAN (θ G , θ D ) + λ 1 CY C (θ * R , θ G ),<label>(7)</label></formula><p>where W GAN is defined in <ref type="formula" target="#formula_2">(2)</ref>, CY C is defined in <ref type="bibr" target="#b4">(5)</ref>, and λ 1 weights the importance of the second optimization term. The optimization in <ref type="bibr" target="#b6">(7)</ref> can use both the seen and unseen classes, or it can rely only the seen classes, in which case the loss CY C in (5) has to be modified so that its second term (that depends on unseen classes) is left out of the optimization. The feature generator model in <ref type="formula" target="#formula_8">(7)</ref> trained with seen and unseen classes is referred to as cycle-(U)WGAN, while the feature generator trained with only seen classes is labeled cycle-WGAN.</p><p>The second strategy explored in this paper to build a feature generator involves pre-training the regressor in (6) using samples from seen classes to produce θ * R , and pre-training a softmax classifier for the seen classes using CLS , defined in (3), which results in θ * C . Then we train the combined loss function:</p><formula xml:id="formula_9">θ * G , θ * D = arg min θ G max θ D W GAN (θ G , θ D ) + λ 1 CY C (θ * R , θ G ) + λ 2 CLS (θ * C , θ G ). (8)</formula><p>The feature generator model in <ref type="bibr" target="#b7">(8)</ref> trained with seen classes is referred to as cycle-CLSWGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning and Testing</head><p>As shown in <ref type="bibr" target="#b0">[1]</ref> the training of a classifier using a potentially unlimited number of samples from the seen and unseen classes generated with x ∼ G(a, z; θ * G ) produces more accurate classification results compared with multi-modal embedding models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Therefore, we train a final softmax classifier P (y|x, θ C ), defined in (4), using the generated visual features by minimizing the negative log likelihood loss CLS (θ C , θ * G ), as defined in <ref type="formula" target="#formula_4">(3)</ref>, where θ * G has been learned from one of the feature learning strategies discussed in Sec. 3.3 -the training of the classifier produces θ * C . The samples used for training the classifier are generated based on the task to be solved. For instance, for ZSL, we only use generated visual representations from the set of unseen classes; while for GZSL, we use the generated samples from seen and unseen classes.</p><p>Finally, the testing is based on the prediction of a class for an input test visual representation x, as follows:</p><formula xml:id="formula_10">y * = arg max y∈ Y P (y|x, θ * C ),<label>(9)</label></formula><p>where Y = Y for GZSL or Y = Y U for ZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the datasets and evaluation criteria used in the experiments, then we discuss the experimental set-up and finally show the results of our approach, comparing with the state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the proposed method on the following ZSL/GZSL benchmark datasets, using the experimental setup of <ref type="bibr" target="#b1">[2]</ref>, namely: CUB-200-2011 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref>, FLO <ref type="bibr" target="#b12">[13]</ref>, SUN <ref type="bibr" target="#b1">[2]</ref>, and AWA <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b1">2]</ref> -where CUB, FLO and SUN are fine-grained datasets, and AWA coarse. <ref type="table">Table 4</ref>.1 shows some basic information about these datasets in terms of number of seen and unseen classes and number of training and testing images. For CUB-200-2011 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref> and Oxford-Flowers <ref type="bibr" target="#b12">[13]</ref>, the semantic feature has 1024 dimensions produced by the character-based CNN-RNN <ref type="bibr" target="#b30">[31]</ref> that encodes the textual description of an image containing fine-grained visual descriptions (10 sentences per image). The sentences from the unseen classes are not used for training the CNN-RNN and the per-class sentence is obtained by averaging the CNN-RNN semantic features that belong to the same class. For the FLO dataset <ref type="bibr" target="#b12">[13]</ref>, we used the same type of semantic feature with 1024 dimensions <ref type="bibr" target="#b30">[31]</ref> as was used for CUB (please see description above). For the SUN dataset <ref type="bibr" target="#b1">[2]</ref>, the semantic features have 102 dimensions. Following the protocol from Xian et al. <ref type="bibr" target="#b1">[2]</ref>, visual features are represented by the activations of the 2048dim top-layer pooling units of ResNet-101 <ref type="bibr" target="#b25">[26]</ref>, obtained from the entire image. For AWA <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b1">2]</ref>, we use a semantic feature containing 85 dimensions denoting <ref type="table">Table 1</ref>. Information about the datasets CUB <ref type="bibr" target="#b11">[12]</ref>, FLO <ref type="bibr" target="#b12">[13]</ref>, SUN <ref type="bibr" target="#b32">[33]</ref>, AWA <ref type="bibr" target="#b1">[2]</ref>, and ImageNet <ref type="bibr" target="#b14">[15]</ref>. Column (1) shows the number of seen classes, denoted by |YS|, split into the number of training and validation classes (train+val), <ref type="formula" target="#formula_2">(2)</ref>  per-class attributes. In addition, we also test our approach on ImageNet <ref type="bibr" target="#b14">[15]</ref>, for a split containing 100 classes for testing <ref type="bibr" target="#b31">[32]</ref>. The input images do not suffer any pre-processing (cropping, background subtraction, etc.) and we do not use any type of data augmentation. This ResNet-101 is pre-trained on ImageNet with 1K classes <ref type="bibr" target="#b14">[15]</ref> and is not fine tuned. For the synthetic visual representations, we generate 2048-dim CNN features using one of the feature generation models, presented in Sec. 3.3.</p><p>For CUB, FLO, SUN, and AWA we use the zero-shot splits proposed by Xian et al. <ref type="bibr" target="#b1">[2]</ref>, making sure that none of the training classes are present on ImageNet <ref type="bibr" target="#b14">[15]</ref>. Differently from these datasets (i.e., CUB, FLO, SUN, AWA), we observed that there is a lack of standardized experimental setup for GZSL on Imagenet. Recently, papers have used ImageNet for GZSL using several splits (e.g., 2-hop, 3-hop), but we noticed that some of the supposedly unseen classes can actually be seen during training (e.g., in split 2-hop, we note that the class American mink is assumed to be unseen, while class Mink is seen, but these two classes are arguably the same). Nevertheless, in order to demonstrate the competitiveness of our proposed cycle-WGAN, we compare it to the baseline using carefully selected 100 unseen classes <ref type="bibr" target="#b31">[32]</ref> (i.e., no overlap with 1k training seen classes) from ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Protocol</head><p>We follow the evaluation protocol proposed by Xian et al. <ref type="bibr" target="#b1">[2]</ref>, where results are based on average per-class top-1 accuracy. For the ZSL evaluation, top-1 accuracy results are computed with respect to the set of unseen classes Y U , where the average accuracy is independently computed for each class, which is then averaged over all unseen classes. For the GZSL evaluation, we compute the average per-class top-1 accuracy on seen classes Y S , denoted by s, the average per-class top-1 accuracy on unseen classes Y U , denoted by u, and their harmonic mean, i.e. H = 2 × (s × u)/(s + u). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>In this section, we explain the implementation details of the generator G(.), the discriminator D(.), the regressor R(.), and the weights used for the hyperparameters in the loss functions in (2),(3), <ref type="formula" target="#formula_8">(7)</ref> and <ref type="formula">(8)</ref> -all these terms have been formally defined in Sec. 3 and depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. The generator consists of a multi-layer perceptron (MLP) with a single hidden layer containing 4096 nodes, where this hidden layer is activated by LeakyReLU <ref type="bibr" target="#b33">[34]</ref>, and the output layer, with 2048 nodes, has a ReLU activation <ref type="bibr" target="#b34">[35]</ref>. The weights of G(.) are initialized with a truncated normal initialization with mean 0 and standard deviation 0.01 and the biases are initialized with 0. The discriminator D(.) is also an MLP consisting of a single hidden layer with 4096 nodes, which is activated by LeakyReLU, and the output layer has no activation. The initialization of D(.) is the same as for G(.). The regressor R(.) is a linear transform from the visual space X to the semantic space A. Following <ref type="bibr" target="#b0">[1]</ref>, we set λ = 10 in (2), β = 0.01 in (3) and λ 1 = λ 2 = 0.01 in <ref type="formula" target="#formula_8">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. We ran an empirical evaluation with the training set and noticed that when λ 1 and λ 2 share the same value, the training becomes stable, but a more systematic evaluation to assess the relative importance of these two hyper-parameters is still needed. <ref type="table" target="#tab_1">Table 2</ref> shows the learning rates for each model (denoted by lr {R(.),G(.),D(.)} ), batch sizes (batch) and number of epochs (#ep) used for each dataset and model -the values for G(.) and D(.) have been estimated to reproduce the published results of our implementation of f-CLSWGAN (explained below), and the values for R(.) have been estimated by cross validation using the training and validation sets. Regarding the number of visual representations generated to train the classifier, we performed a few experiments and reached similar conclusions, compared to <ref type="bibr" target="#b0">[1]</ref>. For all experiments in the paper, we generated 300 visual representations per class <ref type="bibr" target="#b0">[1]</ref>. We reached this number after a study that shows that for a small number of representations (below 100), the classification results were not competitive; for values superior to 200 or more, results became competitive, but unstable; and above 300, results were competitive and stable.</p><p>Since our approach is based on the f-CLSWGAN <ref type="bibr" target="#b0">[1]</ref>, we re-implemented this methodology.In the experiments, the results from our implementation of f-CLSWGAN using a softmax classifier is labeled as baseline. The results that we obtained from our baseline are very similar to the reported results in <ref type="bibr" target="#b0">[1]</ref>, as shown in <ref type="table">Table 3</ref>. For ImageNet, note that we use a split <ref type="bibr" target="#b31">[32]</ref> that is different <ref type="table">Table 3</ref>. Comparison between the reported results of f-CLSWGAN <ref type="bibr" target="#b0">[1]</ref> and our implementation of it, labeled baseline, where we show the top-1 accuracy on the unseen test YU (GZSL), the top-1 accuracy for seen test YS (GZSL), the harmonic mean H (GZSL), and the top-1 accuracy for ZSL (T 1Z ).  <ref type="table" target="#tab_4">Table 6</ref> that the results we obtain for the split <ref type="bibr" target="#b31">[32]</ref> are in fact similar to the reported results for f-CLSWGAN <ref type="bibr" target="#b0">[1]</ref> for similar ImageNet splits. We developed our code and perform all experiments using Tensorflow <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section we show the GZSL and ZSL results using our proposed models cycle-WGAN, cycle-(U)WGAN and cycle-CLSWGAN, the baseline model f-CLSWGAN, denoted by baseline, and several other baseline methods previously used in the field for benchmarking <ref type="bibr" target="#b1">[2]</ref>. <ref type="table">Table 4</ref> shows the GZSL results and <ref type="table">Table 5</ref> shows the ZSL results obtained from our proposed methods, and several baseline approaches on CUB, FLO, SUN and AWA datasets. The results in <ref type="table" target="#tab_4">Table 6</ref> shows that the top-1 accuracy on ImageNet for cycle-WGAN and baseline [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Regarding the GZSL results in <ref type="table">Table 4</ref>, we notice that there is a clear trend of all of our proposed feature generation methods (cycle-WGAN, cycle-(U)WGAN), and cycle-CLSWGAN) to perform better than baseline on the unseen test set. In particular, it seems advantageous to use the synthetic samples from unseen classes to train the cycle-(U)WGAN model since it achieves the best top-1 accuracy results in 3 out of the 4 datasets, with improvements from 0.7% to more than 4%. In general, the top-1 accuracy improvement achieved by our approaches in the seen test set is less remarkable, which is expected given that we prioritize to improve the results for the unseen classes. Nevertheless, our approaches achieved improvements from 0.4% to more than 2.5% for the seen classes. Finally, the harmonic mean results also show that our approaches improve over the baseline in a range of between 1% and 2.2%. Notice that this results are remarkable considering the outstanding improvements achieved by</p><p>Code is available at: https://github.com/rfelixmg/frwgan-eccv18 f-CLSWGAN <ref type="bibr" target="#b0">[1]</ref>, represented here by baseline. In fact, our proposed methods produce the current state of the art GZSL results for these four datasets.</p><p>Analyzing the ZSL results in <ref type="table">Table 5</ref>, we again notice that, similarly to the GZSL case, there is a clear advantage in using the synthetic samples from unseen classes to train the cycle-(U)WGAN model. For instance, top-1 accuracy results show that we can improve over the baseline from 0.9% to 3.5%. The results in this table show that our proposed approaches currently hold the best ZSL results for these datasets.</p><p>It is interesting to see that, compared to GZSL, the ZSL results from previous method in the literature are far more competitive, achieving results that are relatively close to ours and the baseline. This performance gap between ZSL and GZSL, shown by previous methods, enforces the argument in favor of using generative models to synthesize images from seen and unseen classes to train GZSL models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>. As argued throughout this paper, the performance produced by generative models can be improved further with methods that help the training of GANs, such as the cycle consistency loss <ref type="bibr" target="#b9">[10]</ref>.</p><p>In fact, the experiments clearly demonstrate the advantage of using our proposed multi-modal cycle consistency loss in training GANs for GZSL and ZSL. In particular, it is interesting to see that the use of synthetic examples of unseen classes generated by cycle-(U)WGAN to train the GZSL classifier provides remarkable improvements over the baseline, represented by f-CLSWGAN <ref type="bibr" target="#b0">[1]</ref>. The only exception is with the SUN dataset, where the best result is achieved by cycle-CLSWGAN. We believe that cycle-(U)WGAN is not the top performer on SUN due to the number of classes and the proportion of seen/unseen classes in this dataset. For CUB, FLO and AWA we notice that there is roughly a (80%, 20%) ratio between seen and unseen classes. In contrast, SUN has a (91%, 9%) ratio between seen and unseen classes. We also notice a sharp increase in the number of classes from 50 to 817 -GAN models tend not to work well with such a large number of classes. Given the wide variety of GZSL datasets available in the field, with different number of classes and seen/unseen proportions, we believe that there is still lots of room for improvement for GZSL models.</p><p>Regarding the large-scale study on ImageNet, the results in <ref type="table" target="#tab_4">Table 6</ref> show that the top-1 accuracy classification results for Baseline and cycle-WGAN are quite low (similarly to the results observed in <ref type="bibr" target="#b0">[1]</ref> for several ImageNet splits), but our proposed approach still shows more accurate ZSL and GZSL classification.</p><p>An important question about out approach is whether the regularisation succeeds in mapping the generated visual representations back to the semantic space. In order to answer this question, we show in <ref type="figure" target="#fig_1">Fig. 3</ref> the evolution of the reconstruction loss REG in (6) as a function of the number of epochs. In general, the reconstruction loss decreases steadily over training, showing that our model succeeds at such mapping. Another relevant question is if our proposed methods take more or less epochs to converge, compared to the Baseline - <ref type="figure" target="#fig_2">Fig. 4</ref> shows the classification accuracy of the generated training samples from the seen classes for the proposed models cycle-WGAN and cycle-CLSWGAN, and also for the baseline (note that cycle-(U)WGAN is a fine-tuned model from the cycle-WGAN, so their loss functions are in fact identical for the seen classes shown in the graph). For three out of four datasets, our proposed cycle-WGAN converges faster. However, when the CLS in included in <ref type="bibr" target="#b6">(7)</ref> to form the loss in (8) (transforming cycle-WGAN into cycle-CLSWGAN), then the convergence  of cycle-CLSWGAN is comparable to that of the baseline. Hence, cycle-WGAN tends to converge faster than the baseline and cycle-CLSWGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we propose a new method to regularize the training of GANs in GZSL models. The main argument explored in the paper is that the use of GANs to generate seen and unseen synthetic examples for training GZSL models has shown clear advantages over previous approaches. However, the unconstrained nature of the generation of samples from unseen classes can produce models that may not work robustly for some unseen classes. Therefore, by constraining the generation of samples from unseen classes, we target to improve the GZSL classification accuracy. Our proposed constraint is motivated by the cycle consistency loss <ref type="bibr" target="#b9">[10]</ref>, where we enforce that the generated visual representations maps back to their original semantic feature -this represents the multi-modal cycle consistency loss. Experiments show that the use of such loss is clearly advantageous, providing improvements over the current state of the art f-CLSWGAN <ref type="bibr" target="#b0">[1]</ref> both in terms of GZSL and ZSL.</p><p>As noticed in Sec. 6, GAN-based GZSL approaches offer indisputable advantage over previously proposed methods. However, the reliance on GANs to generate samples from unseen classes is challenging because GANs are notoriously difficult to train, particularly in unconstrained and large scale problems. Therefore, future work in this field should be focused on targeting these problems. In this paper, we provide a solution that addresses the unconstrained problem, but it is clear that other regularization approaches could also be used. In addition, the use of GANs in large scale problems (regarding the number of classes) should also be more intensively studied, particularly when dealing with real-life datasets and scenarios. Therefore, we will focus our future research activities in solving these two issues in GZSL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the multi-modal cycle-consistent GZSL model. The visual features, represented by x, are extracted from a state-of-art CNN model, and the semantic features, represented by a, are available from the training set. The generator G(.) synthesizes new visual features x using the semantic feature and a randomly sampled noise vector z ∼ N (0, I), and the discriminator D(.) tries to distinguish between real and synthesized visual features. Our main contribution is focused on the integration of a multi-modal cycle consistency loss (at the bottom) that minimizes the error between the original semantic feature a and its reconstruction a, produced by the regressor R(.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Evolution of REG in terms of the number of epochs for CUB, FLO, SUN and AWA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Convergence of the top-1 accuracy in terms of the number of epochs for the generated training samples from the seen classes for CUB, FLO, SUN and AWA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>presents the number of unseen classes |YU |, (3) displays the number of samples available for training |D T r | and (4) shows number of testing samples that belong to the unseen classes |D T e U | and number of testing samples that belong to the seen classes |D T e S |.</figDesc><table><row><cell>Name</cell><cell cols="4">|YS| (train+val) |YU | |D T r | |D T e U | + |D T e S |</cell></row><row><cell>CUB</cell><cell cols="2">150 (100+50) 50</cell><cell>7057</cell><cell>1764+2967</cell></row><row><cell>FLO</cell><cell>82 (62+20)</cell><cell>20</cell><cell>1640</cell><cell>1155+5394</cell></row><row><cell>SUN</cell><cell cols="2">745 (580+65) 72</cell><cell>14340</cell><cell>2580+1440</cell></row><row><cell>AWA</cell><cell>40 (27+13)</cell><cell>10</cell><cell>19832</cell><cell>4958+5685</cell></row><row><cell cols="4">ImageNet 1000 (1000 + 0) 100 1.2 × 10 6</cell><cell>5200+0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Summary of cross-validated hyper-parameters in our experiments. Classifier lr R(.) batch #ep lr G(.) lr D(.) batch #ep lr batch #ep CUB 1e</figDesc><table><row><cell></cell><cell>R(.)</cell><cell cols="3">GAN: G(.) and D(.)</cell></row><row><cell></cell><cell>−4 64</cell><cell>100 1e −4 1e −3</cell><cell>64</cell><cell>926 1e −4 4096 80</cell></row><row><cell>FLO</cell><cell>1e −4 64</cell><cell>100 1e −4 1e −3</cell><cell>64</cell><cell>926 1e −4 2048 100</cell></row><row><cell cols="2">SUN 1e −4 64</cell><cell>100 1e −2 1e −2</cell><cell>64</cell><cell>926 1e −4 4096 298</cell></row><row><cell cols="2">AWA 1e −3 64</cell><cell>50 1e −4 1e −3</cell><cell>64</cell><cell>350 1e −4 2048 37</cell></row><row><cell cols="2">ImageNet 1e −4 2048</cell><cell cols="3">5 1e −4 1e −3 256 300 1e −3 2048 300</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>GZSL results using per-class average top-1 accuracy on the test sets of unseen classes YU , seen classes YS, and the harmonic mean result H -all results shown in percentage. Results from previously proposed methods in the field extracted from<ref type="bibr" target="#b1">[2]</ref>.CLSWGAN 45.7 61.0 52.3 59.2 72.5 65.1 49.4 33.6 40.0 56.9 64.0 60.2 cycle-(U)WGAN 47.9 59.3 53.0 61.6 69.2 65.2 47.2 33.8 39.4 59.6 63.4 59.8 ZSL results using per-class average top-1 accuracy on the test set of unseen classes YU -all results shown in percentage. Results from previously proposed methods in the field extracted from [2].</figDesc><table><row><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell>FLO</cell><cell></cell><cell>SUN</cell><cell></cell><cell>AWA</cell></row><row><cell>Classifier</cell><cell>YU YS</cell><cell>H</cell><cell cols="3">YU YS H</cell><cell>YU YS</cell><cell>H</cell><cell>YU YS</cell><cell>H</cell></row><row><cell>DAP [30]</cell><cell cols="3">4.2 25.1 7.2 −</cell><cell>−</cell><cell>−</cell><cell cols="3">1.7 67.9 3.3 0.0 88.7 0.0</cell></row><row><cell>IAP [30]</cell><cell cols="3">1.0 37.8 1.8 −</cell><cell>−</cell><cell>−</cell><cell cols="3">0.2 72.8 0.4 2.1 78.2 4.1</cell></row><row><cell cols="9">DEVISE [19] 23.8 53.0 32.8 9.9 44.2 16.2 16.9 27.4 20.9 13.4 68.7 22.4</cell></row><row><cell>SJE [20]</cell><cell cols="8">23.5 59.2 33.6 13.9 47.6 21.5 14.7 30.5 19.8 11.3 74.6 19.6</cell></row><row><cell>LATEM [23]</cell><cell cols="8">15.2 57.3 24.0 6.6 47.6 11.5 14.7 28.8 19.5 7.3 71.7 13.3</cell></row><row><cell>ESZSL [21]</cell><cell cols="8">12.6 63.8 21.0 11.4 56.8 19.0 11.0 27.9 15.8 6.6 75.6 12.1</cell></row><row><cell>ALE [18]</cell><cell cols="8">23.7 62.8 34.4 13.3 61.6 21.9 21.8 33.1 26.3 16.8 76.1 27.5</cell></row><row><cell>SAE [22]</cell><cell cols="3">8.8 18.0 11.8 −</cell><cell>−</cell><cell>−</cell><cell cols="3">7.8 54.0 13.6 1.8 77.1 3.5</cell></row><row><cell>baseline [1]</cell><cell cols="8">43.8 60.6 50.8 58.8 70.0 63.9 47.9 32.4 38.7 56.0 62.8 59.2</cell></row><row><cell>cycle-WGAN</cell><cell cols="8">46.0 60.3 52.2 59.1 71.1 64.5 48.3 33.1 39.2 56.4 63.5 59.7</cell></row><row><cell cols="6">cycle-ZSL</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Classifier</cell><cell></cell><cell cols="4">CUB FLO SUN AWA</cell></row><row><cell></cell><cell cols="3">DEVISE [19]</cell><cell cols="3">52.0 45.9 56.5 54.2</cell><cell></cell></row><row><cell></cell><cell cols="2">SJE [20]</cell><cell></cell><cell cols="3">53.9 53.4 53.7 65.6</cell><cell></cell></row><row><cell></cell><cell cols="3">LATEM [23]</cell><cell cols="3">49.3 40.4 55.3 55.1</cell><cell></cell></row><row><cell></cell><cell cols="2">ESZSL [21]</cell><cell></cell><cell cols="3">53.9 51.0 54.5 58.2</cell><cell></cell></row><row><cell></cell><cell cols="2">ALE [18]</cell><cell></cell><cell cols="3">54.9 48.5 58.1 59.9</cell><cell></cell></row><row><cell></cell><cell cols="3">baseline [1]</cell><cell cols="3">57.7 66.8 58.5 64.1</cell><cell></cell></row><row><cell></cell><cell cols="2">cycle-WGAN</cell><cell></cell><cell cols="3">57.8 68.6 59.7 65.6</cell><cell></cell></row><row><cell></cell><cell cols="6">cycle-CLSWGAN 58.4 70.1 60.0 66.3</cell><cell></cell></row><row><cell></cell><cell cols="7">cycle-(U)WGAN 58.6 70.3 59.9 66.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>ZSL and GZSL ImageNet results using per-class average top-1 accuracy on the test sets of unseen classes YU -all results shown in percentage.</figDesc><table><row><cell cols="2">Classifier ZSL GZSL</cell></row><row><cell>baseline [1] 7.5</cell><cell>0.7</cell></row><row><cell>cycle-WGAN 8.7</cell><cell>1.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature generating networks for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot learning -the Good, the Bad and the Ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3077" to="3086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV). ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4166" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zeroshot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Less is more: zero-shot learning from online textual documents with noise suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zero-shot learning through crossmodal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Designing categorylevel attributes for discriminative visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-shot learning using synthesised unseen visual data with diffusion regularisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating visual representations for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop TASK-CV on The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A bayesian data augmentation approach for learning deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<title level="m">Caltech-ucsd birds 200</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
	<note>ICVGIP&apos;08. Sixth Indian Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition using semantics-preserving adversarial embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Preserving semantic relations for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic Autoencoder for Zero-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Elyor Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<editor>Wasserstein gan. arXiv</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep representations of finegrained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiattention network for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
