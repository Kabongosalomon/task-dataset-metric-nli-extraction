<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">T. ISOBE: REVISITING TEMPORAL MODELING FOR VIDEO SUPER-RESOLUTION Revisiting Temporal Modeling for Video Super-resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>11201</postCode>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
							<email>x.jia@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="department">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">T. ISOBE: REVISITING TEMPORAL MODELING FOR VIDEO SUPER-RESOLUTION Revisiting Temporal Modeling for Video Super-resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 Code is available at https://github.com/junpan19/RRN.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video super-resolution plays an important role in surveillance video analysis and ultra-high-definition video display, which has drawn much attention in both the research and industrial communities. Although many deep learning-based VSR methods have been proposed, it is hard to directly compare these methods since the different loss functions and training datasets have a significant impact on the super-resolution results. In this work, we carefully study and compare three temporal modeling methods (2D CNN with early fusion, 3D CNN with slow fusion and Recurrent Neural Network) for video super-resolution. We also propose a novel Recurrent Residual Network (RRN) for efficient video super-resolution, where residual learning is utilized to stabilize the training of RNN and meanwhile to boost the super-resolution performance. Extensive experiments show that the proposed RRN is highly computational efficiency and produces temporal consistent VSR results with finer details than other temporal modeling methods. Besides, the proposed method achieves state-of-the-art results on several widely used benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> and 2) implicit motion compensation based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. As for explicit motion compensation based methods, kappeler et at. proposes to warp all neighboring frames to the reference frame based on the offline estimated optical flow; VESCPN <ref type="bibr" target="#b0">[1]</ref> is the first end-to-end video SR method by jointly training optical flow estimation and spatial-temporal networks. However, these works are not ideal for VSR since inaccurate motion estimation and alignment would result in errors and deteriorated superresolution performance. Besides, the computation of optical flow often introduces heavy computational load, which restricts deploying these methods in real systems. Another branch of VSR explores advanced temporal modeling frameworks to utilize motion information in an implicit manner. Typically, there temporal modeling framework have been widely used: 2D with early fusion CNN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>, 3D CNN with slow fusion <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> and Recurrent Neural Network (RNN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Although extensive experiments have been reported on the aforementioned methods, it is hard to directly compare the effectiveness of these temporal modeling approaches because they adopt different training sets and loss functions to develop their model, which significantly influences the quality of the estimated high-resolution frames. For example, <ref type="bibr" target="#b11">[12]</ref> trained their model on the private dataset and supervised an elaborately designed Huber Loss. <ref type="bibr" target="#b7">[8]</ref> developed their model on Vimeo-90k <ref type="bibr" target="#b25">[26]</ref> dataset and supervised by L1 Loss. Moreover, different network depth also limits the direct comparison among these temporal modeling methods, e,g. <ref type="bibr" target="#b11">[12]</ref> adopted 52 layers in their large model and <ref type="bibr" target="#b24">[25]</ref> exploited more deep network.</p><p>In this paper, we comprehensively investigate the effectiveness of different temporal modeling approaches on the VSR task by using the fixed loss function (L1 Loss) and training data. Specifically, we explore three commonly used temporal modeling methods: 1) 2D CNN with early fusion, 2) 3D CNN with slow fusion and 3) RNN. Inspired by <ref type="bibr" target="#b17">[18]</ref>, we design the 2D CNN with several modified 2D residual blocks. As for 3D CNN, we further modify these 2D residual blocks to 3D residual blocks. We also incorporate such residual connection into the hidden state of the recurrent network and propose Recurrent Residual Network (RRN) for video super-resolution. In the proposed hidden state, the identity branch not only carries rich image details from the previous layers to the next layers but also helps to avoid gradient vanishing in RNN training. For fair comparison of these temporal modeling methods, we evaluate these models on widely used benchmarks with the same network depth. The experimental results show that Recurrent-based methods are highly efficient and effective in dealing with the VSR problem. Besides, the proposed RRN achieves state-ofthe-art performance on three benchmarks.</p><p>To sum up, we make the following contributions:</p><p>• We carefully study three commonly used temporal modeling methods (2D CNN with early fusion, 3D CNN with slow fusion, and RNN) for the VSR problem.</p><p>• We propose a novel hidden state for the recurrent network, which achieves the best performance among all temporal modeling methods. To more surprise, the proposed method outperforms the previous state-of-the-art methods on all three public benchmarks. <ref type="figure">Figure 1</ref>: × 4 VSR results for the scene Calendar in Vid4 <ref type="bibr" target="#b18">[19]</ref> dataset. Our method produces sharper edges and more detailed textures than other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Single image super-resolution. With the development of deep learning, the convolutional neural network method has achieved dramatic advantages against conventional methods in single image super-resolution (SISR). In <ref type="bibr" target="#b2">[3]</ref>, Deng et at. first proposed to use a 3-layer end-to-end convolutional neural network to fill the missing details of the interpolated lowresolution images and showed promising results. Since then, there have been numerous learning-based SISR methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> constantly emerging. VDSR <ref type="bibr" target="#b13">[14]</ref> further improved CNN depth by stacking more convolutional layers with global residual learning. DRCN <ref type="bibr" target="#b14">[15]</ref> first proposed to reduce model parameters with recursive learning in deep CNN. However, DRCN also suffers from performance degradation problems when increasing more convolutional layers (up to 16 convolutional recursions). To further increase CNN depth, DRRN <ref type="bibr" target="#b21">[22]</ref> was proposed with local residual learning and global residual learning strategy. Much deeper CNN, including RDN <ref type="bibr" target="#b29">[30]</ref>, DBPN <ref type="bibr" target="#b6">[7]</ref>, RCAN <ref type="bibr" target="#b28">[29]</ref> were then introduced, which outperformed previous works by a large margin. In this paper, we also enjoy the merits of residual learning and incorporate residual connection in the Recurrent neural network (RNN). The proposed RRN not only carries rich details from the previous layers to later layers in the hidden state, where information can be stably propagated even through a large number of convolutional layers but also carries on historical information through a long range of time steps as the additional complementary information for the later time step. Video super-resolution. Temporal modeling plays a key role in VSR. Previous works performing temporal aggregation fall into three branches: 1) 2D CNN with early fusion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> 2) 3D CNN with slow fusion <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> and 3) RNN based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref> methods. TDAN <ref type="bibr" target="#b23">[24]</ref> and EDVR <ref type="bibr" target="#b24">[25]</ref> aggregated multi-frames features with several 2D convolutional layers on the top of the feature-wise alignment. PFNL <ref type="bibr" target="#b26">[27]</ref> captured long-range dependencies through a kind of non-local operation, and then aggregated the correlations maps with several 2D convolutional layers.Kim et at. used several stacked 3D convolutional layers to extract both spatial and temporal information within a temporal sliding window in a slow fusion manner and implemented this fashion over the entire video sequence. DUF <ref type="bibr" target="#b11">[12]</ref> estimated dynamic filters with stacked 3D convolutional layers for implicit motion compensation and upsampling. With the advanced temporal modeling strategy, CNN based methods show superior performance on several benchmarks. However, the overlap of sliding windows leads to redundant computation, which limits the VSR efficiency. As for recurrent based methods, both historical information across time step and current information between consecutive frames … t Temporal Aggregation</p><formula xml:id="formula_0">… I t−1 I t−2 I t I t+1 I t+2 ො I t Bicubic Upsampling R t I t+3 I t−3 A B C D I t−1 I t−2 I t I t+1 I t+2 I t+3 I t−3 C 2D CNN I t−1 I t−2 I t I t+1 I t+2 I t+3 I t−3 0 0 F t−3 1 F t−2 1 F t−1 1 F t 1 F t+1 1 F t+2 1 F t+3 1 0 0 Fusion h t−1 O t−1 [I t−1 , I t ] Conv2D ReLU Conv2D ReLU Conv2D h t O t Time Step t h t−1 O t−1 [I t−1 , I t ] Conv2D Time Step t Conv2D Conv2D O t C C 0 C Concatenation Elemental-wise Addition Zero Padding ReLU Conv2D ReLU h t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv2D ReLU</head><p>Depth-to-space Layer can be used to enhance details for an LR frame. In <ref type="bibr" target="#b19">[20]</ref>, Sajjadi et at. proposed to conduct motion estimation and warp operation between the previous frame and current frame, and then super-resolve the aligned frame in a recurrent manner. However, inaccurate motion estimation would cause severe artifacts and increase the risk of error accumulation. Recently, Fouli et at. proposed RLSP <ref type="bibr" target="#b3">[4]</ref>, which propagated historical information in feature space and without explicit motion estimation. Related to <ref type="bibr" target="#b3">[4]</ref>, our method also propagates historical information in feature space. However, in <ref type="bibr" target="#b3">[4]</ref>, they adopt seven simply connected convolutional layers as the hidden state, which is difficult to preserve texture details when propagate so many layers in the hidden state. In addition, RLSP fed three consecutive frames into each hidden state. With more input frames, the hidden state would easily suffers from error accumulation, especially when there is large motion between consecutive frames. In this work, we exploit two frames (previous and current) as hidden state input, and incorporate identity mapping in hidden state to preserve the texture details through so many layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In this section, we introduce the overall system pipeline and detailed configurations of the temporal modeling methods. The whole system consists of two parts: a temporal modeling network which takes consecutive frames as input and integrates them with the reference frame, and a loss function to optimize the network utilizing motion information in an implicit manner. We comprehensively study and compare three temporal modeling methods, including 2D CNN with early fusion, 3D CNN with slow fusion and RNN. Schematic illustration of these networks is shown in <ref type="figure" target="#fig_0">Fig. 2 (a)</ref>, (b) and (c), respectively. The detailed architecture about the proposed hidden state is shown in <ref type="figure" target="#fig_0">Fig. 2 (d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Design</head><p>We consider three types of deep neural networks: (1) 2D CNN (2) 3D CNN and <ref type="formula" target="#formula_3">(3)</ref> RNN. For 2D CNN, the input frames are first concatenated along the channel axis, and then aggregated with a stack of 2D convolutional layer. 3D CNN takes a video sequence as input and then exploits a stack of 3D convolutional layer to extract the spatial-temporal information from the video sequence. Comparing to the CNN methods, RNN takes fewer frame as the hidden state input and handles a long video sequence with a recurrent manner. 2D CNN with early fusion. Inspired by <ref type="bibr" target="#b17">[18]</ref>, we design the 2D CNN with several modified 2D residual blocks, where each block consists of a 3 × 3 convolutional layer followed by ReLU <ref type="bibr" target="#b4">[5]</ref>. Such model takes 2T + 1 consecutive frames as input. The aggregation process can be formulated as:</p><formula xml:id="formula_1">R t = W net2D {W f usion2D [I t−T , . . . , I t+T ]}<label>(1)</label></formula><p>Where [·, ·, ·] denotes the concatenation operation. The input tensor shape of W f usion2D is NC × H ×W , where N = 2T + 1, C is the number of color channels, and H, W are the height and weight of the input LR frames, respectively. W f usion2D and W net2D represent a set of weights (the biases are omitted to simplify notations) of the early fusion layer and 2D CNN, respectively. The shape of the produced residual map is R t is H × W × Cr 2 , where r is the upscale factor. The high-resolution residual maps R ↑ t is obtained by adopting depth-to-space operation <ref type="bibr" target="#b20">[21]</ref>. Finally, the high-resolution imageŷ t is obtained by adding the predicted high-resolution residual map R ↑ t to a bicubic up-sampled high-resolution reference image I ↑ t at the end of the network. 3D CNN with slow fusion. In 3D CNN, we modify the 2D convolutional layers in the 2D residual blocks to 3 × 3 × 3 convolutional layers for extracting spatial-temporal information. We use the same network depth for both 2D and 3D CNN for a fair comparison. In contrast with 2D CNN, 3D CNN takes a video sequence as input and extracts the spatialtemporal information in a slow fusion manner. Specifically, a 3-dimensional filter moves both in temporal and spatial axis directions to extract both spatial and temporal information. Typically, the temporal dimension depth of 3D filter is much smaller than the length of the input sequence. Such slow fusion process can be described as:</p><formula xml:id="formula_2">R t = W f usion3D {W net3D {I t−T :t+T }}<label>(2)</label></formula><p>Where W net3D and W f usion3D represent a set of weights (the biases are omitted to simplify notations) of 3D CNN and the later fusion layer, respectively. The input tensor shape of W net3D is C × N × H ×W , where C is the number of color channels, N = 2T + 1, and H, W are the height and weight of the input LR frames, respectively. To prevent the number of frames from decreasing, we add two frames with pixel value of zero in the temporal axis. RNN. Typically, a hidden state at time step t takes three parts as input: (1) the previous output o t−1 , (2) the previous hidden state features h t−1 and (3) two adjacent frames I {t−1,t} . Intuitively, in an video sequence, pixels within successive frames usually bear a strong similarity. The high-frequency texture details in t-th time step should be further refined by borrowing the complementary information from the previous layer. However, RNN in VSR <ref type="bibr" target="#b3">[4]</ref> also suffers gradient vanishing issue as many other video processing tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref>. To address this issue, we propose a novel recurrent network, termed as Residual Recurrent Network (RRN), which adopts residual mapping between layers with identity skip connections. Such design ensures a fluent information flow and has the ability to preserve the texture information over long periods making RNN easier to process a longer sequences, and meanwhile reduce the risk of gradient vanishing in training. At time step t, the RRN uses following equations to generate output h t and o t for the next time step t+1:</p><formula xml:id="formula_3">x 0 = σ (W conv2D {[I t−1 , I t , o t−1 , h t−1 ]})x k = g(x k−1 ) + F(x k−1 ), k ∈ [1, K] h t = σ (W conv2D {x K }) o t = W conv2D {x K }<label>(3)</label></formula><p>Where σ (·) represents the ReLU function. g(x k−1 ) denotes an identity mapping in k-th residual block: g(x k−1 ) =x k−1 , and F(x k−1 ) denotes the residual mapping to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Previous works use different training sets and different down-sampling kernels, which restricts fair comparisons. In this work, we adopt Vimeo-90k <ref type="bibr" target="#b25">[26]</ref> as the training set. Vimeo-90k is a public dataset for video restoration tasks, including video denoising, deblocking as well as super-resolution. Vimeo-90k contains around 90k 7-frame video clips with various motions and diverse scenes. To develop our model, the low-resolution patches in the size of 64 × 64 are obtained by applying Gaussian blur with σ = 1.6 to a high-resolution frame and further downsampling by 4× scale factor. We evaluate the developed models on Vid4 <ref type="bibr" target="#b18">[19]</ref>, SPMCS <ref type="bibr" target="#b22">[23]</ref> and UDM10 <ref type="bibr" target="#b26">[27]</ref> datasets. Vid4 consists of four scenes with various motion and occlusion. SPMCS and UDM10 are the recently proposed validation sets, which contain diverse senses with considerable high-resolution frames than Vid4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We consider two models which have different network depth for all temporal modeling method.</p><p>As for 2D CNN, 2D CNN-S, 2D CNN-L adopt five and ten 2D residual blocks, respectively. As for 3D CNN, 3D CNN-S and 3D CNN-L adopt five and ten 3D residual blocks, respectively.  The channel size for 2D CNN and 3D CNN is set to 128. For a fair comparison with the CNN based methods, we also adopt five and ten residual blocks as the hidden state for RRN-S and RRN-L, respectively. Each block consists of a convolutional layer, a ReLU layer and following another convolutional layer. The channel size of convolutional layer is set to 128. At the time step t 0 , the previous estimation is initialized with zero. To train the CNN based models, the learning rate is initially set to 1 × 10 −4 and 1 × 10 −3 for 2D CNN and 3D CNN, respectively, and multiplied by 0.1 after 10 epochs. The training step completes after 30 epochs. To train the RNN based model, the learning rate is initially set to 1 × 10 −4 and later down-scaled by a factor of 0.1 every 60 epoch till 70 epochs. All models are supervised by pixel-wise L1 loss function with Adam <ref type="bibr" target="#b16">[17]</ref> optimizer by setting β 1 = 0.9, β 2 = 0.999 and weight decay of 5 × 10 −4 . We set the size of mini-batch as 64 and 4 for CNN based and RNN based methods, respectively. The L1 loss is applied on all pixels between the ground truth frames y t and the networkâȂŹs outputŷ t , defined by L = y t −ŷ t . All experiments are conducted using Python 3.6.4 and Pytorch 1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with different temporal modeling methods</head><p>In this part, we compare three temporal modeling approaches, including 2D CNN, 3D CNN and RNN on Vid4 <ref type="bibr" target="#b18">[19]</ref>, SPMCS <ref type="bibr" target="#b22">[23]</ref> and UMD10 <ref type="bibr" target="#b26">[27]</ref> datasets. The quantitative and qualitative results are shown in Tab. 1 and <ref type="figure" target="#fig_1">Fig. 3</ref>, respectively. We also present the trade-off between runtime and accuracy in Tab. 1.</p><p>In CNN-based methods, 3D CNN-S and 3D CNN-L outperform 2D CNN-S and 2D CNN-L by a large margin. However, VSR with 3D CNN is very time-consuming as shown in Tab. 1, where 3D CNN-L is almost ten times slower than 2D CNN-L on processing an LR frame of size 320 × 180. Comparing with CNN based methods, RNN is highly computational efficiency and achieves excellent results with fewer parameters. RRN-L is 0.44, 0.20 and 0.54 dB higher than 3D CNN-L on Vid4, SPMCS and UDM10, respectively, and meanwhile being more than 23× faster. Moreover, RNN-S and RNN-L can produce a 720p video sequences in 33fps and 22fps, respectively. The qualitative results also show that RRN-L can produce finer details and fewer artifacts than 2D CNN-L and 3D CNN-L. In addition, we visualize the temporal profiles in <ref type="figure" target="#fig_3">Fig. 5</ref>. RRN-L produces temporal consistent frames and suffers less flickering artifacts than other temporal modeling methods. To investigate the information flow of different temporal modeling methods for handling a long video sequence, we plot the PSNR over time for Calendar sequence in Vid4. As shown in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>,the RNN-based method falls behind the CNN-based methods at the first five frames. By accumulating information over time, the RNN-based method outperforms CNNbased methods from the fifth frame. More interestingly, the CNN-based methods suffer from performance degradation at the fifth frames, but the RNN-based method keeps improving, which demonstrates that the information accumulation in the previous hidden state provides a complementary information for recovering missing details. Necessity of residual connection in the hidden state of RNN. To investigate the necessity of residual learning in the hidden state, we create a simple baseline by removing by simply stacking the convolutional layer in a hidden state. The advanced model is obtained by incorporating the identity mapping in a hidden state. The PSNR (dB) and SSIM results on Vid4 are shown in Tab. 2. The quantitative results are measured on the luminance (Y) channel. As shown in Tab. 2, the best performance of the baseline model is 27.09 dB in PSNR, which uses three blocks as the hidden state. However, it suffers from gradient vanishing when increasing the number of blocks to the number of four. With the help of a residual connection in the hidden state, stable improvement is achieved when increasing the number of blocks. These results illustrate that identity mapping can not only stabilize training but also boost the VSR performance. Note that the performance of RRN can be further improved by adopting more blocks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with state-of-the-art methods</head><p>We compare our best model (RRN-L) with eight state-of-the-art VSR approaches: SPMC <ref type="bibr" target="#b22">[23]</ref>, TOFlow <ref type="bibr" target="#b25">[26]</ref>, FRVSR <ref type="bibr" target="#b19">[20]</ref>, DUF <ref type="bibr" target="#b11">[12]</ref>, RBPN <ref type="bibr" target="#b7">[8]</ref>, EDVR <ref type="bibr" target="#b24">[25]</ref>, RLSP <ref type="bibr" target="#b3">[4]</ref> and PFNL <ref type="bibr" target="#b26">[27]</ref>. SPMC, TOFlow and FRVSR apply for explicit motion estimation and compensation. EDVR conducts motion alignment in feature level. RBPN also computes optical flow but uses it as additional input instead of explicit motion compensation. DUF and PFNL use an advanced temporal integration network to utilize motion information in an implicit way. RLSP is the most related work, which also propagates historical information in feature space. However, the design of the hidden state of RLSP is simple, which easily causes gradient vanishing problem (see <ref type="table" target="#tab_3">Tab 2</ref>  sharper edges and finer details than other VSR methods. In addition, our method produces temporal-consistent results than the previous methods, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Video super-resolution is an important task, which has drawn much attention in both research and industrial communities. We comprehensively investigate and compare three commonly used temporal modeling methods for video super-resolution, including 2D CNN with early fusion, 3D CNN with slow fusion and RNN. For a fair comparison, all models are developed on the public Vimeo-90k dataset with the fixed down-sampling filters and loss function. Extensive experiments on Vid4, SPMCS and UDM10 benchmarks, demonstrate RNN is highly efficient and benefit in dealing with the VSR problem. In addition, we also propose a novel hidden state structure for recurrent network, termed as RRN. The proposed method achieves state-of-the-art performance on three benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Schematic illustration of three commonly used temporal modeling frameworks (A: 2D CNN with early fusion, B: 3D CNN with slow fusion, C: RNN). D is the proposed RRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a): Qualitative results for different temporal modeling methods on Vid4 [19] and SPMCS [23] dataset for 4×VSR. (b): Information flow over time for 2D CNN, 3D CNN and RNN on the Calendar sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison on the UDM10 [27] and SPMCS [23] datasets for 4× VSR. Zoom in for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of temporal consistency for the Photography sequence in UDM10<ref type="bibr" target="#b26">[27]</ref>. The temporal profile is produced by recording a single-pixel line (green line) spanning time and stacked vertically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of PSNR values on Vid4<ref type="bibr" target="#b18">[19]</ref>, SPMCS<ref type="bibr" target="#b22">[23]</ref> and UDM10<ref type="bibr" target="#b26">[27]</ref> and runtime between different temporal modeling methods for × 4 VSR. Y denotes the evaluation on luminance channel. Runtime is calculated on an LR image of size 320×180. Red text indicates the best and blue text indicates the second best performance. Best view in color.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation on the residual learning in the hidden state of RRN. " * " represents that the model in training suffers from gradient vanishing.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). Most of the previous methods use different training sets and different down-sampling operations. For fair comparison, we fix the down-sampling filters, i.e.σ = 1.6, and carefully re-implement these methods on the public Vimeo-90k dataset. The quantitative results on Vid4, SPMCS and UDM10 are shown in Tab. 3. We can see that methods with explicit motion compensation do not perform very well. By carefully analyzing, the occlusion or complex motion easily influences the per-pixel motion estimation, such as optical flow. Inaccurate motion estimation would introduce artifacts which deteriorate super-resolution performance. As shown in Tab. 3, our method outperforms the CNN-based methods<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> by a large margin, and meanwhile runs been 70× and 6× faster than the recent proposed RBPN and PFNL, respectively. Comparing with other RNN-.8104 27.38/0.8329 27.17/0.8205 27.35/0.8264 27.48/0.8388 27.16/0.8365 27.69/0.8488 Vid4 (RGB) 20.37/0.5106 -/-24.39/0.7438 25.01/0.7917 25.91/0.8166 25.65/0.7997 25.83/0.8077 25.69/0.8153 25.67/0.8189 26.16/0.8209 SPMCS (Y) 23.29/0.6385 -/-27.86/0.8237 28.16/0.8421 29.63/0.8719 29.73/0.8663 -/-29.59/0.8762 29.74/0.8792 29.84/0.8827 SPMCS (RGB)) 21.83/0.6133 -/-26.38/0.8072 26.68/0.8271 28.10/0.8582 28.23/0.8561 -/-27.25/0.8495 27.24/0.8495 28.28/0.8690 UDM10 (Y) 28.47/0.8523 -/-36.26/0.9438 37.09/0.9522 38.48/0.9605 38.66/0.9596 -/-38.48/0.9606 38.74/0.9627 38.96/0.9644 UDM10 (RGB) 27.05/0.8267 -/-34.46/0.9298 35.39/0.9403 36.78/0.9514 36.53/0.9462 -/-36.39/0.9465 36.91/0.9526 37.03/0.9534</figDesc><table><row><cell>Method</cell><cell>Bicubic</cell><cell cols="3">SPMC  † [23] TOFLOW [26] FRVSR [20]</cell><cell>DUF [12]</cell><cell>RBPN [8]</cell><cell>EDVR [25]</cell><cell>RLSP [4]</cell><cell>PFNL [27]</cell><cell>RRN-L (Ours)</cell></row><row><cell># Param. [M]</cell><cell>N/A</cell><cell>-</cell><cell>1.4</cell><cell>5.1</cell><cell>5.8</cell><cell>12.8</cell><cell>20.1</cell><cell>4.3</cell><cell>3.0</cell><cell>3.4</cell></row><row><cell>Runtime [ms]</cell><cell>N/A</cell><cell>-</cell><cell>1658</cell><cell>129</cell><cell>1393</cell><cell>3482</cell><cell>621</cell><cell>50</cell><cell>295</cell><cell>45</cell></row><row><cell>Vid4 (Y)</cell><cell>21.80/0.5426</cell><cell>25.52/0.76</cell><cell>25.85/0.7659</cell><cell>26.48/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>based methods [4, 20], our method outperforms FRVSR by a large margin even with fewer parameters and 2× faster. Our method achieves comparable runtimes with RLSP, but it outperforms RLSP by 0.21, 0.25 and 0.48 dB in PSNR on Vid4, SPMCS and UDM10, re- spectively. Qualitative comparisons are presented in Fig. 4. The proposed method produces</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison (PSNR(dB) and SSIM) on Vid4<ref type="bibr" target="#b18">[19]</ref>, SPMCS<ref type="bibr" target="#b22">[23]</ref> and UDM10<ref type="bibr" target="#b26">[27]</ref> for 4×VSR, respectively. ' †' means the values are taken from original publications or calculated by provided models. Y and RGB indicate the evaluation on luminance channel or RGB channels, respectively. Runtime is calculated on an LR image of size 320×180. Red text indicates the best and blue text indicates the second best performance.Best view in color.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal deformable convolutional encoder-decoder networks for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient video super-resolution through recurrent latent space propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Fuoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno>abs/1909.08080</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Re3: Re al-time recurrent regression networks for visual tracking of generic objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="788" to="795" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent backprojection network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Video super-resolution with recurrent structure-detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video super-resolution with temporal group attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep video superresolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">3dsrnet: Video superresolution using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongyeon</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno>abs/1812.09079</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On bayesian adaptive video super resolution. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tdan: Temporally deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1812.02898</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Eleatt-rnn: Adding attentiveness to neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
