<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Concepts and Order for Image and Sentence Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<email>yhuang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic Concepts and Order for Image and Sentence Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image and sentence matching has made great progress recently, but it remains challenging due to the large visualsemantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of image and sentence matching refers to measuring the visual-semantic similarity between an image and a sentence. It has been widely applied to the application of image-sentence cross-modal retrieval, e.g., given an image query to find similar sentences, namely image annotation, and given a sentence query to retrieve matched images, A quick cheetah is chasing a young gazelle on grass. namely text-based image search. Although much progress in this area has been achieved, it is still nontrivial to accurately measure the similarity between image and sentence, due to the existing huge visualsemantic discrepancy. Taking an image and its matched sentence in <ref type="figure" target="#fig_0">Figure 1</ref> for example, main objects, properties and actions appearing in the image are: {cheetah, gazelle, grass}, {quick, young, green} and {chasing, running}, respectively. These high-level semantic concepts are the essential content to be compared with the matched sentence, but they cannot be easily represented from the pixel-level image. Most existing methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> jointly represent all the concepts by extracting a global CNN <ref type="bibr" target="#b27">[28]</ref> feature vector, in which the concepts are tangled with each other. As a result, some primary foreground concepts tend to be dominant, while other secondary background ones will probably be ignored, which is not optimal for finegrained image and sentence matching. To comprehensively predict all the semantic concepts for the image, a possible way is to adaptively explore the attribute learning frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref>. But such a method has not been well investigated in the context of image and sentence matching.</p><p>In addition to semantic concepts, how to correctly organize them, namely semantic order, plays an even more important role in the visual-semantic discrepancy. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, given the semantic concepts mentioned above, if we incorrectly set their semantic order as: a quick gazelle is chasing a young cheetah on grass, then it would have completely different meanings compared with the image content and matched sentence. But directly learning the correct semantic order from semantic concepts is very difficult, since there exist various incorrect orders that semantically make sense. We could resort to the image global context, since it already indicates the correct semantic order from the appearing spatial relations among semantic concepts, e.g., the cheetah is on the left of the gazelle. But it is unclear how to suitably combine them with the semantic concepts, and make them directly comparable to the semantic order in the sentence.</p><p>Alternatively, we could generate a descriptive sentence from the image as its representation. However, the imagebased sentence generation itself, namely image captioning, is also a very challenging problem. Even those state-ofthe-art image captioning methods cannot always generate very realistic sentences that capture all image details. The image details are essential to the matching task, since the global image-sentence similarity is aggregated from local similarities in image details. Accordingly, these methods cannot achieve very high performance for image and sentence matching <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In this work, to bridge the visual-semantic discrepancy between image and sentence, we propose a semanticenhanced image and sentence matching model, which improves the image representation by learning semantic concepts and then organizing them in a correct semantic order. To learn the semantic concepts, we exploit a multiregional multi-label CNN that can simultaneously predict multiple concepts in terms of objects, properties, actions, etc. The inputs of this CNN are multiple selectively extracted regions from the image, which can comprehensively capture all the concepts regardless of whether they are primary foreground ones. To organize the extracted semantic concepts in a correct semantic order, we first fuse them with the global context of the image in a gated manner. The context includes the spatial relations of all the semantic concepts, which can be used as the reference to facilitate the semantic order learning. Then we use the groundtruth semantic order in the matched sentence as the supervision, by forcing the fused image representation to generate the matched sentence.</p><p>After enhancing the image representation with both semantic concepts and order, we learn the sentence representation with a conventional LSTM <ref type="bibr" target="#b9">[10]</ref>. Then the representations of image and sentence are matched with a structured objective, which is in conjunction with another objective of sentence generation for joint model learning. To demonstrate the effectiveness of the proposed model, we perform several experiments of image annotation and retrieval on two publicly available datasets, and achieve the state-of-theart results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual-semantic Embedding Based Methods</head><p>Frome et al. <ref type="bibr" target="#b6">[7]</ref> propose the first visual-semantic embedding framework, in which ranking loss, CNN <ref type="bibr" target="#b14">[15]</ref> and Skip-Gram <ref type="bibr" target="#b21">[22]</ref> are used as the objective, image and word encoders, respectively. Under the similar framework, Kiros et al. <ref type="bibr" target="#b12">[13]</ref> replace the Skip-Gram with LSTM <ref type="bibr" target="#b9">[10]</ref> for sentence representation learning, Vendrov et al. <ref type="bibr" target="#b28">[29]</ref> use a new objective that can preserve the order structure of visualsemantic hierarchy, and Wang et al. <ref type="bibr" target="#b31">[32]</ref> additionally consider within-view constraints to learn structure-preserving representations.</p><p>Yan and Mikolajczyk <ref type="bibr" target="#b35">[36]</ref> associate the image and sentence using deep canonical correlation analysis as the objective, where the matched image-sentence pairs have high correlation. Based on the similar framework, Klein et al. <ref type="bibr" target="#b13">[14]</ref> use Fisher Vectors (FV) <ref type="bibr" target="#b24">[25]</ref> to learn more discriminative representations for sentences, Lev et al. <ref type="bibr" target="#b15">[16]</ref> alternatively use RNN to aggregate FV and further improve the performance, and Plummer et al. <ref type="bibr" target="#b25">[26]</ref> explore the use of region-to-phrase correspondences. In contrast, our proposed model considers to bridge the visual-semantic discrepancy by learning semantic concepts and order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image Captioning Based Methods</head><p>Chen and Zitnick <ref type="bibr" target="#b1">[2]</ref> use a multimodal auto-encoder for bidirectional mapping, and measure the similarity using the cross-modal likelihood and reconstruction error. Mao et al. <ref type="bibr" target="#b20">[21]</ref> propose a multimodal RNN model to generate sentences from images, in which the perplexity of generating a sentence is used as the similarity. Donahue et al. <ref type="bibr" target="#b2">[3]</ref> design a long-term recurrent convolutional network for image captioning, which can be extended to image and sentence matching as well. Vinyals et al. <ref type="bibr" target="#b29">[30]</ref> develop a neural image captioning generator and show the effectiveness on the image and sentence matching. These models are originally designed to predict grammatically-complete sentences, so their performance on measuring the image-sentence similarity is not very well. Different from them, our work focuses on the similarity measurement, which is especially suitable for the task of image and sentence matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semantic-enhanced Image and Sentence Matching</head><p>In this section, we will detail our proposed semanticenhanced image and sentence matching model from the following aspects: 1) sentence representation learning with a conventional LSTM, 2) semantic concept extraction with a multi-regional multi-label CNN, 3) semantic order learning with a context-gated sentence generation scheme, and 4) model learning with joint image and sentence matching and sentence generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sentence Representation Learning</head><p>For a sentence, its included nouns, verbs and adjectives directly correspond to the visual semantic concepts of object, property and action, respectively, which are already given. The semantic order of these semantic-related words is intrinsically exhibited by the sequential nature of sentence. To learn the sentence representation that can capture those semantic-related words and model their semantic order, we use a conventional LSTM, similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. The LSTM has multiple components for information memorizing and forgetting, which can well suit the complex properties of semantic concepts and order. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), we sequentially feed all the words of the sentence into the LSTM at different timesteps, and then regard the hidden state at the last timestep as the desired sentence representation s ∈ R H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Semantic Concept Extraction</head><p>For images, their semantic concepts refer to various objects, properties, actions, etc. The existing datasets do not provide these information at all but only matched sentences, so we have to predict them with an additional model. To learn such a model, we manually build a training dataset following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref>. In particular, we only keep the nouns, adjectives, verbs and numbers as semantic concepts, and eliminate all the semantic-irrelevant words from the sentences. Considering that the size of the concept vocabulary is very large, we ignore those words that have very low use frequencies. In addition, we unify the different tenses of verbs, and the singular and plural forms of nouns to further reduce the vocabulary size. Finally, we obtain a vocabulary containing K semantic concepts. Based on this vocabulary, we can generate the training dataset by selecting multiple words from sentences as the groundtruth semantic concepts.</p><p>Then, the prediction of semantic concepts is equivalent to a multi-label classification problem. Many effective models on this problem have been proposed recently <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34]</ref>, which mostly learn various CNN-based models as nonlinear mappings from images to the desired multiple labels. Similar to <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, we simply use the VGGNet <ref type="bibr" target="#b27">[28]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b26">[27]</ref> as our multi-label CNN. To suit the multi-label classification, we modify the output layer to have K outputs, each corresponding to the predicted confidence score of a semantic concept. We then use the sigmoid activation instead of softmax on the outputs, so that the task of multi-label classification is transformed to multiple tasks of binary classification. Given an image, its multi-hot representation of groundtruth semantic concepts is y i ∈ {0, 1} K and the predicted score vector by the multi-label CNN is y i ∈ [0, 1] K , then the model can be learned by optimizing the following objective:</p><formula xml:id="formula_0">L cnn = K c=1 log(1 + e (−y i,c y i,c ) )<label>(1)</label></formula><p>During testing, considering that the semantic concepts usually appear in image local regions and vary in size, we perform the concept prediction in a regional way. Given a testing image, we first selectively extract r image regions in a similar way as <ref type="bibr" target="#b32">[33]</ref>, and then resize them to square shapes. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b), by separately feeding these regions into the learned multi-label CNN, we can obtain a set of predicted confidence score vectors. Note that the model parameters are shared among all the regions. We then perform element-wise max-pooling across these score vectors to obtain a single vector, which includes the desired confidence scores for all the semantic concepts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Image Semantic Order Learning</head><p>After obtaining the semantic concepts, how to reasonably organize them in a correct semantic order plays an essential role to the image and sentence matching. Even though based on the same set of semantic concepts, combining them in different orders could lead to completely opposite meanings. For example in <ref type="figure" target="#fig_1">Figure 2</ref> (b), if we organize the extracted semantic concepts: giraffes, eating and basket as: a basket is eating two giraffes, then its meaning is very different from the image content. To learn the semantic order, we propose a context-gated sentence generation scheme that uses the image global context as reference and the sentence generation as supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Global Context as Reference</head><p>It is not easy to learn the semantic order directly from separated semantic concepts, since the semantic order involves not only the hypernym relations between concepts, but also the textual entailment among phrases in high levels of semantic hierarchy <ref type="bibr" target="#b28">[29]</ref>. To deal with this, we propose to use the image global context as auxiliary reference for semantic order learning. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, the global context can not only describe all the semantic concepts in a coarse level, but also indicate their spatial relations with each other, e.g., two giraffe are standing in the left while the basket is in the top left corner. When organizing the separated semantic concepts, our model can refer to the global context to find their relations and then combine them to facilitate the prediction of semantic order. In practice, for efficient implementation, we use a pre-trained VGGNet to process the whole image content, and then extract the vector in the last fully-connected layer as the desired global context, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c).</p><p>To model such a reference procedure, a simple way is to sum the global context with semantic concepts together. But considering that the content of different images can be diverse, thus the relative importance of semantic concepts and context is not equivalent in most cases. For those images with complex content, their global context might be a bit of ambiguous, so the semantic concepts are more discriminative. To handle this, we design a gated fusion unit that can selectively balance the relative importance of semantic concepts and context. The unit acts as a gate that controls how much information of the semantic concepts and context contributes to their fused representation. As illustrated in <ref type="figure" target="#fig_1">Figure 2 (d)</ref>, after obtaining the normalized context vector x ∈ R I and concept score vector p ∈ R K , their fusion by the gated fusion unit can be formulated as:</p><formula xml:id="formula_1">p = W l p 2 , c = W g x 2 , t = σ(U l p + U g x) v = t p + (1 − t) x<label>(2)</label></formula><p>where · 2 denotes the l 2 -normalization, and v ∈ R H is the fused representation of semantic concepts and global context. The use of sigmoid function σ is to rescale each element in the gate vector t ∈ R H to [0, 1], so that v becomes an element-wise weighted sum of p and x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Sentence Generation as Supervision</head><p>To learn the semantic order based on the fused representation, a straightforward approach is to directly generate a sentence from it, similar to image captioning <ref type="bibr" target="#b34">[35]</ref>. However, such an approach is infeasible resulting from the following problem. Although the current image captioning methods can generate semantically meaningful sentences, the accuracy of their generated sentences on capturing image details is not very high. And even a little error in the sentences can be amplified and further affect the measurement of similarity, since the generated sentences are highly semantic and the similarity is computed in a fine-grained level. Accordingly, even the state-of-the-art image captioning models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref> cannot perform very well on the image and sentence matching task. We also implement a similar model (as "ctx + sen") in Section 4.3, but find it only achieves inferior results. In fact, it is unnecessary for the image and sentence matching task to generate a grammatically-complete sentence. We can alternatively regard the fused context and concepts as the image representation, and supervise it using the groundtruth semantic order in the matched sentence during the sentence generation. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (e), we feed the image representation into the initial hidden state of a generative LSTM, and ask it to be capable of generating the matched sentence. During the cross-word and crossphrase generations, the image representation can thus learn the hypernym relations between words and textual entailment among phrases as the semantic order.</p><p>Given a sentence w j |w j ∈ {0, 1} G j=1,··· ,J , where each word w j is represented as an one-hot vector, J is the length of the sentence, and G is the size of word dictionary, we can formulate the sentence generation as follows:</p><formula xml:id="formula_2">i t = σ(W wi (F w t ) + W hi h t−1 + b i ), f t = σ(W wf (F w t ) + W hf h t−1 + b f ), o t = σ(W wo (F w t ) + W ho h t−1 + b o ), c t = tanh(W wc (F w t ) + W hc h t−1 + b c ), c t = f t c t−1 + i t c t , h t = o t tanh(c t ), q t = softmax(F T h t + b p ), e = arg max(w t ), P (w t |w t−1 , w t−2 , · · · , w 0 , x, p) = q t,e<label>(3)</label></formula><p>where c t , h t , i t , f t and o t are memory state, hidden state, input gate, forget gate and output gate, respectively, e is the index of w t in the word vocabulary, and F ∈ R D×G is a word embedding matrix. During the sentence generation, since all the words are predicted in a chain manner, the probability P of current predicted word is conditioned on all its previous words, as well as the input semantic concepts p and context x at the initial timestep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Joint Matching and Generation</head><p>During the model learning, to jointly perform image and sentence matching and sentence generation, we need to minimize the following combined objectives:</p><formula xml:id="formula_3">L = L mat + λ × L gen<label>(4)</label></formula><p>where λ is a tuning parameter for balancing. The L mat is a structured objective that encourages the cosine similarity scores of matched images and sentences to be larger than those of mismatched ones:</p><formula xml:id="formula_4">ik max {0, m − s ii + s ik } + max {0, m − s ii + s ki }</formula><p>where m is a margin parameter, s ii is the score of matched i-th image and i-th sentence, s ik is the score of mismatched i-th image and k-th sentence, and vice-versa with s ki . We empirically set the total number of mismatched pairs for each matched pair as 128 in our experiments.</p><p>The L gen is the negative conditional log-likelihood of the matched sentence given the semantic concepts p and context x:</p><formula xml:id="formula_5">− t log P (w t |w t−1 , w t−2 , · · · , w 0 , x, p)</formula><p>where the detailed formulation of probability P is shown in <ref type="bibr">Equation 3</ref>. Note that we use the predicted semantic concepts rather than groundtruth ones in our experiments.</p><p>All modules of our model excepting for the multiregional multi-label CNN can constitute a whole deep network, which can be jointly trained in an end-to-end manner from raw image and sentence to their similarity score. It should be noted that we do not need to generate the sentence during testing. We only have to compute the image representation v from x and p, and then compare it with the sentence representation s to obtain their cosine similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To demonstrate the effectiveness of the proposed model, we perform several experiments in terms of image annotation and retrieval on two publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Protocols</head><p>The two evaluation datasets and their experimental protocols are described as follows. 1) Flickr30k <ref type="bibr" target="#b36">[37]</ref> consists of 31783 images collected from the Flickr website. Each image is accompanied with 5 human annotated sentences. We use the public training, validation and testing splits <ref type="bibr" target="#b12">[13]</ref>, which contain 28000, 1000 and 1000 images, respectively. 2) MSCOCO <ref type="bibr" target="#b16">[17]</ref> consists of 82783 training and 40504 validation images, each of which is associated with 5 sentences. We use the public training, validation and testing splits <ref type="bibr" target="#b12">[13]</ref>, with 82783, 4000 and 1000 (or 5000) images, respectively. When using 1000 images for testing, we perform 5-fold cross-validation and report the averaged results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The commonly used evaluation criterions for image annotation and retrieval are "R@1", "R@5" and "R@10", i.e., recall rates at the top 1, 5 and 10 results. We also compute  an additional criterion "mR" by averaging all the 6 recall rates, to evaluate the overall performance for both image annotation and retrieval. For images, the dimension of global context is I=4096 for VGGNet <ref type="bibr" target="#b27">[28]</ref> or I=1000 for ResNet <ref type="bibr" target="#b8">[9]</ref>. We perform 10-cropping <ref type="bibr" target="#b13">[14]</ref> from the images and then separately feed the cropped regions into the network. The final global context is averaged over 10 regions. For sentences, the dimension of embedded word is D=300. We set the max length for all the sentences as 50, i.e., the number of words J=50, and use zero-padding when a sentence is not long enough. Other parameters are empirically set as follows: H=1024, K=256, λ=1, r=50 and m=0.2.</p><p>To systematically evaluate the contributions of different model components, we design various ablation models as shown in <ref type="table" target="#tab_0">Table 1</ref>. The variable model components are explained as follows: 1) "1-crop" and "10-crop" refer to cropping 1 or 10 regions from images, respectively, when extracting the global context. 2) "concept" and "context" denote using semantic concepts and global context, respectively. 3) "sum" and "gate" are two different ways that combine semantic concepts and context via feature summation and gated fusion unit, respectively. 4) "sentence", "generation" and "sampling" are three different ways to learn the semantic order, in which "sentence" uses the state-ofthe-art image captioning method <ref type="bibr" target="#b29">[30]</ref> to generate sentences from images and then regard the sentences as the image representations, "generation" uses the sentence generation as supervision as described in Section 3.3.2, and "sampling" additionally uses the scheduled sampling <ref type="bibr" target="#b0">[1]</ref>. 5) "share" and "non-shared" indicate whether the parameters of two word embedding matrices for sentence representation learning and sentence generation are shared or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Ablation Models</head><p>The results of the ablation models on the Flickr30k and MSCOCO datasets are shown in <ref type="table" target="#tab_1">Table 2</ref>, from which we can obtain the following conclusions. 1) Cropping 10 image regions (as "ctx") can achieve much robust global context features than cropping only 1 region (as "ctx (1-crop)"). 2) Directly using the pre-generated sentences as image representations (as "ctx + sen") cannot improve the performance, since the generated sentences might not accurately include the image details. 3) Using the sentence generation as supervision for semantic order learning (as "ctx + gen") is very effective. But additionally performing the scheduled sampling (as "ctx + gen (S)") cannot further improve the performance. It is probably because the groundtruth semantic order is degenerated during sampling, accordingly the model cannot learn it well. 4) Using a shared word embedding matrix (as "ctx + gen (E)") cannot improve the performance, which might result from that learning a unified matrix for two tasks is difficult. 5) Only using the semantic concepts (as "cnp") can already achieve good performance, especially when the training data are sufficient on the MSCOCO dataset. 6) Simply summing the concept and context (as "cnp + ctx (C)") can further improve the result, because the context contains the spatial relations of concepts which are very useful. 7) Using the proposed gated fusion unit (as "cnp + ctx") performs better, due to the effective importance balancing scheme. 8) The best performance is achieve by the "cnp + ctx + gen", which combines the 10-cropped extracted context with semantic concepts via the gated fusion unit, and exploits the sentence generation for semantic order learning. Without using either semantic concepts (as "ctx + gen") or context (as "cnp + gen"), the performance drops heavily. In the follow experiments, we regard the "cnp + ctx + gen" as our default model. In addition, we test the balancing parameter λ in Equation 4, by varying it from 0 to 100. The corresponding results are presented in <ref type="table" target="#tab_2">Table 3</ref>, we can find that when λ=1, the model can achieve its best performance. It indicates that the generation objective plays an equally important role as the matching objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art Methods</head><p>We compare our proposed model with several recent state-of-the-art models on the Flickr30k and MSCOCO datasets in <ref type="table" target="#tab_3">Table 4</ref>. The methods marked by "(Res)" use the 152-layer ResNet <ref type="bibr" target="#b8">[9]</ref> for context extraction, while the rest ones use the default 19-layer VGGNet <ref type="bibr" target="#b27">[28]</ref>.</p><p>Using either VGGNet or ResNet on the MSCOCO dataset, our proposed model outperforms the current stateof-the-art models by a large margin on all 7 evaluation criterions. It demonstrates that learning semantic concepts and order for image representations is very effective. When using VGGNet on the Flickr30k dataset, our model gets lower performance than 2WayNet on the R@1 evaluation criterion, but obtains much better overall performance on the rest evaluation criterions. When using ResNet on the Flickr30k dataset, our model is able to achieve the best result. Note that our model obtains much larger improvements on the MSCOCO dataset than Flickr30k. It is because the MSCOCO dataset has more training data, so that our model can be better fitted to predict more accurate image-sentence similarities.</p><p>The above experiments on the MSCOCO dataset follow the first protocol <ref type="bibr" target="#b11">[12]</ref>, which uses 1000 images and their associated sentences for testing. We also test the second protocol that uses all the 5000 images and their sentences for testing, and present the comparison results in <ref type="table" target="#tab_4">Table 5</ref>. From the table we can observe that the overall results by all the methods are lower than the first protocol. It probably results from that the target set is much larger so there exist more distracters for a given query. Among all the models, the proposed model still achieves the best performance, which again demonstrates its effectiveness. Note that our model has much larger improvements using VGGNet than ResNet, which results from that "Ours (Res)" only uses the ResNet for extracting global context but not semantic concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis of Image Annotation Results</head><p>To qualitatively validate the effectiveness of our proposed model, we analyze its image annotation results as follows. We select several representative images with complex content, and retrieve relevant sentences by 3 ablation models: "ctx", "cnp + ctx" and "cnp + ctx + gen". We show 1. a <ref type="table" target="#tab_1">dinner table with various plates of food  and a glass of water on the table  2. a table top with some plates of food on it  3. a table set for three with food and</ref>   the retrieved top-3 relevant sentences by the 3 models in <ref type="figure">Figure 4</ref>, and the predicted top-10 semantic concepts with confidence scores in <ref type="figure">Figure 5</ref>. From <ref type="figure">Figure 5</ref>, we can see that our multi-regional multilabel CNN can accurately predict the semantic concepts with high confidence scores for describing the detailed image content. For example, road, motorcycle and riding are predicted from the second image. We also note that the skate is incorrectly assigned, which might result from the reason that this image content is complicated and the smooth country road looks like some skating scenes.</p><p>As shown in <ref type="figure">Figure 4</ref>, without the aid of the predicted semantic concepts, "ctx" cannot accurately capture the semantic concepts from complex image content. For example, the retrieved sentences contain some clearly wrong semantic concepts including water and wine for the first image, and lose important concepts such as eating and basket for the third image. After incorporating the predicted semantic concepts, the retrieved sentences by "cnp + ctx" have very similar meanings as the images, and are able to rank groundtruth sentences into top-3. But the top-1 sentences still do not involve partial image details, e.g., bowl, sun and eating for the three images, respectively. By further learning the semantic order 1 with sentence generation, the "cnp + ctx + gen" is able to associate all the related concepts and retrieve the matched sentences with all the image details.  <ref type="figure">Figure 5</ref>. Predicted top-10 semantic concepts with their confidence scores from the 3 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this work, we have proposed a semantic-enhanced image and sentence matching model. Our main contribution is improving the image representation by learning semantic concepts and then organizing them in a correct semantic order. This is accomplished by a series of model components in terms of multi-regional multi-label CNN, gated fusion unit, and joint matching and generation learning. We have systematically studied the impact of these components on the image and sentence matching, and demonstrated the effectiveness of our model by achieving significant performance improvements.</p><p>In the future, we will replace the used VGGNet with ResNet in the multi-regional multi-label CNN to predict the semantic concepts more accurately, and jointly train it with the rest of our model in an end-to-end manner. Our model can perform image and sentence matching and sentence generation, so we would like to extend it for the image captioning task. Although Pan et al. <ref type="bibr" target="#b23">[24]</ref> have shown the effectiveness of using visual-semantic embedding for video captioning, yet in the context of image captioning, its effectiveness has not been well investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Matched Illustration of the semantic concepts and order (best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed semantic-enhanced image and sentence matching model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of using the global context as reference for semantic order learning (best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>wine 1 .Figure 4 .</head><label>14</label><figDesc>a couple of giraffes eating hay from a trough 2. a couple of giraffes eating out of a basket 3. two giraffes stand and eat food out of a basket Results of image annotation by 3 ablation models. Groundtruth matched sentences are marked as red and bold, while some sentences sharing similar meanings as groundtruths are marked as underline (best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The experimental settings of ablation models. 1-crop 10-crop context concept sum gate sentence generation sampling shared non-shared ctx(1-crop)   </figDesc><table><row><cell>√</cell><cell></cell><cell>√</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ctx ctx + sen ctx + gen (S) ctx + gen (E) ctx + gen cnp cnp + gen cnp + ctx (C) cnp + ctx cnp + ctx + gen</cell><cell>√ √ √ √ √ √ √ √</cell><cell>√ √ √ √ √ √ √ √</cell><cell>√ √ √ √ √</cell><cell>√</cell><cell>√ √</cell><cell>√</cell><cell>√ √ √ √ √</cell><cell>√</cell><cell>√</cell><cell>√ √ √ √</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison results of image annotation and retrieval by ablation models on the Flickr30k and MSCOCO (1000 testing) datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Flickr30k dataset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSCOCO dataset</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Image Annotation R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>mR</cell><cell cols="4">Image Annotation R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>mR</cell></row><row><cell>ctx (1-crop)</cell><cell>29.8 58.4</cell><cell>70.5</cell><cell>22.0 47.9</cell><cell>59.3</cell><cell cols="2">48.0 43.3 75.7</cell><cell>85.8</cell><cell>31.0 66.7</cell><cell>79.9</cell><cell>63.8</cell></row><row><cell>ctx</cell><cell>33.8 63.7</cell><cell>75.9</cell><cell>26.3 55.4</cell><cell>67.6</cell><cell cols="2">53.8 44.7 78.2</cell><cell>88.3</cell><cell>37.0 73.2</cell><cell>85.7</cell><cell>67.9</cell></row><row><cell>ctx + sen</cell><cell>22.8 48.6</cell><cell>60.8</cell><cell>19.1 46.0</cell><cell>59.7</cell><cell cols="2">42.8 39.2 73.3</cell><cell>85.5</cell><cell>32.4 70.1</cell><cell>83.7</cell><cell>64.0</cell></row><row><cell>ctx + gen (S)</cell><cell>34.4 64.5</cell><cell>77.0</cell><cell>27.1 56.3</cell><cell>68.3</cell><cell cols="2">54.6 45.7 78.7</cell><cell>88.7</cell><cell>37.3 73.8</cell><cell>85.8</cell><cell>68.4</cell></row><row><cell>ctx + gen (E)</cell><cell>35.5 63.8</cell><cell>75.9</cell><cell>27.4 55.9</cell><cell>67.6</cell><cell cols="2">54.3 46.9 78.8</cell><cell>89.2</cell><cell>37.3 73.9</cell><cell>85.9</cell><cell>68.7</cell></row><row><cell>ctx + gen</cell><cell>35.6 66.3</cell><cell>76.9</cell><cell>27.9 56.8</cell><cell>68.2</cell><cell cols="2">55.3 46.9 79.2</cell><cell>89.3</cell><cell>37.9 74.0</cell><cell>85.9</cell><cell>68.9</cell></row><row><cell>cnp</cell><cell>30.9 60.9</cell><cell>72.4</cell><cell>23.1 52.5</cell><cell>64.8</cell><cell cols="2">50.8 59.5 86.9</cell><cell>93.6</cell><cell>48.5 81.4</cell><cell>90.9</cell><cell>76.8</cell></row><row><cell>cnp + gen</cell><cell>31.5 61.7</cell><cell>74.5</cell><cell>25.0 53.4</cell><cell>64.9</cell><cell cols="2">51.8 62.6 89.0</cell><cell>94.7</cell><cell>50.6 82.4</cell><cell>91.2</cell><cell>78.4</cell></row><row><cell>cnp + ctx (C)</cell><cell>39.9 71.2</cell><cell>81.3</cell><cell>31.4 61.7</cell><cell>72.8</cell><cell cols="2">59.7 62.8 89.2</cell><cell>95.5</cell><cell>53.2 85.1</cell><cell>93.0</cell><cell>79.8</cell></row><row><cell>cnp + ctx</cell><cell>42.4 72.9</cell><cell>81.5</cell><cell>32.4 63.5</cell><cell>73.9</cell><cell cols="2">61.1 65.3 90.0</cell><cell>96.0</cell><cell>54.2 85.9</cell><cell>93.5</cell><cell>80.8</cell></row><row><cell cols="2">cnp + ctx + gen 44.2 74.1</cell><cell>83.6</cell><cell>32.8 64.3</cell><cell>74.9</cell><cell cols="2">62.3 66.4 91.3</cell><cell>96.6</cell><cell>55.5 86.5</cell><cell>93.7</cell><cell>81.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison results of image annotation and retrieval on the MSCOCO (1000 testing) dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Flickr30k dataset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSCOCO dataset</cell><cell></cell></row><row><cell>λ</cell><cell cols="4">Image Annotation R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>mR</cell><cell cols="4">Image Annotation R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>mR</cell></row><row><cell>0</cell><cell>42.4 72.9</cell><cell>81.5</cell><cell>32.4 63.5</cell><cell>73.9</cell><cell cols="2">61.1 65.3 90.0</cell><cell>96.0</cell><cell>54.2 85.9</cell><cell>93.5</cell><cell>80.8</cell></row><row><cell>0.01</cell><cell>43.1 72.8</cell><cell>83.5</cell><cell>32.8 63.2</cell><cell>73.6</cell><cell cols="2">61.5 66.3 91.2</cell><cell>96.5</cell><cell>55.4 86.5</cell><cell>93.7</cell><cell>81.6</cell></row><row><cell>1</cell><cell>44.2 74.1</cell><cell>83.6</cell><cell>32.8 64.3</cell><cell>74.9</cell><cell cols="2">62.3 66.6 91.8</cell><cell>96.6</cell><cell>55.5 86.6</cell><cell>93.8</cell><cell>81.8</cell></row><row><cell>100</cell><cell>42.3 73.8</cell><cell>83.1</cell><cell>32.5 63.3</cell><cell>74.0</cell><cell cols="2">61.5 65.0 90.5</cell><cell>96.1</cell><cell>54.9 86.3</cell><cell>93.7</cell><cell>81.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison results of image annotation and retrieval on the Flickr30k and MSCOCO (1000 testing) datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Flickr30k dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSCOCO dataset</cell></row><row><cell>Method</cell><cell cols="6">Image Annotation R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>mR</cell><cell cols="4">Image Annotation R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>mR</cell></row><row><cell>m-RNN [21]</cell><cell cols="3">35.4 63.8 73.7</cell><cell cols="7">22.8 50.7 63.1 51.6 41.0 73.0 83.5</cell><cell>29.0 42.2 77.0 57.6</cell></row><row><cell>FV [14]</cell><cell cols="3">35.0 62.0 73.8</cell><cell cols="7">25.0 52.7 66.0 52.4 39.4 67.9 80.9</cell><cell>25.1 59.8 76.6 58.3</cell></row><row><cell>DVSA [12]</cell><cell cols="3">22.2 48.2 61.4</cell><cell cols="7">15.2 37.7 50.5 39.2 38.4 69.9 80.5</cell><cell>27.4 60.2 74.8 58.5</cell></row><row><cell>MNLM [13]</cell><cell cols="3">23.0 50.7 62.9</cell><cell cols="7">16.8 42.0 56.5 42.0 43.4 75.7 85.8</cell><cell>31.0 66.7 79.9 63.8</cell></row><row><cell>m-CNN [20]</cell><cell cols="3">33.6 64.1 74.9</cell><cell cols="7">26.2 56.3 69.6 54.1 42.8 73.1 84.1</cell><cell>32.6 68.6 82.8 64.0</cell></row><row><cell>RNN+FV [16]</cell><cell cols="3">34.7 62.7 72.6</cell><cell cols="7">26.2 55.1 69.2 53.4 40.8 71.9 83.2</cell><cell>29.6 64.8 80.5 61.8</cell></row><row><cell>OEM [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">46.7 78.6 88.9</cell><cell>37.9 73.7 85.9 68.6</cell></row><row><cell>VQA [18]</cell><cell cols="3">33.9 62.5 74.5</cell><cell cols="7">24.9 52.6 64.8 52.2 50.5 80.1 89.7</cell><cell>37.0 70.9 82.9 68.5</cell></row><row><cell>RTP [26]</cell><cell cols="3">37.4 63.1 74.3</cell><cell cols="4">26.0 56.0 69.3 54.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DSPE [32]</cell><cell cols="3">40.3 68.9 79.9</cell><cell cols="7">29.7 60.1 72.1 58.5 50.1 79.7 89.2</cell><cell>39.6 75.2 86.9 70.1</cell></row><row><cell>sm-LSTM [11]</cell><cell cols="3">42.5 71.9 81.5</cell><cell cols="7">30.2 60.4 72.3 59.8 53.2 83.1 91.5</cell><cell>40.7 75.8 87.4 72.0</cell></row><row><cell>2WayNet [4]</cell><cell cols="2">49.8 67.5</cell><cell>-</cell><cell cols="2">36.0 55.6</cell><cell>-</cell><cell>-</cell><cell cols="2">55.8 75.2</cell><cell>-</cell><cell>39.7 63.3</cell><cell>-</cell><cell>-</cell></row><row><cell>DAN [23]</cell><cell cols="3">41.4 73.5 82.5</cell><cell cols="4">31.8 61.7 72.5 60.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VSE++ [5]</cell><cell cols="3">41.3 69.0 77.9</cell><cell cols="7">31.4 59.7 71.2 58.4 57.2 85.1 93.3</cell><cell>45.9 78.9 89.1 74.6</cell></row><row><cell>Ours</cell><cell cols="3">44.2 74.1 83.6</cell><cell cols="7">32.8 64.3 74.9 62.3 66.6 91.8 96.6</cell><cell>55.5 86.6 93.8 81.8</cell></row><row><cell>RRF (Res) [19]</cell><cell cols="3">47.6 77.4 87.1</cell><cell cols="7">35.4 68.3 79.9 66.0 56.4 85.3 91.5</cell><cell>43.9 78.1 88.6 73.9</cell></row><row><cell>DAN (Res) [23]</cell><cell cols="3">55.0 81.8 89.0</cell><cell cols="4">39.4 69.2 79.1 68.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">VSE++ (Res) [5] 52.9 79.1 87.2</cell><cell cols="7">39.6 69.6 79.5 68.0 64.6 89.1 95.7</cell><cell>52.0 83.1 92.0 79.4</cell></row><row><cell>Ours (Res)</cell><cell cols="3">55.5 82.0 89.3</cell><cell cols="7">41.1 70.5 80.1 69.7 69.9 92.9 97.5</cell><cell>56.7 87.5 94.8 83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison results of image annotation and retrieval on the MSCOCO (5000 testing) dataset.</figDesc><table><row><cell>Method</cell><cell>Image Annotation R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval mR</cell></row><row><cell>DVSA [12]</cell><cell>11.8 32.5 45.4 8.9 24.9 36.3 26.6</cell></row><row><cell>FV[14]</cell><cell>17.3 39.0 50.2 10.8 28.3 40.1 31.0</cell></row><row><cell>VQA [18]</cell><cell>23.5 50.7 63.6 16.7 40.5 53.8 41.5</cell></row><row><cell>OEM [29]</cell><cell>23.3 50.5 65.0 18.0 43.6 57.6 43.0</cell></row><row><cell>VSE++ [5]</cell><cell>32.9 61.6 74.7 24.1 52.0 66.2 51.9</cell></row><row><cell>Ours</cell><cell>40.2 70.1 81.3 31.3 61.5 73.9 59.7</cell></row><row><cell cols="2">VSE++ (Res) [5] 41.3 69.2 81.2 30.3 59.1 72.4 58.9</cell></row><row><cell>Ours (Res)</cell><cell>42.8 72.3 83.0 33.1 62.9 75.5 61.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is difficult to visualize the learned semantic order like semantic concepts, since it is implied in the image representation. Here we validate its effectiveness by showing it can help to find more accurate sentences.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. a man on a skateboard performing a trick 2. a man riding a skateboard up the side of a ramp 3. a man at a skate park with his foot on the side of the skateboard 1. its a cloudy night for a ride on the </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vse++: Improved visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<title level="m">Deep convolutional ranking for multilabel image annotation. arXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">TACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rnn fisher vectors for action recognition and image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging visual question answering for image-caption ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Explain images with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Orderembeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Cnn: Single-label to multi-label. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
