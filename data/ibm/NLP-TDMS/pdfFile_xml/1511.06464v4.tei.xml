<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unitary Evolution Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">MARJOVSKY@DC.UBA.AR</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Universidad de Buenos Aires</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">Université de Montréal. Yoshua</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unitary Evolution Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Bengio is a CIFAR Senior Fellow. * Indicates first authors. Ordering determined by coin flip.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very longterm dependencies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks have shown remarkably good performance on a wide range of complex data problems including speech recognition , image recognition <ref type="bibr" target="#b12">(Krizhevsky et al., 2012)</ref> and natural language processing <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>. However, training very deep models remains a difficult task. The main issue surrounding the training of deep networks is the vanishing and exploding gradients problems introduced by Hochre-Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s). iter <ref type="bibr">(1991)</ref> and shown by <ref type="bibr" target="#b0">Bengio et al. (1994)</ref> to be necessarily arising when trying to learn to reliably store bits of information in any parametrized dynamical system. If gradients propagated back through a network vanish, the credit assignment role of backpropagation is lost, as information about small changes in states in the far past has no influence on future states. If gradients explode, gradientbased optimization algorithms struggle to traverse down a cost surface, because gradient-based optimization assumes small changes in parameters yield small changes in the objective function. As the number of time steps considered in the sequence of states grows, the shrinking or expanding effects associated with the state-to-state transformation at individual time steps can grow exponentially, yielding respectively vanishing or exploding gradients. See <ref type="bibr" target="#b18">Pascanu et al. (2010)</ref> for a review.</p><p>Although the long-term dependencies problem appears intractable in the absolute <ref type="bibr" target="#b0">(Bengio et al., 1994)</ref> for parametrized dynamical systems, several heuristics have recently been found to help reduce its effect, such as the use of self-loops and gating units in the LSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b2">(Cho et al., 2014)</ref> recurrent architectures. Recent work also supports the idea of using orthogonal weight matrices to assist optimization <ref type="bibr" target="#b19">(Saxe et al., 2014;</ref><ref type="bibr" target="#b14">Le et al., 2015)</ref>.</p><p>In this paper, we explore the use of orthogonal and unitary matrices in recurrent neural networks. We start in Section 2 by showing a novel bound on the propagated gradients in recurrent nets when the recurrent matrix is orthogonal. Section 3 discusses the difficulties of parameterizing real valued orthogonal matrices and how they can be alleviated by moving to the complex domain.</p><p>We discuss a novel approach to constructing expressive unitary matrices as the composition of simple unitary matrices which require at most O(n log n) computation and O(n) memory, when the state vector has dimension n. These are unlike general matrices, which require O(n 2 ) computation and memory. Complex valued representations arXiv:1511.06464v4 <ref type="bibr">[cs.</ref>LG] 25 May 2016 have been considered for neural networks in the past, but with limited success and adoption <ref type="bibr" target="#b8">(Hirose, 2003;</ref><ref type="bibr" target="#b22">Zimmermann et al., 2011)</ref>. We hope our findings will change this.</p><p>Whilst our model uses complex valued matrices and parameters, all implementation and optimization is possible with real numbers and has been done in Theano <ref type="bibr" target="#b1">(Bergstra et al., 2010)</ref>. This along with other implementation details are discussed in Section 4, and the code used for the experiments is available online. The potential of the developed model for learning long term dependencies with relatively few parameters is explored in Section 5. We find that the proposed architecture generally outperforms LSTMs and previous approaches based on orthogonal initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Orthogonal Weights and Bounding the</head><p>Long-Term Gradient</p><formula xml:id="formula_0">A matrix, W, is orthogonal if W W = WW = I.</formula><p>Orthogonal matrices have the property that they preserve norm (i.e. Wh 2 = h 2 ) and hence repeated iterative multiplication of a vector by an orthogonal matrix leaves the norm of the vector unchanged.</p><p>Let h T and h t be the hidden unit vectors for hidden layers T and t of a neural network with T hidden layers and T t. If C is the objective we are trying to minimize, then the vanishing and exploding gradient problems refer to the decay or growth of ∂C ∂ht as the number of layers, T , grows. Let σ be a pointwise nonlinearity function, and</p><formula xml:id="formula_1">z t+1 = W t h t + V t x t+1 h t+1 = σ(z t+1 )<label>(1)</label></formula><p>then by the chain rule</p><formula xml:id="formula_2">∂C ∂h t = ∂C ∂h T ∂h T ∂h t = ∂C ∂h T T −1 k=t ∂h k+1 ∂h k = ∂C ∂h T T −1 k=t D k+1 W T k<label>(2)</label></formula><p>where D k+1 = diag(σ (z k+1 )) is the Jacobian matrix of the pointwise nonlinearity.</p><p>In the following we define the norm of a matrix to refer to the spectral radius norm (or operator 2-norm) and the norm of a vector to mean L 2 -norm. By definition of the operator norms, for any matrices A, B and vector v we have Av ≤ A v and AB ≤ A B . If the weight matrices W k are norm preserving (i.e. orthogonal), then we prove</p><formula xml:id="formula_3">∂C ∂h t = ∂C ∂h T T −1 k=t D k+1 W T k ≤ ∂C ∂h T T −1 k=t D k+1 W T k = ∂C ∂h T T −1 k=t D k+1 . (3) Since D k is diagonal, D k = max j=1,...,n |σ (z (j) k )|, with z (j)</formula><p>k the j-th pre-activation of the k-th hidden layer. If the absolute value of the derivative σ can take some value τ &gt; 1, then this bound is useless, since ∂C ∂ht ≤ ∂C ∂h T τ T −t which grows exponentially in T . We therefore cannot effectively bound ∂C ∂ht for deep networks, resulting potentially in exploding gradients.</p><p>In the case |σ | &lt; τ &lt; 1, equation 3 proves that that ∂C ∂ht tends to 0 exponentially fast as T grows, resulting in guaranteed vanishing gradients. This argument makes the rectified linear unit (ReLU) nonlinearity an attractive choice <ref type="bibr" target="#b5">(Glorot et al., 2011;</ref><ref type="bibr" target="#b17">Nair &amp; Hinton, 2010)</ref>. Unless all the activations are killed at one layer, the maximum entry of D k is 1, resulting in D k = 1 for all layers k. With ReLU nonlinearities, we thus have</p><formula xml:id="formula_4">∂C ∂h t ≤ ∂C ∂h T T −1 k=t D k+1 = ∂C ∂h T .<label>(4)</label></formula><p>Most notably, this result holds for a network of arbitrary depth and renders engineering tricks like gradient clipping unnecessary .</p><p>To the best of our knowledge, this analysis is a novel contribution and the first time a neural network architecture has been mathematically proven to avoid exploding gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unitary Evolution RNNs</head><p>Unitary matrices generalize orthogonal matrices to the complex domain. A complex valued, norm preserving matrix, U, is called a unitary matrix and is such that U * U = UU * = I, where U * is the conjugate transpose of U. Directly parametrizing the set of unitary matrices in such a way that gradient-based optimization can be applied is not straightforward because a gradient step will typically yield a matrix that is not unitary, and projecting on the set of unitary matrices (e.g., by performing an eigendecomposition) generally costs O(n 3 ) computation when U is n × n.</p><p>The most important feature of unitary and orthogonal matrices for our purpose is that they have eigenvalues λ j with absolute value 1. The following lemma, proved in <ref type="bibr" target="#b11">(Hoffman &amp; Kunze, 1971)</ref>, may shed light on a method which can be used to efficiently span a large set of unitary matrices.</p><p>Lemma 1. A complex square matrix W is unitary if and only if it has an eigendecomposition of the form W = VDV * , where * denotes the conjugate transpose. Here, V, D ∈ C n×n are complex matrices, where V is unitary, and D is a diagonal such that |D j,j | = 1. Furthermore, W is a real orthogonal matrix if and only if for every eigenvalue D j,j = λ j with eigenvector v j , there is also a complex conjugate eigenvalue λ k = λ j with corresponding eigenvector v k = v j .</p><p>Writing λ j = e iwj with w j ∈ R, a naive method to learn a unitary matrix would be to fix a basis of eigenvectors V ∈ C n×n and set W = VDV * ,</p><p>where D is a diagonal such that D j,j = λ j .</p><p>Lemma 1 informs us how to construct a real orthogonal matrix, W. We must (i) ensure the columns of V come in complex conjugate pairs, v k = v j , and (ii) tie weights w k = −w j in order to achieve e iwj = e iw k . Most neural network objective functions are differentiable with respect to the weight matrices, and consequently w j may be learned by gradient descent.</p><p>Unfortunately the above approach has undesirable properties. Fixing V and learning w requires O n 2 memory, which is unacceptable given that the number of learned parameters is O(n). Further note that calculating Vu for an arbitrary vector u requires O(n 2 ) computation. Setting V to the identity would satisfy the conditions of the lemma, whilst reducing memory and computation requirements to O(n), however, W would remain diagonal, and have poor representation capacity.</p><p>We propose an alternative strategy to parameterize unitary matrices. Since the product of unitary matrices is itself a unitary matrix, we compose several simple, parameteric, unitary matrices to construct a single, expressive unitary matrix. The four unitary building blocks considered are</p><p>• D, a diagonal matrix with D j,j = e iwj , with parameters w j ∈ R,</p><formula xml:id="formula_6">• R = I − 2 vv * v 2 , a reflection matrix in the complex vector v ∈ C n ,</formula><p>• Π, a fixed random index permutation matrix, and • F and F −1 , the Fourier and inverse Fourier transforms.</p><p>Appealingly, D, R and Π all permit O(n) storage and O(n) computation for matrix vector products. F and F −1 require no storage and O(n log n) matrix vector multiplication using the Fast Fourier Transform algorithm. A major advantage of composing unitary matrices of the form listed above, is that the number of parameters, memory and computational cost increase almost linearly in the size of the hidden layer. With such a weight matrix, immensely large hidden layers are feasible to train, whilst being impossible in traditional neural networks.</p><p>With this in mind, in this work we choose to consider recurrent neural networks with unitary hidden to hidden weight matrices. Our claim is that the ability to have large hidden layers where hidden states norms are preserved provides a powerful tool for modeling long term dependencies in sequence data. <ref type="bibr" target="#b0">(Bengio et al., 1994)</ref> suggest that having a large memory may be crucial for solving difficult tasks with long ranging dependencies: the smaller the state dimension, the more information necessarily has to be eliminated when mapping a long sequence to a fixed-dimension state.</p><p>We call any RNN architecture which uses a unitary hidden to hidden matrix a unitary evolution RNN (uRNN). After experimenting with several structures, we settled on the following composition</p><formula xml:id="formula_7">W = D 3 R 2 F −1 D 2 ΠR 1 FD 1 .<label>(6)</label></formula><p>Whilst each but the permutation matrix is complex, we parameterize and represent them with real numbers for implementation purposes. When the final cost is real and differentiable, we may perform gradient descent optimization to learn the parameters. <ref type="bibr" target="#b21">(Yang et al., 2015)</ref> construct a real valued, non-orthogonal matrix using a similar parameterization with the motivation of parameter reduction by an order of magnitude on an industrial sized network. This combined with earlier work <ref type="bibr" target="#b13">(Le et al., 2010)</ref> suggests that it is possible to create highly expressive matrices by composing simple matrices with few parameters. In the following section, we explain details on how to implement our model and illustrate how we bypass the potential difficulties of working in the complex domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Architecture details</head><p>In this section, we describe the nonlinearity we used, how we incorporate real valued inputs with complex valued hidden units and map from complex hidden states to real outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Complex hidden units</head><p>Our implementation represents all complex numbers using real values in terms of their real and imaginary parts.</p><p>Under this framework, we sidestep the lack of support for complex numbers by most deep learning frameworks. Consider multiplying the complex weight matrix W = A + iB by the complex hidden vector h = x + iy, where A, B, x, y are real. It is trivially true that Wh = (Ax − By) + i(Ay + Bx). When we represent v ∈ C n as Re(v) , Im(v) ∈ R 2n , we compute complex matrix vector products with real numbers as follows</p><formula xml:id="formula_8">Re(Wh) Im(Wh) = A −B B A Re(h) Im(h) .<label>(7)</label></formula><p>More generally, let f : C n → C n be any complex function and z = x + iy any complex vector. We may write</p><formula xml:id="formula_9">f (z) = α(x, y) + iβ(x, y) where α, β : R n → R n .</formula><p>This allows us to implement everything using real valued operations, compatible with any any deep learning framework with automatic differentiation such as Theano.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Input to Hidden, Nonlinearity, Hidden to Output</head><p>As is the case with most recurrent networks, our uRNN follows the same hidden to hidden mapping as equation 1 with V t = V and W t = W. Denote the size of the complex valued hidden states as n h . The input to hidden matrix is complex valued, V ∈ C n h ×nin . We learn the initial hidden state h 0 ∈ C n h as a parameter of the model.</p><p>Choosing an appropriate nonlinearity is not trivial in the complex domain. As discussed in the introduction, using a ReLU is a natural choice in combination with a norm preserving weight matrix. We first experimented with placing separate ReLU activations on the real and imaginary parts of the hidden states. However, we found that such a nonlinearity usually performed poorly. Our intuition is that applying separate ReLU nonlinearities to the real and imaginary parts brutally impacts the phase of a complex number, making it difficult to learn structure.</p><p>We speculate that maintaining the phase of hidden states may be important for storing information across a large number of time steps, and our experiments supported this claim. A variation of the ReLU that we name modReLU, is what we finally chose. It is a pointwise nonlinearity, σ modReLU (z) : C → C, which affects only the absolute value of a complex number, defined as</p><formula xml:id="formula_10">σ modReLU (z) = (|z| + b) z |z| if |z| + b ≥ 0 0 if |z| + b &lt; 0<label>(8)</label></formula><p>where b ∈ R is a bias parameter of the nonlinearity. For a n h dimensional hidden space we learn n h nonlinearity bias parameters, one per dimension. Note that the modReLU is similar to the ReLU in spirit, in fact more concretely σ modReLU (z) = σ ReLU (|z| + b) z |z| .</p><p>To map hidden states to output, we define a matrix U ∈ R no×2n h , where n o is the output dimension. We calculate a linear output as</p><formula xml:id="formula_11">o t = U Re(h t ) Im(h t ) + b o ,<label>(9)</label></formula><p>where b o ∈ R no is the output bias. The linear output is real valued (o t ∈ R no ) and can be used for prediction and loss function calculation akin to typical neural networks (e.g. it may be passed through a softmax which is used for cross entropy calculation for classification tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Initialization</head><p>Due to the stability of the norm preserving operations of our network, we found that performance was not very sensitive to initialization of parameters. For full disclosure and reproducibility, we explain our initialization strategy for each parameter below.</p><p>• We initialize V and U (the input and output matrices) as in <ref type="bibr" target="#b4">(Glorot &amp; Bengio, 2010)</ref>, with weights sampled independently from uniforms,</p><formula xml:id="formula_12">U − √ 6 √ nin+nout , √ 6 √ nin+nout .</formula><p>• The biases, b and b o are initialized to 0. This implies that at initialization, the network is linear with unitary weights, which seems to help early optimization <ref type="bibr" target="#b19">(Saxe et al., 2014)</ref>.</p><p>• The reflection vectors for R 1 and R 2 are initialized coordinate-wise from a uniform U[−1, 1]. Note that the reflection matrices are invariant to scalar multiplication of the parameter vector, hence the width of the uniform initialization is unimportant.</p><p>• The diagonal weights for D 1 , D 2 and D 3 are sampled from a uniform, U[−π, π]. This ensures that the diagonal entries D j,j are sampled uniformly over the complex unit circle.</p><p>• We initialize h 0 with a uniform, U − 3 2n h , 3 2n h , which results in E h 0 2 = 1. Since the norm of the hidden units are roughly preserved through unitary evolution and inputs are typically whitened to have norm 1, we have hidden states, inputs and linear outputs of the same order of magnitude, which seems to help optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we explore the performance of our uRNN in relation to (a) RNN with tanh activations, (b) IRNN , that is an RNN with ReLU activations and with the recurrent weight matrix initialized to the identity, <ref type="figure">Figure 1</ref>. Results of the copying memory problem for time lags of 100, 200, 300, 500. The LSTM is able to beat the baseline only for 100 times steps. Conversely the uRNN is able to completely solve each time length in very few training iterations, without getting stuck at the baseline. and (c) LSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997</ref>) models. We show that the uRNN shines quantitatively when it comes to modeling long term dependencies and exhibits qualitatively different learning properties to the other models.</p><p>We chose a handful of tasks to evaluate the performance of the various models. The tasks were especially created to be be pathologically hard, and have been used as benchmarks for testing the ability of a model to capture long-term memory <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b14">Le et al., 2015;</ref><ref type="bibr" target="#b6">Graves et al., 2014;</ref><ref type="bibr" target="#b16">Martens &amp; Sutskever, 2011)</ref> Of the handful of optimization algorithms we tried on the various models, RMSProp <ref type="bibr" target="#b20">(Tieleman &amp; Hinton, 2012)</ref> lead to fastest convergence and is what we stuck to for all experiments here on in. However, we found the IRNN to be particularly unstable; it only ran without blowing up with incredibly low learning rates and gradient clipping. Since the performance was so poor relative to other models we compare against, we do not show IRNN curves in the figures. In each experiment we use a learning rate of 10 −3 and a decay rate of 0.9. For the LSTM and RNN models, we had to clip gradients at 1 to avoid exploding gradients.</p><p>Gradient clipping was unnecessary for the uRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Copying memory problem</head><p>Recurrent networks have been known to have trouble remembering information about inputs seen many time steps previously <ref type="bibr" target="#b0">(Bengio et al., 1994;</ref><ref type="bibr" target="#b18">Pascanu et al., 2010)</ref>. We therefore want to test the uRNN's ability to recall exactly data seen a long time ago.</p><p>Following a similar setup to <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref>, we outline the copy memory task. Consider 10 categories, {a i } 9 i=0 . The input takes the form of a T +20 length vector of categories, where we test over a range of values of T . The first 10 entries are sampled uniformly, independently and with replacement from {a i } 7 i=0 , and represent the sequence which will need to be remembered. The next T − 1 entries are set to a 8 , which can be thought of as the 'blank' category. The next single entry is a 9 , which represents a delimiter, which should indicate to the algorithm that it is now required to reproduce the initial 10 categories in the output. The remaining 10 entries are set to a 8 . The required output sequence consists of T + 10 repeated entries of a 8 , followed by the first 10 categories of the input sequence in exactly the same order. The goal is to minimize the average cross entropy of category predictions at each time step of the sequence. The task amounts to having to remember a categorical sequence of length 10, for T time steps.</p><p>A simple baseline can be established by considering an optimal strategy when no memory is available, which we deem the memoryless strategy. The memoryless strategy would be to predict a 8 for T + 10 entries and then predict each of the final 10 categories from the set {a i } 7 i=0 independently and uniformly at random. The categorical cross entropy of this strategy is 10 log <ref type="formula" target="#formula_10">(8)</ref> T +20 . We ran experiments where the RNN with tanh activations, IRNN, LSTM and uRNN had hidden layers of size 80, 80, 40 and 128 respectively. This equates to roughly 6500 parameters per model. In <ref type="figure">Figure 1</ref>, we see that aside from the simplest case, both the RNN with tanh and more surprisingly the LSTMs get almost exactly the same cost as the memoryless strategy. This behaviour is consistent with the results of <ref type="bibr" target="#b6">(Graves et al., 2014)</ref>, in which poor performance is reported for the LSTM for a very similar long term memory problem.</p><p>The uRNN consistently achieves perfect performance in relatively few iterations, even when having to recall sequences after 500 time steps. What is remarkable is that the uRNN does not get stuck at the baseline at all, whilst the LSTM and RNN do. This behaviour suggests that the representations learned by the uRNN have qualitatively different properties from both the LSTM and classical RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Adding Problem</head><p>We closely follow the adding problem defined in (Hochreiter &amp; Schmidhuber, 1997) to explain the task at hand. Each input consists of two sequences of length T . The first sequence, which we denote x, consists of numbers sampled uniformly at random U[0, 1]. The second sequence is an indicator sequence consisting of exactly two entries of 1 and remaining entries 0. The first 1 entry is located uniformly at random in the first half of the sequence, whilst the second 1 entry is located uniformly at random in the second half. The output is the sum of the two entries of the first sequence, corresponding to where the 1 entries are located in the second sequence. A naive strategy of predicting 1 as the output regardless of the input sequence gives an expected mean squared error of 0.167, the variance of the sum of two independent uniform distributions. This is our baseline to beat. <ref type="figure">Figure 3</ref>. Results on pixel by pixel MNIST classification tasks. The uRNN is able to converge in a fraction of the iterations that the LSTM requires. The LSTM performs better on MNIST classification, but the uRNN outperforms on the more complicated task of permuted pixels.</p><p>We chose to use 128 hidden units for the RNN with tanh, IRNN and LSTM and 512 for the uRNN. This equates to roughly 16K parameters for the RNN with tanh and IRNN, 60K for the LSTM and almost 9K for the uRNN. All models were trained using batch sizes of 20 and 50 with the best results being reported. Our results are shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The LSTM and uRNN models are able to convincingly beat the baseline up to T = 400 time steps. Both models do well when T = 750, but the mean squared error does not reach close to 0. The uRNN achieves lower test error, but it's curve is more noisy. Despite having vastly more parameters, we monitored the LSTM performance to ensure no overfitting.</p><p>The RNN with tanh and IRNN were not able to beat the baseline for any number of time steps.  report that their RNN solve the problem for T = 150 and the IRNN for T = 300, but they require over a million iterations before they start learning. Neither of the two models came close to either the uRNN or the LSTM in performance. The stark difference in our findings are best explained by our use of RMSprop with significantly higher learning rates (10 −3 as opposed to 10 −8 ) than  use for SGD with momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Pixel-by-pixel MNIST</head><p>In this task, suggested by , algorithms are fed pixels of <ref type="bibr">MNIST (LeCun et al., 1998)</ref> sequentially and required to output a class label at the end. We consider two tasks: one where pixels are read in order (from left to right, bottom to top) and one where the pixels are all randomly permuted using the same randomly generated permutation matrix. The same model architectures as for the adding problem were used for this task, except we now use a softmax for category classification. We ran the optimization algorithms until convergence of the mean categorical cross entropy on test data, and plot test accuracy in <ref type="figure">Figure 3</ref>.</p><p>Both the uRNN and LSTM perform applaudably well here. On the correct unpermuted MNIST pixels, the LSTM performs better, achieving 98.2 % test accurracy versus 95.1% for the uRNN. However, when we permute the ordering of the pixels, the uRNN dominates with 91.4% of accuracy in contrast to the 88% of the LSTM, despite having less than a quarter of the parameters. This result is state of the art on this task, beating the IRNN , which reaches close to 82% after 1 million training iterations. Notice that uRNN reaches convergence in less than 20 thousand iterations, while it takes the LSTM from 5 to 10 times as many to finish learning.</p><p>Permuting the pixels of MNIST images creates many longer term dependencies across pixels than in the original pixel ordering, where a lot of structure is local. This makes it necessary for a network to learn and remember more complicated dependencies across varying time scales. The results suggest that the uRNN is better able to deal with such structure over the data, where the LSTM is better suited to more local sequence structure tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Exploratory experiments</head><p>Norms of hidden state gradients. As discussed in Section 2, key to being able to learn long term dependencies is in controlling ∂C ∂ht . With this in mind, we explored how each model propagated gradients, by examining ∂C ∂ht as a function of t. Gradient norms were computed at the be- (iii) Norms of the hidden states and (iv) L2 distance between hidden states and final hidden state. The gradient norms of uRNNs do not decay as fast as for other models as training progresses. uRNN hidden state norms stay much more consistent over time than the LSTM. LSTM hidden states stay almost the same after a number of time steps, suggesting that it is not able to use new input information.</p><p>ginning of training and again after 100 iterations of training on the adding problem. The curves are plotted in <ref type="figure" target="#fig_1">Figure 4</ref>. It is clear that at first, the uRNN propagates gradients perfectly, while each other model has exponentially vanishing gradients. After 100 iterations of training, each model experiences vanishing gradients, but the uRNN is best able to propagate information, having much less decay.</p><p>Hidden state saturation. We claim that typical recurrent architectures saturate, in the sense that after they acquire some information, it becomes much more difficult to acquire further information pertaining to longer dependencies. We took the uRNN and LSTM models trained on the adding problem with T = 200, and computed a forward pass with newly generated data for the adding problem with T = 1000. In order to show saturation effects, we plot the norms of the hidden states and the L 2 distance between each state and the last in <ref type="figure" target="#fig_1">Figure 4</ref>.</p><p>In our experiments, it is clear that the uRNN does not suffer as much as other models do. Notice that whilst the norms of hidden states in the uRNN grow very steadily over time, in the LSTM they grow very fast, and then stay constant after about 500 time steps. This behaviour may suggest that the LSTM hidden states saturate in their ability to incorporate new information, which is vital for modeling long complicated sequences. It is interesting to see that the LSTM hidden state at t = 500, is close to that of t = 1000, whilst this is far from the case in the uRNN. Again, this suggests that the LSTM's capacity to use new information to alter its hidden state severly degrades with sequence length. The uRNN does not suffer from this difficulty nearly as badly.</p><p>A clear example of this phenomenon was observed in the adding problem with T = 750. We found that the Pearson correlation between the LSTM output prediction and the first of the two uniform samples (whose sum is the target output) was ρ = 0.991. This suggests that the LSTM learnt to simply find and store the first sample, as it was unable to incorporate any more information by the time it reached the second, due to saturation of the hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>There are a plethora of further ideas that may be explored from our findings, both with regards to learning representation and efficient implementation. For example, one hurdle of modeling long sequences with recurrent networks is the requirement of storing all hidden state values for the purpose of gradient backpropagation. This can be prohibitive, since GPU memory is typically a limiting factor of neural network optimization. However, since our weight matrix is unitary, its inverse is its conjugate transpose, which is just as easy to operate with. If further we were to use an invertible nonlinearity function, we would no longer need to store hidden states, since they can be recomputed in the backward pass. This could have potentially huge implications, as we would be able to reduce memory usage by an order of T , the number of time steps. This would make having immensely large hidden layers possible, perhaps enabling vast memory representations.</p><p>In this paper we demonstrate state of the art performance on hard problems requiring long term reasoning and memory. These results are based on a novel parameterization of unitary matrices which permit efficient matrix computations and parameter optimization. Whilst complex domain modeling has been widely succesful in the signal processing community (e.g. Fourier transforms, wavelets), we have yet to exploit the power of complex valued representation in the deep learning community. Our hope is that this work will be a step forward in this direction. We motivate the idea of unitary evolution as a novel way to mitigate the problems of vanishing and exploding gradients. Empirical evidence suggests that our uRNN is better able to pass gradient information through long sequences and does not suffer from saturating hidden states as much as LSTMs, typical RNNs, or RNNs initialized with the identity weight matrix (IRNNs).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Results of the adding problem for T = 100, 200, 400, 750. The RNN with tanh is not able to beat the baseline for any time length. The LSTM and the uRNN show similar performance across time lengths, consistently beating the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>From left to right. Norms of the gradients with respect to hidden states i.e. ∂C ∂h t at (i) beginning of training, (ii) after 100 iterations.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We thank the developers of Theano <ref type="bibr" target="#b1">(Bergstra et al., 2010)</ref> for their great work. We thank NSERC, Compute Canada, Canada Research Chairs and CIFAR for their support. We would also like to thank Ç aglar Gulçehre, David Krueger, Soroush Mehri, Marcin Moczulski, Mohammad Pezeshki and Saizheng Zhang for helpful discussions, comments and code sharing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-Decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep sparse rectifier neural networks. International Conference on Artificial Intelligence and Statistics (AIS-TATS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Complex-valued neural networks: theories and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Hirose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>World Scientific Publishing Company Incorporated</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<title level="m">Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. Münich</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Linear Algebra. Pearson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kunze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fastfood -approximating kernel expansions in loglinear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaitly</forename><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Deep fried convnets. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparison of the complex valued and real valued neural networks trained with gradient descent and random search algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Georg</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Minin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Kusherbaeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
