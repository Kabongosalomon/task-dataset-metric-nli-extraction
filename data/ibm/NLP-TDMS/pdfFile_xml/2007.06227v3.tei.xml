<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>RGB-D Salient Object Detection</term>
					<term>Cross-modal Fusion</term>
					<term>Dy- namic Dilated Pyramid Module</term>
					<term>Hybrid Enhanced Loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main purpose of RGB-D salient object detection (SOD) is how to better integrate and utilize cross-modal fusion information. In this paper, we explore these issues from a new perspective. We integrate the features of different modalities through densely connected structures and use their mixed features to generate dynamic filters with receptive fields of different sizes. In the end, we implement a kind of more flexible and efficient multi-scale cross-modal feature processing, i.e. dynamic dilated pyramid module. In order to make the predictions have sharper edges and consistent saliency regions, we design a hybrid enhanced loss function to further optimize the results. This loss function is also validated to be effective in the single-modal RGB SOD task. In terms of six metrics, the proposed method outperforms the existing twelve methods on eight challenging benchmark datasets. A large number of experiments verify the effectiveness of the proposed module and loss function. Our code, model and results are available at https://github.com/lartpang/HDFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Salient object detection (SOD) aims to model the mechanism of human visual attention and mine the most salient objects or regions in data such as images or videos. SOD has been widely applied in many computer vision tasks, such as scene classification <ref type="bibr" target="#b38">[39]</ref>, video segmentation <ref type="bibr" target="#b13">[14]</ref>, semantic segmentation <ref type="bibr" target="#b43">[44]</ref>, foreground map evaluation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> visual tracking <ref type="bibr" target="#b30">[31]</ref>, person reidentification <ref type="bibr" target="#b39">[40]</ref> and so on.</p><p>With the advent of the fully convolutional network <ref type="bibr" target="#b29">[30]</ref>, deep learning-based SOD models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref> have made great progress. Some methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b51">52]</ref> have achieved very good performance on the existing benchmark datasets. However, these works are mainly based on RGB data. They still face severe challenges when handling the cluttered or low-contrast scenes. Recently, some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>  introduce the depth data as an aid to further improve the detection performance. The depth information can more intuitively express spatial structures of the objects in a scene and provide a powerful supplement for the detection and recognition of salient objects. Using complementary modal cues, the scene can be further deeply and intelligently understood. However, limited by the way of using the depth information, RGB-D salient object detection is still great challenging.</p><p>It is well known that RGB images contain rich appearance and detail information while depth images contain more spatial structure information. They complement each other for many vision tasks. RGB-D SOD approaches aim to formulate cross-modal fusion in different manners. Most of them integrate depth and RGB features by element-wise addition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>, concatenation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3]</ref> and convolution operations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43]</ref>. Some methods compute attention map <ref type="bibr" target="#b49">[50]</ref> or saliency map <ref type="bibr" target="#b42">[43]</ref> via a shallow or deep CNN network from pure depth images. Because of using the fixed parameters for different samples during the testing phase, the generalization capability of these models is weakened.</p><p>Moreover, for dense prediction task, the loss in each spatial position is usually different. Thus, the actual optimization direction of gradients in different positions may be varying. The weight-sharing convolution operation across different positions, which is used in the existing methods, causes that the training process of each parameter relies on the global gradient. This forces the network to learn trade-off and sub-optimal parameters. To address these problems, we propose a dynamic dilated pyramid module (DDPM), which uses RGB-depth mixed features to adaptively adjust convolution kernels for different input samples and processing locations. These kernels can capture rich semantic cues at multiple scales with the help of the pyramid structure and the dilated convolution. This design is capable of making more efficient convolution operations for current RGB features and promotes the network to obtain more flexible and targeted features for saliency prediction. Early deep learning-based SOD models <ref type="bibr" target="#b14">[15]</ref>, which use fully connected layers, destroy the spatial structure of the data. This issue is alleviated to some extent by using the fully convolutional network. But the intrinsic gridding operation and the repeated down-sampling lead to the loss of numerous details in the predicted results. Although many methods frequently combine shallower features to restore feature resolution, the improvement is still limited. While some approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref> leverage CRF post-processing to refine subtle structures, which has a large computational cost. In this work, we design a new hybrid enhanced loss function (HEL). The HEL encourages the consistency between the area around edges and the interior of objects, thereby achieving sharper boundaries and a solid saliency area.</p><p>Our main contributions are summarized as follows: -We propose a simple yet effective hierarchical dynamic filtering network (HDFNet) for RGB-D SOD. Especially, we provide a new perspective to utilize depth information. The depth and RGB features are combined to generate region-aware dynamic filters to guide the decoding in RGB stream. -We propose a hybrid enhanced loss and verify its effectiveness in both RGB and RGB-D SOD tasks. It can effectively optimize the details of predictions and enhance the consistency of salient regions without additional parameters. -We compare the proposed method with twelve state-of-the-art methods on eight datasets. It achieves the best performance under six evaluation metrics. Meanwhile, we implement a forward reasoning speed of 52 FPS on an NVIDIA GTX 1080 Ti GPU. The size of our VGG16-based model is about 170 MB ( <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Word</head><p>RGB-D Salient Object Detection. The early methods are mainly based on hand-crafted features, such as contrast <ref type="bibr" target="#b5">[6]</ref> and shape <ref type="bibr" target="#b7">[8]</ref>. Limited by the representation ability of the features, they can not cope with complex scenes. Please refer to <ref type="bibr" target="#b12">[13]</ref> for more details about traditional methods. In recent years, FCN-based methods have shown great potential and some of them achieve very good performance in the RGB-D SOD task <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13]</ref>. Chen and Li <ref type="bibr" target="#b1">[2]</ref> progressively combine the current depth/RGB features and the preceding fused feature by a series of convolution and element-wise addition operations to build the cross fusion modules. Recently, they concatenate depth and RGB features and feed them into an additional CNN stream to achieve multi-level cross-modal fusion <ref type="bibr" target="#b2">[3]</ref>. Wang and Gong <ref type="bibr" target="#b42">[43]</ref> respectively build a saliency prediction stream for RGB and depth inputs and then fuse their predictions and their preceding features to obtain final prediction via several convolutional layers. Zhao et. al <ref type="bibr" target="#b49">[50]</ref> insert a lightweight net between adjacent encoding blocks to compute a contrast map from the depth input and use it to enhance the features from the RGB stream. Piao et. al <ref type="bibr" target="#b36">[37]</ref> combine multi-level paired complementary features from RGB and depth streams by convolution and nonlinear operations. Fan et. al <ref type="bibr" target="#b12">[13]</ref> design a depth depurator to remove the low-quality depth input, and for high-quality one they feed the concatenated 4-channel input into a convolutional neural network to achieve cross-modal fusion. Different from these methods, we use the RGB-depth mixed features to generate "adaptive" multi-scale convolution kernels to filter and enhance the decoding features from the RGB stream. The overall architecture of HDFNet. The network is based on two-stream structure. The two encoders use the same network (such as VGG-16 <ref type="bibr" target="#b40">[41]</ref>, VGG-19 <ref type="bibr" target="#b40">[41]</ref>, or ResNet-50 <ref type="bibr" target="#b16">[17]</ref>), and are fed RGB and depth images, respectively. The details of HDFNet are introduced in Sec. 3.</p><p>Dynamic Filters. The works closely related to ours are <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b15">[16]</ref>. The conception of the dynamic filter is firstly proposed in video and stereo prediction task <ref type="bibr" target="#b20">[21]</ref>. The filter is utilized to enhance the representation of its corresponding input in a self-learning manner. While we use multi-modal information to generate multi-scale filters to dynamically strengthen the cross-modal complementarity and suppress the inter-modality incompatibility. Besides, the kernel computation in <ref type="bibr" target="#b20">[21]</ref> introduces a large number of parameters and is difficultly extended at multiple scales, which significantly increases parameters and causes optimization difficulties. To efficiently achieve hierarchical dynamic filters, we introduce the idea of depth-wise separable convolution <ref type="bibr" target="#b18">[19]</ref> and dilated convolution <ref type="bibr" target="#b48">[49]</ref>. In <ref type="bibr" target="#b15">[16]</ref>, the filters are computed by pooling the input feature. They share kernel parameters across different positions, which is only an image-specific filter generator. In contrast, we design position-specific and image-specific filters to provide cross-modal contextual guidance for the decoder. The parameter update of dynamic filters is determined by the gradients of local neighborhoods to achieve more targeted adjustments and guarantee the overall performance of optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we first introduce the overall structure of the proposed method and then detail two main components, including the dynamic dilated pyramid module (DDPM) and the hybrid enhanced loss (HEL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two Stream Structure</head><p>We build a two-stream network, which structure is shown in <ref type="figure">Fig. 2</ref>. It has two inputs: one is an RGB image and the other is a depth image, which corresponds to the RGB and depth streams, respectively. Through convolution blocks</p><formula xml:id="formula_0">{E i rgb } 5 i=1 and {E i d } 5 i=1</formula><p>in two encoding networks, we can obtain the intermediate features with different resolutions, which are recorded as f 1 , f 2 , f 3 , f 4 , f 5 from large to small. The third-level features still retain enough valid information. Besides, the shallower features contain more noise and also cause higher computational cost due to the larger resolution. To balance efficiency and effectiveness, we only utilize the features f 3 d , f 4 d , f 5 d from the deepest three blocks in the depth stream. These features are respectively combined with the features f 3 rgb , f 4 rgb , f 5 rgb from the RGB stream. Then, we use a dense block <ref type="bibr" target="#b19">[20]</ref> to build the transport layer, which combines rich and various receptive fields and generates powerful mixed features f Tm with both spatial structures and appearance details. These features are fed into the DDPM to produce multi-scale convolution kernels that are used to filter the features f D rgb from the decoder. The resulted features f M are merged in the top-down pathway by element-wise addition. After recovering the resolution layer by layer, we obtain the final prediction P , which is supervised by the ground truth G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Dilated Pyramid Module</head><p>In order to make more reasonable and effective use of the mixed features f Tm from the dense transport layer, we employ DDPMs to generate the adaptive kernel for decoding RGB features. The DDPMs contain two inputs: the mixed feature f Tm and the feature f D rgb from the decoder. On one hand, for specific position in feature maps f D rgb , we use kernel generation units (KGUs) to yield independent weight tensors, i.e. f g , that can cover a 3 × 3, 7 × 7 or 11 × 11 square neighborhood. KGUs are also a kind of dense structure <ref type="bibr" target="#b19">[20]</ref>. The module Algorithm 1: The operation process of adaptive convolution ⊗ related to KTU j in DDPM i .</p><formula xml:id="formula_1">Input: f i r = R(f i D rgb ) ∈ R N ×C ×H ×W , f i g j ∈ R N ×(9×C )×H ×W Output: f i B j ∈ R N ×C ×H ×W . 1 d ← j × 2 − 1; 2 pad f i r with 0 from (H , W ) to (H + 2 × d, W + 2 × d); 3 for n ← 0 to N − 1 do 4 for c ← 0 to C − 1 do 5 for h ← d to H + d − 1 do 6 for w ← d to W + d − 1 do 7 (f i B j ) [n,c,h,w] ← 1 l=−1 1 m=−1 {(f i g j ) [n,(l+1)×3+(m+1),h,w] ×(f i r ) [n,c,h+l×d,w+m×d] };</formula><p>contains 4 densely connected layers and each layer is connected to all the others in a feed-forward fashion, which can further strengthen feature propagation and expression capabilities, encourage feature reuse and greatly improve parameter efficiency. Then, by recombining kernel tensors and inserting different numbers of zeros, kernel transformation units (KTUs) construct regular convolution kernels with different dilation rates. Please see "KTU" shown in <ref type="figure" target="#fig_1">Fig. 3</ref> and introduced in Alg. 1 for a more intuitive presentation. On the other hand, after preliminary dimension reduction, the other input f D rgb is re-weighted and integrated into three parallel branches to obtain the enhanced features {f B j } 3 j=1 . Note that this is actually a channel-wise adjustment and the operation of each channel is independent. Finally, after concating and merging {f B j } 3 j=1 and the reduced f D rgb , the resulted features {f i M } 5 i=3 become more discriminative. The entire process can be formulated as follows:</p><formula xml:id="formula_2">f i M = DDPM i (f i D rgb , f i Tm ) = F(C(R(f i D rgb ), f i B 1 , f i B 2 , f i B 3 ) = F(C(R(f i D rgb ), KT U i 1 (KGU i 1 (f i Tm )) ⊗ R(f i D rgb ), KT U i 2 (KGU i 2 (f i Tm )) ⊗ R(f i D rgb ), KT U i 3 (KGU i 3 (f i Tm )) ⊗ R(f i D rgb ))),<label>(1)</label></formula><p>where f i M represents the feature from the DDPM i related to the f i D rgb . DDPM(·), KGU(·) and KT U(·) denote the operation of the corresponding module. R(·) is a 1×1 convolution operation, which is used to reduce the number of channels from 64 to 16. ⊗ is an adaptive convolution operation as shown in Alg. 1. C(·) is a concatenation operation and F(·) is a 3 × 3 convolution to fuse the concatenated features from different branches. More details is as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hybrid Enhanced Loss</head><p>No matter for RGB or RGB-D based SOD tasks, good prediction requires the salient area to be clearly and completely highlighted. This contains two aspects: one is the sharpness of boundaries and the other is the consistency of intra-class. We start with the loss function and design a new loss to constrain the edges and the fore-/background regions to separately achieve high-contrast predictions.</p><p>The common loss function in the SOD task is binary cross entropy (BCE). It is a pixel-level loss, which independently performs error calculation and supervision at different positions. The main form is as follows:</p><formula xml:id="formula_3">L bce = 1 N × H × W N n H h W w [g log p + (1 − g) log(1 − p)] ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">P = {p|0 &lt; p &lt; 1} ∈ R N ×1×H×W and G = {g|0 &lt; g &lt; 1} ∈ R N ×1×H×W</formula><p>respectively represent the prediction and the corresponding ground truch. N , H and W are the batchsize, height and width of the input data, respectively. It calculates the error between the ground truth g and the prediction p at each position, and the loss L bce accumulates and averages the errors of all positions.</p><p>In order to further enhance the strength of supervision at higher levels such as edges and regions, we specially constrain and optimize the regions near the edges. In particular, the loss is formulated as follows:</p><formula xml:id="formula_5">L e = H h W w (e * |p − g|) H h W w e , e = 0 if (G − P(G)) [h,w] = 0, 1 if (G − P(G)) [h,w] = 0,<label>(3)</label></formula><p>where L e represents the edge enhanced loss (EEL), and P(·) denotes the average pooling operation with a 5 × 5 slide window. In Equ. 3, we can obtain the local region near the contour of the ground truth by calculating e. In this region, the difference L e between the prediction p and the ground truth g can be calculated. Through this loss, the optimization process can target the contours of salient objects.</p><p>In addition, we also design a region enhanced loss (REL) to constrain the prediction of intra-class. By respectively calculating the prediction errors within the foreground class and the background class, fore-/background predictions can be independently optimized. Specifically, the REL L r is written as:</p><formula xml:id="formula_6">L r = N n (L f + L b )</formula><p>N ,</p><formula xml:id="formula_7">L f = H h W w (g − g * p) H h W w g , L b = H h W w (1 − g) * p H h W w (1 − g) ,<label>(4)</label></formula><p>where L f and L b denote the fore-/background losses, respectively. The losses compute the normalized prediction errors in the intra-class regions. They depict the region-level supervision. Finally, we integrate these three losses (L bce , L e and L r ) to obtain the hybrid enhanced loss (HEL), which can optimize the prediction at two different levels. The total loss is expressed as follows:</p><formula xml:id="formula_8">L = L bce + L e + L r .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To fully verify the effectiveness of the proposed method, we evaluated the results on eight benchmark datasets. LFSD <ref type="bibr" target="#b24">[25]</ref> is a small dataset that contains 100 images with depth information and human-labeled ground truths and is built for saliency detection on the light filed. NJUD [22] contains 1,985 groups of RGB, depth, and label images, which are collected from the Internet, 3D movies, and photographs taken by a Fuji W3 stereo camera. NLPR <ref type="bibr" target="#b34">[35]</ref> is also called RGBD1000, which contains 1,000 natural RGBD images captured by Microsoft Kinect together with the human-marked ground truth. RGBD135 <ref type="bibr" target="#b6">[7]</ref> is also named DES, which consists of 135 images about indoor scenes collected by Microsoft Kinect. SIP <ref type="bibr" target="#b12">[13]</ref> includes 1,000 images with many challenging situations from various outdoor scenarios and these images emphasize salient persons in real-world scenes. SSD <ref type="bibr" target="#b52">[53]</ref> contains 80 images picked up from three stereo movies. STEREO <ref type="bibr" target="#b32">[33]</ref> is also called SSB, which contains 1,000 stereoscopic images downloaded from the Internet. DUTRGBD <ref type="bibr" target="#b36">[37]</ref> is a new and large dataset and contains 800 indoor and 400 outdoor scenes paired with the depth maps and ground truths. For comprehensively and fairly evaluating different methods, we follow the setting of <ref type="bibr" target="#b36">[37]</ref>. On the DUTRGBD, we use 800 images for training and 400 images for testing. For the other seven datasets, we follow the data partition of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37]</ref> to use 1,485 samples from the NJUD and 700 samples from the NLPR as the training set and the remaining samples in these datasets are used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>There are six widely used metrics for evaluating RGB and RGB-D SOD models: Precision-Recall (PR) curve, F-measure <ref type="bibr" target="#b0">[1]</ref>, weighted F-measure <ref type="bibr" target="#b31">[32]</ref>, MAE <ref type="bibr" target="#b35">[36]</ref>, S-measure <ref type="bibr" target="#b10">[11]</ref> and E-measure <ref type="bibr" target="#b11">[12]</ref>. PR Curve. We use a series of fixed thresholds from 0 to 255 to binarize the gray prediction map, and then calculate several groups of precision (P re) and recall (Rec) with ground truth by P re = T P T P +F P and Rec = T P T P +F N . Based on them, we can plot a precision-recall curve to describe the performance of the model. F-measure <ref type="bibr" target="#b0">[1]</ref>. It is a region-based similarity metric and is formulated as the weighted harmonic mean (the weight is set to 0.3) of P re and Rec. In this paper, we employ the threshold changing from 0 to 255 to get F max , and use twice the mean value of the prediction P as the threshold to obtain F ada . In addition, since F-measure reflects the performance of the binary predictions under different thresholds, we evaluate the consistency and uniformity at the regional level according to F-measure threshold curves. weighted F-measure (F ω β ) <ref type="bibr" target="#b31">[32]</ref>. It is proposed to improve the existing metric F-measure. It defines a weighted precision, which is a measure of exactness, and a weighted recall, which is a measure of completeness and follows the form of F-measure. MAE <ref type="bibr" target="#b35">[36]</ref>. This metric estimates the approximation degree between the saliency map and ground-truth map, and it is normalized to [0, 1]. It focuses on pixel-level performance. S-measure (S m ) <ref type="bibr" target="#b10">[11]</ref>. It calculates the object-/region-aware structure similarities S o / S r between prediction and ground truth by the equation: S m = α · S o + (1 − α) · S r , α = 0.5. Emeasure (E m ) <ref type="bibr" target="#b11">[12]</ref>. This measure utilizes the mean-removed predictions and ground truths to compute the similarity, which characterizes both image-level statistics and local pixel matching.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Parameter setting. Two encoders of the proposed model are based on the same model, such as VGG-16 <ref type="bibr" target="#b40">[41]</ref>, VGG-19 <ref type="bibr" target="#b40">[41]</ref>, and ResNet-50 <ref type="bibr" target="#b16">[17]</ref>. In both encoders, only the convolutional layers in corresponding classification networks are retained, and the last pooling layer of VGG-16 and VGG-19 is removed at the same time. During the training phase, we use the weight parameters pretrained on the ImageNet to initialize the encoders. Also, since the depth image is a single channel data, we change the channel number of its corresponding input layer from 3 to 1, and its parameters are initialized randomly by PyTorch. The parameters of the remaining structures are all initialized randomly.</p><p>Training setting. During the training stage, we apply random horizontal flipping, random rotating as data augmentation for RGB images and depth images. In addition, we employ random color jittering and normalization for RGB images. We use the momentum SGD optimizer with a weight decay of 5e-4, an initial learning rate of 5e-3, and a momentum of 0.9. Besides, we apply a "poly" strategy <ref type="bibr" target="#b28">[29]</ref> with a factor of 0.9. The input images are resized to 320 × 320. We train the model for 30 epochs on an NVIDIA GTX 1080 Ti GPU with a batch size of 4 to obtain the final model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons</head><p>In order to fully demonstrate the effectiveness of the proposed method, we compared it with the existing twelve RGB-D based SOD models, including DES <ref type="bibr" target="#b5">[6]</ref>, DCMC <ref type="bibr" target="#b8">[9]</ref>, CDCP <ref type="bibr" target="#b53">[54]</ref>, DF <ref type="bibr" target="#b37">[38]</ref>, CTMF <ref type="bibr" target="#b14">[15]</ref>, PCANet <ref type="bibr" target="#b1">[2]</ref>, MMCI <ref type="bibr" target="#b3">[4]</ref>, TANet <ref type="bibr" target="#b2">[3]</ref>, AFNet <ref type="bibr" target="#b42">[43]</ref>, CPFP <ref type="bibr" target="#b49">[50]</ref>, DMRA <ref type="bibr" target="#b36">[37]</ref> and D3Net <ref type="bibr" target="#b12">[13]</ref>. For fair comparisons, all saliency maps of these methods are directly provided by authors or computed by their released codes. Besides, the codes and results of AFNet <ref type="bibr" target="#b42">[43]</ref> and D3Net <ref type="bibr" target="#b12">[13]</ref> on the DUTRGBD <ref type="bibr" target="#b36">[37]</ref> dataset are not publicly available. Therefore, their results on this dataset are not listed. Quantitative Evaluation. In Tab. 1, we list the results of all competitors on eight datasets and six metrics. It can be seen that the proposed method performs best on most datasets and achieve significant performance improvement. On the DUTRGBD <ref type="bibr" target="#b36">[37]</ref>, our models based on VGG-  <ref type="bibr" target="#b12">[13]</ref>. Because the existing RGB-D SOD datasets are relatively small, we propose a new calculation method to measure the performance of models. According to the proportion of each testing set in all testing datasets, the results on all datasets are weighted and summed to obtain an overall performance evaluation, which is listed in the row "AveMetric" in Tab. 1. It can be seen that our structure achieves similar and excellent results on different backbones, which shows that our structure has less dependence on the performance of the backbone. In addition, we show a scatter plot based on the average performance of each model on all datasets and the model size in <ref type="figure">Fig. 1</ref>. Our model has the smallest size while achieving the best result. We demonstrate the PR curves and the F-measure curves in <ref type="figure">Fig. 4</ref> and <ref type="figure" target="#fig_3">Fig. 5</ref>. Our approach (red solid line) achieves very good results on these datasets. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, our results are much flatter at most thresholds, which reflects that our prediction results are more uniform and consistent. Qualitative Evaluation. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we list some representative results. These examples include scenarios with varying complexity, as well as different types of objects, including cluttered background (Column 1 and 2), simple scene (Column 3 and 4), small objects (Column 5), complex objects (Column 6 and 7), large objects (Column 8), multiple objects (Column 9 and 10) and low contrast between foreground and background (Column 11 and 12). It can be seen that the proposed method can consistently produce more accurate and complete saliency maps with higher contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In this section, we perform ablation analysis over the main components of the HDFNet and further investigate their importance and contributions. Our base- line model, i.e. Model 1, uses the commonly used encoder-decoder structure, and all ablation experiments are based on the VGG-16 backbone. In the baseline model, the output features of the last three stages in the depth stream are added to the decoder after compressing the channel to 64 through an independent 1 × 1 convolution. In order to evaluate the benefits of cross-modal fusion at the dense transport layer (i.e. Model 6), we feed single-modal features into this layer to build Model 2 (i.e. "+T d ") and Model 4 (i.e. "+T rgb "). Thus, the followed dynamic filters in the DDPM will be determined only by depth features or RGB features, respectively. Dynamic Dilated Pyramid Module. Based on Model 2, Model 4, and Model 6, we add the dynamic dilated pyramid module to obtain Model 3, Model 5, and Model 7, respectively. In Tab. 2, we show the performance improvement contributed by different structures in terms of the weighted average metrics "AveMetric". It can be seen that the DDPM significantly improves performance. Specifically, by comparing Model 3, 5 and 7 with Model 2, 4 and 6, we achieve a relative improvement of 1.47%, 3.11% and 2.11% in terms of F ω β and 5.01%, 10.29% and 6.77% in terms of MAE, respectively. We can see that even without the HEL, the average performance of Model 7 already exceeds these existing models. More comparisons can be found in Appendix A.</p><p>In addition, we compare the design of the dynamic filter in DCM <ref type="bibr" target="#b15">[16]</ref> with ours. It can be seen that the proposed DDPM (Model 7) has obvious advantages over the DCM (Model 8), and it respectively increases by 3.91%, 5.60%, and 18.07% in terms of F ada , F ω β and MAE. In <ref type="figure" target="#fig_5">Fig. 7</ref>, we can see that the noise in depth images interferes with the final predictions. By the cross-modal guidance from the DDPMs, the interference is effectively suppressed. Hybrid Enhanced Loss. As shown in Tab. 2, the proposed hybrid enhanced loss brings huge performance improvements by comparing Model 7 with Model 12. We evaluate each component in the HEL <ref type="figure" target="#fig_7">(Model 9, 10, and 11</ref>) and all of <ref type="table" target="#tab_5">Table 2</ref>. Ablation experiments. +T d : Using a dense transport layer for depth features. +T rgb : Using a dense transport layer for RGB features. +DDPM: Using a DDPM after the transport layer. +DCM: Using the DCM <ref type="bibr" target="#b15">[16]</ref> after the transport layer. +Le: Using the edge loss as the auxiliary loss. +L f : Using the foreground loss as the auxiliary loss. +L b : Using the background loss as the auxiliary loss. them contribute to the final performance. In addition, the benefits of this loss are also clearly reflected in <ref type="figure" target="#fig_3">Fig. 5</ref> where the curves of the proposed model are more straight, and <ref type="figure" target="#fig_5">Fig. 7</ref> where the predictions of the model "B+R+D+M+L" have higher contrast than ones of the model "B+D+R+M". Since the design goal of the HEL is to solve the general requirements of SOD tasks, we evaluate its effectiveness on several recent RGB SOD models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref>. For a fair comparison, we retrain these models using the released code. Most of hyper-parameters are the same as the default values given by their corresponding code. The average performance "AveMetric" on five main RGB SOD datasets (DUTS <ref type="bibr" target="#b41">[42]</ref>, ECSSD <ref type="bibr" target="#b46">[47]</ref>, HKU-IS <ref type="bibr" target="#b23">[24]</ref>, PASCAL-S <ref type="bibr" target="#b25">[26]</ref> and DUT-OMRON <ref type="bibr" target="#b47">[48]</ref>) is shown in Tab. 2. More experimental details and results can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we revisit the role that depth information should play in the RGB-D based SOD task. We consider the characteristics of spatial structures contained in depth information and combine it with RGB information with rich appearance details. After that, the model generates adaptive filters with different receptive field sizes through the dynamic dilated pyramid module. It can make full use of semantic cues from multi-modal mixed features to achieve multi-scale cross-modal guidance, thereby enhancing the representation capabilities of the decoder. At the same time, we can obtain clearer predictions with the aid of additional region-level supervision to the regions around the edges and fore-/background regions. Expensive experiments on eight datasets and six metrics demonstrate the effectiveness of the designed components. The proposed approach achieves state-of-the-art performance with small model size and high running speed. Acknowledgements. This work was supported in part by the National Key R&amp;D Program of China #2018AAA0102003, National Natural Science Foundation of China #61876202, #61725202, #61751212 and #61829102, the Dalian Science and Technology Innovation Foundation #2019J12GX039, and the Fundamental Research Funds for the Central Universities #DUT20ZD212.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Effectiveness of the HEL</head><p>Tab 4 shows the performance gains of the proposed loss function in some recent RGB saliency models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref>. Here, "AveMetric" still denotes the weighted average results on all datasets and is consistent with the data in <ref type="table" target="#tab_5">Table 2</ref> of the original paper. It is worth noting that there are some differences in our experimental settings for these models. 1) R3Net <ref type="bibr" target="#b9">[10]</ref>: We use ResNeXt-101 <ref type="bibr" target="#b45">[46]</ref> as the backbone as the original paper. We only change the supervision for the final prediction to the proposed HEL. 2) CPD <ref type="bibr" target="#b44">[45]</ref>: ResNet-50 <ref type="bibr" target="#b16">[17]</ref> is used as the backbone. For each branch, we use the proposed HEL to supervise the prediction. 3) PoolNet <ref type="bibr" target="#b26">[27]</ref>: The backbone network is ResNet-50. We do not use the strategy of joint training with the edge and we apply the HEL on the final prediction. 4) GCPANet <ref type="bibr" target="#b4">[5]</ref>: We also use ResNet-50, and the HEL to supervise the final result with the same resolution as the input.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2. The overall architecture of HDFNet. The network is based on two-stream structure. The two encoders use the same network (such as VGG-16 [41], VGG-19 [41], or ResNet-50 [17]), and are fed RGB and depth images, respectively. The details of HDFNet are introduced in Sec. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The structure of the dynamic dilated pyramid module. The DDPM contains two submodules: kernel generation units (KGUs) and kernel transformation units (KTUs). KGUs generate adaptive kernel tensors and KTUs transform these tensors to the regular form of convolution kernels with different dilation rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>F-measure (vertical axis) threshold (horizontal axis) curves on eight RGB-D salient object detection datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The visualization results of some recent methods and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Visual comparisons for showing the benefits of the proposed components. GT: Ground truth; B: Baseline; D: Dense transport layer for depth features; R: Dense transport layer for RGB features; M: DDPM; L: HEL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Model No.</head><label></label><figDesc>Baseline +Td +Trgb +DDPM +DCM +Le +Lf +Lb Fmax Fada F ω β 0.714 0.716 0.072 0.831 0.830 14 0.832 0.731 0.740 0.069 0.835 0.844 CPD19 [45] 15 0.848 0.790 0.769 0.052 0.856 0.889 16 0.849 0.804 0.792 0.049 0.857 0.898 PoolNet19 [27] 15 0.832 0.755 0.728 0.060 0.841 0.865 16 0.861 0.811 0.799 0.046 0.862 0.902 GCPANet20 [5] 17 0.847 0.766 0.744 0.061 0.854 0.869 18 0.854 0.779 0.773 0.055 0.856 0.880</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fmax 0 .</head><label>0</label><figDesc>823 0.827 0.858 0.859 0.844 0.879 0.856 0.865 F ada 0.688 0.710 0.786 0.807 0.748 0.818 0.759 0.777 F ω β 0.701 0.726 0.774 0.800 0.733 0.814 0.744 0.778 MAE 0.071 0.069 0.047 0.045 0.055 0.040 0.055 0.049 Sm 0.821 0.826 0.863 0.865 0.849 0.873 0.860 0.864 Em 0.818 0.833 0.889 0.903 0.864 0.909 0.867 0.881 DUT-OMRON [48] Fmax 0.785 0.794 0.799 0.797 0.778 0.807 0.798 0.806 F ada 0.668 0.685 0.739 0.749 0.699 0.754 0.713 0.726 F ω β 0.669 0.693 0.714 0.732 0.668 0.736 0.689 0.715 MAE 0.079 0.078 0.059 0.058 0.066 0.054 0.070 0.066 Sm 0.812 0.816 0.828 0.826 0.810 0.829 0.826 0.826 Em 0.804 0.818 0.864 0.869 0.839 0.874 0.841 0.851 ECSSD [47] Fmax 0.927 0.932 0.934 0.939 0.921 0.943 0.933 0.933 F ada 0.858 0.866 0.908 0.918 0.882 0.919 0.892 0.896 F ω β 0.858 0.882 0.881 0.906 0.846 0.905 0.863 0.882 MAE 0.053 0.045 0.044 0.035 0.054 0.037 0.049 0.042 Sm 0.908 0.910 0.911 0.918 0.897 0.918 0.912 0.912 Em 0.911 0.918 0.941 0.951 0.920 0.947 0.930 0.935 HKU-IS [24]Fmax 0.914 0.917 0.922 0.927 0.915 0.931 0.923 0.928 F ada 0.842 0.856 0.886 0.899 0.869 0.903 0.878 0.885 F ω β 0.831 0.857 0.864 0.892 0.838 0.893 0.851 0.876 MAE 0.047 0.040 0.037 0.030 0.043 0.029 0.041 0.035 Sm 0.890 0.895 0.904 0.911 0.896 0.911 0.907 0.910 Em 0.918 0.929 0.945 0.956 0.937 0.958 0.943 0.947 PASCAL-S [26] Fmax 0.844 0.836 0.868 0.867 0.850 0.873 0.856 0.857 F ada 0.757 0.764 0.819 0.829 0.786 0.829 0.798 0.803 F ω β 0.733 0.750 0.784 0.806 0.745 0.806 0.764 0.782 MAE 0.101 0.091 0.079 0.072 0.093 0.073 0.087 0.081 Sm 0.813 0.822 0.842 0.844 0.822 0.843 0.837 0.835 Em 0.823 0.832 0.872 0.889 0.844 0.878 0.856 0.863 AveMetric Fmax 0.828 0.832 0.848 0.849 0.832 0.861 0.847 0.854 F ada 0.714 0.731 0.790 0.804 0.755 0.811 0.766 0.779 F ω β 0.716 0.740 0.769 0.792 0.728 0.799 0.744 0.773 MAE 0.072 0.069 0.052 0.049 0.060 0.046 0.061 0.055 Sm 0.831 0.835 0.856 0.857 0.841 0.862 0.854 0.856 Em 0.830 0.844 0.889 0.898 0.865 0.902 0.869 0.880</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 1. Comparisons in model size and accuracy.</figDesc><table><row><cell>0$(</cell><cell>2856</cell><cell></cell><cell>)Ada</cell><cell>2856</cell><cell></cell><cell>6m</cell><cell>2856</cell><cell></cell><cell>(m</cell><cell>2856</cell><cell></cell></row><row><cell></cell><cell>&amp;3)3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>'05$</cell><cell>3&amp;$1HW</cell><cell>00&amp;, 7$1HW</cell><cell>&amp;3)3 '05$</cell><cell>3&amp;$1HW</cell><cell>00&amp;, 7$1HW</cell><cell>&amp;3)3 '05$</cell><cell>3&amp;$1HW</cell><cell>00&amp;, 7$1HW</cell><cell>&amp;3)3 '05$</cell><cell>3&amp;$1HW</cell><cell>7$1HW 00&amp;,</cell></row><row><cell></cell><cell cols="2">0RGHO6L]H×10 3 0%</cell><cell></cell><cell cols="2">0RGHO6L]H×10 3 0%</cell><cell></cell><cell cols="2">0RGHO6L]H×10 3 0%</cell><cell></cell><cell cols="2">0RGHO6L]H×10 3 0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results (↑: Fmax, F ada<ref type="bibr" target="#b0">[1]</ref>, F ω β<ref type="bibr" target="#b31">[32]</ref>, Sm<ref type="bibr" target="#b10">[11]</ref> and Em<ref type="bibr" target="#b11">[12]</ref>; ↓: MAE<ref type="bibr" target="#b35">[36]</ref>) of different RGB-D SOD methods across eight datasets. The best results are highlight in red. : Traditional methods. †: VGG-16<ref type="bibr" target="#b40">[41]</ref> as backbone. ‡: VGG-19<ref type="bibr" target="#b40">[41]</ref> as backbone.: ResNet-50<ref type="bibr" target="#b16">[17]</ref> as backbone. -: No data available. DES 14 [6] DCMC 16 [9] CDCP 17 [54] DF † 17 [38] CTMF † 18 [15] PCANet † 18 [2] MMCI † 19 [4] TANet † 19 [3] AFNet † 19 [43] CPFP † 19 [50] OURS † DMRA ‡ 19 [37] OURS ‡ D3Net 19 [13] OURS</figDesc><table><row><cell cols="2">Fmax Fada F ω β MAE Metric LFSD [25] Sm</cell><cell>0.377 0.227 0.274 0.416 0.440</cell><cell>0.850 0.815 0.601 0.155 0.754</cell><cell>0.680 0.634 0.518 0.199 0.658</cell><cell>0.854 0.810 0.642 0.142 0.786</cell><cell>0.815 0.781 0.696 0.120 0.796</cell><cell>0.829 0.793 0.716 0.112 0.800</cell><cell>0.813 0.779 0.663 0.132 0.787</cell><cell>0.827 0.794 0.719 0.111 0.801</cell><cell>0.780 0.742 0.671 0.133 0.738</cell><cell>0.850 0.813 0.775 0.088 0.828</cell><cell>0.860 0.831 0.792 0.085 0.847</cell><cell>0.872 0.849 0.811 0.076 0.847</cell><cell>0.858 0.833 0.793 0.083 0.844</cell><cell>0.849 0.801 0.756 0.099 0.832</cell><cell>0.883 0.843 0.806 0.076 0.854</cell></row><row><cell></cell><cell>Em</cell><cell>0.492</cell><cell>0.842</cell><cell>0.737</cell><cell>0.841</cell><cell>0.851</cell><cell>0.856</cell><cell>0.840</cell><cell>0.851</cell><cell>0.810</cell><cell>0.867</cell><cell>0.883</cell><cell>0.899</cell><cell>0.886</cell><cell>0.860</cell><cell>0.891</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.328</cell><cell>0.769</cell><cell>0.661</cell><cell>0.789</cell><cell>0.857</cell><cell>0.887</cell><cell>0.868</cell><cell>0.888</cell><cell>0.804</cell><cell>0.890</cell><cell>0.924</cell><cell>0.896</cell><cell>0.922</cell><cell>0.903</cell><cell>0.922</cell></row><row><cell>NJUD [22]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.165 0.234 0.448 0.413</cell><cell>0.715 0.497 0.167 0.703</cell><cell>0.618 0.510 0.182 0.672</cell><cell>0.744 0.545 0.151 0.735</cell><cell>0.788 0.720 0.085 0.849</cell><cell>0.844 0.803 0.059 0.877</cell><cell>0.813 0.739 0.079 0.859</cell><cell>0.844 0.805 0.061 0.878</cell><cell>0.768 0.696 0.100 0.772</cell><cell>0.837 0.828 0.053 0.878</cell><cell>0.894 0.881 0.037 0.911</cell><cell>0.872 0.847 0.051 0.885</cell><cell>0.887 0.877 0.038 0.911</cell><cell>0.840 0.833 0.051 0.895</cell><cell>0.889 0.877 0.038 0.908</cell></row><row><cell></cell><cell>Em</cell><cell>0.491</cell><cell>0.796</cell><cell>0.751</cell><cell>0.818</cell><cell>0.866</cell><cell>0.909</cell><cell>0.882</cell><cell>0.909</cell><cell>0.847</cell><cell>0.900</cell><cell>0.934</cell><cell>0.920</cell><cell>0.932</cell><cell>0.901</cell><cell>0.932</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.695</cell><cell>0.413</cell><cell>0.687</cell><cell>0.752</cell><cell>0.841</cell><cell>0.864</cell><cell>0.841</cell><cell>0.876</cell><cell>0.816</cell><cell>0.883</cell><cell>0.917</cell><cell>0.888</cell><cell>0.919</cell><cell>0.904</cell><cell>0.927</cell></row><row><cell>NLPR [35]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.583 0.254 0.300 0.582</cell><cell>0.328 0.259 0.196 0.550</cell><cell>0.591 0.501 0.114 0.724</cell><cell>0.683 0.516 0.100 0.769</cell><cell>0.724 0.679 0.056 0.860</cell><cell>0.795 0.762 0.044 0.873</cell><cell>0.730 0.676 0.059 0.856</cell><cell>0.796 0.780 0.041 0.886</cell><cell>0.747 0.693 0.058 0.799</cell><cell>0.818 0.807 0.038 0.884</cell><cell>0.878 0.869 0.027 0.916</cell><cell>0.855 0.839 0.031 0.898</cell><cell>0.883 0.871 0.027 0.915</cell><cell>0.834 0.826 0.034 0.906</cell><cell>0.889 0.882 0.023 0.923</cell></row><row><cell></cell><cell>Em</cell><cell>0.760</cell><cell>0.685</cell><cell>0.786</cell><cell>0.840</cell><cell>0.869</cell><cell>0.916</cell><cell>0.872</cell><cell>0.916</cell><cell>0.884</cell><cell>0.920</cell><cell>0.948</cell><cell>0.942</cell><cell>0.951</cell><cell>0.934</cell><cell>0.957</cell></row><row><cell>RGBD135 [7]</cell><cell>Fmax Fada F ω β MAE Sm Em</cell><cell>0.800 0.695 0.301 0.288 0.632 0.817</cell><cell>0.311 0.234 0.169 0.196 0.469 0.676</cell><cell>0.651 0.594 0.478 0.120 0.709 0.810</cell><cell>0.625 0.573 0.392 0.131 0.685 0.806</cell><cell>0.865 0.778 0.686 0.055 0.863 0.911</cell><cell>0.842 0.774 0.711 0.050 0.843 0.912</cell><cell>0.839 0.762 0.650 0.065 0.848 0.904</cell><cell>0.853 0.795 0.740 0.046 0.858 0.919</cell><cell>0.775 0.730 0.641 0.068 0.770 0.874</cell><cell>0.882 0.829 0.787 0.038 0.872 0.927</cell><cell>0.934 0.919 0.902 0.020 0.932 0.973</cell><cell>0.906 0.867 0.843 0.030 0.899 0.944</cell><cell>0.941 0.918 0.913 0.017 0.937 0.976</cell><cell>0.917 0.876 0.831 0.030 0.904 0.956</cell><cell>0.932 0.912 0.895 0.021 0.926 0.971</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.720</cell><cell>0.680</cell><cell>0.544</cell><cell>0.704</cell><cell>0.720</cell><cell>0.860</cell><cell>0.840</cell><cell>0.851</cell><cell>0.756</cell><cell>0.870</cell><cell>0.904</cell><cell>0.847</cell><cell>0.907</cell><cell>0.882</cell><cell>0.910</cell></row><row><cell>SIP [13]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.644 0.342 0.298 0.616</cell><cell>0.645 0.413 0.186 0.683</cell><cell>0.495 0.397 0.224 0.595</cell><cell>0.673 0.406 0.185 0.653</cell><cell>0.684 0.535 0.139 0.716</cell><cell>0.825 0.768 0.071 0.842</cell><cell>0.795 0.711 0.086 0.833</cell><cell>0.809 0.748 0.075 0.835</cell><cell>0.705 0.617 0.118 0.720</cell><cell>0.819 0.788 0.064 0.850</cell><cell>0.863 0.835 0.050 0.878</cell><cell>0.815 0.734 0.088 0.800</cell><cell>0.870 0.844 0.047 0.885</cell><cell>0.831 0.793 0.063 0.864</cell><cell>0.875 0.848 0.047 0.886</cell></row><row><cell></cell><cell>Em</cell><cell>0.751</cell><cell>0.786</cell><cell>0.722</cell><cell>0.794</cell><cell>0.824</cell><cell>0.900</cell><cell>0.886</cell><cell>0.894</cell><cell>0.815</cell><cell>0.899</cell><cell>0.920</cell><cell>0.858</cell><cell>0.924</cell><cell>0.903</cell><cell>0.924</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.260</cell><cell>0.750</cell><cell>0.576</cell><cell>0.763</cell><cell>0.755</cell><cell>0.844</cell><cell>0.823</cell><cell>0.834</cell><cell>0.735</cell><cell>0.801</cell><cell>0.872</cell><cell>0.858</cell><cell>0.883</cell><cell>0.872</cell><cell>0.885</cell></row><row><cell>SSD [53]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.073 0.172 0.500 0.341</cell><cell>0.684 0.480 0.168 0.706</cell><cell>0.524 0.429 0.219 0.603</cell><cell>0.709 0.536 0.151 0.741</cell><cell>0.709 0.622 0.100 0.776</cell><cell>0.786 0.733 0.063 0.842</cell><cell>0.748 0.662 0.082 0.813</cell><cell>0.766 0.727 0.063 0.839</cell><cell>0.694 0.589 0.118 0.714</cell><cell>0.726 0.708 0.082 0.807</cell><cell>0.844 0.808 0.048 0.866</cell><cell>0.821 0.787 0.058 0.856</cell><cell>0.847 0.819 0.046 0.875</cell><cell>0.793 0.780 0.058 0.866</cell><cell>0.842 0.821 0.045 0.879</cell></row><row><cell></cell><cell>Em</cell><cell>0.475</cell><cell>0.790</cell><cell>0.714</cell><cell>0.801</cell><cell>0.838</cell><cell>0.890</cell><cell>0.860</cell><cell>0.886</cell><cell>0.803</cell><cell>0.832</cell><cell>0.913</cell><cell>0.898</cell><cell>0.911</cell><cell>0.892</cell><cell>0.911</cell></row><row><cell>STEREO [33]</cell><cell>Fmax Fada F ω β MAE Sm Em</cell><cell>0.738 0.594 0.375 0.295 0.642 0.696</cell><cell>0.789 0.742 0.520 0.148 0.731 0.831</cell><cell>0.704 0.666 0.558 0.149 0.713 0.796</cell><cell>0.789 0.742 0.549 0.141 0.757 0.838</cell><cell>0.848 0.771 0.698 0.086 0.848 0.870</cell><cell>0.875 0.826 0.778 0.064 0.875 0.907</cell><cell>0.877 0.829 0.760 0.068 0.873 0.905</cell><cell>0.878 0.835 0.787 0.060 0.871 0.916</cell><cell>0.848 0.807 0.752 0.075 0.825 0.887</cell><cell>0.889 0.830 0.817 0.051 0.879 0.907</cell><cell>0.918 0.879 0.863 0.039 0.906 0.937</cell><cell>0.802 0.762 0.647 0.087 0.752 0.816</cell><cell>0.916 0.875 0.859 0.040 0.903 0.934</cell><cell>0.897 0.833 0.815 0.054 0.891 0.911</cell><cell>0.910 0.867 0.853 0.041 0.900 0.931</cell></row><row><cell>DUTRGBD [37]</cell><cell>Fmax Fada F ω β MAE Sm Em</cell><cell>0.770 0.667 0.380 0.280 0.659 0.751</cell><cell>0.444 0.405 0.284 0.243 0.499 0.712</cell><cell>0.658 0.633 0.521 0.159 0.687 0.794</cell><cell>0.774 0.747 0.536 0.145 0.729 0.842</cell><cell>0.842 0.792 0.682 0.097 0.831 0.882</cell><cell>0.809 0.760 0.688 0.100 0.801 0.863</cell><cell>0.804 0.753 0.628 0.112 0.791 0.856</cell><cell>0.823 0.778 0.705 0.093 0.808 0.871</cell><cell>------</cell><cell>0.787 0.735 0.638 0.100 0.749 0.815</cell><cell>0.926 0.892 0.865 0.040 0.905 0.938</cell><cell>0.908 0.883 0.852 0.048 0.887 0.930</cell><cell>0.934 0.894 0.871 0.039 0.911 0.941</cell><cell>------</cell><cell>0.930 0.885 0.864 0.041 0.907 0.938</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.654</cell><cell>0.666</cell><cell>0.642</cell><cell>0.756</cell><cell>0.811</cell><cell>0.861</cell><cell>0.850</cell><cell>0.862</cell><cell>0.801</cell><cell>0.868</cell><cell>0.914</cell><cell>0.855</cell><cell>0.915</cell><cell>0.893</cell><cell>0.915</cell></row><row><cell>AveMetric</cell><cell>Fada F ω β MAE Sm</cell><cell>0.534 0.325 0.325 0.585</cell><cell>0.618 0.425 0.179 0.661</cell><cell>0.595 0.491 0.174 0.669</cell><cell>0.714 0.502 0.151 0.721</cell><cell>0.747 0.652 0.099 0.809</cell><cell>0.814 0.761 0.068 0.853</cell><cell>0.794 0.712 0.081 0.844</cell><cell>0.815 0.764 0.067 0.853</cell><cell>0.755 0.684 0.093 0.773</cell><cell>0.813 0.784 0.061 0.853</cell><cell>0.878 0.857 0.041 0.898</cell><cell>0.822 0.756 0.069 0.824</cell><cell>0.878 0.859 0.041 0.900</cell><cell>0.834 0.810 0.055 0.883</cell><cell>0.877 0.858 0.041 0.899</cell></row><row><cell></cell><cell>Em</cell><cell>0.686</cell><cell>0.781</cell><cell>0.765</cell><cell>0.822</cell><cell>0.859</cell><cell>0.899</cell><cell>0.885</cell><cell>0.901</cell><cell>0.853</cell><cell>0.892</cell><cell>0.933</cell><cell>0.876</cell><cell>0.933</cell><cell>0.909</cell><cell>0.932</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>16, VGG-19 and ResNet-50 have surpassed the second-best model DMRA [37] by 2.02%, 2.85% and 2.45% on F max , and 16.09%, 17.88% and 13.56% on MAE. At the same time, on the recent dataset SIP [13], they have increased by 3.83%, 4.65% and 5.23% on F ada , 5.22%, 6.37% and 6.84% on F ω β , and 20.94%, 24.65% and 24.91% on MAE, over the D3Net</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>of the original paper, we show the weighted average results of each model in terms of six metrics. In Sec. A.1 and Sec A.2 of this document, we respectively list the results of these models across different datasets.This supplementary document is organized as follows: -More details about the performance contributed by different components in the proposed HDFNet.-More detailed comparisons of RGB SOD models with the HEL and without the HEL. Model 2 vs. Model 3: Effectiveness of DDPM using only depth features to compute dynamic filters. 2. Model 4 vs. Model 5: Effectiveness of DDPM using only RGB features to compute dynamic filters. 3. Model 6 vs. Model 7 vs. Model 8: Effectiveness of DDPM using twomodality features to compute dynamic filters. 4. Model 9 vs. Model 10 vs. Model 11 vs. Model 12: Effectiveness of three components in the HEL (L e , L f and L b ) and the overall HEL.</figDesc><table><row><cell>A.1 Ablation Study</cell></row><row><cell>Tab. 3 shows the performance improvement contributed by different components.</cell></row><row><cell>Note that Model 2, 4 and 6 without the DDPM directly combine the features of</cell></row><row><cell>dense transport layer into the decoder by element-wise addition instead of using</cell></row><row><cell>convolution operation of Model 3, 5, 7.</cell></row><row><cell>The experiments in Tab 3 are divided into different groups:</cell></row><row><cell>1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiments of different components. "Model " corresponds to the model "No. " inTable 2of the original paper. In each set of comparative experiments, we emphasize the best test results in red.</figDesc><table><row><cell></cell><cell>Metric</cell><cell>Baseline</cell><cell cols="2">Depth Input</cell><cell cols="2">DDPM RGB Input</cell><cell></cell><cell>RGB-D Input</cell><cell></cell><cell>Le</cell><cell>Lf</cell><cell>HEL</cell><cell>Lb</cell><cell>ALL</cell></row><row><cell></cell><cell></cell><cell cols="13">Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Model 8 Model 9 Model 10 Model 11 Model 12</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.808</cell><cell>0.819</cell><cell>0.832</cell><cell>0.792</cell><cell>0.825</cell><cell>0.846</cell><cell>0.851</cell><cell>0.809</cell><cell>0.846</cell><cell>0.859</cell><cell></cell><cell>0.858</cell><cell>0.860</cell></row><row><cell>LFSD [25]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.776 0.703 0.116 0.800</cell><cell>0.778 0.698 0.117 0.801</cell><cell>0.788 0.716 0.113 0.815</cell><cell>0.762 0.676 0.124 0.782</cell><cell>0.796 0.722 0.114 0.816</cell><cell>0.816 0.745 0.099 0.833</cell><cell>0.812 0.747 0.101 0.835</cell><cell>0.760 0.685 0.129 0.791</cell><cell>0.819 0.772 0.092 0.838</cell><cell>0.806 0.765 0.092 0.838</cell><cell cols="2">0.837 0.773 0.089 0.847</cell><cell>0.831 0.792 0.085 0.847</cell></row><row><cell></cell><cell>Em</cell><cell>0.846</cell><cell>0.845</cell><cell>0.847</cell><cell>0.832</cell><cell>0.852</cell><cell>0.867</cell><cell>0.860</cell><cell>0.826</cell><cell>0.869</cell><cell>0.867</cell><cell></cell><cell>0.878</cell><cell>0.883</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.889</cell><cell>0.897</cell><cell>0.902</cell><cell>0.891</cell><cell>0.899</cell><cell>0.909</cell><cell>0.916</cell><cell>0.892</cell><cell>0.925</cell><cell>0.920</cell><cell></cell><cell>0.916</cell><cell>0.924</cell></row><row><cell>NJUD [22]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.840 0.801 0.061 0.880</cell><cell>0.848 0.807 0.059 0.887</cell><cell>0.849 0.819 0.055 0.891</cell><cell>0.851 0.809 0.057 0.884</cell><cell>0.856 0.826 0.054 0.892</cell><cell>0.858 0.827 0.051 0.898</cell><cell>0.872 0.846 0.047 0.905</cell><cell>0.843 0.808 0.058 0.884</cell><cell>0.895 0.876 0.039 0.911</cell><cell>0.867 0.858 0.043 0.903</cell><cell></cell><cell>0.886 0.857 0.045 0.906</cell><cell>0.894 0.881 0.037 0.911</cell></row><row><cell></cell><cell>Em</cell><cell>0.897</cell><cell>0.903</cell><cell>0.905</cell><cell>0.907</cell><cell>0.907</cell><cell>0.908</cell><cell>0.916</cell><cell>0.903</cell><cell>0.932</cell><cell>0.921</cell><cell></cell><cell>0.923</cell><cell>0.934</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.882</cell><cell>0.886</cell><cell>0.889</cell><cell>0.890</cell><cell>0.899</cell><cell>0.900</cell><cell>0.908</cell><cell>0.885</cell><cell>0.910</cell><cell>0.907</cell><cell></cell><cell>0.912</cell><cell>0.917</cell></row><row><cell>NLPR [35]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.819 0.790 0.042 0.889</cell><cell>0.810 0.786 0.042 0.892</cell><cell>0.810 0.801 0.039 0.899</cell><cell>0.830 0.802 0.038 0.897</cell><cell>0.845 0.824 0.034 0.906</cell><cell>0.828 0.809 0.038 0.900</cell><cell>0.848 0.835 0.033 0.915</cell><cell>0.819 0.804 0.039 0.897</cell><cell>0.882 0.864 0.029 0.916</cell><cell>0.829 0.834 0.032 0.902</cell><cell></cell><cell>0.869 0.847 0.030 0.912</cell><cell>0.878 0.869 0.027 0.916</cell></row><row><cell></cell><cell>Em</cell><cell>0.925</cell><cell>0.921</cell><cell>0.921</cell><cell>0.929</cell><cell>0.935</cell><cell>0.927</cell><cell>0.936</cell><cell>0.925</cell><cell>0.948</cell><cell>0.929</cell><cell cols="2">0.949</cell><cell>0.948</cell></row><row><cell>RGBD135 [7]</cell><cell>Fmax Fada F ω β MAE Sm Em</cell><cell>0.892 0.834 0.770 0.040 0.873 0.929</cell><cell>0.910 0.864 0.807 0.035 0.898 0.950</cell><cell>0.904 0.865 0.819 0.034 0.907 0.959</cell><cell>0.889 0.847 0.774 0.039 0.873 0.934</cell><cell>0.894 0.841 0.790 0.037 0.885 0.934</cell><cell>0.917 0.883 0.835 0.029 0.910 0.962</cell><cell>0.927 0.890 0.853 0.027 0.922 0.965</cell><cell>0.879 0.828 0.770 0.042 0.878 0.930</cell><cell>0.931 0.917 0.889 0.022 0.925 0.974</cell><cell>0.933 0.873 0.873 0.023 0.925 0.959</cell><cell></cell><cell>0.926 0.906 0.855 0.026 0.915 0.971</cell><cell>0.934 0.919 0.902 0.020 0.932 0.973</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.865</cell><cell>0.875</cell><cell>0.888</cell><cell>0.875</cell><cell>0.886</cell><cell>0.888</cell><cell>0.889</cell><cell>0.861</cell><cell>0.897</cell><cell>0.899</cell><cell></cell><cell>0.897</cell><cell>0.904</cell></row><row><cell>SIP [13]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.810 0.753 0.074 0.853</cell><cell>0.821 0.761 0.072 0.858</cell><cell>0.832 0.783 0.064 0.872</cell><cell>0.832 0.772 0.069 0.858</cell><cell>0.843 0.795 0.063 0.867</cell><cell>0.844 0.788 0.065 0.867</cell><cell>0.844 0.797 0.062 0.872</cell><cell>0.806 0.749 0.074 0.849</cell><cell>0.865 0.829 0.052 0.879</cell><cell>0.840 0.812 0.058 0.870</cell><cell></cell><cell>0.863 0.819 0.056 0.878</cell><cell>0.863 0.835 0.050 0.878</cell></row><row><cell></cell><cell>Em</cell><cell>0.893</cell><cell>0.894</cell><cell>0.903</cell><cell>0.902</cell><cell>0.909</cell><cell>0.903</cell><cell>0.906</cell><cell>0.893</cell><cell>0.916</cell><cell>0.909</cell><cell></cell><cell>0.914</cell><cell>0.920</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.844</cell><cell>0.859</cell><cell>0.863</cell><cell>0.825</cell><cell>0.853</cell><cell>0.862</cell><cell>0.861</cell><cell>0.841</cell><cell>0.877</cell><cell>0.880</cell><cell></cell><cell>0.875</cell><cell>0.872</cell></row><row><cell>SSD [53]</cell><cell>Fada F ω β MAE Sm</cell><cell>0.770 0.730 0.072 0.841</cell><cell>0.790 0.744 0.067 0.854</cell><cell>0.786 0.750 0.063 0.859</cell><cell>0.782 0.735 0.068 0.840</cell><cell>0.802 0.758 0.059 0.855</cell><cell>0.799 0.759 0.058 0.859</cell><cell>0.818 0.784 0.054 0.871</cell><cell>0.791 0.739 0.067 0.846</cell><cell>0.843 0.806 0.047 0.871</cell><cell>0.811 0.787 0.054 0.863</cell><cell></cell><cell>0.828 0.789 0.049 0.870</cell><cell>0.844 0.808 0.048 0.866</cell></row><row><cell></cell><cell>Em</cell><cell>0.863</cell><cell>0.874</cell><cell>0.876</cell><cell>0.871</cell><cell>0.897</cell><cell>0.887</cell><cell>0.902</cell><cell>0.885</cell><cell>0.911</cell><cell>0.888</cell><cell></cell><cell>0.899</cell><cell>0.913</cell></row><row><cell>STEREO [33]</cell><cell>Fmax Fada F ω β MAE Sm Em</cell><cell>0.883 0.822 0.780 0.062 0.873 0.901</cell><cell>0.871 0.806 0.762 0.068 0.865 0.896</cell><cell>0.861 0.787 0.753 0.071 0.858 0.883</cell><cell>0.898 0.853 0.807 0.055 0.888 0.919</cell><cell>0.906 0.863 0.829 0.048 0.901 0.924</cell><cell>0.900 0.846 0.809 0.053 0.892 0.918</cell><cell>0.909 0.857 0.829 0.049 0.901 0.922</cell><cell>0.895 0.843 0.805 0.055 0.890 0.914</cell><cell>0.912 0.877 0.854 0.042 0.903 0.932</cell><cell>0.911 0.841 0.826 0.048 0.891 0.917</cell><cell></cell><cell>0.910 0.875 0.843 0.045 0.903 0.931</cell><cell>0.918 0.879 0.863 0.039 0.906 0.937</cell></row><row><cell>DUTRGBD [37]</cell><cell>Fmax Fada F ω β MAE Sm Em</cell><cell>0.875 0.817 0.740 0.079 0.853 0.893</cell><cell>0.886 0.822 0.749 0.076 0.865 0.899</cell><cell>0.901 0.846 0.784 0.066 0.877 0.912</cell><cell>0.893 0.839 0.772 0.067 0.874 0.911</cell><cell>0.916 0.872 0.817 0.054 0.892 0.928</cell><cell>0.911 0.858 0.800 0.059 0.886 0.915</cell><cell>0.920 0.871 0.820 0.054 0.895 0.922</cell><cell>0.872 0.812 0.742 0.077 0.859 0.896</cell><cell>0.926 0.892 0.857 0.044 0.908 0.937</cell><cell>0.922 0.861 0.827 0.052 0.891 0.921</cell><cell></cell><cell>0.924 0.890 0.840 0.050 0.896 0.930</cell><cell>0.926 0.892 0.865 0.040 0.905 0.938</cell></row><row><cell></cell><cell>Fmax</cell><cell>0.875</cell><cell>0.879</cell><cell>0.882</cell><cell>0.884</cell><cell>0.896</cell><cell>0.898</cell><cell>0.904</cell><cell>0.878</cell><cell>0.909</cell><cell>0.909</cell><cell></cell><cell>0.907</cell><cell>0.914</cell></row><row><cell>AveMetric</cell><cell>Fada F ω β MAE Sm</cell><cell>0.819 0.768 0.067 0.865</cell><cell>0.820 0.768 0.066 0.868</cell><cell>0.820 0.780 0.063 0.873</cell><cell>0.839 0.787 0.060 0.874</cell><cell>0.852 0.811 0.054 0.886</cell><cell>0.846 0.803 0.056 0.884</cell><cell>0.856 0.820 0.052 0.893</cell><cell>0.823 0.777 0.064 0.871</cell><cell>0.878 0.849 0.044 0.898</cell><cell>0.845 0.827 0.050 0.887</cell><cell></cell><cell>0.874 0.836 0.048 0.895</cell><cell>0.878 0.857 0.041 0.898</cell></row><row><cell></cell><cell>Em</cell><cell>0.898</cell><cell>0.899</cell><cell>0.900</cell><cell>0.909</cell><cell>0.916</cell><cell>0.913</cell><cell>0.918</cell><cell>0.903</cell><cell>0.929</cell><cell>0.916</cell><cell></cell><cell>0.926</cell><cell>0.933</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of RGB SOD models with the HEL (w) and without the HEL (w/o). The best result of each group is highlight in red.</figDesc><table><row><cell>Metric</cell><cell>R3Net18 [10] CPD19 [45] PoolNet19 [27] GCPANet20 [5] w/o w w/o w w/o w w/o w</cell></row><row><cell>DUTS [42]</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global context-aware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Internet Multimedia Computing and Service</title>
		<meeting>the International Conference on Internet Multimedia Computing and Service</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Internet Multimedia Computing and Service</title>
		<meeting>the International Conference on Internet Multimedia Computing and Service</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<title level="m">An in depth view of saliency</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">R3net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rethinking rgb-d salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06781</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic multi-scale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference and Workshop on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection using anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">F3net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency-based discriminant tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Region-based saliency detection and its application in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W H</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive fusion for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Saliency detection via graphbased manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.071224" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Egnet:edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Suppress and balance: A simple gated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
