<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Semi-supervised Learning for Text Classification Under Large-Scale Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zĳun</forename><surname>Sun</surname></persName>
							<email>zĳun_sun@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Fan</surname></persName>
							<email>fanchun@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Center</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><forename type="middle">♣</forename><surname>Xiaofei</surname></persName>
							<email>xiaofei_sun@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
							<email>yuxian_meng@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiwei_li@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Semi-supervised Learning for Text Classification Under Large-Scale Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of semi-supervised learning is to utilize the unlabeled, in-domain dataset to improve models trained on the labeled dataset . Under the context of large-scale languagemodel (LM) pretraining, how we can make the best use of is poorly understood: Is semisupervised learning still beneficial with the presence of large-scale pretraining? Should be used for in-domain LM pretraining or pseudo-label generation? How should the pseudo-label based semi-supervised model be actually implemented? How different semisupervised strategies (e.g., self-learning) affect performances regarding of different sizes, of different sizes, etc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we conduct comprehensive studies on semi-supervised learning in the task of text classification under the context of largescale LM pretraining. Our studies shed important lights on the behavior of semi-supervised learning methods. We find that: (1) with the presence of in-domain LM pretraining on , open-domain LM pretraining <ref type="bibr" target="#b17">(Devlin et al., 2018)</ref> is unnecessary, and we are able to achieve better performance with pretraining on the in-domain dataset ; (2) both the in-domain pretraining strategy and the pseudolabel based strategy introduce significant performance boosts, with the former performing better with larger , the latter performing better with smaller , and the combination leading to the largest performance gain; (3) vanilla selftraining (pretraining first on the pseudo-label dataset and then fine-tuning on ) yields better performances when is small, while joint training on the combination of and yields better performances when is large.</p><p>Using semi-supervised learning strategies, we are able to achieve a performance of around 93.8% accuracy with only 50 training data points on the IMDB dataset, and a competitive performance of 96.6% with the full IMDB dataset. Our work marks an initial step toward understanding the behavior of semi-supervised learning models under the context of largescale pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Because of the fact that obtaining supervised training labels is costly and time-intensive, and that unlabeled data is relatively easy to obtain, semisupervised learning <ref type="bibr" target="#b8">(Chapelle et al., 2006;</ref><ref type="bibr" target="#b97">Zhu, 2005)</ref>, which utilizes in-domain unlabeled data to improve models trained on the labeled dataset , is of growing interest. Under the context of large-scale of language model pretraining <ref type="bibr" target="#b17">(Devlin et al., 2018;</ref><ref type="bibr" target="#b94">Yang et al., 2019;</ref><ref type="bibr" target="#b4">Bao et al., 2020;</ref><ref type="bibr" target="#b32">Joshi et al., 2020)</ref>, where a language model is pretrained on an extremely large, open-domain dataset (denoted by largeU, with |largeU| |U|), how we can make the best use of the in-domain unlabeled dataset is poorly understood. There are basically two ways to take advantages of the unlabeled, in-domain dataset : in-domain pretraining , where a language model is pretrained on the in-domain dataset , and then fine-tuned on ; pseudo-label based approach <ref type="bibr" target="#b41">(Lee, 2013;</ref><ref type="bibr" target="#b69">Reed et al., 2015;</ref><ref type="bibr" target="#b31">Iscen et al., 2019;</ref><ref type="bibr" target="#b76">Shi et al., 2018;</ref><ref type="bibr" target="#b1">Arazo et al., 2020)</ref>, where unlabeled data points are assigned with labels predicted by the model trained on (referred to as the teacher model), forming a new dataset . A new model (referred to as the student model) is trained for final predictions by considering .</p><p>Many important questions regarding the behavior of Code, models and datasets can be found at https://github.com/ShannonAI/Neural-Semi-Supervised-Learning-for-Text-Classification</p><p>To note, the pretraining on the in-domain dataset is distinguished from the pretraining on the large-scale, opendomain dataset largeU. The model for in-domain pretraining can be randomly initialized or taking a pretrained model based on the open-domain dataset largeU <ref type="bibr" target="#b24">(Gururangan et al., 2020).</ref> semi-supervised learning models under the context of large-scale LM pretraining remain unanswered: Is semi-supervised training still beneficial with the presence of large scale pretraining on largeU? Should be used for in-domain LM pretraining or pseudo-label generation? How should pseudo-label based semi-supervised models be implemented? How different semi-supervised strategies (e.g., self learning) affect performances regarding of different sizes, and of different sizes, etc.</p><p>In this paper, we conduct comprehensive studies on the behavior of semi-supervised learning in NLP with the presence of large-scale language model pretraining. We use the task of text classification as an example, the method of which can be easily adapted to different NLP tasks. Our work sheds important lights on the behavior of semi-supervised learning models: we find that (1) with the presence of in-domain pretraining LM on , open-domain LM pretraining <ref type="bibr" target="#b17">(Devlin et al., 2018)</ref> is unnecessary, and we are able to achieve better performance with pretraining on the in-domain dataset ; (2) both the in-domain pretraining strategy and the pseudolabel based strategy lead to significant performance boosts, with the former performing better with larger , the latter performing better with smaller , and the combination of both performing the best; (3) for pseudo-label based strategies, self-training (pretraining first on the pseudo-label dataset and then fine-tuning on ) yields better performances when is small, while joint training on the combination of and yields better performances when is large.</p><p>Using semi-supervised learning models, we are able to achieve a performance of around 93 − 94% accuracy with only 50 training data points on the IMDB dataset, and a competitive performance of 96.6% with the full dataset. More importantly, our work marks an initial step toward understanding the behavior of semi-supervised learning models in the context of large-scale pretraining.</p><p>The rest of this paper is organized as follows: related work is detailed in Section 2. Different strategies for training semi-supervised models are shown in Section 3. Experimental results and findings are shown in Section 4, followed by a brief conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semi-Supervised Learning</head><p>The goal of semi-supervised learning <ref type="bibr" target="#b8">(Chapelle et al., 2006;</ref><ref type="bibr" target="#b97">Zhu, 2005)</ref> is to use massive amount of unlabeled data to improve the models trained on labeled data. One widely-used type of semisupervised method is the pseudo-label based method <ref type="bibr" target="#b41">(Lee, 2013;</ref><ref type="bibr" target="#b69">Reed et al., 2015;</ref><ref type="bibr" target="#b31">Iscen et al., 2019;</ref><ref type="bibr" target="#b76">Shi et al., 2018;</ref><ref type="bibr" target="#b1">Arazo et al., 2020)</ref>, where unlabeled data points are assigned with labels predicted by a trained model, forming a large pseudo labeled dataset to train a model. Self-training <ref type="bibr" target="#b73">(Scudder, 1965;</ref><ref type="bibr" target="#b70">Riloff and Wiebe, 2003</ref>) is a specific type of pseudo-label based method that is of growing interest. Self-training involves training two models: a "teacher" used to label unlabeled data, which is used as an augmented labeled dataset. Then a "student" is trained on the newly augmented dataset. This process can be iterated to further boost performances. Self-training has been successfully applied in different fields such as computer vision <ref type="bibr" target="#b93">(Yalniz et al., 2019;</ref><ref type="bibr" target="#b3">Babakhin et al., 2019;</ref><ref type="bibr" target="#b90">Xie et al., 2020;</ref><ref type="bibr" target="#b32">Chen et al., 2020;</ref>, automatic speech recognition (ASR) <ref type="bibr">(Parthasarathi and Strom, 2019)</ref>. In Vision, Yalniz et al. <ref type="bibr" target="#b62">(2019)</ref> adopted the self-training paradigm in image classification, and achieves the state-of-the-art top-1 result on ImageNet benchmark; <ref type="bibr" target="#b90">Xie et al. (2020)</ref> proposed the strategy of Noisy Student Training, a variant of self-training built on top of EfficientNet <ref type="bibr" target="#b79">(Tan and Le, 2020)</ref>. In ASR, Parthasarathi and Strom <ref type="bibr" target="#b62">(2019)</ref> trained an ASR model on one million hours of unlabeled speech data using a teacher-student self-training model. <ref type="bibr" target="#b60">Park et al. (2020)</ref> proposed the concept of normalized filtering score that filters out low-confident utterance-transcript pairs generated by the teacher to mitigate the noise introduced by the teacher model.</p><p>In the context of natural language processing (NLP), the concept of semi-supervised learning has been adopted in different NLP tasks such as machine translation <ref type="bibr" target="#b11">(Cheng et al., 2016;</ref><ref type="bibr" target="#b83">Tu et al., 2016;</ref><ref type="bibr" target="#b67">Ramachandran et al., 2016;</ref><ref type="bibr" target="#b18">Edunov et al., 2018;</ref><ref type="bibr" target="#b12">Clark et al., 2018)</ref>, information extraction <ref type="bibr" target="#b47">(Liao and Veeramachaneni, 2009;</ref><ref type="bibr" target="#b64">Peters et al., 2017)</ref>, text classification <ref type="bibr">(Nigam et al., 2006;</ref><ref type="bibr" target="#b15">Dai and Le, 2015;</ref><ref type="bibr" target="#b54">Miyato et al., 2016;</ref><ref type="bibr" target="#b29">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b33">Karamanolakis et al., 2019;</ref><ref type="bibr" target="#b45">Li et al., 2019a)</ref>, and text generation <ref type="bibr" target="#b95">(Zang and Wan, 2019;</ref><ref type="bibr" target="#b65">Qader et al., 2019;</ref><ref type="bibr" target="#b75">Shang et al., 2019)</ref>. Particularly, <ref type="bibr" target="#b27">He et al. (2019)</ref> studied the efficacy of self-training on sequence generation tasks and found that self-training can significantly boost performances, particularly when labeled data is scarce. Besides, they also pointed out that the core of self-training for sequence generation tasks is the noise injected into the neural model, which can be interpreted to smooth the latent sequence space.</p><p>Word vector models <ref type="bibr">(Mikolov et al., 2013b,a;</ref><ref type="bibr" target="#b63">Pennington et al., 2014;</ref><ref type="bibr" target="#b52">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b42">Levy and Goldberg, 2014)</ref> and language modeling pretraining <ref type="bibr" target="#b17">(Devlin et al., 2018;</ref><ref type="bibr" target="#b94">Yang et al., 2019;</ref><ref type="bibr" target="#b4">Bao et al., 2020;</ref><ref type="bibr" target="#b32">Joshi et al., 2020)</ref> can also be viewed as a specific type of semi-supervised learning model, by leveraging the information of data in the general domain. Other strategies for training semi-supervised models involve co-training <ref type="bibr" target="#b66">(Qiao et al., 2018;</ref><ref type="bibr" target="#b10">Chen et al., 2018;</ref><ref type="bibr">Han et al., 2018)</ref>, low-density separation <ref type="bibr" target="#b21">(Grandvalet and Bengio, 2005;</ref><ref type="bibr" target="#b16">Dai et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>Data augmentation aims at increasing the amount of training data by adding slightly modified copies of existing data points or created new synthetic data based on existing data points <ref type="bibr" target="#b37">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b62">Paulin et al., 2014;</ref><ref type="bibr" target="#b39">Laine and Aila, 2016;</ref><ref type="bibr" target="#b72">Sajjadi et al., 2016;</ref><ref type="bibr" target="#b13">Cubuk et al., 2018;</ref><ref type="bibr" target="#b30">Inoue, 2018;</ref>. The concept consistency training <ref type="bibr" target="#b68">(Rasmus et al., 2015;</ref><ref type="bibr" target="#b72">Sajjadi et al., 2016;</ref><ref type="bibr" target="#b40">Laine and Aila, 2017;</ref><ref type="bibr" target="#b82">Tarvainen and Valpola, 2018;</ref><ref type="bibr" target="#b55">Miyato et al., 2018;</ref><ref type="bibr" target="#b50">Luo et al., 2018;</ref><ref type="bibr" target="#b2">Athiwaratkun et al., 2019;</ref><ref type="bibr" target="#b46">Li et al., 2019b;</ref><ref type="bibr" target="#b84">Verma et al., 2019;</ref> is widely used as a regulation to force the label of modified copies to the same as the original label.</p><p>In NLP, modified copies of existing data points are generated usually by synonym replacement and text editing <ref type="bibr" target="#b96">(Zhang et al., 2015;</ref><ref type="bibr" target="#b36">Kobayashi, 2018;</ref><ref type="bibr" target="#b86">Wei and Zou, 2019;</ref><ref type="bibr" target="#b20">Gao et al., 2019)</ref>, back-translation <ref type="bibr" target="#b74">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b18">Edunov et al., 2018;</ref><ref type="bibr" target="#b89">Xie et al., 2019)</ref>, noise injection (Wang and <ref type="bibr" target="#b85">Yang, 2015;</ref><ref type="bibr" target="#b92">Xie et al., 2017</ref><ref type="bibr" target="#b91">Xie et al., , 2018</ref>, mixup <ref type="bibr" target="#b23">(Guo et al., 2019)</ref>, generation (Anaby-Tavor et al., <ref type="bibr" target="#b77">2019;</ref><ref type="bibr" target="#b38">Kumar et al., 2020)</ref>. Data augmentation has introduced significant performance boost especially in low-resource scenarios <ref type="bibr" target="#b19">(Fadaee et al., 2017;</ref><ref type="bibr" target="#b5">Bergmanis et al., 2017;</ref><ref type="bibr" target="#b71">Şahin and Steedman, 2018;</ref><ref type="bibr" target="#b77">Shleifer, 2019;</ref><ref type="bibr" target="#b78">Singh et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>We use the task of text classification for illustration purposes, in which the goal is to assign a label to a given input . is a sequence of words. We have a given labeled set = { , }, ∈ [1, ], where denotes the number of data points in . In addition, we have an in-domain unlabeled dataset of size , where . The goal of semi-supervised learning is to explore how the unlabeled in-domain dataset can be leveraged at the training time. At test time, inference remains the same as the original setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">In-domain LM Pretraining</head><p>A direct way to take advantage of is to pretrain a BERT <ref type="bibr" target="#b17">(Devlin et al., 2018)</ref>, RoBERTa  or GPT3 (Brown et al., 2020) style language model on by predicting a masked word given surrounding contexts or predicting a subsequent word given proceeding contexts. Pretraining on the in-domain data facilitates the learning of in-domain semantics and word compositions.</p><p>For training, the LM model can be trained from scratch with random initialization or initialized using an existing BERT or RoBERTa model trained on an open-domain dataset, the latter of which is similar to the idea in Gururangan et al. <ref type="formula">(2020)</ref>, which continues LM pretraining in different domains and tasks. The LM model trained on is used as initialization to be further finetuned on .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pseudo-label Based Approach</head><p>Another way to take advantage of is to use the model trained on to assign pseudo labels to data points in . Specifically, the model trained on is referred to as the teacher model. The teacher model is used to assign pseudo labels to or a specific portion of , forming the augmented dataset . Let denote the number of selected examples in . There are different options on how to generate , how and are combined, and how the new model can be trained on the combination. The model trained on the combination is referred to as the student model.</p><p>The teacher and the student can share a similar model backbone. In the text classification task, various structures can be used as the backbone such as LSTMs <ref type="bibr" target="#b28">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr">Tang et al., 2016)</ref>, CNNs <ref type="bibr" target="#b34">(Kim, 2014)</ref>, BERT <ref type="bibr" target="#b17">(Devlin et al., 2018;</ref><ref type="bibr" target="#b7">Chai et al., 2020)</ref>, etc, with the trained model to maximize the probability of predicting the golden label given . The student model can be randomly initialized or initialized using the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Different strategies to construct</head><p>Here we discuss different strategies to generate .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naive Strategy</head><p>= : The teacher model is used to label all instances in to form . The shortcoming for this strategy is that incorrect and low-confident labels included in can be severely detrimental to the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-K Model Predictions:</head><p>To avoid the negative effect from the low-confident labels assigned by the teacher, we can only pick the confident ones. The teacher model is run on each example in to obtain the probability of all classes. Then for each class , we rank all instances in based on the corresponding probabilities. The top-instances for each class are selected to form .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Different Strategies for Student Training</head><p>Here we discuss how models can be trained given and .</p><p>Training on + : This strategy is denoted by ( + ). The most straightforward strategy is to train the student model on the union of and . The potential risk with this naive strategy is that the influence from clean labels in can be diluted if the size of is large and that incorrect labels in can exert negative effects on the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining on</head><p>and Fine-tuning on : This strategy is denoted by ( ) ( ). To make the model more immune to incorrectly labeled examples in , and be able to take advantage of the large amount of data in at the same time, we can first train the student model on the newly collected dataset to predict the pseudo labels, and then fine-tune the model on the original labeled dataset . This strategy ensures that the final model is fine-tuned on the dataset with clean labels. This process actually mimics the idea of the self-training <ref type="bibr" target="#b17">(Devlin et al., 2018;</ref><ref type="bibr" target="#b90">Xie et al., 2020;</ref><ref type="bibr" target="#b32">Chen et al., 2020;</ref><ref type="bibr" target="#b22">Grill et al., 2020)</ref> in semi-supervised learning literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining on +</head><p>and Fine-tuning on : This strategy is denoted by ( + ) ( ). A minor change can be made to the strategy described above, where the model is pretrained on the concatenation of and , and then fine-tuned on . Practically, we find that ( + ) ( ) consistently outperforms ( ) ( ), which is expected since adding examples with golden labels for pretraining does no harm the performance. We thus only report results for ( + ) ( ), and omit ( ) ( ) for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Training</head><p>The process of training the teacher and the student can be iterated: the student model trained in the previous iteration can be used as a new teacher model to relabel the unlabeled dataset , from which the top-examples are regenerated to form the new . Based on the new , a new student model is trained. This process is repeated until the pre-defined value of iterations is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combining In-domain LM Pretraining and Pseudo-label Based Approach</head><p>The in-domain LM pretraining and the pseudo-label based strategy can be combined: an LM model is first pretrained on the in-domain data . Next, the model is used to initialize the teacher, which will be trained on . The teacher model is then used to generate pseudo labels for , which are used to train the student. In this way, is used twice, both in LM pretraining and pseudo-label generation for student training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct extensive experiments to better understand the behavior of semi-supervised learning. We give insights gathered from extensive experimental studies and also discuss several considerations towards obtaining a successful model. We use the following two datasets for detailed explorations:</p><p>(1) The labeled dataset is the IMDB dataset collected by <ref type="bibr" target="#b51">Maas et al. (2011)</ref>. This dataset contains an even number of positive and negative movie reviews. The training and test sets respectively contain 25k and 25k examples. The task is formalized as a binary classification task to decide the polarity of sentiment for a review. To explore models' behavior on training datasets of different sizes, we use 10, 20, 50, 100, 1k, 5k and 25k for training.</p><p>For the unlabeled dataset , we crawled the IMDB and collected about 3.4M movie reviews. We release this large-scale IMDB movie review dataset to public.</p><p>(2) The labeled dataset is the deceptive opinion spam dataset <ref type="bibr">(Ott et al., 2011;</ref><ref type="bibr" target="#b44">Li et al., 2014)</ref> to separate fake hotel reviews generated by Turkers and hotel employees from genuine reviews from real customers. The task is formulated as a three-class classification task, and contains 800/280/800 reviews, which respectively denote fake reviews from Turkers, fake reviews from hotel employees and genuine reviews from real customers. For the unlabeled dataset , we crawled TripAdvisor, an online travel website and collected about 1M reviews from roughly 5k hotels.</p><p>The sentiment analysis task on movie reviews is a significantly easier task than the deceptive review detection task, since the latter requires to model to identify subtle changes in language usage for spam generation, while the former focuses more on identifying sentiment-indicative tokens.</p><p>Training Details For in-domain LM pretraining, we use the RoBERTa structure  as the backbone. We use both the small model, a 12layer transformer with the hidden size of each layer being 768, and a large model, a 24-layer transformer with the hidden size of each layer being 1,024. Models are trained using using Adam (Kingma and Ba, 2014) with = (0.9, 0.98), = 10 −6 , a polynomial learning rate schedule, warmup for 4K steps and weight decay with 10 −3 . Dropout rate is set to 0.2. The pretrained LM model on the in-domain is then finetuned on . For training the teacher and the student, we use Adam for optimization. Learning rate, batch-size and dropout rate are treated hyperparamters tuned on the dev set.</p><p>For reference purposes, we also implement the BiL-STM model, where the representation at the last time step from the left-to-right direction is concatenated with the the representation at the first time step from the right-to-left direction, which is next fed to the softmax function for golden label prediction. Word vectors are initialized using GloVe <ref type="bibr" target="#b63">(Pennington et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Teacher Performances</head><p>We first examine teacher performances for BiL-STMs, In-domain pretraining, Open-domain pretraining and Open-domain+In-domain pretraining  , then on the in-domain dataset , and last fine-tuned on . The pseudolabel based semi-supervised strategy is applied in none of these setups. The accuracy progressively increases as we increase the size of the labeled dataset , which is in line with our expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is open-domain pretraining still necessary?</head><p>Take the IMDB dataset as an example. As can be seen from <ref type="figure">Figure 1, the</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">In-domain Pretraining</head><p>As shown in <ref type="figure" target="#fig_3">Figures 1 and 4</ref>, the in-domain pretraining strategy has significantly better few-shot learning abilities and requires much smaller amount of data for training: the performance from in-domain pretraining drastically improves as | | increases from 10 to 50, achieving a performance of 93.8 accuracy on the IMDB dataset with only 50 training examples. This performance is higher than BiLSTMs with 25K training examples, and similar to the performance of vanilla RoBERTa with 5K training examples. This shows that LM pretraining on the in-domain dataset provides the model with the generality to rapidly figure out the necessary task-specific information for predictions.</p><p>Comparing with existing pretraining models that have few-shot learning ability such as GPT3, the advantages of the semi-supervised in-domain pretraining are obvious: the model is easier to train, has significantly fewer parameters and does not have to rely on a vast amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Influence from the size of | |</head><p>It is widely accepted that LM pretraining requires a massive amount of training data. Results for of different sizes for LM pretraining are shown in <ref type="figure">Figure 2</ref>. The model is randomly initialized and then trained on the in-domain until convergence. The pretrained LM model is next fine-tuned on = 25 .</p><p>As can be seen, the performance of in-domain pretraining highly relies on the size of the in-domain dataset . With smaller sizes of , in-domain pretraining underperforms open-domain pretraining, which is in line with our expectation since small cannot provide enough evidence for learning in-domain word semantics and compositions. Specifically, the performance of | | = 10 is only slightly better than the no-pretraining setup; the performance for in-domain pretraining outperforms open-domain pretraining only when the size of exceeds 1M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pseudo-label Based Approaches</head><p>Detailed results for pseudo-label based approaches based on open-domain pretraining are shown in <ref type="table">Table 1</ref>. The trends can be summarized as follows:</p><p>(1) Generally, both T( + )F( ) and T( + ) perform better than the vanilla setup ( ). ( ) underperforms the vanilla setup ( ) when is small and is large, but outperforms ( ) when is of medium size.</p><p>(2) The performance boost introduced by pseudolabel based methods gradually increases as the size of increases, and then shrinks. The explanation is as follows: with a small , the accuracy of the teacher model is low. Most predicted labels on are thus unreliable. Therefore, the advantage from the model trained on the noisy is relatively small; with a large , the teacher model is already good enough, reaching an accuracy higher than 0.9. Though almost all predicted labels on are correct, their improvement upon an already pretty good teacher model is small. There is a sweet spot for the size of , where pseudo-label based methods introduce the largest boost: with a medium-sized , where the teacher model reaches an acceptable accuracy when the correctly labeled examples in outweigh incorrectly labeled ones, the student model can take the most advantage of , leading to significant performance boosts of +5%. Similar trends are observed for ( ), where ( ) underperforms the vanilla setup ( ) both when is small and is large, but outperforms ( ) when is of medium size.</p><p>(3) For the three pseudo-label based strategies, ( ), T( + )F( ) and T( + ), ( ) performs the worst for all | |. This is in line with our expectation because for ( ), only is used for final predictions, and thus the influence of the golden-labeled dataset is diluted; T( + )F( ) works best when | | is small, while T( + ) works best when | | is large. Our explanations are as follows: when | | is small, the student trained on + is inferior due to the massive amount of incorrect labels in . The model thus needs to be further fine-tuned on , making T( + )F( ) perform better than T( + ); When | | is large, most labels in are correct and the student is more immune to the small proportion of incorrect labels in . Directly training on the larger + dataset provides the model with more generalization ability, while fine-tuning only on dilutes the influence from massive amount of correct labels in , making the performance of T( + )F( ) worse than T( + ) when | | is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Influence from the size of</head><p>With a fixed in-domain dataset , we select top-K examples for each label . Different values of lead to different sizes of . There is apparently a tradeoff between the size of and the confidence for examples included in : larger size of means that more less-confident examples are selected.</p><p>Trends regarding different values of | | are shown in <ref type="figure">Figure 5</ref>. We only run setups with | | larger than | |. As can be seen, for smaller (| | = 50, 100), the final performance first increases as | | gets larger, which means the model is taking advantage of evidence provided by the pseudo labels. Then, the performance decreases as | | continues to increase, which means the model starts suffering from the incorrectness of the less-confident examples. For larger values of (| | larger than 1k), performances keep increasing as | | gets larger. This is because the teacher model trained on is good enough to generate confident examples.  <ref type="figure">Figure 5</ref>: The influence of | | on pseudo-label based approaches. | | is required to be larger than | |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Influence from the size of</head><p>The influence of the size of is already manifested in the size of , since the size of should be larger than the size of ( is selected from ). Performances for different sizes of are shown in <ref type="figure">Figure 3</ref>, where we randomly sample examples from the 3.4 million IMDB reviews to form of different sizes. For a given , we plot the performance achieved with the best | |. Larger should lead to better performances since more confident examples can be selected. <ref type="figure">Figure 3</ref>, for of different sizes, the performance first increases as | | grows, but then immediately plateaus. Explanations are as follows: for larger | |, the model is already good enough to provide confidently correct labels for points in , and the improvement from extra confident examples is marginal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Iterative Training</head><p>We can iterate the teacher-student pattern for pseudolabel based approaches, where the teacher for the current iteration is initialized with the student in the previous iteration. Results are shown in Figure 6. As can be seen, additional performance boosts are observed for different | | as the iterative process goes on. For larger | |, the performance becomes stagnant across the iterative process, while for smaller | |, the curve keeps rising until convergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combing In-domain Pretraining and Pseudo-label Based Approaches</head><p>In-domain pretraining and pseudo-label based approaches can be combined, where the teacher model is initialized with the pretrained model on the indomain dataset . Results for the combined strategy are shown in <ref type="table" target="#tab_4">Table 2</ref>. As can be seen, both strategies lead to progressive performance boosts over Open-domain Pretraining, with a combined boost of +1.1 for the small model, and +1.0 for the large model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we conduct comprehensive analysis on semi-supervised learning in NLP under the con-  text of large-scale language model pretraining. We find that even with the presence of large-scale LM pretraining, both the in-domain pretraining strategy and the pseudo-label based strategy introduce additional significant performance boost, with the former performing better with larger , the latter performing better with smaller , and the combination leading to the best performance. Using semi-supervised learning models, we are able to achieve a performance of around 93 − 94% accuracy with only 50 training data points on the IMDB dataset, and a competitive performance of 96.6% with the full dataset. Our work sheds light on the behavior of semi-supervised learning models in the context of large-scale pretraining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1: Results for different strategies for pseudo-label based approaches on the IMDB dataset based on opendomain pretraining. "T" represents "Train" and "F" represents "Fintune". Brackets represent the used dataset(s), e.g., "T(D+D')F(D)" means the model is trained on + and finetuned on . The best result for each row is marked in bold. of which significantly outperform open-domain pretraining (95.20). Similar phenomena are observed for the deceptive spam dataset, where In-domain pretraining, open-domain+In-domain pretraining and open-domain pretraining respectively obtains 82.5, 82.0 and 78.2 accuracy. This demonstrates that with the presence of relatively large in-domain data, the extremely time-intensive training of LM on a huge amount of open-domain data is unnecessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>The influence of | | for in-domain pretraining with | | = 25 . The influence of | | for pseudo-label based approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The influence of | | on pseudo-label based approaches for different | |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Iterative Training for Pseudo-label Based Approaches on IMDB based on Open-domain Pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>54.00 51.46 53.95 53.60 55.10 51.54 52.49 53.91 55.25 55.35 55.47 54.60 (+0.66) (-1.88) (+0.61) (+0.26) (+1.76) (-1.80) (-0.85) (+0.57) (+1.91) (+2.01) (+2.13) (+1.26) 20 54.62 51.73 53.28 54.16 55.43 54.35 54.50 53.92 54.43 58.15 58.63 58.</figDesc><table><row><cell></cell><cell></cell><cell>T( )</cell><cell></cell><cell cols="2">T( )</cell><cell></cell><cell></cell><cell cols="2">T( + )</cell><cell></cell><cell></cell><cell cols="2">T( + )F( )</cell><cell></cell></row><row><cell>| |</cell><cell>| |</cell><cell>0</cell><cell>1K</cell><cell>10K</cell><cell>100K</cell><cell>1M</cell><cell>1K</cell><cell>10K</cell><cell>100K</cell><cell>1M</cell><cell>1K</cell><cell>10K</cell><cell>100K</cell><cell>1M</cell></row><row><cell>10</cell><cell></cell><cell cols="13">53.34 77 55.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="12">(-2.89) (-1.34) (-0.46) (+0.81) (-0.27) (-0.12) (-0.70) (-0.19) (+3.53) (+4.01) (+4.15) (+1.01)</cell></row><row><cell>50</cell><cell></cell><cell cols="9">80.67 56.91 72.25 82.19 81.30 73.81 77.36 83.80 81.63</cell><cell cols="4">81.32 84.96 85.86 82.85</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="12">(-23.76) (-8.42) (+1.52) (+0.63) (-6.86) (-3.31) (+3.13) (+0.96) (+0.65) (+4.29) (+5.19) (+2.18)</cell></row><row><cell cols="2">100</cell><cell cols="9">83.12 65.67 83.49 85.52 83.95 80.30 83.50 84.89 83.60</cell><cell cols="4">81.86 86.25 87.27 84.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="12">(-17.48) (+0.37) (+2.40) (+0.83) (-2.82) (+0.38) (+1.77) (+0.48) (-1.26) (+3.13) (+4.15) (+1.28)</cell></row><row><cell cols="2">1K</cell><cell>89.90</cell><cell>-</cell><cell cols="3">84.68 89.84 90.66</cell><cell>-</cell><cell cols="3">90.10 90.15 91.12</cell><cell>-</cell><cell cols="3">90.45 90.78 91.32</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell cols="3">(-5.22) (-0.06) (+0.76)</cell><cell>-</cell><cell cols="3">(+0.20) (+0.25) (+1.22)</cell><cell>-</cell><cell cols="3">(+0.55) (+0.88) (+1.42)</cell></row><row><cell cols="2">5K</cell><cell>93.16</cell><cell>-</cell><cell cols="3">87.09 91.43 92.98</cell><cell>-</cell><cell cols="3">93.02 93.57 93.79</cell><cell>-</cell><cell cols="3">93.07 93.05 93.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell cols="3">(-6.07) (-1.73) (-0.18)</cell><cell>-</cell><cell cols="3">(-0.14) (+0.41) (+0.63)</cell><cell>-</cell><cell cols="3">(-0.09) (-0.11) (+0.56)</cell></row><row><cell cols="2">25K</cell><cell>95.20</cell><cell>-</cell><cell>-</cell><cell cols="2">90.56 92.75</cell><cell>-</cell><cell>-</cell><cell cols="2">95.51 95.80</cell><cell>-</cell><cell>-</cell><cell>95.42</cell><cell>95.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">(-3.24) (-1.05)</cell><cell>-</cell><cell>-</cell><cell cols="2">(+0.31) (+0.60)</cell><cell>-</cell><cell>-</cell><cell cols="2">(+0.22) (+0.30)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">performance of In-domain</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">pretraining (95.87 when | | = 25 ) on the 3.4M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">unlabeled reviews performs nearly the same as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">open-domain+In-domain pretraining (95.82), both</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results for Combining In-domain Pretraining and Pseudo-label based Approaches</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Not enough data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ateret</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Naama Tepper, and Naama Zwerdling</pubPlace>
		</imprint>
	</monogr>
	<note>deep learning to the rescue!</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pseudolabeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (ĲCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised segmentation of salt bodies in seismic images using an ensemble of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yauhen</forename><surname>Babakhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotoshi</forename><surname>Kitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="218" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training data augmentation for low-resource morphological inflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toms</forename><surname>Bergmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection</title>
		<meeting>the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Description based text classification with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mohammad Norouzi, and Geoffrey Hinton. 2020. Big selfsupervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning with memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="275" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1965" to="1974" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dandelion Mane, Vĳay Vasudevan, and Quoc V Le</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
	</analytic>
	<monogr>
		<title level="m">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Good semisupervised learning that requires a bad gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00440</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Soft contextual data augmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lĳun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5539" to="5544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semisupervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13788</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Label propagation for deep semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondrej Chum</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Leveraging just a few keywords for fine-grained aspect detection through weakly supervised co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Karamanolakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06201</idno>
		<title level="m">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Data augmentation using pre-trained transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards a general rule for identifying deceptive opinion spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1566" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tat-Seng Chua, and Bernt Schiele</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Decoupled certainty-driven consistency loss for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05657</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A simple semi-supervised algorithm for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriharsha</forename><surname>Veeramachaneni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing</title>
		<meeting>the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Decoupled certainty-driven consistency loss for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Semi-supervised text classification using em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">T</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Improved noisy student training for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09629</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikko</forename><surname>Sree Hari Krishnan Parthasarathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strom</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transformation pursuit for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattis</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3646" to="3653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Semi-supervised neural text generation by joint learning of natural language generation and natural language understanding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raheel</forename><surname>Qader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Labbé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Deep co-training for semisupervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning extraction patterns for subjective expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in natural language processing</title>
		<meeting>the 2003 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Data augmentation via dependency tree morphing for lowresource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözde</forename><surname>Gül Şahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5004" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Semisupervised text style transfer: Cross projection in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pĳi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Transductive semi-supervised deep learning using min-max features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Zhiheng MaXiaoyu Tao, and Nanning Zheng</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Low resource text classification with ulmfit and backtranslation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Xlda: Cross-lingual data augmentation for natural language inference and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasdeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Nitish Shirish Keskar, Caiming Xiong, and Richard Socher</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01874</idno>
		<title level="m">Neural machine translation with reconstruction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2557" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Conditional bert contextual augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Generalized data augmentation for low-resource translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5786" to="5796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Noising and denoising natural language: Diverse backtranslation for grammar correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Genthial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="619" to="628" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02573</idno>
		<title level="m">Data noising as smoothing in neural network language models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">A semisupervised approach for low-resourced text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
