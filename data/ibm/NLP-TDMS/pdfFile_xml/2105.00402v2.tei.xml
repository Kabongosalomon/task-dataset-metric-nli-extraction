<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-04">4 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dinh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Quang</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Phan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao</forename><surname>Van Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">Thi</forename><surname>Thuy</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">National University of Agriculture</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country>Vietnam, Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-04">4 May 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Artificial Intelligence in Medicine May 6, 2021</note>
					<note>(Phan Ngoc Lan), daoviethang@hmu.edu.vn (Dao Viet Hang), bsdaolong@yahoo.com (Dao Van Long), ntthuy@vnua.edu.vn (Nguyen Thi Thuy) 1 These authors contributed equally to this work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Attention Mechanism</term>
					<term>Polyp Segmentation</term>
					<term>Colonoscopy * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Colorectal cancer is among the most common malignancies and can develop from high-risk colon polyps. Colonoscopy is an effective screening tool to detect and remove polyps, especially in the case of precancerous lesions. However, the missing rate in clinical practice is relatively high due to many factors. The procedure could benefit greatly from using AI models for automatic polyp segmentation, which provide valuable insights for improving colon polyp detection.</p><p>However, precise segmentation is still challenging due to variations of polyps in size, shape, texture, and color. This paper proposes a novel neural network architecture called AG-CUResNeSt, which enhances Coupled UNets using the robust ResNeSt backbone and attention gates. The network is capable of effectively combining multi-level features to yield accurate polyp segmentation.</p><p>Experimental results on five popular benchmark datasets show that our proposed method achieves state-of-the-art accuracy compared to existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide, with roughly 694,000 fatalities each year <ref type="bibr" target="#b0">[1]</ref>. Most CRC arises from colon polyps, especially the adenomas with high-grade dysplasia <ref type="bibr" target="#b2">[2]</ref>. According to a longitudinal study <ref type="bibr" target="#b3">[3]</ref>, each 1% of adenoma detection rate increase is associated with a 3% decrease in the risk of colon cancer. Therefore, the detection and removal of polyps at the early stage are of great importance to prevent CRC. Nowadays, colonoscopy is considered as the gold standard for colon screening and recommended in many guidelines of different societies <ref type="bibr" target="#b4">[4]</ref>. Nevertheless, the overloaded healthcare systems in many countries, especially in limited-resource settings, might result in shorter endoscopy duration to guarantee the required number of procedures per day. This factor, combined with low-quality endoscopy systems and experience gaps among endoscopists at different levels of the healthcare system, seriously affects the quality of colonoscopy procedures and increases the risk of missing lesions and inaccurate diagnosis <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>. A literature review has shown that the colon polyp missing rate in endoscopies could range from 20-47% <ref type="bibr" target="#b7">[7]</ref>. Hence, studies to develop computer-aided systems to support endoscopists in providing accurate polyp regions are much needed in both aspects of training endoscopists and application in clinical practice.</p><p>Despite the growth of many advanced machine learning and computer vision techniques in recent years, automatic polyp segmentation still a challenging problem. The first challenge is that polyps are often diverse in appearance, such as shape, size, texture, and color. Secondly, as the nature of medical images, the boundary between polyps and their surrounding mucosa, especially in case of flat lesions or unclean bowel preparation, is not always clear during colonoscopy, leading to confusion for segmentation methods.</p><p>There are different approaches to polyp segmentation. Traditional machine learning methods are based on hand-crafted features for image representation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b10">9]</ref>. These methods rely on color, texture, shape, or edge information as ex-tracted features and train classifiers to distinguish polyps from surrounding normal mucosa. However, hand-crafted features are limited in representing polyps due to their high intra-class diversity and low inter-class variation between them and hard negative mucosa regions. Recently, deep neural networks have proven to be more effective in solving medical image segmentation problems, particularly the ones related to endoscopic images of the human gastrointestinal (GI) tract. Among various deep models, encoder-decoder based networks like UNet family <ref type="bibr" target="#b11">[10]</ref> have demonstrated impressive performance. In UNets, high-level semantic features in the decoder are gradually up-sampled and fused with corresponding low-level detailed information in the encoder through skip connections. Inspired by the success of UNets, UNet++ <ref type="bibr" target="#b12">[11]</ref> and ResUNet++ <ref type="bibr" target="#b13">[12]</ref> were proposed for polyp segmentation and yielded promising results. However, these methods heavily depend on the dense concatenation of feature maps at multiple levels, resulting in high computational resource requirements and timeconsuming procedures. Recently, the attention mechanism has been widely used in various deep learning models. In <ref type="bibr" target="#b14">[13]</ref>, Oktay et al. introduce attention gates to UNets in order to suppress irrelevant low-level information from encoders before concatenating them with high-level feature maps in decoders. Fan et al. <ref type="bibr" target="#b15">[14]</ref> enhance an FCN-like model by a parallel partial decoder and reverse attention module and obtained impressive results. On the other hand, previous works show that stacking multiple UNets allows the networks to learn a better feature representation and considerably improves the accuracy. DoubleUNet <ref type="bibr" target="#b16">[15]</ref> stacks two UNets on top of each other and was applied for polyp segmentation. However, DoubleUNet lacks skip connections between the two UNets, which limits the information flow within the network. Another weakness of DoubleUNet is the use of an old VGG-19 backbone, which can be replaced with more efficient models proposed recently, e.g., the ResNet family <ref type="bibr" target="#b17">[16]</ref><ref type="bibr" target="#b18">[17]</ref><ref type="bibr" target="#b19">[18]</ref>. In <ref type="bibr" target="#b20">[19]</ref>, Tang et al.</p><p>introduce the coupled UNets (CUNet) architecture, where coupling connections are utilized to improve the information flow across UNets. In <ref type="bibr" target="#b21">[20]</ref>, Na et al.</p><p>introduce coupled attention residual UNets and use it as a generator for adversarial learning based Facial UV map completion. The model in <ref type="bibr" target="#b21">[20]</ref> is based on ResNet backbone and utilizes the fast normalized fusion <ref type="bibr" target="#b22">[21]</ref> to combined the information between the two UNets, which can result in potential information loss.</p><p>This paper proposes a novel deep network, called AG-CUResNeSt, where the encoders of coupled UNets are strengthened by residual connections and split-attention blocks in ResNeSt <ref type="bibr" target="#b19">[18]</ref>. Attention gates are integrated into skip connections within each UNet to suppress the redundant low-level information from the encoders. We also leverage skip connections across the two UNets <ref type="bibr" target="#b20">[19]</ref> to reduce gradient vanishing and promote feature reuse. Extensive experiments on five popular benchmark datasets demonstrate that our method yields superior accuracy compared to other state-of-the-art approaches.</p><p>The rest of the paper is structured as follows. Section 2 reviews the literature regarding CNN backbones and semantic segmentation in medical image analysis.</p><p>In Section 3, we describe the proposed network architecture in detail. Section 4 outlines our experiment settings. The results are presented and discussed in Section 5. Finally, we conclude the paper and outline future works in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There have been many methods proposed for semantic image segmentation in general and for the purpose of segmenting the polyps in colonoscopy images in particular. In this section, we briefly review prior methods related to our work, focusing on deep neural network models for colorectal polyps segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CNN Architectures</head><p>Since AlexNet <ref type="bibr" target="#b23">[22]</ref>, Convolutional Neural Networks (CNNs) have dominated in solving computer vision tasks. VGG <ref type="bibr" target="#b24">[23]</ref> proposes a simple yet effective modularized network design exploiting the efficiency of small 3 × 3 kernels.</p><p>However, plain networks like VGG suffer from degradation when their depth increases. ResNet <ref type="bibr" target="#b18">[17]</ref> introduces identity skip connections to smooth out the objective function's landscape. Skip connections also reduce gradient vanishing and allow very deep networks to learn better feature representations. GoogleNet <ref type="bibr" target="#b25">[24]</ref> demonstrates the success of multi-branch networks, where each branch is carefully designed using different convolutional kernel sizes. ResNeXt <ref type="bibr" target="#b17">[16]</ref> improves ResNet with a unified multi-branch design, where all branches have the same architecture. SE-Net <ref type="bibr" target="#b26">[25]</ref> introduces a channel attention mechanism that adaptively recalibrates channel-wise feature responses. SK-Net <ref type="bibr" target="#b27">[26]</ref> proposes an adaptive selection mechanism to fuse two network branches to adaptively adjust receptive field sizes of neurons according to the input. Recently, ResNeSt <ref type="bibr" target="#b19">[18]</ref> integrates the channel-wise attention on different network branches to exploit their success in capturing cross-feature interactions and learning diverse representations. Besides, with the growth of computing capability, some efficient CNNs such as EfficientNet <ref type="bibr" target="#b28">[27]</ref> are automatically designed by machine thanks to neural architecture search techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Segmentation for Medical Image Analysis</head><p>Semantic image segmentation has been an area of very active research in recent years. Many network architectures and learning techniques have been proposed to improve segmentation accuracy, latency, and throughput. In <ref type="bibr" target="#b29">[28]</ref>, the authors utilize several well-known classification networks (AlexNet <ref type="bibr" target="#b23">[22]</ref>, VGG <ref type="bibr" target="#b24">[23]</ref> and GoogLeNet <ref type="bibr" target="#b25">[24]</ref>) in segmentation networks, coupled with transfer learning techniques. UNet <ref type="bibr" target="#b11">[10]</ref> is among the most famous network architectures for segmentation. The network consists of an encoder and a decoder, with skip connections between corresponding levels. More recently, DeepLabV3 <ref type="bibr" target="#b30">[29]</ref> introduces atrous convolutions to extract denser features for better performance in segmentation tasks.</p><p>Semantic segmentation specifically for medical images has also attracted much attention. UNet is among the first successful applications of neural architecture in medical images and brings several variants in the following years. In  <ref type="bibr" target="#b13">[12]</ref> propose ResUNet++ that takes the advantages of residual blocks, squeeze and excitation units, atrous spatial pyramidal pooling (ASPP), and the attention mechanism. DoubleUNet <ref type="bibr" target="#b16">[15]</ref> uses a pre-trained VGG backbone, squeeze and excitation units, and ASPP modules. The performance of this network surpasses previous methods on several different datasets.</p><p>However, DoubleUNet suffers from using the old VGG backbone and the lack of skip connections across the two UNet blocks, limiting the information flow.</p><p>CUNet <ref type="bibr" target="#b20">[19]</ref> improves the information flow by adding skip connections across UNet blocks. Attention UNet <ref type="bibr" target="#b14">[13]</ref> introduces attention gates, which suppress unimportant regions while highlighting salient features useful for a specific task.</p><p>Non-UNet architectures are also utilized for medical segmentation. Fan et al. <ref type="bibr" target="#b15">[14]</ref> propose PraNet, enhancing an FCN-like model using a parallel partial decoder and reverse attention modules. PraNet achieves state-of-the-art performance on five challenging benchmark medical datasets. In this paper, we analyze and integrate the three aforementioned architectures <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b20">19]</ref> to propose a novel network that outperforms previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>The overall architecture of our proposed network is depicted in fused with low-level information in the encoder via gated skip connections called attention gates. Next, we use a 1x1 conv layer followed by a sigmoid layer at the end of each UNet to yield its output. The last feature map of the first UNet is combined with the raw input image and then fed to the second UNet for further refinement. Inspired by <ref type="bibr" target="#b20">[19]</ref>, we also use skip connections across the two UNets to enhance the information flow and promote feature reuse in the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone: ResNet Family</head><p>Plain networks tend to decrease performance on both training and test datasets as their depth increases. This is a widely observed phenomenon called the degradation problem. ResNet <ref type="bibr" target="#b18">[17]</ref>  ResNeXt <ref type="bibr" target="#b17">[16]</ref> introduces a homogeneous multi-branch structure that breaks channel information into K repeated smaller bottleneck branches called cardinal groups.</p><p>One of the latest members in the ResNet family is ResNeSt <ref type="bibr" target="#b19">[18]</ref>, which im- </p><formula xml:id="formula_0">splits:Û k = R r=1 U R(k−1)+r , k = 1, 2, . . . , K.</formula><p>Inspired by the ideas of SE-Net <ref type="bibr" target="#b26">[25]</ref> and SK-Net <ref type="bibr" target="#b27">[26]</ref>, ResNeSt introduces the channel-wise attention for multi network splits <ref type="figure" target="#fig_4">(Fig. 2)</ref>. Firstly, the global context information across spatial dimensions s k ∈ R C/K is obtained by applying global average pooling toÛ k . Then a network G of two consecutive fully connected (FC) layers is added to predict the attention weights over splits in each channel a c k = {a c k1 , a c k2 , ..., a c kR } ∈ R R as follows:</p><formula xml:id="formula_1">a c kr =      exp(G c r (s k )) R j=1 exp(G c j (s k )) , if R &gt; 1, 1 1+exp(−G c r (s k )) , otherwise.<label>(1)</label></formula><p>The attention weights corresponding to the r-th split can be denoted as</p><formula xml:id="formula_2">a kr = {a 1 kr , a 2 kr , . . . , a C/K kr }.</formula><p>The output of k-th cardinal group V k ∈ R H×W ×C/K is calculated by a weighted fusion over splits:</p><formula xml:id="formula_3">V c k = R r=1 a c kr U c R(k−1)+r , c = 1, 2, . . . , C/K.<label>(2)</label></formula><p>Next, the representation of all cardinal groups are concatenated along the</p><formula xml:id="formula_4">channel dimension: V = Concat(V 1 , V 2 , . . . , V K ). Finally, a standard skip connection is applied Y = X + T (X), where T (X) is an appropriate transform to align the output shapes if needed.</formula><p>The experiments in <ref type="bibr" target="#b19">[18]</ref> show that ResNeSt even outperforms the machine designed architecture EfficientNet <ref type="bibr" target="#b28">[27]</ref> in accuracy and latency trade-off. In this study, we conduct an ablation study on different backbones including ResNet-50, ResNet-101, ResNeSt-50 and ResNeSt-101. Besides, we also compare the ResNet family with Non-ResNet architectures such as VGG and EfficientNet. Attention gates (AG) <ref type="bibr" target="#b14">[13]</ref> can implicitly learn to suppress the irrelevant information in an input image while strengthening salient features necessary for a specific task. In the encoder of a UNet, input data is gradually down-sampled and transformed from low-level to high-level semantic feature maps with coarser scales. In the decoder, coarse feature maps are upsampled and fused with lowlevel ones to produce a final segmentation result. <ref type="figure" target="#fig_5">Fig. 3</ref> describes how the attention gate works. Suppose that g ∈ R Fg×Hg×Wg is a coarse feature map in the decoder that provides information to suppress the irrelevant content in a low-level feature map x ∈ R Fx×Hx×Wx from the encoder. Each feature map g and x is fed to a 1 × 1 convolutional (conv) layer with F i kernels to reduce its number of channels to an intermediate value F i . The low-level feature map is then down-sampled to align with the shape of g. Next, the two resulting feature maps are combined by passing them to an element-wise summation operation followed by a ReLU function. A 1 × 1 conv layer with only one kernel is further applied to aggregate the information across all channels. After that, a sigmoid function is used to normalize the information and produce a coarse attention map, which is then resampled to match the shape of x. Finally, the fine-grained attention map α ∈ R Hx×Wx is used to scale the feature map x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention Gate</head><p>In the first UNet of our proposed network <ref type="figure" target="#fig_2">(Fig. 1)</ref>, the attention gate takes a coarse feature map D U 1 from decoder and a low-level feature map E U 1 from the encoder as input and produces a filtered mapÊ U 1 . The feature map D U 1 is then upsampled and concatenated withÊ U 1 before fitting them to two successive 3 × 3 conv layers followed by ReLU and Batch Norm. A similar mechanism is applied in the second UNet. However, in our design,Ê U 2 is concatenated with not only D U 2 but also the coarse feature map D U 1 passed through the skip connections across the two UNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>It is known that the problem of medical image segmentation poses an issue of imbalanced data, i.e., lesions or polyps are often small regions in an image.</p><p>Therefore, we propose to employ Tversky loss <ref type="bibr" target="#b31">[30]</ref> to address the issue of data imbalance and achieve a much better tradeoff between precision and recall during training the networks. Assume that P and G are the predicted map taken after the softmax layer and the binary ground-truth, respectively, the Tversky loss is defined as follows:</p><formula xml:id="formula_5">T (α, β, P, G) = N i=1 P i0 G i0 N i=1 P i0 G i0 + α N i=1 P i0 G i1 + β N i=1 P i1 G i0 ,<label>(3)</label></formula><p>where N is the number of pixels in the ground-truth G; P i0 is the the probability that pixel i belongs to a polyp, P i1 = 1−P i0 is the probability that pixel i belongs to a non-polyp region; G i0 = 1 for a polyp pixel, G i0 = 0 for a non-polyp pixel and vice verse for G i1 ; α and β control the magnitude of penalties for false positives and false negatives, respectively.</p><p>An auxiliary Tversky loss is also applied to the first UNet to boost the gradient flow during training. Thus, the final loss function is:</p><formula xml:id="formula_6">L = T (α, β, P U 2 , G) + T aux (α, β, P U 1 , G),<label>(4)</label></formula><p>where P U 1 and P U 2 are the output of the first and the second UNet, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Several benchmark datasets are available for evaluating polyp segmentation models. The CVC-ClinincDB <ref type="bibr" target="#b32">[31]</ref> and ETIS-Larib <ref type="bibr" target="#b10">[9]</ref> datasets are provided in the 2015 MICCAI automatic polyp detection sub-challenge. These datasets consist of frames extracted from colonoscopy videos, annotated by expert video endoscopists. CVC-ClinicDB has 612 images (384x288) extracted from 29 different video studies. ETIS-Larib has a total of 196 high-resolution images (1225x966).</p><p>The CVC-ColonDB <ref type="bibr" target="#b33">[32]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Settings</head><p>We implement the proposed models using the PyTorch framework. A single training run takes approximately 12 hours using an NVIDIA GTX 2080 GPU.</p><p>Weights pre-trained on ImageNet for ResNet and ResNeSt are used as initialization for the respective backbones. The training process consists of two phases.</p><p>The first phase trains the first UNet to convergence, and the second phase trains the entire coupled network model. Both phases use stochastic gradient descent (SGD) with a learning rate of 5.10 −3 , and a momentum of 0.9. We use the Tversky loss function, with α = 0.5 and β = 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Augmentation</head><p>The aforementioned datasets are generally small compared to other computer vision datasets, as annotations require expert endoscopists. Thus, image augmentation is quite often used to help diversify training data. Our experiments follow the Augmentation-II strategy proposed in <ref type="bibr" target="#b35">[34]</ref>. Particularly, we apply the following transformations to every training image:</p><p>• Rotating the images by 90, 180, and 270 degrees, respectively;</p><p>• Flipping the images both horizontally and vertically;</p><p>• Resizing the images with four scale factors of 0.9, 1.1, 1.15, and 1.2, respectively;</p><p>• Blurring the images with a kernel size of 5 × 5;</p><p>• Brightening the images by using RandomBrightness in <ref type="bibr" target="#b36">[35]</ref> with alpha = 1.5;</p><p>• Darkening the images by using RandomContrast in <ref type="bibr" target="#b36">[35]</ref> with alpha = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation Metrics</head><p>We  <ref type="table" target="#tab_1">Table 1</ref> shows the overall results of ablation study. It can be seen that AG-CUResNeSt-101 architecture obtained the best results over state-of-the-art models in terms of mDice and mIoU scores, the second-best in terms of precision, and is comparable to other models in terms of recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">The Effectiveness of Backbone Networks</head><p>We first evaluate the use of different encoder backbones. Several ResNet variants including ResNet34, ResNet50, ResNet101, ResNeSt50, and ResNeSt101 have been used. Besides, we also try other CNN architectures such as VGG16 and EfficientNet family from B0 to B4. The ResNeSt architecture uses channelwise attention on separate branches to enrich their features. <ref type="table" target="#tab_1">Table 1</ref> shows that ResNeSt101 gives the best overall performance. ResNet backbones generally perform better as size increases. ResNeSt101 also improves over ResNest50, but the improvement is quite marginal. ResNest101 achieves lower recall (to 0.813 from 0.829), suggesting that a larger ResNeSt such as ResNeSt152 would likely not yield significant improvements. ResNeSt101 backbone significantly outperforms VGG16 and yields slightly better results compared to EfficientNet-B4 in terms of mDice and mIoU scores.  <ref type="table" target="#tab_1">Table 1</ref> shows a considerable increase in mDice score when applying AG.</p><p>More specifically, mDice for ResNet101-UNet increases from 0.811 to 0.815 when AG is added, while ResNeSt101-UNet increases from 0.816 to 0.829. We note that while the overall Dice score increases, adding AG causes a drop in precision score for both models. This is likely due to increased focus on potential polyp regions that had previously been ignored without AG. As attention gates bring more focus to these regions, the network can cover more polyps but also more likely to make false predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">The Effectiveness of coupled connections</head><p>The Attention CUNet architecture adds one additional UNet, as well as skip connections across the two UNets. We denote the variant with ResNet backbone as Attention ResCUNet, and that with the ResNeSt backbone as AG-CUResNeSt. For each backbone, the mDice score increases by roughly 0.5%.</p><p>AG-CUResNeSt achieves a mDice of 0.833 and a mIOU of 0.754, the best scores among models in <ref type="table">Table (</ref> <ref type="bibr" target="#b0">1)</ref>. Both network size and the enrichment of semantic features play a factor in this improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to Existing Methods</head><p>This section compares our proposed AG-CUResNeSt to several state-of-theart models for polyp segmentation. From the previous ablation study, we select the best-performing ResNeSt101 backbone as the comparison model for this section. Therefore, the model is briefly called AG-CUResNeSt-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Cross-dataset Evaluation</head><p>The following experiments are for evaluating the performance of AG-CUResNeSt-101 and previous state-of-the-art models when training and testing across different datasets, i.e., Scenario 2 and Scenario 3. This setting implies that models need to generalize well to have good performance, as different polyp datasets have different image properties and feature distributions.</p><p>We first compare AG-CUResNeSt-101 with Mask-RCNN <ref type="bibr" target="#b37">[37]</ref> for segmentation, using Scenario 2, i.e., using CVC-Colon for training, CVC-Clinic for testing. We use the implementation of Mask-RCNN in the detectron2 project and train the model from scratch using the original paper's hyperparameter configurations mentioned in <ref type="bibr" target="#b37">[37]</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows that both Attention ResNeSt101-UNet and AG-CUResNeSt-101 outperform Mask-RCNN by a large margin (over 10%). Notably, AG-CUResNeSt-101 seems capable of detecting both tiny and large polyps that occupy the whole image. We also compare the final output taken from the second UNet and the auxiliary output taken from the first UNet. <ref type="figure" target="#fig_9">Fig. 5</ref> shows that the second UNet can correct some regions that the first UNet fails to predict.   Mask-RCNN <ref type="bibr" target="#b37">[37]</ref> and DoubleUNet <ref type="bibr" target="#b16">[15]</ref> are trained from scratch using the original papers' configurations. <ref type="table" target="#tab_4">Table 3</ref> shows that AG-CUResNeSt-101 outperforms both models in terms of mDice and mIoU, both by significant margins. Specifically, AG-CUResNeSt-101 outperforms the best Mask-RCNN by 13.6% in mDice and 14.4% in mIoU. Against DoubleUNet, both of these figures are 11.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Intra-dataset Evaluation</head><p>The following experiments compare AG-CUResNeSt-101 with several existing models when the training and test set are drawn from the same dataset, i.e., Scenario 4, 5, and 6).  SFA <ref type="bibr" target="#b38">[38]</ref>, ResUNet-mod <ref type="bibr" target="#b39">[39]</ref>, ResUNet++ <ref type="bibr" target="#b13">[12]</ref>, UNet <ref type="bibr" target="#b11">[10]</ref> and UNet++ <ref type="bibr" target="#b12">[11]</ref>.</p><p>Results for the compared models are reported in their respective papers. On the Kvasir-SEG test set, AG-CUResNeSt-101 outperforms the second-best PraNet by 0.4% in mDice and 0.5% in mIoU. For the CVC-ClinicDB test set, AG-CUResNeSt-101 is also the best performing model, exceeding PraNet by 1.9% in mDice and 1.8% in mIoU, respectively.   for the compared models are reported in their respective papers. <ref type="table" target="#tab_7">Table 6</ref> shows that the proposed model achieves the best mDice score, mIoU, recall, and precision. Specifically, AG-CUResNeSt-101 achieves an average Dice score of 0.912, outperforming the second-place PraNet by 2.9%. Besides, metric scores across different folds demonstrate the stability of AG-CUResNeSt-101, with a slightly lower standard deviation than PraNet.</p><p>A qualitative comparison between different models is shown in <ref type="figure" target="#fig_10">Fig. 6</ref>. One can see that in many cases, our model performs significantly better than other state-of-the-art methods. The lesion regions are well-segmented and delineated.</p><p>Nevertheless, in some other case shown in <ref type="figure" target="#fig_12">Fig. 7</ref>, AG-CUResNeSt-101 is confused in estimating the attention maps, which lead to poor segmentation results. Usually, these imprecise predictions may occur when the input image contains very large polyps or colon mucosa folds, with many similar appearance characteristics as polyps. These are challenging cases for all the segmentation models and even junior endoscopists in practice.   practice as a second-look tool or an assisting system during the procedure. The high accuracy of our novel models in benchmark datasets proposes a feasible solution to reduce the missing rate in real practice, which may have a significant impact on improving the quality of colorectal cancer screening strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has introduced a novel neural network architecture for polyp segmentation called AG-CUResNeSt. The architecture combines several components, namely ResNeSt, attention gates, and Coupled UNets, to improve performance. The proposed model is verified using extensive experiments and compared against several state-of-the-art methods on public benchmark datasets.</p><p>Results show that AG-CUResNeSt consistently improves over all compared models, with a slight tradeoff in model size.</p><p>We hope that our proposal will provide a strong baseline for developing deep neural networks in medical image analysis. Our future research will focus  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[ 11 ]</head><label>11</label><figDesc>, Zhou et al. introduce UNet++, an ensemble of nested UNets of varying depths, which partially share an encoder and jointly learn using deep supervision. Later, Jha et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 1. The network consists of two coupled UNets with a similar architecture. Each UNet has an encoder and a decoder with skip connections between them. The encoder takes an image input of size 512 × 512, then passes through five top-down blocks to produce a high-level semantic feature map of size 16 × 16. This feature map is gradually up-sampled through five bottom-up blocks of the decoder and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the proposed AG-CUResNeSt. Attention gates within each UNet are used to suppress irrelevant information in the encoder's feature maps. Skip connections across the two UNets are also utilized to boost the information flow and promote feature reuse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>addresses this problem by introducing skip connections. Suppose that H(x) as an underlying mapping to be fitted by a block of few nonlinear layers, where x is the input to the first layer. Instead of directly approximating H(x), we can let the block approximate the corresponding residual mapping F (x) = H(x) − x. The original mapping can be obtained using a skip connection as H(x) = F (x) + x. By this trick, if the network wants to learn identity mappings, it just simply needs to drive the weights of the nonlinear layers in the block toward zero. Hence, residual connections facilitate the optimization of the network at almost no cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Split attention in the k-th cardinal group with R splits. proves the feature representation to boost the performance across multiple computer vision tasks. ResNeSt proposes to split each cardinal group into R smaller feature groups, where R is called a new radix hyperparameter. Hence, the total number of feature groups is G = K × R. Each group is associated with a transformation F i , i = 1, 2, . . . , G and outputs an intermediate result U i = F i (X), where U i ∈ R H×W ×C/K , and H, W, C are the sizes of a cardinal group's output. The output of k-th cardinal group is an element-wise summation of all R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>The Attention Gate (AG) receives two inputs: a low-level feature map x from an encoder and a coarse feature map g from a corresponding decoder. The feature map x is firstly down-sampled and fused with g, then fed to some hidden layers to yield an attention coefficient map α. Finally, the input features x are scaled with attention coefficients to suppress irrelevant information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>use the performance metrics listed in the MICCAI 2015 challenge[36] to evaluate model performance: precision, recall, IoU (Jaccard score), and Dice score (F1). These are the most well-known measures for segmentation accuracy evaluation. Metrics are measured on the macro level: measurements are made on every image, then averaged on the whole dataset across all images. P + F P + F N IoU (P, G) = T P T P + F P + F N where P represents the model's prediction, G is the ground-truth, TP is true positives, FP is false positives, and FN is false negatives. 5. Results and Discussion 5.1. Ablation Study In this section, we measure the impact of each component in the proposed model. For the ablation study, we choose Scenario 1, i.e., CVC-Colon and ETIS-Larib are used for training, and CVC-Clinic is used for testing, due to two following reasons. Firstly, there are a number of options for the model's architecture. Hence, we should choose a combination with a small training dataset to speed up the evaluation process. The CVC-ColonDB dataset, including 380 images, seems to be not enough to fit large backbones. The ETIS-Larib may be a good addition to the training dataset. Secondly, the training and test datasets are taking separately from different sources with different image properties and characteristics. This cross-dataset experiment setup is useful to evaluate the generalization capability of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4</head><label>4</label><figDesc>provides additional references for the output produced by each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative result comparison using Colon for training and Clinic for testing. From left to righ: input image, ground truth, visualization of ResNet101-MaskR-CNN's output in overlay mode, binary output of ResNet101-MaskR-CNN, visualization of ResNet50-MaskR-CNN's output in overlay mode, binary output of ResNet50-MaskR-CNN, binary output of AG-CUResNeSt-101, and attention map in the last attention gate denote by S9 in Fig. 1. The red color in the attention map indicates the region where the model focus on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>The results of AG-CUResNeSt-101 on CVC-Clinic dataset. From left to right: input image, ground truth, output of the first UNet, output of the second UNet, and attention map in the last attention gate S9. The red areas in the attention map are high probability where polyps appear. Next, we compare AG-CUResNeSt-101 with Mask-RCNN and DoubleUNet in Scenario 3, i.e., using CVC-ClinicDB for training, ETIS-Larib for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative result comparison of different models trained in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8</head><label>8</label><figDesc>shows the ROC curve and PR curve for each model in this experiment. Our AG-CUResNeSt-101 again reports the best AUC value of 0.9612 and the best MAP value of 0.886. From the perspectives of endoscopists, the use of our proposed models is expected to support the training of junior staff in colon polyp detection. The improvements of our proposed model over state-of-the-art methods are especially helpful for inexperienced endoscopists in delineating lesions in challenging</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Some failed cases of our model on the Kvasir-SEG dataset cases. Furthermore, it could be considered the possibility of setting up in clinical</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>ROC curves and PR curves for AG-CUResNeSt-101, PraNet, ResUNet++ and UNet in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset. All curves are averaged over five folds. on reducing the model size without sacrificing accuracy and conducting a deep study on failed cases to further understand the characteristic of the cases for further improvement in terms of segmentation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance metrics for model variants in Scenario 1, i.e., training on CVC-Colon and ETIS-Larib, testing on CVC-Clinic</figDesc><table><row><cell>Method</cell><cell cols="4">mDice ↑ mIoU ↑ Recall ↑ Precision ↑</cell></row><row><cell>VGG16-UNet</cell><cell>0.759</cell><cell>0.660</cell><cell>0.831</cell><cell>0.778</cell></row><row><cell>Efficientnet-B0-UNet</cell><cell>0.747</cell><cell>0.650</cell><cell>0.871</cell><cell>0.737</cell></row><row><cell>Efficientnet-B1-UNet</cell><cell>0.754</cell><cell>0.662</cell><cell>0.877</cell><cell>0.747</cell></row><row><cell>Efficientnet-B2-UNet</cell><cell>0.800</cell><cell>0.715</cell><cell>0.806</cell><cell>0.860</cell></row><row><cell>Efficientnet-B3-UNet</cell><cell>0.803</cell><cell>0.716</cell><cell>0.849</cell><cell>0.824</cell></row><row><cell>Efficientnet-B4-UNet</cell><cell>0.813</cell><cell>0.731</cell><cell>0.835</cell><cell>0.860</cell></row><row><cell>ResNet34-UNet</cell><cell>0.783</cell><cell>0.692</cell><cell>0.827</cell><cell>0.821</cell></row><row><cell>ResNet50-UNet</cell><cell>0.805</cell><cell>0.719</cell><cell>0.843</cell><cell>0.827</cell></row><row><cell>ResNet101-UNet</cell><cell>0.811</cell><cell>0.731</cell><cell>0.849</cell><cell>0.838</cell></row><row><cell>ResNeSt50-UNet</cell><cell>0.814</cell><cell>0.725</cell><cell>0.829</cell><cell>0.861</cell></row><row><cell>ResNeSt101-UNet</cell><cell>0.816</cell><cell>0.739</cell><cell>0.813</cell><cell>0.888</cell></row><row><cell>Attention ResNet101-UNet</cell><cell>0.815</cell><cell>0.730</cell><cell>0.863</cell><cell>0.825</cell></row><row><cell>Attention ResNeSt101-UNet</cell><cell>0.829</cell><cell>0.749</cell><cell>0.842</cell><cell>0.877</cell></row><row><cell>Attention ResCUNet-101</cell><cell>0.820</cell><cell>0.736</cell><cell>0.859</cell><cell>0.838</cell></row><row><cell>AG-CUResNeSt-101</cell><cell>0.833</cell><cell>0.754</cell><cell>0.840</cell><cell>0.883</cell></row><row><cell cols="2">5.1.2. The Effectiveness of Attention Gate</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Next, we conduct experiments using two backbones, ResNet101 and ResNeSt101,</cell></row><row><cell cols="5">integrated with the Attention Gate (AG) module. The integrated models are</cell></row><row><cell cols="5">called Attention ResNet101-UNet and Attention ResUNeSt101-UNet, respec-</cell></row><row><cell>tively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance metrics for Mask-RCNN and AG-CUResNeSt using Scenario 2, i.e.,</figDesc><table><row><cell cols="2">using CVC-Colon for training, CVC-Clinic for testing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">mDice ↑ mIoU ↑ Recall ↑ Precision ↑</cell></row><row><cell>ResNet50-Mask-RCNN [37]</cell><cell>0.639</cell><cell>0.560</cell><cell>0.648</cell><cell>0.710</cell></row><row><cell>ResNet101-Mask-RCNN [37]</cell><cell>0.641</cell><cell>0.565</cell><cell>0.646</cell><cell>0.725</cell></row><row><cell>Attention ResNeSt101-UNet</cell><cell>0.765</cell><cell>0.681</cell><cell>0.773</cell><cell>0.842</cell></row><row><cell>Our AG-CUResNeSt-101</cell><cell>0.771</cell><cell>0.686</cell><cell>0.793</cell><cell>0.830</cell></row><row><cell cols="5">indicates a model retrained with the original reported configurations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows our evaluation results in Scenario 4, i.e., Kvasir-SEG and</cell></row><row><cell>CVC-Clinic datasets are merged, then split 80/10/10 for training, validation,</cell></row><row><cell>and testing. The proposed AG-CUResNeSt-101 is compared with PraNet [14],</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance metrics for Mask-RCNN, Double UNet and AG-CUResNeSt in Scenario 3, i.e., using CVC-ClinicDB for training, ETIS-Larib for testing</figDesc><table><row><cell>Method</cell><cell cols="4">mDice ↑ mIoU ↑ Recall ↑ Precision ↑</cell></row><row><cell>ResNet50-Mask-RCNN [37]</cell><cell>0.501</cell><cell>0.412</cell><cell>0.546</cell><cell>0.573</cell></row><row><cell>ResNet101-Mask-RCNN [37]</cell><cell>0.565</cell><cell>0.469</cell><cell>0.565</cell><cell>0.639</cell></row><row><cell>DoubleUNet (BCE loss) [15]</cell><cell>0.482</cell><cell>0.400</cell><cell>0.713</cell><cell>0.475</cell></row><row><cell>Double UNet (Dice loss) [15]</cell><cell>0.588</cell><cell>0.500</cell><cell>0.689</cell><cell>0.599</cell></row><row><cell>Attention ResNeSt101-UNet</cell><cell>0.688</cell><cell>0.605</cell><cell>0.740</cell><cell>0.687</cell></row><row><cell>Our AG-CUResNeSt-101</cell><cell>0.701</cell><cell>0.613</cell><cell>0.755</cell><cell>0.693</cell></row><row><cell cols="5">indicates a model retrained with the original reported configurations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>mDice and mIoU scores for models trained in Scenario 4 on the Kvasir-SEG and CVC-ClinicDB test sets</figDesc><table><row><cell></cell><cell cols="2">Kvasir-SEG</cell><cell cols="2">CVC-ClinicDB</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">mDice ↑ mIoU ↑ mDice ↑ mIoU ↑</cell></row><row><cell>UNet [10]</cell><cell>0.818</cell><cell>0.746</cell><cell>0.823</cell><cell>0.755</cell></row><row><cell>UNet++ [11]</cell><cell>0.821</cell><cell>0.743</cell><cell>0.794</cell><cell>0.729</cell></row><row><cell>ResUNet-mod [39]</cell><cell>0.791</cell><cell>n/a</cell><cell>0.779</cell><cell>n/a</cell></row><row><cell>ResUNet++ [12]</cell><cell>0.813</cell><cell>0.793</cell><cell>0.796</cell><cell>0.796</cell></row><row><cell>SFA [38]</cell><cell>0.723</cell><cell>0.611</cell><cell>0.700</cell><cell>0.607</cell></row><row><cell>PraNet [14]</cell><cell>0.898</cell><cell>0.840</cell><cell>0.899</cell><cell>0.849</cell></row><row><cell>Our AG-CUResNeSt-101</cell><cell>0.902</cell><cell>0.845</cell><cell>0.917</cell><cell>0.867</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance metrics for UNet, MultiResUNet and AG-CUResNeSt-101 in Scenario 5, i.e., 5-fold cross-validation on the CVC-Clinic dataset</figDesc><table><row><cell>Method</cell><cell>mDice ↑</cell><cell>mIoU ↑</cell><cell>Recall ↑</cell><cell>Precision ↑</cell></row><row><cell>UNet [10]</cell><cell>-</cell><cell>0.792</cell><cell>-</cell><cell>-</cell></row><row><cell>MultiResUNet [40]</cell><cell>-</cell><cell>0.849</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">Our AG-CUResNeSt-101 0.946±0.01 0.902±0.015 0.953±0.013 0.944±0.009</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance metrics for UNet, ResUNet++, PraNet and AG-CUResNeSt-101 in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset</figDesc><table><row><cell>Method</cell><cell>mDice ↑</cell><cell>mIoU ↑</cell><cell>Recall ↑</cell><cell>Precision ↑</cell></row><row><cell>UNet [10]</cell><cell>0.708±0.017</cell><cell>0.602±0.01</cell><cell>0.805±0.014</cell><cell>0.716±0.02</cell></row><row><cell>ResUNet++ [12]</cell><cell>0.780±0.01</cell><cell>0.681±0.008</cell><cell>0.834±0.01</cell><cell>0.799±0.01</cell></row><row><cell>PraNet [14]</cell><cell>0.883±0.02</cell><cell>0.822±0.02</cell><cell>0.897±0.02</cell><cell>0.906±0.01</cell></row><row><cell cols="3">Our AG-CUResNeSt-101 0.912±0.01 0.860±0.011</cell><cell cols="2">0.923±0.009 0.927±0.014</cell></row><row><cell cols="4">Next, we compare AG-CUResNeSt-101 against UNet and MultiResUNet [40]</cell><cell></cell></row><row><cell cols="4">in Scenario 5, i.e., 5-fold cross-validation on the CVC-Clinic dataset. Results</cell><cell></cell></row><row><cell cols="4">are shown in Table 5. Note that the authors of UNet and MultiResUNet only re-</cell><cell></cell></row><row><cell cols="4">ported their IoU scores on this dataset. Regardless, we can see AG-CUResNeSt-</cell><cell></cell></row><row><cell cols="4">101 shows significant improvement in this metric, outperforming MultiResUNet</cell><cell></cell></row><row><cell cols="2">by almost 5%, and UNet by 10%.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Finally, AG-CUResNeSt-101 is compared with UNet, ResUNet++, and PraNet</cell></row><row><cell cols="4">in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset. Results</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajkbaksh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Angermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rustad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Debard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Córdova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sánchez-Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparative validation of polyp detection methods in video colonoscopy: Results from the miccai 2015 endoscopic vision challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2017.2664042</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-grade dysplasia and invasive carcinoma in colorectal adenomas: a multivariate analysis of the impact of adenoma and patient characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gschwantler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriwanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Göritzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schrutka-Kölbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brownstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feichtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of gastroenterology &amp; hepatology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adenoma detection rate and risk of colorectal cancer and death</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Doubeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Zauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Fireman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Schottinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New england journal of medicine</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1298" to="1306" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Colorectal cancer screening: An updated review of the available options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noureddine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World journal of gastroenterology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page">5086</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adequate level of training for technical competence in screening and diagnostic colonoscopy: a prospective multicenter evaluation of the learning curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-O</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointestinal endoscopy</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="683" to="689" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visibility map: a new method in evaluation quality of optical colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Armin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grimpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Salvado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factors influencing the miss rate of polyps in a back-to-back colonoscopy study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leufkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Oijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vleggaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siersema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="470" to="475" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwahori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Automatic polyp detection in endoscope images using a hessian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasugai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>MVA</publisher>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">De</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Attention u-net: Learning where to look for the pancreas</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Doubleunet: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CBMS49503.2020.00111</idno>
	</analytic>
	<monogr>
		<title level="m">33rd IEEE International Symposium on Computer-Based Medical Systems</title>
		<editor>A. G. S. de Herrera, A. R. González, K. C. Santosh, Z. Temesgen, B. Kane, P. Soda</editor>
		<meeting><address><addrLine>Rochester, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Resnest: Split-attention networks, CoRR abs/2004.08955</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<title level="m">29th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Cu-net: Coupled u-nets</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial uv map completion for poseinvariant face recognition: a novel adversarial approach based on coupled attention residual unets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dinh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-centric Computing and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientdet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio, Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
	<note>Selective kernel networks</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientnet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tversky loss function for image segmentation using 3d fully convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on machine learning in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards automatic polyp detection with a polyp appearance model</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic colon polyp detection using region based deep cnn and post learning approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40950" to="40962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albumentations</forename></persName>
		</author>
		<ptr target="https://github.com/albumentations-team/albumentations" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Online; accessed 10</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Balasingham, Polyp detection and segmentation using mask r-cnn: Does a deeper feature extractor cnn always perform better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solhusvik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Symposium on Medical Information and Communication Technology (IS-MICT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selective feature aggregation network with area-boundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
