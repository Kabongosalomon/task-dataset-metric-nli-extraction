<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Ao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
							<email>qingyong.hu@cs.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
							<email>bo.yang@polyu.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
							<email>guoyulan@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting robust and general 3D local features is key to downstream tasks such as point cloud registration and reconstruction. Existing learning-based local descriptors are either sensitive to rotation transformations, or rely on classical handcrafted features which are neither general nor representative. In this paper, we introduce a new, yet conceptually simple, neural architecture, termed SpinNet, to extract local features which are rotationally invariant whilst sufficiently informative to enable accurate registration. A Spatial Point Transformer is first introduced to map the input local surface into a carefully designed cylindrical space, enabling end-to-end optimization with SO(2) equivariant representation. A Neural Feature Extractor which leverages the powerful point-based and 3D cylindrical convolutional neural layers is then utilized to derive a compact and representative descriptor for matching. Extensive experiments on both indoor and outdoor datasets demonstrate that SpinNet outperforms existing state-of-theart techniques by a large margin. More critically, it has the best generalization ability across unseen scenarios with different sensor modalities. The code is available at https: //github.com/QingyongHu/SpinNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate matching of partial 3D surfaces is critical for point cloud registration <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>, segmentation <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>, and recognition <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45]</ref>. Given multiple partially overlapped 3D scans, the goal of surface matching is to align these fragments according to a set of point correspondences, thus obtaining a complete 3D scene structure. To achieve this, it is of key importance to identify general and robust local geometric patterns shared between two scans. However, this is challenging, primarily because 1) different scans usually have different viewing angles, 2) the raw 3D scans are typically incomplete, noisy, and have <ref type="bibr">*</ref>   <ref type="figure">Figure 1</ref>: The Feature Matching Recall (FMR) scores of different approaches on the indoor 3DMatch <ref type="bibr" target="#b62">[63]</ref> and outdoor ETH <ref type="bibr" target="#b43">[44]</ref> dataset. Note that, all methods are trained only on the 3DMatch dataset. Our method not only achieves the highest score on 3DMatch, but also has the best generalization ability across the unseen ETH dataset. significantly different point densities.</p><p>Early methods to extract local geometries include PS <ref type="bibr" target="#b8">[9]</ref>, ISS <ref type="bibr" target="#b64">[65]</ref>, SHOT <ref type="bibr" target="#b37">[38]</ref> and RoPS <ref type="bibr" target="#b23">[24]</ref>, which simply compute the low-level features such as faces <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b61">62]</ref>, corners <ref type="bibr" target="#b49">[50]</ref>, and handcrafted statistical histograms <ref type="bibr" target="#b40">[41]</ref>. Although they achieve encouraging results on high-quality 3D point clouds, they are not capable of generalizing to highly noisy and large-scale real-world 3D point clouds.</p><p>Recent deep neural network based approaches <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43]</ref> have yielded excellent results in learning better local point features, thanks to the availability of large-scale labeled 3D datasets. However, they have two major limitations. First, many of these methods such as D3Feat <ref type="bibr" target="#b1">[2]</ref> and FCGF <ref type="bibr" target="#b7">[8]</ref> rely on kernel-based point convolution <ref type="bibr" target="#b50">[51]</ref> or submanifold sparse convolution <ref type="bibr" target="#b20">[21]</ref> to extract per-point features, resulting in the learned point local patterns being rotationally variant. Consequently, their performance drops dramatically when they are applied to novel 3D scans with strong rotational changes. Second, although a number of recent approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref> introduce rotationinvariant point descriptors, they simply integrate the hand-crafted features such as point-pairs <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b14">15]</ref> and point density <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b48">49]</ref>, or external local reference frames (LRFs) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b63">64]</ref> into the pipeline, fundamentally limiting the representational power of the framework <ref type="bibr" target="#b22">[23]</ref>. As a result, the extracted point features, albeit being rotation invariant, are not robust and general when being applied to unseen 3D scans with noise and different point densities.</p><p>In this paper, we aim to design a new neural architecture, which is able to learn descriptive local features and generalize well to unseen scenarios. This network clearly satisfies three key properties: 1) It is rotation invariant. Particularly, it learns consistent local features from 3D scans with different rotation angles; 2) It is descriptive. In essence, it preserves the prominent local patterns despite the noise, possible surface incompleteness, or different point densities; 3) It does not include any handcrafted features. Instead, it only consists of multiple point transformations and simple neural layers coupled with true end-to-end optimization. This allows the learned descriptor to be extremely representative and general for complex real-world 3D surfaces.</p><p>Our network, named SpinNet, mainly consists of two modules, 1) a Spatial Point Transformer 1 , which explicitly transforms the input 3D scans into a carefully designed cylindrical space, driving the transformed scans to be SO(2) equivariant, whilst retaining point local information; 2) a Neural Feature Extractor, which leverages powerful point-based and convolutional neural layers to learn representative and general local patterns.</p><p>The Spatial Point Transformer firstly aligns the input 3D surface according to a reference axis, eliminating the rotational variance along the Z-axis. This is followed by an additional coordinate transformation over the XY-plane with the aid of spherical voxelization, further removing the rotation variance of each spherical voxel. Lastly, the transformed local surface is formulated as a simple yet novel 3D cylindrical volume, which is amenable to consumption by the subsequent point-based and convolutional neural layers. The Neural Feature Extractor firstly uses simple pointbased MLPs to extract a unique signature for each voxel within the cylindrical volume, generating an initial set of cylindrical feature maps. These maps are further fed into a series of novel 3D cylindrical convolutional layers, which fully exploit the rich spatial and contextual information and generate a compact and representative feature vector.</p><p>These two modules enable our SpinNet to learn remarkably robust and general local features for accurate 3D point cloud registration. It achieves state-of-the-art performance both on the indoor 3DMatch <ref type="bibr" target="#b62">[63]</ref> dataset and the outdoor ETH <ref type="bibr" target="#b43">[44]</ref> dataset. Notably, it shows superior generalization ability across unseen scenarios. As shown in <ref type="figure">Figure 1</ref>, being trained only on the 3DMatch dataset, the learned descriptor of our SpinNet can achieve an average recall score of 92.8% 1 This is different from the Transformer for natural language processing. on the unseen outdoor ETH dataset for feature matching, significantly surpassing the state of the art by nearly 13%. Overall, our contributions are three-fold:</p><p>• We propose a new neural feature learner for 3D surface matching. It is rotation invariant, representative, and has superior generalization ability across unseen scenarios. • By formulating the transformed 3D surface into a cylindrical volume, we introduce a powerful 3D cylindrical convolution to learn rich and general features. • We conduct extensive experiments and ablation studies, demonstrating the remarkable generalization of our method and providing the intuition behind our choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Handcrafted Descriptors</head><p>Traditional handcrafted descriptors can be roughly divided into two categories: 1) LRF-free methods and 2) LRF-based. The LRF-free descriptors including Spin Images (SIs) <ref type="bibr" target="#b27">[28]</ref>, Local Surface Patch (LSP) <ref type="bibr" target="#b3">[4]</ref> and Fast Point Feature Histograms (FPFHs) <ref type="bibr" target="#b45">[46]</ref> are typically constructed by exploiting geometrical properties (e.g. curvatures and normal deviations) of a local surface. The main drawback of these descriptors is the lack of sufficient geometric details for the local surface. The LRF-based descriptors such as Point Signature (PS) <ref type="bibr" target="#b8">[9]</ref>, SHOT <ref type="bibr" target="#b51">[52]</ref> and Rotational Projection Statistics (RoPS) <ref type="bibr" target="#b23">[24]</ref> are not only able to characterize the geometric patterns of the local support region, but also effectively exploit the 3D spatial attributes. However, LRF-based methods inherently introduce rotation errors, sacrificing the feature robustness. Overall, all these handcrafted descriptors are usually tailored to specific tasks and sensitive to noise, thus not being sufficiently flexible and descriptive for complicated and novel scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning-based Descriptors</head><p>In contrast to traditional handcrafted descriptors, recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b58">59]</ref> leverage data-driven deep neural networks to learn local features from large-scale datasets. These learned descriptors tend to have strong descriptive ability and robustness.</p><p>Rotation Variant Descriptors. Zeng et al. propose the pioneering work 3DMatch <ref type="bibr" target="#b62">[63]</ref>, which takes the local volumetric patches as input, and then leverages 3D Convolutional Neural Networks (CNNs) to learn local geometric patterns. Yew and Lee introduce a weakly-supervised framework 3DFeat-Net <ref type="bibr" target="#b57">[58]</ref> to learn both the 3D feature detector and descriptor simultaneously. Choy et al. build a dense feature descriptor FCGF <ref type="bibr" target="#b7">[8]</ref> based on <ref type="bibr" target="#b6">[7]</ref>. Recently, Bai et al. <ref type="bibr" target="#b1">[2]</ref> design a pipeline to jointly learn both dense feature detectors and local feature descriptors, achieving the state-of-the-art performance on 3DMatch <ref type="bibr" target="#b62">[63]</ref> and KITTI <ref type="bibr" target="#b17">[18]</ref> datasets for point cloud registration. However, all these methods are sensitive to rigid transformation in Euclidian space. Extensive data augmentation can be used to alleviate this problem, however, the overall performance of subsequent tasks is still sub-optimal <ref type="bibr" target="#b16">[17]</ref>.</p><p>Rotation Invariant Descriptors. A number of recent methods have started to learn rotation-invariant descriptors. Khoury et al. <ref type="bibr" target="#b29">[30]</ref> parameterize the raw point clouds with oriented spherical histograms, and then map the highdimensional embedding to a compact descriptor through a deep neural network. Deng et al. <ref type="bibr" target="#b13">[14]</ref> encode the local surface using rotation-invariant Point Pair Features (PPFs). These features are then fed into multiple MLPs to learn a global descriptor. In the follow-up work <ref type="bibr" target="#b12">[13]</ref>, Fold-ingNet <ref type="bibr" target="#b56">[57]</ref> is adopted as the backbone network to learn 3D local descriptors. Gojcic et al. <ref type="bibr" target="#b19">[20]</ref> introduce the voxelized Smoothed Density Value (SDV) to encode the local surface as a compact and rotation-invariant representation, which is fed into a Siamese architecture to learn the final descriptor. Overall, although these methods are indeed able to learn rotationally invariant features from the local surface, they initially rely on classical handcrafted features which significantly limits the descriptiveness of the descriptors. Additionally, most of the above handcrafted features are based on the point-pairs and point density, both of which are sensitive to noise, clutter, and distance variations, making the learned features hardly generalize to novel scenarios.</p><p>A handful of recent works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> try to learn rotation-invariant local descriptors with end-to-end optimization. However, they either require the computation of the point density or rely on external reference frames to achieve rotation invariance. This is usually unstable and does not generalize well to unseen datasets. In contrast, our SpinNet directly transforms the point clouds into a cylindrical volume followed by a series of powerful neural layers. It learns rotation-invariant, compact, and highly descriptive local features in a truly end-to-end fashion, without relying on any handcrafted features or unstable external LRFs. This enables the learned descriptors to be extremely general for use on unseen 3D surfaces across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SpinNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>Given two partially overlapped point clouds P = {p i ∈ R 3 |i = 1, . . . , N } and Q = {q j ∈ R 3 |j = 1, . . . , M }. The task of point cloud registration is to find an optimal rigid transformation T = {R, t}, as well as the point correspondences to align pairs of fragments, and finally recover the complete scene. The pair of point correspondence (p i , q j ) is expected to satisfy:</p><formula xml:id="formula_0">q j = Rp i + t + i ,<label>(1)</label></formula><p>where R ∈ SO(3) denotes the rotation matrix, t ∈ R 3 is the translation vector, and i is the residual error. In practice, it is infeasible to simultaneously find the correspondences and estimate the transformation, due to the non-convexity of this problem <ref type="bibr" target="#b33">[34]</ref>. However, if the point subsets P c and Q c with one-to-one correspondences can be determined, the registration problem can be simplified as a minimization problem for the following L 2 distance:</p><formula xml:id="formula_1">L(P c , Q c |P, R, t) = 1 N Q c − RP c P − t 2 (2)</formula><p>where N is the number of successfully matched correspondences, P ∈ R N ×N is a permutation matrix whose entries satisfy P u,v = 1 if the u th point in P c corresponds to v th point in Q c and 0 otherwise. We propose a new surface feature learner SpinNet, which is a mapping function M, where M(p i ) is equal to M(q j ) under arbitrary rigid transformations such as rotation and translation, if p i and q j are indeed a correct match. In particular, our feature learner mainly consists of a Spatial Point Transformer and a Neural Feature Extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Point Transformer</head><p>This module is designed to spatially transform the input 3D surfaces into a cylindrical volume, overcoming the rotation variance, whilst without dropping critical information of local patterns. As shown in <ref type="figure">Figure 2</ref>, it consists of four components, as discussed below.</p><p>Alignment with a Reference Axis. Given a specific point p ∈ P in a local surface, we first estimate a reference axis n p oriented to the viewpoint <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1]</ref> from its neighbouring point set P s = {p i : p i − p 2 ≤ R} within a support radius R. We then align n p with the Z-axis using a rotation matrix R z . Compared with the external local reference frames which are likely to be ambiguous and unstable, our estimated n p tends to be more robust and stable with regard to rotation changes <ref type="bibr" target="#b41">[42]</ref>. Subsequently, the neighbouring point set P s is transformed to P s r = R z P s . To achieve translation invariance, we further normalize P s r by offsetting to the center point, i.e., P s r = P s r − R z p. Hence, the obtained local patch P s r is aligned with the z-axis, leaving the remaining rotational degree of freedom entirely on the XY-plane.</p><p>Spherical Voxelization. To further eliminate the rotational variance on the XY-plane, we leverage a rotationrobust spherical representation. In particular, we treat the patch P s r as a sphere, and evenly divide it into J × K × L voxels along the radial distance ρ, elevation angle φ and azimuth angle θ. The center of each voxel is denoted as v jkl , where j ∈ {1, ..., J}, k ∈ {1, ..., K}, l ∈ {1, ..., L}. We then explicitly identify a set of neighboring points for the center point v jkl of each voxel. In particular, we use the radius query to find the neighboring points P jkl ⊂ P s r based </p><formula xml:id="formula_2">K L J Z X Y Z X Y Z X Y v R Z X Y Figure 2:</formula><p>The detailed components and processing steps of our Spatial Point Transformer.</p><formula xml:id="formula_3">on a fixed radius R v , where P jkl = { p i : p i − v jkl 2 ≤ R v , p i ∈ P s r }.</formula><p>Lastly, we randomly sample and preserve a fixed number of k v points for each voxel, aiming for efficient computation in parallel. This spherical voxelization step is key to the successive spatial point transformation.</p><p>Transformation on the XY-Plane. To enable each spherical voxel to be rotationally invariant on the XY-plane, we proactively rotate each voxel around the Z-axis to align its center v jkl with the YZ-plane, where the rotation matrix R jkl is defined as:</p><formula xml:id="formula_4">R jkl =   cos(π/2 − 2πl/L) − sin(π/2 − 2πl/L) 0 sin(π/2 − 2πl/L) cos(π/2 − 2πl/L) 0 0 0 1   (3)</formula><p>This removes an additional rotational degree of freedom for each voxel on the XY-plane, without dropping any local geometric patterns of each voxel. Note that, the existing methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b59">60]</ref> usually use handcrafted features to achieve rotation invariance, resulting in the loss of the rich local patterns. Uniquely, our simple strategy to transform voxels can preserve these patterns, leaving them to be learned by the powerful neural layers.</p><p>Cylindrical Volume Formulation. Once the local patterns of each voxel are transformed, it is crucial to further preserve the larger spatial structures across multiple voxels. This requires the relative positions of all voxels to be represented in the whole framework. To this end, we reformulate the spherical voxels into a cylindrical volume. This is amenable to the proposed 3D cylindrical convolutional network, which guarantees the SO(2) equivariance of the input local surface and preserves the topological patterns of multiple voxels. In particular, given the transformed spherical voxels, each of which has a set of neighbouring points, we logically project them into a cylindrical volume, denoted as C ∈ R J×K×L×kv×3 and illustrated in <ref type="figure">Figure 2</ref>.</p><p>In summary, given an input surface patch, our Spatial Point Transformer explicitly aligns its Z-axis with a reference axis, and proactively transforms the spherical voxel patterns on the XY-plane, and further preserves the topolog-ical surface structures through the cylindrical volume formulation. Clearly, this module keeps all surface patterns intact for the subsequent Neural Feature Extractor to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Neural Feature Extractor</head><p>This module is designed to learn the general features from the transformed points within each cylindrical voxel using the powerful neural layers. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, it consists of two components, as discussed below.</p><p>Point-based Layers. Given the points within each cylindrical voxel, we use shared MLPs followed by a maxpooling function A(·) to learn an initial signature for each voxel. Formally, the point-based layers are defined as:</p><formula xml:id="formula_5">f jkl = A(MLPs(R jkl P jkl ))<label>(4)</label></formula><p>where f jkl is the learned features with D dimension. Note that, the MLP weights are shared across all spherical voxels. Eventually, we obtain a set of 3D cylindrical feature maps F ∈ R J×K×L×D . 3D Cylindrical Convolutional Layers. To further learn spatial structures across multiple voxels of the volume, we propose an efficient 3D Cylindrical Convolution Network (3DCCN) inspired by <ref type="bibr" target="#b28">[29]</ref>. In particular, given a voxel located at the position (j, k, l) on the d th cylindrical feature map in the s th layer, our 3DCCN is defined as follows.</p><formula xml:id="formula_6">F sd jkl = D d=1 Rs r=1 Ys y=1 Xs x=1 w sd d ryx F (s−1)d (j+r)(k+y)(l+x) . (5)</formula><p>where R s is the size of the kernel along the radial dimension, Y s and X s are the height and width of the kernel respectively, w sd d ryx are the learnable parameters. Being quite different from existing convolution operations, our proposed 3DCCN is novel in the following two aspects. First, since the cylindrical feature maps are 360 • continuous over a cylinder, our 3DCCN is designed to wrap around these feature maps, i.e., over the periodic boundary from −180 • to 180 • . Therefore, explicit padding is not required in our 3DCCN, but required by 3D-CNN at the boundary of feature maps. Second, compared with the existing 3D manifold sparse convolution <ref type="bibr" target="#b7">[8]</ref> or kernel point  convolutions <ref type="bibr" target="#b1">[2]</ref>, the continuous convolution around the 360 • volume enables the obtained feature map to be SO(2) equivariant, hence to achieve the final rotation-invariance.</p><p>After stacking multiple of these 3DCCN layers followed by max-pooling, the original cylindrical feature maps are compressed to a compact and representative feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">End-to-end Implementation</head><p>The Spatial Point Transformer is directly connected with the Neural Feature Extractor, followed by the existing contrastive loss <ref type="bibr" target="#b1">[2]</ref> for end-to-end optimization. The widelyused hardest in batch sampling <ref type="bibr" target="#b39">[40]</ref> is also adopted on-thefly to maximize the distance between the closest positive and the closest negative patches. Details of the neural layers are presented in the appendix.</p><p>We implement our SpinNet based on the PyTorch framework. The Adam optimizer <ref type="bibr" target="#b32">[33]</ref> with default parameters is used. The initial learning rate is set to 0.001 and decayed with a rate of 0.5 for every 5 epochs. We train the network for 20 epochs, the best-performed model on the validation set is then used for testing. For a fair comparison, we keep the same setting for all experiments. All experiments are conducted on the platform with Intel Xeon CPU @2.30GHZ with an NVIDIA RTX2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first evaluate our SpinNet on the indoor 3DMatch dataset <ref type="bibr" target="#b62">[63]</ref> and the outdoor KITTI dataset <ref type="bibr" target="#b17">[18]</ref>. We then evaluate the generalization ability of our approach across multiple unseen datasets <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44]</ref> acquired by different sensors. Lastly, extensive ablation studies are conducted.</p><p>Experimental Setup. We follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b62">63]</ref> to generate training samples by only considering the point cloud fragment pairs with more than 30% overlap in the whole dataset. For each paired fragment P and Q, we randomly sample a fixed number of anchor points from the overlapping region of P, and then apply the ground-truth transformation T = {R, t} to determine the corresponding points in fragment Q. Considering the varying number of point cloud fragments in different datasets, we uniformly select 20 and 500 anchor points from each fragment in the 3DMatch and KITTI dataset, so as to keep a similar number of samples for training. For each anchor point, we randomly sample 2048 points from its support region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on Indoor 3DMatch Dataset</head><p>3DMatch is a RGBD-reconstruction dataset, which consists of 62 real-world indoor scenes collected from existing dataset <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b10">11]</ref>. We follow the official protocol provided in <ref type="bibr" target="#b1">[2]</ref> to divide the scenes into training and testing splits. Each scene contains several partially overlapped fragments, and has the ground truth transformation parameters available for evaluation. Feature Matching Recall (FMR) <ref type="bibr" target="#b12">[13]</ref> is used as the standard metric. Comparisons with the state-of-the-arts. We first compare the FMR scores achieved by our SpinNet and strong baselines (including LMVD <ref type="bibr" target="#b34">[35]</ref>, D3Feat <ref type="bibr" target="#b1">[2]</ref>, FCGF <ref type="bibr" target="#b7">[8]</ref>, PerfectMatch <ref type="bibr" target="#b19">[20]</ref>, PPFNet <ref type="bibr" target="#b13">[14]</ref>, and PPF-FoldNet <ref type="bibr" target="#b12">[13]</ref>) on the 3DMatch dataset, under the conditions of sampling points f =5000, distance threshold τ 1 =10 cm and inlier ratio threshold τ 2 =5%. To further evaluate the robustness of all approaches against rotations, we follow <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref> to build a rotated 3DMatch benchmark by applying arbitrary rotations in SO(3) group to all fragments of the dataset.   <ref type="table">Table 2</ref>: Quantitative results on the 3DMatch dataset using different numbers of sampled points. <ref type="figure">Figure 4</ref>: Feature matching recall on the 3DMatch dataset under different inlier distance threshold τ 1 (Left) and inlier ratio threshold τ 2 (Right). <ref type="table" target="#tab_3">Table 1</ref>, the descriptor generated by our method achieves the highest average FMR score and the lowest standard deviation on both the original and rotated datasets, outperforming the state-of-the-art methods. Note that, several baselines <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref> require rotation-based data augmentation for training, whilst ours does not. Performance under different number of sampled points. We further evaluate the performance of our SpinNet on the 3DMatch by taking different number of sampled points as input. As shown in <ref type="table">Table 2</ref>, the descriptor learned by our SpinNet consistently achieves the best FMR scores when the number of sampled points is reduced from 5000 to 250. In particular, by randomly selecting points, our method even outperforms D3Feat-pred which has an explicit keypoint detection module. This demonstrates our network is highly robust and not sensitive to the number of sampled points. Performance under Different Error Thresholds. Additionally, we evaluate the robustness of SpinNet by varying the error thresholds (τ 1 and τ 2 ). As shown in <ref type="figure">Figure  4</ref>, the descriptor generated by SpinNet consistently outperforms other methods under all thresholds. It is worth noting that the FMR score of our method is significantly higher than others, when the inlier ratio threshold increases.  <ref type="table">Table 3</ref>: Quantitative results of different approaches on the KITTI odometry dataset. The scores of baselines are retrieved from <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>For a stricter condition τ 2 = 0.2, our method maintains a high FMR score of 85.7%, while D3Feat and FCGF drop to 75.8% and 67.4%, respectively. This highlights that our approach is more robust in harder scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Outdoor KITTI Dataset</head><p>KITTI odometry <ref type="bibr" target="#b17">[18]</ref> is an outdoor sparse point cloud dataset acquired by Velodyne-64 3D LiDAR scanners. It consists of 11 sequences of outdoor scans. For fair comparison, we follow the same dataset splits and preprocessing methods as used in D3Feat <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. Similar to <ref type="bibr" target="#b36">[37]</ref>, Relative Translational Error (RTE), Relative Rotation Error (RRE), and Success rate are used as the evaluation metrics. The registration is regarded as successful if the RTE and RRE of a pair of fragments are both below the predefined thresholds 2m and 5 • . It is noted that the point clouds are gravityaligned in this dataset, we follow <ref type="bibr" target="#b57">[58]</ref> to skip the alignment with a reference axis in our method. As shown in <ref type="table">Table 3</ref>, the results of our SpinNet are on par with the strong baseline D3Feat. Admittedly, our SpinNet is marginally lower than the state-of-the-art D3Feat-pred, primarily because D3Feat has a powerful joint learned descriptor and keypoint detector. Also, the well aligned point clouds in this dataset are indeed in favor of D3Feat. We leave the integration of keypoint detection for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalization across Unseen Datasets</head><p>We have conducted several groups of experiments to extensively evaluate the generalization ability of our SpinNet. In each group, our network is trained on one dataset, and then directly tested on a completely unseen dataset.</p><p>Generalization from 3DMatch to ETH dataset. Following the settings in <ref type="bibr" target="#b1">[2]</ref>, all models are only trained on the 3DMatch dataset, and then directly tested on the ETH dataset <ref type="bibr" target="#b43">[44]</ref>. Note that, the ETH dataset consists of four scenes, i.e., Gazebo-Summer, Gazebo-Winter, Wood-Summer, and Wood-Autumn. Different from 3DMatch, the ETH dataset is acquired by static terrestrial scanners and dominated by outdoor vegetation, such as trees and bushes. In addition, the fragments of point clouds in the ETH dataset have lower resolution and contain more com-  plex geometries compared with the 3DMatch dataset. The large domain gap between these two datasets poses a great challenge to the generalization of all approaches. As shown in <ref type="table" target="#tab_6">Table 4</ref>, the performance of all baselines, namely D3Feat, FCGF, 3DMatch, and CGF, show a significant drop on the ETH dataset. Their FMR scores decrease up to 80% compared with their results on the original 3DMatch dataset, as shown in <ref type="table" target="#tab_3">Table 1</ref>, and some techniques are even lower in performance than handcrafted descriptors such as SHOT. Fundamentally, the poor generalization of these methods is attributed to the fact that the descriptors learned by D3Feat, FCGF, and 3DMatch are variant to rigid transformations such as rotation and translation.</p><p>The descriptor generated by our SpinNet achieves the highest FMR scores on all four scenes, significantly surpassing the second-best method (LMVD) by about 13%. This clearly shows that our method has excellent generalization ability across the unseen dataset collected by a new sensor modality. This is primarily because our SpinNet is explicitly designed to achieve rotational invariance. The first row of <ref type="figure">Figure 5</ref> shows the qualitative results.   ages. As presented in <ref type="table" target="#tab_8">Table 5</ref>, both D3Feat and FCGF achieve poor results on the 3DMatch dataset, especially when arbitary rotation in SO(3) exists. Their scores are even lower than the traditional methods such as FPFH, primarily because both D3Feat and FCGF have large numbers of parameters and tend to overfit the KITTI dataset, without learning the representative and general local patterns that can be applicable to the unseen dataset. By comparison, our SpinNet achieves an overall FMR score of 79.6%, demonstrating the superior generalization across novel scenarios. The second row of <ref type="figure">Figure 5</ref> shows the qualitative results.</p><p>Generalization from 3DMatch to KITTI dataset. Additionaly, we evaluate the generalization ability from 3DMatch to KITTI dataset. All models are only trained on the indoor 3DMatch dataset, and then directly tested on the outdoor KITTI dataset. <ref type="table" target="#tab_9">Table 6</ref> presents the quantitative results. Because these two datasets are collected by different types of sensors, there is a large gap between the data distributions. Neither FCGF nor D3Feat can effectively generalize from 3DMatch to KITTI dataset. However, our method still demonstrates an excellent success rate of 69.19%, doubling that of the second best method. The third row of <ref type="figure">Figure 5</ref> shows the qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To systematically evaluate the effectiveness of each component in our SpinNet, we conduct extensive ablative experiments on both 3DMatch and ETH datasets. In particular, we train all ablated models on the 3DMatch dataset, and then directly test them on both 3DMatch and ETH datasets.</p><p>(1) Only removing the alignment with a reference axis. Initially, the reference axis is computed to align the input patch with the Z-axis. By removing this step, the rotation invariance on SO(3) is no longer maintained.</p><p>(2) Only removing the transformation on the XY-plane. The transformation employed on the XY-plane is originally designed to eliminate the rotation variance of each voxel in the plane. In this experiment, we remove the transformation and directly operate on the non-transformed spherical voxels to formulate the cylindrical volume.  D3Feat [2] FCGF <ref type="bibr" target="#b6">[7]</ref> Ours Ground Truth <ref type="figure">Figure 5</ref>: Qualitative results of our method on unseen datasets. The first row is from 3DMatch to ETH, the second row is from KITTI to 3DMatch, and the third row is from 3DMatch to KITTI.  <ref type="table">Table 7</ref>: The FMR scores of all ablated models on the 3DMatch and ETH datasets with τ 1 = 0.1cm.</p><p>for each cylindrical voxel, we manually compute the point density of each voxel as its signature. Basically, this is to validate whether our point-based learned features are more general and representative than the commonly used, yet limited, handcrafted feature.</p><p>(4) Only replacing 3DCCN by MLPs. The 3DCNN is designed to learn larger spatial structures from multiple voxels, whilst maintaining rotation equivariance. In this experiment, we replace the 3DCNN layers with the same number of MLP layers shared by all cylindrical voxels. These MLPs are unable to learn a wide context. Analysis. <ref type="table">Table 7</ref> shows the results of all ablated networks on the 3DMatch dataset, as well as the generalization performance on the ETH datasets. It can be seen that: 1) Without using the alignment of a reference axis or the transformation of spherical voxels, the ablated models are unable to effectively match the point clouds either in 3DMatch or ETH datasets, especially for the point clouds with random rotations. This shows that the proposed Spatial Point Transformer indeed plays an important role to achieve rotation invariance in our SpinNet. 2) Without using the advanced point-based neural layers to learn the signatures for spherical voxels, the ablated method can obtain consistent results on the 3DMatch dataset using the simple handcrafted feature, i.e., point density, but fails to generalize to the unseen ETH dataset. This clearly demonstrates that the learned local features tend to be much more powerful and general than the handcrafted features. 3) Without using the 3DCCN to learn larger surface structures, the ablated model only obtains significantly lower scores on both the 3DMatch and ETH datasets. This demonstrates that our 3DCCN is a key to preserving the local spatial patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a new neural descriptor to learn compact representations for complex 3D surfaces. The learned representations are rotation invariant, descriptive, and able to preserve complex local geometric patterns. Extensive experiments demonstrate that our descriptor has remarkable generalization ability across unseen scenarios and achieves superior results for 3D point cloud registration. In future, we will investigate the integration of keypoint detector, as well as the fully-convolutional architecture.  <ref type="figure" target="#fig_1">3×3×3, filters×32, stride×1, dilation×1  : 3DCConv 3×3×3, filters×64, stride×1, dilation×1  : 3DCConv 1×3×3, filters×128, stride×1, dilation×2  : 3DCConv 1×3×3, filters×128, stride×1, dilation×1   : 3DCConv 1×3×3, filters×64, stride×1, dilation×2  : 3DCConv 1×3×3, filters×64, stride×1, dilation×1  : 3DCConv 1×2×2, filters×32, stride×1</ref>, dilation×2 <ref type="figure">Figure 6</ref>: Detailed architecture of our proposed 3D cylindrical convolution networks. feature representations. In particular, the maximum number of channels used in our cylindrical feature map is 128, which is much smaller than 1024 used in D3Feat <ref type="bibr" target="#b1">[2]</ref>. This further makes our network very lightweight and less prone to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed Evaluation Metrics</head><p>We further provide the detailed evaluation metrics used in our experiments (Sec. 4).</p><p>Evaluation Metrics on 3DMatch and ETH. We adopt Feature Matching Recall (FMR) as the main evaluation metric to evaluate the performance of the learned descriptors. Similar to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8]</ref>, we also provide a formal definition for each metric as follows.</p><p>First, suppose there are a total of H pairs of fragments in the 3DMatch dataset, where the overlap is greater than 30%. Each pair of fragments P h and Q h can be aligned by the ground-truth rigid transformation T h = {R h , t h }. Then, we randomly select n points from the two point clouds to obtain P n h = {p 1 , p 2 , ..., p n } and Q n h = {q 1 , q 2 , ..., q n }. In particular, a set of point correspondences Ω h between P n h and Q n h is also generated by applying nearest neighbor search NN in the feature space M:</p><p>Then the average feature matching recall on the 3DMatch dataset is defined as:</p><formula xml:id="formula_7">FMR = 1 H H h=1 1 1 |Ω h | (pi,qj )∈Ω h 1( p i − q j &lt; τ 1 ) &gt; τ 2 ,<label>(14)</label></formula><p>where p i = R h p i + t h , || · || denotes the Euclidean distance, τ 1 and τ 2 is the inlier distance threshold and inliner ratio threshold, respectively. 1 is the indicator function. Ω h denotes a set of point correspondences between P n h and Q n h . In particular, it is generated by applying nearest neighbor search NN in the feature space M:</p><formula xml:id="formula_8">Ω h = {{p i , q j }|M(p i ) = NN(M(q j ), M(P n h )), M(q j ) = NN(M(p i ), M(Q n h ))}.<label>(15)</label></formula><p>Evaluation Metrics on KITTI. Different from the indoor 3DMatch dataset, the evaluation metrics on the KITTI dataset are Relative Translational Error (RTE), Relative Rotation Error (RRE), and Success Rate (SR), respectively. According to the definitions in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b7">8]</ref>, for a pair of fragments P h and Q h , the relative rotation error RRE is calculated as:</p><formula xml:id="formula_9">RRE = arccos trace(R T h R h ) − 1 2 180 π ,<label>(16)</label></formula><p>where R h andR h denote the ground-truth and the estimated rotation matrix, respectively. Analogously, the relative translation error RTE can be calculated by:</p><formula xml:id="formula_10">RTE = t h − t h ,<label>(17)</label></formula><p>where t h andt h denote the ground-truth and the estimated translation matrix, respectively. Finally, success rate SR is defined as:</p><formula xml:id="formula_11">SR = 1 H H h=1 1 RRE h &lt; 2m &amp;&amp; RTE h &lt; 5 • . (18)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>Here we provide extra implementation details in this section. The detailed hyperparameter settings of our SpinNet on different datasets are listed in <ref type="table" target="#tab_3">Table 10</ref>. In particular, we keep the same parameter settings as the training dataset when generalized to unseen datasets, except for the support radius R and query radius R v , due to the varying point densities in different datasets. Specifically, we follow the scheme in D3Feat <ref type="bibr" target="#b1">[2]</ref> to adaptively adjust the radius according to the ratio.   <ref type="table">Table 9</ref>: Average recall (%) of different methods on the rotated 3DMatch benchmark with τ 1 = 10cm and τ 2 = 0.05. The symbol '-' means the results are unavailable and † means the results are reported from <ref type="bibr" target="#b12">[13]</ref>, which is different from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Results on 3DMatch</head><p>For comparison, we also report the detailed quantitative results of our SpinNet on the 3DMatch dataset in <ref type="table" target="#tab_12">Table 8</ref> and the rotated 3DMatch dataset in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional qualitative results.</head><p>As illustrated in Sec. 4.3, our SpinNet has demonstrated superior quantitative generalization performance across different datasets with different sensor modalities. Here, we further show additional qualitative results in this section.</p><p>Additional qualitative results on the 3DMatch dataset. We first show the additional qualitative results achieved by FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b1">[2]</ref>, and our SpinNet on the 3DMatch dataset in <ref type="figure">Fig. 7</ref>. It can be seen that the FCGF and D3Feat are prone to mismatching the fragments when the two input partial scans have relatively significant differences. However, our simple SpinNet can always achieve consistent reg-istration performance on this dataset, despite only being trained on the outdoor KITTI dataset with sparse LiDAR point clouds. Additional qualitative results on the KITTI dataset.</p><p>Then, we show the extra qualitative results achieved by FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b1">[2]</ref>, and our SpinNet on the KITTI dataset in <ref type="figure">Fig. 8</ref>. We can clearly see that the point cloud in the KITTI dataset is significantly different from the point cloud in 3DMatch, since the KITTI dataset is mainly composed of large-scale, sparse, and partial Li-DAR scans. As shown in <ref type="figure">Figure,</ref> FCGF and D3Feat tend to misalign the input fragments when the scene contains lots of geometrically-similar objects (e.g., cars). However, our method can still achieve satisfactory registration results when only trained on the indoor 3DMatch dataset. This further validates the superior generalization ability of our method. Additional qualitative results on the ETH dataset. We finally show the extra qualitative results achieved by FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b1">[2]</ref>, and our SpinNet on the ETH dataset in <ref type="figure">Fig.  9</ref>. Compared with the 3DMatch and KITTI data sets, the ETH dataset is collected by static terrestrial lasers in outdoor scenes, and is mainly composed of bushes and vegetation. As shown in <ref type="figure">Figure,</ref> it is highly challenging for FCGF and D3Feat to successfully align the input scans together, since this dataset suffers from issues such as noise, clutter, and occlusions. Nevertheless, the proposed SpinNet can still achieve excellent performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fragment 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fragment 2</head><p>D3Feat [2] FCGF <ref type="bibr" target="#b6">[7]</ref> Ours Ground Truth <ref type="figure">Figure 7</ref>: Additional qualitative results achieved by FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b1">[2]</ref>, and our SpinNet on the 3DMatch dataset. Note that, all methods are only trained on the outdoor KITTI <ref type="bibr" target="#b17">[18]</ref> dataset. Red boxes/circles show the failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fragment 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fragment 2 D3Feat [2] FCGF [7]</head><p>Ours Ground Truth <ref type="figure">Figure 8</ref>: Additional qualitative results achieved by FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b1">[2]</ref>, and our SpinNet on the KITTI dataset. Note that, all methods are only trained on the indoor 3DMatch <ref type="bibr" target="#b62">[63]</ref> dataset. Red boxes show the failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fragment 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fragment 2 D3Feat [2] FCGF [7]</head><p>Ours Ground Truth <ref type="figure">Figure 9</ref>: Additional qualitative results achieved by FCGF <ref type="bibr" target="#b7">[8]</ref>, D3Feat <ref type="bibr" target="#b1">[2]</ref>, and our SpinNet on the ETH dataset. Note that, all methods are only trained on the indoor 3DMatch <ref type="bibr" target="#b62">[63]</ref> dataset. Red boxes/circles show the failure cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the proposed Neural Feature Extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 3 )</head><label>3</label><figDesc>Only replacing the point-based layers with density. Instead of using the Point-based layers to learn a signature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Equal contribution 3DMatch [59.6% 16.9%] CGF [58.2% 20.2%] FCGF [95.2% 16.1%] Ours [97.6% 92.8%] D3Feat-pred [95.8% 61.6%] D3Feat-rand [95.3% 26.2%]</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>J L K Spatial Point Transformer Input Local Surface R p Input Support Patch Alignment with a Reference Axis Transformation on the XY-plane Cylindrical Volume Formulation Spherical Voxelization</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">#Sampled points 5000 2500 1000 500 250 Average</cell></row><row><cell>Feature Matching Recall (%)</cell><cell></cell></row><row><cell>PerfectMatch [20] 94.7 94.2 92.6 90.1 82.9</cell><cell>90.9</cell></row><row><cell>FCGF [8] 95.2 95.5 94.6 93.0 89.9</cell><cell>93.6</cell></row><row><cell>D3Feat-rand [2] 95.3 95.1 94.2 93.6 90.8</cell><cell>93.8</cell></row><row><cell>D3Feat-pred [2] 95.8 95.6 94.6 94.3 93.3</cell><cell>94.7</cell></row><row><cell>SpinNet (Ours) 97.6 97.5 97.3 96.3 94.3</cell><cell>96.6</cell></row></table><note>Quantitative results on the 3DMatch dataset, STD: standard deviation. The symbol '-' means the results are unavailable or STD under low FMRs (&lt;10%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results on the ETH dataset. Note that, all methods are only trained on the indoor 3DMatch dataset. The FMR scores at τ 1 = 10cm, τ 2 = 5% are compared.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results of different methods on the indoor 3DMatch dataset. Note that, all methods are only trained on the outdoor KITTI dataset.Generalization from KITTI to 3DMatch dataset. All models are trained on the outdoor KITTI dataset which is mainly composed of sparse LiDAR point clouds, and then directly tested on the indoor 3DMatch dataset which consists of dense point clouds reconstructed from RGBD im-] 27.1 5.<ref type="bibr" target="#b57">58</ref> 1.61 1.51 24.19 D3Feat-rand [2] 37.8 9.98 1.58 1.47 18.47 D3Feat-pred [2] 31.6 10.1 1.44 1.35 36.76 SpinNet (Ours) 15.6 1.89 0.98 0.63 81.44</figDesc><table><row><cell>RTE (cm) AVG STD AVG STD RRE ( • )</cell><cell>Success (%)</cell></row><row><cell>FCGF [8</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results on the KITTI dataset. Note that, all models are trained on the indoor 3DMatch dataset, while being directly tested on the outdoor KITTI dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>FPFH<ref type="bibr" target="#b45">[46]</ref> SHOT<ref type="bibr" target="#b52">[53]</ref> 3DMatch<ref type="bibr" target="#b62">[63]</ref> CGF †<ref type="bibr" target="#b29">[30]</ref> PPFNet<ref type="bibr" target="#b13">[14]</ref> PPF-FoldNet<ref type="bibr" target="#b12">[13]</ref> PerfectMatch<ref type="bibr" target="#b19">[20]</ref> FCGF<ref type="bibr" target="#b7">[8]</ref> D3Feat<ref type="bibr" target="#b1">[2]</ref> LMVD<ref type="bibr" target="#b34">[35]</ref> Ours</figDesc><table><row><cell>Kitchen</cell><cell>30.6</cell><cell>17.8</cell><cell>57.5</cell><cell>46.1</cell><cell>89.7</cell><cell>78.7</cell><cell>97.0</cell><cell>-</cell><cell>-</cell><cell>99.4</cell><cell>99.2</cell></row><row><cell>Home 1</cell><cell>58.3</cell><cell>37.2</cell><cell>73.7</cell><cell>61.5</cell><cell>55.8</cell><cell>76.3</cell><cell>95.5</cell><cell>-</cell><cell>-</cell><cell>98.7</cell><cell>98.1</cell></row><row><cell>Home 2</cell><cell>46.6</cell><cell>33.7</cell><cell>70.7</cell><cell>56.3</cell><cell>59.1</cell><cell>61.5</cell><cell>89.4</cell><cell>-</cell><cell>-</cell><cell>94.7</cell><cell>96.2</cell></row><row><cell>Hotel 1</cell><cell>26.1</cell><cell>20.8</cell><cell>57.1</cell><cell>44.7</cell><cell>58.0</cell><cell>68.1</cell><cell>96.5</cell><cell>-</cell><cell>-</cell><cell>99.6</cell><cell>99.6</cell></row><row><cell>Hotel 2</cell><cell>32.7</cell><cell>22.1</cell><cell>44.2</cell><cell>38.5</cell><cell>57.7</cell><cell>71.2</cell><cell>93.3</cell><cell>-</cell><cell>-</cell><cell>100.0</cell><cell>97.1</cell></row><row><cell>Hotel 3</cell><cell>50.0</cell><cell>38.9</cell><cell>63.0</cell><cell>59.3</cell><cell>61.1</cell><cell>94.4</cell><cell>98.2</cell><cell>-</cell><cell>-</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>Study</cell><cell>15.4</cell><cell>7.2</cell><cell>56.2</cell><cell>40.8</cell><cell>53.4</cell><cell>62.0</cell><cell>94.5</cell><cell>-</cell><cell>-</cell><cell>95.5</cell><cell>95.6</cell></row><row><cell>MIT Lab</cell><cell>27.3</cell><cell>13.0</cell><cell>54.6</cell><cell>35.1</cell><cell>63.6</cell><cell>62.3</cell><cell>93.5</cell><cell>-</cell><cell>-</cell><cell>92.2</cell><cell>94.8</cell></row><row><cell>Average</cell><cell>35.9</cell><cell>23.8</cell><cell>59.6</cell><cell>47.8</cell><cell>62.3</cell><cell>71.8</cell><cell>94.7</cell><cell>95.2</cell><cell>95.8</cell><cell>97.5</cell><cell>97.6</cell></row><row><cell>STD</cell><cell>13.4</cell><cell>10.9</cell><cell>8.8</cell><cell>9.4</cell><cell>10.8</cell><cell>10.5</cell><cell>2.7</cell><cell>2.9</cell><cell>2.9</cell><cell>2.8</cell><cell>1.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Average recall (%) of different methods on the 3DMatch benchmark with τ 1 = 10cm and τ 2 = 0.05. The symbol '-' means the results are unavailable and † means the results are reported from<ref type="bibr" target="#b12">[13]</ref>, which is different fromTable 1.</figDesc><table><row><cell></cell><cell cols="11">FPFH [46] SHOT [53] 3DMatch [63] CGF  † [30] PPFNet [14] PPF-FoldNet [13] PerfectMatch [20] FCGF [8] D3Feat [2] LMVD [35] Ours</cell></row><row><cell>Kitchen</cell><cell>29.1</cell><cell>17.8</cell><cell>0.4</cell><cell>44.7</cell><cell>0.2</cell><cell>78.9</cell><cell>97.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.0</cell></row><row><cell>Home 1</cell><cell>59.0</cell><cell>35.6</cell><cell>1.3</cell><cell>66.7</cell><cell>0.0</cell><cell>78.2</cell><cell>96.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>98.7</cell></row><row><cell>Home 2</cell><cell>47.1</cell><cell>33.7</cell><cell>3.4</cell><cell>52.9</cell><cell>1.4</cell><cell>64.4</cell><cell>90.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>96.2</cell></row><row><cell>Hotel 1</cell><cell>30.1</cell><cell>21.7</cell><cell>0.4</cell><cell>44.3</cell><cell>0.4</cell><cell>67.7</cell><cell>96.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.6</cell></row><row><cell>Hotel 2</cell><cell>30.0</cell><cell>24.0</cell><cell>0.0</cell><cell>44.2</cell><cell>0.0</cell><cell>62.9</cell><cell>92.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.1</cell></row><row><cell>Hotel 3</cell><cell>51.9</cell><cell>33.3</cell><cell>1.0</cell><cell>63.0</cell><cell>0.0</cell><cell>96.3</cell><cell>98.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>100.0</cell></row><row><cell>Study</cell><cell>15.8</cell><cell>8.2</cell><cell>0.0</cell><cell>41.8</cell><cell>0.0</cell><cell>62.7</cell><cell>94.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.9</cell></row><row><cell>MIT Lab</cell><cell>41.6</cell><cell>62.3</cell><cell>3.9</cell><cell>45.5</cell><cell>0.0</cell><cell>67.5</cell><cell>93.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.8</cell></row><row><cell>Average</cell><cell>36.4</cell><cell>23.4</cell><cell>1.1</cell><cell>49.9</cell><cell>0.3</cell><cell>73.1</cell><cell>94.9</cell><cell>95.3</cell><cell>95.5</cell><cell>96.9</cell><cell>97.5</cell></row><row><cell>STD</cell><cell>13.6</cell><cell>9.5</cell><cell>1.2</cell><cell>8.9</cell><cell>0.5</cell><cell>10.4</cell><cell>2.5</cell><cell>3.3</cell><cell>3.5</cell><cell>-</cell><cell>1.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset J K L</cell><cell>R</cell><cell>R v</cell><cell>k v</cell></row><row><cell cols="4">3DMatch [63] 9 40 80 0.3m 0.04m 30</cell></row><row><cell cols="4">KITTI [18] 9 30 60 2.0m 0.30m 30</cell></row><row><cell cols="4">ETH [44] 9 40 80 0.8m 0.10m 30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>The hyperparameters set by our method in different datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definitions of Equivariance and Invariance</head><p>For a specified function f : X → Y as well as a specified group action G, f is said to be equivariant with respect to transformation action g ∈ G if,</p><p>Analogously, f is said to be invariant to transformations g ∈ G when the following equation is satisfied:</p><p>B. Theoretical Proof of Equivariance Lemma 1. Given a discrete 2D rotation group 2 R ⊂ SO(2), where R = {r i ∈ R 3×3 , i = 1, 2, ..., L}, then the proposed spatial point transformer is an equivariant map for the 2D rotation group R.</p><p>Proof: For a local patch P s , the spatial point transformer in our framework can be regarded as a mapping M v from P s to cylindrical volume C ∈ R J×K×L×kv×3 : R 3×|P s | → R J×K×L×kv×3 . For a group action r i in R, suppose P s = r i • P s = r i P s , and the rotated local neighbouring set P jk(l+i) = r i P jkl . On the other hand, for the rotation matrix defined in Eq. 3, we have R jkl = R jk(l+i) 112r i . Then, the (j th , k th , l th ) element c p jkl of cylindrical volume C satisfies:</p><p>where c p jk(l+i) ∈ C, which is the cylindrical volume corresponding to the P s . Based on Eq. 8, we can infer that c p jk(l−i) = c p jkl , hence the transformed cylindrical volume C can be formulated as:</p><p>where c p jkl = c p jk(l+L) if l &lt; 1, due to the periodic property of the cylindrical volume in the XY plane. On the other hand, r i • M v means rotating the cylindrical volume C around the Z-axis, that is: <ref type="bibr" target="#b1">2</ref> The minimum rotation unit depends on the way partion along the azimuth axis. i.e., 2π L .</p><p>which completes our proof that the spatial point transformer M v is an equivariant map for the rotation group R.</p><p>Lemma 2. Given a discrete 2D rotation group R ⊂ SO(2), where R = {r i ∈ R 3×3 , i = 1, 2, ..., L}, then 3DCCN is an equivariant map for the 2D rotation group R.</p><p>Proof: The proposed 3D cylindrical convolution can be formulated as a set of convolution filter ψ i on the cylindrical feature maps f :</p><p>where ρ, θ and z denote radial distance, azimuth angle and height, respectively. d is the number of channels in feature map. Suppose a group action r i in R operating on cylindrical feature maps f , we have (r i •f )(ρ, z, θ) = f (ρ, z, θ−i). To clarify, the r i -transformed feature maps r i • f at the coordinate (ρ, z, θ) is equivalent to find the value in the original feature map f at the coordinate (ρ, z, θ − i). Leaving out the summation over feature maps for clarity, we have:</p><p>Using the substitution l → l + i, then Eq. 12 can be transformed into:</p><p>which completes our proof that 3DCCN is an equivariant map for the 2D rotation group R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed Network Architecture</head><p>Using 3D Cylindrical Convolution (3D-CCN) as a basic operator, we build a hierarchical learning architecture as depicted in <ref type="figure">Figure 6</ref>. To ensure the reproducibility of our framework, we also provide detailed information on the kernel size, stride, and the number of filters in this figure. A number of cylindrical convolution layers are stacked together to progressively learn descriptive, yet compact local</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Repeatable and Robust Local Reference Frame for 3D Surface Matching. PR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ClusterNet: Deep Hierarchical Cluster Network with Rigorously Rotation-Invariant Representation for Point Cloud Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D Free-form Object Recognition in Range Images Using Local Surface Patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust Reconstruction of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Global Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">4D Spatio-Temporal Convnets: Minkowski Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully Convolutional Geometric Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Point Signatures: A New Representation for 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Jarvis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spherical CNNs. In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BundleFusion: Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeepSphere: A Graph-based Spherical CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martino</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédérick</forename><surname>Gusset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanaël</forename><surname>Perraudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PPFNet: Global Context Aware Local Features for Robust 3D Point Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model Globally, Match Locally: Efficient and Robust 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D Point Cloud Registration for Localization using a Deep Neural Network Auto-Encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Elbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning SO(3) Equivariant Representations with Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Multiview 3D Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Perfect Match: 3D Point Cloud Matching with Smoothed Densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D Object Recognition in Cluttered Scenes with Local Surface Features: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Comprehensive Performance Evaluation of 3D Local Feature Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai Ming</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rotational Projection Statistics for 3D Local Surface Description and Object Recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Learning for 3D Point Clouds: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Featuremetric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using Spin Images for Efficient Object Recognition in Cluttered 3D Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cylindrical Convolutional Networks for Joint Object Detection and Viewpoint Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ig-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Compact Geometric Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rotation-Invariant Local-to-Global Representation Learning for 3D Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
		<title level="m">Shape2pose: Human-Centric Shape Analysis. TOG</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The 3D-3D Registration Problem Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast and Accurate Registration of Structured Point Clouds with Small Overlaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On the Repeatability and Quality of Keypoints for Local Feature-based 3D Object Retrieval from Cluttered Scenes. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Owens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Three-Dimensional Model-Based Object Recognition and Segmentation in Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Ajmal S Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Working Hard to Know Your Neighbor&apos;s Margins: Local Descriptor Learning Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasiia</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scale-Dependent/Invariant Local 3D Shape Descriptors for Fully Automatic Registration of Multiple Sets of Range Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Novatnack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the Repeatability of The Local Reference Frame for Partial Shape Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alioscia</forename><surname>Petrelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distinctive 3D Local Deep Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poiesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Challenging Data Sets for Point Cloud Registration Algorithms. IJRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spherical Fractal Convolutional Neural Networks for Point Cloud Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast Point Feature Histograms (FPFH) for 3D Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning an Effective Equivariant 3D Descriptor Without Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Spezialetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning to Orient Surfaces by Self-supervised Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Spezialetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlon</forename><surname>Marcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<editor>NeurlPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Structural Indexing: Efficient 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fridtjof</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unique Signatures of Histograms for Local Surface Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unique Signatures of Histograms for Local Surface Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning to Navigate the Energy Landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sun3d: A Database of Big Spaces Reconstructed using SfM and Object Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fold-ingNet: Point Cloud Auto-encoder via Deep Grid Deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">RPM-Net: Robust Point Matching using Learned Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pointwise Rotation-Invariant Network with Adaptive Sampling and 3D Spherical Voxel Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep Positional and Relational Feature Learning for Rotation-Invariant Point Cloud Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Surface Feature Detection and Description with Applications to Mesh Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zaharescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning Local Geometric Descriptors from RGB-D Reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Quaternion Equivariant Capsule Networks for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Intrinsic Shape Signatures: A Shape Descriptor for 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning and Matching Multi-view Descriptors for Registration of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
