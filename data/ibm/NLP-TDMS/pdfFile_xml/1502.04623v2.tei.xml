<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DRAW: A Recurrent Neural Network For Image Generation Ivo Danihelka Daan Wierstra</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danihelka@google</forename><surname>Com</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">Jimenez</forename><surname>Rezende</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wierstra@google</forename><surname>Com</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">KAROLG@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Alex Graves</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">DANILOR@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DRAW: A Recurrent Neural Network For Image Generation Ivo Danihelka Daan Wierstra</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A person asked to draw, paint or otherwise recreate a visual scene will naturally do so in a sequential, iterative fashion, reassessing their handiwork after each modification. Rough outlines are gradually replaced by precise forms, lines are sharpened, darkened or erased, shapes are altered, and the final picture emerges. Most approaches to automatic image generation, however, aim to generate entire scenes at once. In the context of generative neural networks, this typically means that all the pixels are conditioned on a single latent distribution <ref type="bibr" target="#b1">(Dayan et al., 1995;</ref><ref type="bibr" target="#b8">Hinton &amp; Salakhutdinov, 2006;</ref><ref type="bibr" target="#b14">Larochelle &amp; Murray, 2011)</ref>. As well as precluding the possibility of iterative self-correction, the "one shot" approach is fundamentally difficult to scale to large images. The Deep Recurrent Attentive Writer (DRAW) architecture represents a shift towards a more natural form of image construction, in which parts of a scene are created independently from others, and approximate sketches are successively refined.</p><p>Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP volume 37. Copyright 2015 by the author(s). Time <ref type="figure">Figure 1</ref>. A trained DRAW network generating MNIST digits. Each row shows successive stages in the generation of a single digit. Note how the lines composing the digits appear to be "drawn" by the network. The red rectangle delimits the area attended to by the network at each time-step, with the focal precision indicated by the width of the rectangle border.</p><p>The core of the DRAW architecture is a pair of recurrent neural networks: an encoder network that compresses the real images presented during training, and a decoder that reconstitutes images after receiving codes. The combined system is trained end-to-end with stochastic gradient descent, where the loss function is a variational upper bound on the log-likelihood of the data. It therefore belongs to the family of variational auto-encoders, a recently emerged hybrid of deep learning and variational inference that has led to significant advances in generative modelling <ref type="bibr" target="#b22">Rezende et al., 2014;</ref><ref type="bibr" target="#b25">Salimans et al., 2014)</ref>. Where DRAW differs from its siblings is that, rather than generat-arXiv:1502.04623v2 [cs.CV] 20 May 2015 ing images in a single pass, it iteratively constructs scenes through an accumulation of modifications emitted by the decoder, each of which is observed by the encoder.</p><p>An obvious correlate of generating images step by step is the ability to selectively attend to parts of the scene while ignoring others. A wealth of results in the past few years suggest that visual structure can be better captured by a sequence of partial glimpses, or foveations, than by a single sweep through the entire image <ref type="bibr" target="#b13">(Larochelle &amp; Hinton, 2010;</ref><ref type="bibr" target="#b2">Denil et al., 2012;</ref><ref type="bibr" target="#b28">Tang et al., 2013;</ref><ref type="bibr" target="#b21">Ranzato, 2014;</ref><ref type="bibr" target="#b31">Zheng et al., 2014;</ref><ref type="bibr" target="#b0">Ba et al., 2014;</ref><ref type="bibr" target="#b26">Sermanet et al., 2014)</ref>. The main challenge faced by sequential attention models is learning where to look, which can be addressed with reinforcement learning techniques such as policy gradients . The attention model in DRAW, however, is fully differentiable, making it possible to train with standard backpropagation. In this sense it resembles the selective read and write operations developed for the Neural Turing Machine .</p><p>The following section defines the DRAW architecture, along with the loss function used for training and the procedure for image generation. Section 3 presents the selective attention model and shows how it is applied to reading and modifying images. Section 4 provides experimental results on the MNIST, Street View House Numbers and CIFAR-10 datasets, with examples of generated images; and concluding remarks are given in Section 5. Lastly, we would like to direct the reader to the video accompanying this paper (https://www.youtube. com/watch?v=Zt-7MI9eKEo) which contains examples of DRAW networks reading and generating images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The DRAW Network</head><p>The basic structure of a DRAW network is similar to that of other variational auto-encoders: an encoder network determines a distribution over latent codes that capture salient information about the input data; a decoder network receives samples from the code distribuion and uses them to condition its own distribution over images. However there are three key differences. Firstly, both the encoder and decoder are recurrent networks in DRAW, so that a sequence of code samples is exchanged between them; moreover the encoder is privy to the decoder's previous outputs, allowing it to tailor the codes it sends according to the decoder's behaviour so far. Secondly, the decoder's outputs are successively added to the distribution that will ultimately generate the data, as opposed to emitting this distribution in a single step. And thirdly, a dynamically updated attention mechanism is used to restrict both the input region observed by the encoder, and the output region modified by the decoder. In simple terms, the network decides at each time-step "where to read" and "where to write" as well  as "what to write". The architecture is sketched in <ref type="figure" target="#fig_1">Fig. 2</ref>, alongside a feedforward variational auto-encoder.</p><formula xml:id="formula_0">c t 1 c t c T h enc t 1 h dec t 1 Q(z t |x, z 1:t 1 ) Q(z t+1 |x, z 1:t ) . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Network Architecture</head><p>Let RNN enc be the function enacted by the encoder network at a single time-step. The output of RNN enc at time t is the encoder hidden vector h enc t . Similarly the output of the decoder RNN dec at t is the hidden vector h dec t . In general the encoder and decoder may be implemented by any recurrent neural network. In our experiments we use the Long Short-Term Memory architecture (LSTM; <ref type="bibr" target="#b9">Hochreiter &amp; Schmidhuber (1997)</ref>) for both, in the extended form with forget gates <ref type="bibr" target="#b3">(Gers et al., 2000)</ref>. We favour LSTM due to its proven track record for handling long-range dependencies in real sequential data <ref type="bibr" target="#b5">(Graves, 2013;</ref><ref type="bibr">Sutskever et al., 2014)</ref>. Throughout the paper, we use the notation b = W (a) to denote a linear weight matrix with bias from the vector a to the vector b.</p><p>At each time-step t, the encoder receives input from both the image x and from the previous decoder hidden vector h dec t−1 . The precise form of the encoder input depends on a read operation, which will be defined in the next section. The output h enc t of the encoder is used to parameterise a distribution Q(Z t |h enc t ) over the latent vector z t . In our experiments the latent distribution is a diagonal Gaussian N (Z t |µ t , σ t ):</p><formula xml:id="formula_1">µ t = W (h enc t ) (1) σ t = exp (W (h enc t ))<label>(2)</label></formula><p>Bernoulli distributions are more common than Gaussians for latent variables in auto-encoders <ref type="bibr" target="#b1">(Dayan et al., 1995;</ref><ref type="bibr" target="#b7">Gregor et al., 2014)</ref>; however a great advantage of Gaussian latents is that the gradient of a function of the samples with respect to the distribution parameters can be easily obtained using the so-called reparameterization trick <ref type="bibr" target="#b22">Rezende et al., 2014)</ref>. This makes it straightforward to back-propagate unbiased, low variance stochastic gradients of the loss function through the latent distribution.</p><p>At each time-step a sample z t ∼ Q(Z t |h enc t ) drawn from the latent distribution is passed as input to the decoder. The output h dec t of the decoder is added (via a write operation, defined in the sequel) to a cumulative canvas matrix c t , which is ultimately used to reconstruct the image. The total number of time-steps T consumed by the network before performing the reconstruction is a free parameter that must be specified in advance.</p><p>For each image x presented to the network, c 0 , h enc 0 , h dec 0 are initialised to learned biases, and the DRAW network iteratively computes the following equations for t = 1 . . . , T :x</p><formula xml:id="formula_2">t = x − σ(c t−1 ) (3) r t = read (x t ,x t , h dec t−1 ) (4) h enc t = RNN enc (h enc t−1 , [r t , h dec t−1 ]) (5) z t ∼ Q(Z t |h enc t ) (6) h dec t = RNN dec (h dec t−1 , z t ) (7) c t = c t−1 + write(h dec t )<label>(8)</label></formula><p>wherex t is the error image, <ref type="bibr">[v, w]</ref> is the concatenation of vectors v and w into a single vector, and σ denotes the logistic sigmoid function: σ(x) = 1 1+exp(−x) . Note that h enc t , and hence Q(Z t |h enc t ), depends on both x and the history z 1:t−1 of previous latent samples. We will sometimes make this dependency explicit by writing Q(Z t |x, z 1:t−1 ), as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. h enc can also be passed as input to the read operation; however we did not find that this helped performance and therefore omitted it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss Function</head><p>The final canvas matrix c T is used to parameterise a model D(X|c T ) of the input data. If the input is binary, the natural choice for D is a Bernoulli distribution with means given by σ(c T ). The reconstruction loss L x is defined as the negative log probability of x under D:</p><formula xml:id="formula_3">L x = − log D(x|c T )<label>(9)</label></formula><p>The latent loss L z for a sequence of latent distributions Q(Z t |h enc t ) is defined as the summed Kullback-Leibler divergence of some latent prior P (Z t ) from Q(Z t |h enc t ):</p><formula xml:id="formula_4">L z = T t=1 KL Q(Z t |h enc t )||P (Z t )<label>(10)</label></formula><p>Note that this loss depends upon the latent samples z t drawn from Q(Z t |h enc t ), which depend in turn on the input x. If the latent distribution is a diagonal Gaussian with µ t , σ t as defined in Eqs 1 and 2, a simple choice for P (Z t ) is a standard Gaussian with mean zero and standard deviation one, in which case Eq. 10 becomes</p><formula xml:id="formula_5">L z = 1 2 T t=1 µ 2 t + σ 2 t − log σ 2 t − T /2<label>(11)</label></formula><p>The total loss L for the network is the expectation of the sum of the reconstruction and latent losses:</p><formula xml:id="formula_6">L = L x + L z z∼Q<label>(12)</label></formula><p>which we optimise using a single sample of z for each stochastic gradient descent step.</p><p>L z can be interpreted as the number of nats required to transmit the latent sample sequence z 1:T to the decoder from the prior, and (if x is discrete) L x is the number of nats required for the decoder to reconstruct x given z 1:T . The total loss is therefore equivalent to the expected compression of the data by the decoder and prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Stochastic Data Generation</head><p>An imagex can be generated by a DRAW network by iteratively picking latent samplesz t from the prior P , then running the decoder to update the canvas matrixc t . After T repetitions of this process the generated image is a sample from D(X|c T ):z</p><formula xml:id="formula_7">t ∼ P (Z t )<label>(13)</label></formula><formula xml:id="formula_8">h dec t = RNN dec (h dec t−1 ,z t )<label>(14)</label></formula><formula xml:id="formula_9">c t =c t−1 + write(h dec t ) (15) x ∼ D(X|c T )<label>(16)</label></formula><p>Note that the encoder is not involved in image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Read and Write Operations</head><p>The DRAW network described in the previous section is not complete until the read and write operations in Eqs. 4 and 8 have been defined. This section describes two ways to do so, one with selective attention and one without.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reading and Writing Without Attention</head><p>In the simplest instantiation of DRAW the entire input image is passed to the encoder at every time-step, and the decoder modifies the entire canvas matrix at every time-step. In this case the read and write operations reduce to</p><formula xml:id="formula_10">read (x,x t , h dec t−1 ) = [x,x t ] (17) write(h dec t ) = W (h dec t )<label>(18)</label></formula><p>However this approach does not allow the encoder to focus on only part of the input when creating the latent distribution; nor does it allow the decoder to modify only a part of the canvas vector. In other words it does not provide the network with an explicit selective attention mechanism, which we believe to be crucial to large scale image generation. We refer to the above configuration as "DRAW without attention".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Selective Attention Model</head><p>To endow the network with selective attention without sacrificing the benefits of gradient descent training, we take inspiration from the differentiable attention mechanisms recently used in handwriting synthesis <ref type="bibr" target="#b5">(Graves, 2013)</ref> and Neural Turing Machines . Unlike the aforementioned works, we consider an explicitly twodimensional form of attention, where an array of 2D Gaussian filters is applied to the image, yielding an image 'patch' of smoothly varying location and zoom. This configuration, which we refer to simply as "DRAW", somewhat resembles the affine transformations used in computer graphics-based autoencoders <ref type="bibr" target="#b29">(Tieleman, 2014)</ref>.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the N ×N grid of Gaussian filters is positioned on the image by specifying the co-ordinates of the grid centre and the stride distance between adjacent filters. The stride controls the 'zoom' of the patch; that is, the larger the stride, the larger an area of the original image will be visible in the attention patch, but the lower the effective resolution of the patch will be. The grid centre (g X , g Y ) and stride δ (both of which are real-valued) determine the mean location µ i X , µ j Y of the filter at row i, column j in the patch as follows:</p><formula xml:id="formula_11">µ i X = g X + (i − N/2 − 0.5) δ (19) µ j Y = g Y + (j − N/2 − 0.5) δ<label>(20)</label></formula><p>Two more parameters are required to fully specify the attention model: the isotropic variance σ 2 of the Gaussian filters, and a scalar intensity γ that multiplies the filter response. Given an A × B input image x, all five attention parameters are dynamically determined at each time step via a linear transformation of the decoder output h dec :</p><formula xml:id="formula_12">(g X ,g Y , log σ 2 , logδ, log γ) = W (h dec )<label>(21)</label></formula><formula xml:id="formula_13">g X = A + 1 2 (g X + 1) (22) g Y = B + 1 2 (g Y + 1) (23) δ = max(A, B) − 1 N − 1δ<label>(24)</label></formula><p>where the variance, stride and intensity are emitted in the log-scale to ensure positivity. The scaling of g X , g Y and δ is chosen to ensure that the initial patch (with a randomly initialised network) roughly covers the whole input image.</p><p>Given the attention parameters emitted by the decoder, the horizontal and vertical filterbank matrices F X and F Y (dimensions N × A and N × B respectively) are defined as follows:</p><formula xml:id="formula_14">F X [i, a] = 1 Z X exp − (a − µ i X ) 2 2σ 2 (25) F Y [j, b] = 1 Z Y exp − (b − µ j Y ) 2 2σ 2<label>(26)</label></formula><p>where (i, j) is a point in the attention <ref type="figure">patch, (a, b)</ref> is a point in the input image, and Z x , Z y are normalisation constants that ensure that a F X [i, a] = 1 and b F Y [j, b] = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reading and Writing With Attention</head><p>Given F X , F Y and intensity γ determined by h dec t−1 , along with an input image x and error imagex t , the read operation returns the concatenation of two N × N patches from the image and error image:</p><formula xml:id="formula_15">read (x,x t , h dec t−1 ) = γ[F Y xF T X , F Yx F T X ]<label>(27)</label></formula><p>Note that the same filterbanks are used for both the image and error image. For the write operation, a distinct set of attention parametersγ,F X andF Y are extracted from h dec t , the order of transposition is reversed, and the intensity is inverted:</p><formula xml:id="formula_16">w t = W (h dec t ) (28) write(h dec t ) = 1 γF T Y w tFX<label>(29)</label></formula><p>where w t is the N × N writing patch emitted by h dec t . For colour images each point in the input and error image (and hence in the reading and writing patches) is an RGB triple. In this case the same reading and writing filters are used for all three channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We assess the ability of DRAW to generate realisticlooking images by training on three datasets of progressively increasing visual complexity: <ref type="bibr">MNIST (LeCun et al., 1998)</ref>, Street View House Numbers (SVHN) <ref type="bibr" target="#b19">(Netzer et al., 2011) and</ref><ref type="bibr">CIFAR-10 (Krizhevsky, 2009</ref>). The images generated by the network are always novel (not simply copies of training examples), and are virtually indistinguishable from real data for MNIST and SVHN; the generated CIFAR images are somewhat blurry, but still contain recognisable structure from natural scenes. The binarized MNIST results substantially improve on the state of the art. As a preliminary exercise, we also evaluate the 2D attention module of the DRAW network on cluttered MNIST classification.</p><p>For all experiments, the model D(X|c T ) of the input data was a Bernoulli distribution with means given by σ(c T ). For the MNIST experiments, the reconstruction loss from Eq 9 was the usual binary cross-entropy term. For the SVHN and CIFAR-10 experiments, the red, green and blue pixel intensities were represented as numbers between 0 and 1, which were then interpreted as independent colour emission probabilities. The reconstruction loss was therefore the cross-entropy between the pixel intensities and the model probabilities. Although this approach worked well in practice, it means that the training loss did not correspond to the true compression cost of RGB images.</p><p>Network hyper-parameters for all the experiments are presented in <ref type="table" target="#tab_3">Table 3</ref>. The Adam optimisation algorithm <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2014)</ref> was used throughout. Examples of generation sequences for MNIST and SVHN are provided in the accompanying video (https://www. youtube.com/watch?v=Zt-7MI9eKEo).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cluttered MNIST Classification</head><p>To test the classification efficacy of the DRAW attention mechanism (as opposed to its ability to aid in image generation), we evaluate its performance on the 100 × 100 cluttered translated MNIST task . Each image in cluttered MNIST contains many digit-like fragments of visual clutter that the network must distinguish from the true digit to be classified. As illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, having an iterative attention model allows the network to progressively zoom in on the relevant region of the image, and ignore the clutter outside it.</p><p>Our model consists of an LSTM recurrent network that receives a 12 × 12 'glimpse' from the input image at each time-step, using the selective read operation defined in Section 3.2. After a fixed number of glimpses the network uses a softmax layer to classify the MNIST digit. The network is similar to the recently introduced Recurrent Attention Model (RAM) , except that our attention method is differentiable; we therefore refer to it as "Differentiable RAM".</p><p>The results in <ref type="table" target="#tab_0">Table 1</ref> demonstrate a significant improvement in test error over the original RAM network. Moreover our model had only a single attention patch at each  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MNIST Generation</head><p>We trained the full DRAW network as a generative model on the binarized MNIST dataset <ref type="bibr" target="#b24">(Salakhutdinov &amp; Murray, 2008)</ref>. This dataset has been widely studied in the literature, allowing us to compare the numerical performance (measured in average nats per image on the test set) of DRAW with existing methods. <ref type="table" target="#tab_1">Table 2</ref> shows that DRAW without selective attention performs comparably to other recent generative models such as DARN, NADE and DBMs, and that DRAW with attention considerably improves on the state of the art.  <ref type="bibr" target="#b23">(Salakhutdinov &amp; Hinton, 2009</ref>), [2] <ref type="bibr">(Murray &amp; Salakhutdinov, 2009), [3]</ref>  <ref type="bibr" target="#b30">(Uria et al., 2014)</ref>, <ref type="bibr">[4]</ref>  <ref type="bibr" target="#b20">(Raiko et al., 2014)</ref>, <ref type="bibr">[5]</ref>  <ref type="bibr" target="#b22">(Rezende et al., 2014)</ref>, <ref type="bibr">[6]</ref>  <ref type="bibr">(Salimans et al., 2014), [7]</ref>    <ref type="figure">Figure 6</ref>. Generated MNIST images. All digits were generated by DRAW except those in the rightmost column, which shows the training set images closest to those in the column second to the right (pixelwise L 2 is the distance measure). Note that the network was trained on binary samples, while the generated images are mean probabilities.</p><p>Once the DRAW network was trained, we generated MNIST digits following the method in Section 2.3, examples of which are presented in <ref type="figure">Fig. 6. Fig. 7</ref> illustrates the image generation sequence for a DRAW network without selective attention (see Section 3.1). It is interesting to compare this with the generation sequence for DRAW with attention, as depicted in <ref type="figure">Fig. 1</ref>. Whereas without attention it progressively sharpens a blurred image in a global way, <ref type="figure">Figure 7</ref>. MNIST generation sequences for DRAW without attention. Notice how the network first generates a very blurry image that is subsequently refined.</p><p>with attention it constructs the digit by tracing the linesmuch like a person with a pen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MNIST Generation with Two Digits</head><p>The main motivation for using an attention-based generative model is that large images can be built up iteratively, by adding to a small part of the image at a time. To test this capability in a controlled fashion, we trained DRAW to generate images with two 28 × 28 MNIST images chosen at random and placed at random locations in a 60 × 60 black background. In cases where the two digits overlap, the pixel intensities were added together at each point and clipped to be no greater than one. Examples of generated data are shown in <ref type="figure">Fig. 8</ref>. The network typically generates one digit and then the other, suggesting an ability to recreate composite scenes from simple pieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Street View House Number Generation</head><p>MNIST digits are very simplistic in terms of visual structure, and we were keen to see how well DRAW performed on natural images. Our first natural image generation experiment used the multi-digit Street View House Numbers dataset <ref type="bibr" target="#b19">(Netzer et al., 2011)</ref>. We used the same preprocessing as <ref type="bibr" target="#b4">(Goodfellow et al., 2013)</ref>, yielding a 64 × 64 house number image for each training example. The network was then trained using 54 × 54 patches extracted at random locations from the preprocessed images. The SVHN training set contains 231,053 images, and the validation set contains 4,701 images.</p><p>The house number images generated by the network are <ref type="figure">Figure 8</ref>. Generated MNIST images with two digits. <ref type="figure">Figure 9</ref>. Generated SVHN images. The rightmost column shows the training images closest (in L 2 distance) to the generated images beside them. Note that the two columns are visually similar, but the numbers are generally different.</p><p>highly realistic, as shown in Figs. 9 and 10. <ref type="figure">Fig. 11</ref> reveals that, despite the long training time, the DRAW network underfit the SVHN training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generating CIFAR Images</head><p>The most challenging dataset we applied DRAW to was the CIFAR-10 collection of natural images (Krizhevsky,  <ref type="figure">Figure 10</ref>. SVHN Generation Sequences. The red rectangle indicates the attention patch. Notice how the network draws the digits one at a time, and how it moves and scales the writing patch to produce numbers with different slopes and sizes.  <ref type="figure">Figure 11</ref>. Training and validation cost on SVHN. The validation cost is consistently lower because the validation set patches were extracted from the image centre (rather than from random locations, as in the training set). The network was never able to overfit on the training data. 2009). CIFAR-10 is very diverse, and with only 50,000 training examples it is very difficult to generate realistic- looking objects without overfitting (in other words, without copying from the training set). Nonetheless the images in <ref type="figure" target="#fig_1">Fig. 12</ref> demonstrate that DRAW is able to capture much of the shape, colour and composition of real photographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduced the Deep Recurrent Attentive Writer (DRAW) neural network architecture, and demonstrated its ability to generate highly realistic natural images such as photographs of house numbers, as well as improving on the best known results for binarized MNIST generation. We also established that the two-dimensional differentiable attention mechanism embedded in DRAW is beneficial not only to image generation, but also to image classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Left: Conventional Variational Auto-Encoder. During generation, a sample z is drawn from a prior P (z) and passed through the feedforward decoder network to compute the probability of the input P (x|z) given the sample. During inference the input x is passed to the encoder network, producing an approximate posterior Q(z|x) over latent variables. During training, z is sampled from Q(z|x) and then used to compute the total description length KL Q(Z|x)||P (Z) − log(P (x|z)), which is minimised with stochastic gradient descent. Right: DRAW Network. At each time-step a sample zt from the prior P (zt) is passed to the recurrent decoder network, which then modifies part of the canvas matrix. The final canvas matrix cT is used to compute P (x|z1:T ). During inference the input is read at every timestep and the result is passed to the encoder RNN. The RNNs at the previous time-step specify where to read. The output of the encoder RNN is used to compute the approximate posterior over the latent variables at that time-step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Left: A 3 × 3 grid of filters superimposed on an image. The stride (δ) and centre location (gX , gY ) are indicated. Right: Three N × N patches extracted from the image (N = 12). The green rectangles on the left indicate the boundary and precision (σ) of the patches, while the patches themselves are shown to the right. The top patch has a small δ and high σ, giving a zoomed-in but blurry view of the centre of the digit; the middle patch has large δ and low σ, effectively downsampling the whole image; and the bottom patch has high δ and σ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Zooming. Top Left: The original 100 × 75 image. Top Middle: A 12 × 12 patch extracted with 144 2D Gaussian filters. Top Right: The reconstructed image when applying transposed filters on the patch. Bottom: Only two 2D Gaussian filters are displayed. The first one is used to produce the top-left patch feature. The last filter is used to produce the bottom-right patch feature. By using different filter weights, the attention can be moved to a different location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Cluttered MNIST classification with attention. Each sequence shows a succession of four glimpses taken by the network while classifying cluttered translated MNIST. The green rectangle indicates the size and location of the attention patch, while the line width represents the variance of the filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 .</head><label>12</label><figDesc>Generated CIFAR images. The rightmost column shows the nearest training examples to the column beside it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification test error on 100 × 100 Cluttered Translated MNIST.</figDesc><table><row><cell>Model</cell><cell>Error</cell></row><row><cell>Convolutional, 2 layers</cell><cell>14.35%</cell></row><row><cell cols="2">9.41% 8.11% Differentiable RAM, 4 glimpses, 12 × 12 4.18% RAM, 4 glimpses, 12 × 12, 4 scales RAM, 8 glimpses, 12 × 12, 4 scales Differentiable RAM, 8 glimpses, 12 × 12 3.36%</cell></row><row><cell cols="2">time-step, whereas RAM used four, at different zooms.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Negative log-likelihood (in nats) per test-set example on the binarised MNIST data set. The right hand column, where present, gives an upper bound (Eq. 12) on the negative loglikelihood. The previous results are from[1]  </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experimental Hyper-Parameters.</figDesc><table><row><cell></cell><cell>Task</cell><cell cols="2">#glimpses LSTM #h</cell><cell cols="3">#z Read Size Write Size</cell></row><row><cell>s</cell><cell>100 × 100 MNIST Classification MNIST Model SVHN Model CIFAR Model</cell><cell>8 64 32 64</cell><cell cols="2">256 256 100 -800 100 400 200</cell><cell>12 × 12 2 × 2 12 × 12 5 × 5</cell><cell>-5 × 5 12 × 12 5 × 5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Of the many who assisted in creating this paper, we are especially thankful to Koray Kavukcuoglu, Volodymyr Mnih, Jimmy Ba, Yaroslav Bulatov, Greg Wayne, Andrei Rusu and Shakir Mohamed.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zemel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning where to attend with deep architectures for image tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2151" to="2184" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yaroslav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sacha</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6082</idno>
		<title level="m">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andriy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a third-order boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluating probabilities under high-dimensional latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Iterative neural autoregressive distribution estimator nade-k</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On learning where to look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">&amp;apos;</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1405.5488</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of Deep Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Conference on Machine Learning</title>
		<meeting>the 25th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.6460</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7054</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recurrent Neural Network For Image Generation Sutskever</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6110</idno>
		<title level="m">Learning generative models with visual attention</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Optimizing Neural Networks that Generate Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to attention-based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu-Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
