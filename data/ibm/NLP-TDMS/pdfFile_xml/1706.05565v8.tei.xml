<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 TOWARDS NEURAL PHRASE-BASED MACHINE TRANSLATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
							<email>pshuang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Huang</surname></persName>
							<email>shuang91@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Citadel</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
							<email>l.deng@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research, † Google</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 TOWARDS NEURAL PHRASE-BASED MACHINE TRANSLATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present Neural Phrase-based Machine Translation (NPMT). 1 Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A word can be considered as a basic unit in languages. However, in many cases, we often need a phrase to express a concrete meaning. For example, consider understanding the following sentence, "machine learning is a field of computer science". It may become easier to comprehend if we segment it as "[machine learning] [is] [a field of] [computer science]", where the words in the bracket '[]' are regarded as "phrases". These phrases have their own meanings, and can often be reused in other contexts.</p><p>The goal of this paper is to explore the use of phrase structures aforementioned for neural networkbased machine translation systems <ref type="bibr" target="#b22">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. To this end, we develop a neural machine translation method that explicitly models phrases in target language sequences. Traditional phrase-based statistical machine translation (SMT) approaches have been shown to consistently outperform word-based ones <ref type="bibr" target="#b14">(Koehn et al., 2003;</ref><ref type="bibr" target="#b14">Koehn, 2009;</ref><ref type="bibr" target="#b15">Lopez, 2008)</ref>. However, modern neural machine translation (NMT) methods <ref type="bibr" target="#b22">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref> do not have an explicit treatment on phrases, but they still work surprisingly well and have been deployed to industrial systems <ref type="bibr" target="#b31">(Zhou et al., 2016;</ref><ref type="bibr" target="#b28">Wu et al., 2016)</ref>. The proposed Neural Phrase-based Machine Translation (NPMT) method tries to explore the advantages from both kingdoms. It builds upon Sleep-WAke Networks (SWAN), a segmentation-based sequence modeling technique described in <ref type="bibr" target="#b25">Wang et al. (2017a)</ref>, where segments (or phrases) are automatically discovered given the data. However, SWAN requires monotonic alignments between inputs and outputs. This is often not an appropriate assumption in many language pairs. To mitigate this issue, we introduce a new layer to perform (soft) local reordering on input sequences. Experimental results show that NPMT outperforms attention-based NMT baselines in terms of the BLEU score <ref type="bibr" target="#b19">(Papineni et al., 2002)</ref> on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese translation tasks. We believe our method is one step towards the full integration of the advantages from neural machine translation and phrase-based SMT.  <ref type="bibr">(a)</ref> sie wollen die entscheidung wirklich richtig treffen sie wirklich wollen treffen die entscheidung richtig you really want to make the decision right (b) <ref type="figure">Figure 1</ref>: <ref type="bibr">(a)</ref> The overall architecture of NPMT. (b) An illustration of using NPMT in German-English translation. Ideally, phrases in the source sentence (German) are first reordered. Given the new order, phrases can be translated one by one to the target phrases. These translated phrases then compose the target sentence <ref type="bibr">(English)</ref>. Phrase boundaries in the target language are not predefined, but automatically discovered by the model. No attention-based decoders are used here.</p><p>This paper is organized as follows. Section 2 presents the neural phrase-based machine translation model. Section 3 demonstrates the usefulness of our approach on several language pairs. We conclude our work with some discussions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NEURAL PHRASE-BASED MACHINE TRANSLATION</head><p>We first give an overview of the proposed NPMT architecture and some related work on incorporating phrases into NMT. We then describe the two key building blocks in NPMT: 1) SWAN, and 2) the soft reordering layer which alleviates the monotonic alignment requirement of SWAN. In the context of machine translation, we use "segment" and "phrase" interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE OVERALL ARCHITECTURE OF NPMT</head><p>Figure 1(a) shows the overall architecture of NPMT. The input sequence is first turned into embedding representations and then they go through a (soft) reordering layer (described below in Section 2.3). We then pass these "reordered" activations to the bi-directional RNN layers, which are finally fed into the SWAN layer to directly output target language in terms of segments (or phrases). While it is possible to replace bi-directional RNN layers with other layers <ref type="bibr" target="#b8">(Gehring et al., 2017)</ref>, in this paper, we have only explored this particular setting to demonstrate our proposed idea.</p><p>There have been several works that propose different ways to incorporate phrases into attentionbased neural machine translation, such as <ref type="bibr" target="#b23">Tang et al. (2016)</ref>; <ref type="bibr" target="#b26">Wang et al. (2017b)</ref>; <ref type="bibr" target="#b6">Dahlmann et al. (2017)</ref>. These approaches typically use predefined phrases (obtained by external methods, e.g., phrase-based SMT) to guide or modify the existing attention-based decoder. The major difference from our approach is that, in NPMT, we do not use attention-based decoding mechanisms, and our phrase structures for the target language are automatically discovered from the training data. Another line of related work is the segment-to-segment neural transduction model (SSNT) , which shows promising results on a Chinese-to-English translation task under a noisy channel framework <ref type="bibr" target="#b30">(Yu et al., 2017)</ref>. In SSNT, the segments are implicit, and the monotonic alignments between the inputs and outputs are achieved using latent variables. The latent variables are marginalized out during training using dynamic programming. </p><formula xml:id="formula_0">, where {a1 = {y1, $}, a2 = {$}, a3 = {$}, a4 = {y2, y3, $}, a5 = {$}}.</formula><p>Here x1 wakes (emitting segment a1) and x4 wakes (emitting segment a4) while x2, x3 and x5 sleep (emitting empty segments a2, a3 and a5 respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MODELING PHRASES WITH SWAN</head><p>Here we review the SWAN model proposed in <ref type="bibr" target="#b25">Wang et al. (2017a)</ref>. SWAN defines a probability distribution for the output sequence given an input sequence. It models all valid output segmentations of the output sequence as well as the monotonic alignments between the output segments and the input sequence. Empty segments are allowed in the output segmentations. It does not make any assumption on the lengths of input or output sequence.</p><p>Assume input sequence for SWAN is x 1:T , which is the outputs from bi-directional RNN of <ref type="figure">Figure  1</ref>(a), and output sequence is y 1:T . Let S y denote the set containing all valid segmentations of y 1:T , with the constraint that the number of segments in a segmentation is the same as the input sequence length, T . Let a t denote a segment or phrase in the target sequence. Empty segments are allowed to ensure that we can correctly align segment a t to input element x t . Otherwise, we might not have a valid alignment for the input and output pair. See <ref type="figure" target="#fig_0">Figure 2</ref> for an example of the emitted segmentation of y 1:T . The probability of the sequence y 1:T is defined as the sum of the probabilities of all the segmentations in S y {a 1:T : π(a 1:T ) = y 1:T }, 2 p(y 1:T |x 1:T )</p><formula xml:id="formula_1">a 1:T ∈Sy T t=1 p(a t |x t ),<label>(1)</label></formula><p>where the p(a t |x t ) is the segment probability given input element x t , which is modeled using a recurrent neural network (RNN) with an additional softmax layer. π(·) is the concatenation operator and the symbol $, end of a segment, is ignored in the concatenation operator π(·). (An empty segment, which only contains $ will thus be ignored as well.) SWAN can be also understood via a generative model, 1. For t = 1, ..., T : (a) Given an initial state of x t , sample words from RNN until we reach an end of segment symbol $. This gives us a segment a t . 2. Concatenate {a 1 , ..., a T } to obtain the output sequence via π(a 1:T ) = y 1:T .</p><p>Since there are more than one way to obtain the same y 1:T using the generative process above, the probability of observing y 1:T is obtained by summing over all possible ways, which is Eq. 1.</p><p>Note that |S y | is exponentially large, direct summation quickly becomes infeasible when T or T is not small. Instead, <ref type="bibr" target="#b25">Wang et al. (2017a)</ref> developed an exact dynamic programming algorithm to tackle the computation challenges. <ref type="bibr">3</ref> The key idea is that although the number of possible segmentations is exponentially large, the number of possible segments is polynomial-O(T 2 ). In other words, it is possible to first compute all possible segment probabilities, p(a t |x t ), ∀a t , x t , and then use dynamic programming to calculate the output sequence probability p(y 1:T |x 1:T ) in Eq. (1). The feasibility of using dynamic programming is due to a property of segmentations-a segmentation of a subsequence is also part of the segmentation of the entire sequence. In practice, a maximum length L for a segment a t is enforced to reduce the computational complexity, since the length of useful segments is often not very long. <ref type="bibr" target="#b25">Wang et al. (2017a)</ref> also discussed a way to carry over information across segments using a separate RNN, which we will not elaborate here. We refer the readers to the original paper for the algorithmic details.</p><p>SWAN defines a conditional probability for an output sequence given an input one. It can be used in many sequence-to-sequence tasks. In practice, a sequence encoder like a bi-directional RNN can be used to process the raw input sequence (like speech signals or source language) to obtain x 1:T that is to be passed into SWAN for decoding. For example, <ref type="bibr" target="#b25">Wang et al. (2017a)</ref> demonstrated the usefulness of SWAN in the context of speech recognition.</p><p>Greedy decoding for SWAN is straightforward. We first note that p(a t |x t ) is modeled as an RNN with an additional softmax layer. Given each p(a t |x t ), ∀t ∈ 1, . . . , T , is independent of each other, we can run the RNN in parallel to produce an output segment (possibly empty) for each p(a t |x t ). We then concatenate these output segments to form the greedy decoding of the entire output sequence. The decoding satisfies the non-autoregressive property <ref type="bibr" target="#b9">(Gu et al., 2018)</ref> and the decoding complexity is O(T L). See <ref type="bibr" target="#b25">Wang et al. (2017a)</ref> for the algorithmic details of the beam search decoder.</p><p>We finally note that, in SWAN (thus in NPMT), only output segments are explicit; input segments are implicitly modeled by allowing empty segments in the output. This is conceptually different from the traditional phrase-based SMT where both inputs and outputs are phrases (or segments). We leave the option of exploring explicit input segments as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LOCAL REORDERING OF INPUT SEQUENCES</head><p>SWAN assumes a monotonic alignment between the output segments and the input elements. For speech recognition experiments in <ref type="bibr" target="#b25">Wang et al. (2017a)</ref>, this is a reasonable assumption. However, for machine translation, this is usually too restrictive. In neural machine translation literature, attention mechanisms were proposed to address alignment problems <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Raffel et al., 2017;</ref><ref type="bibr" target="#b24">Vaswani et al., 2017)</ref>. But it is not clear how to apply a similar attention mechanism to SWAN due to the use of segmentations for output sequences.</p><p>One may note that in NPMT, a bi-directional RNN encoder for the source language can partially mitigate the alignment issue for SWAN, since it can access every source word. However, from our empirical studies, it is not enough to obtain the best performance. Here we augment SWAN with a reordering layer that does (soft) local reordering of the input sequence. This new model leads to promising results on the IWSLT 2014 German-English/English-German, and IWSLT 2015 English-Vietnamese machine translation tasks. One additional advantage of using SWAN is that since SWAN does not use attention mechanisms, decoding can be done in parallel with linear complexity, as now we remove the need to query the entire input source for every output word <ref type="bibr" target="#b20">(Raffel et al., 2017;</ref><ref type="bibr" target="#b9">Gu et al., 2018)</ref>.</p><p>We now describe the details of the local reordering layer shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a). Denote the input to the local reordering layer by e 1:T , which is the output from the word embedding layer of <ref type="figure">Figure  1</ref>(a), and the output of this layer by h 1:T , which is fed as inputs to the bi-directional RNN of <ref type="figure">Figure  1</ref>(a). We compute h t as</p><formula xml:id="formula_2">h t = tanh 2τ i=0 σ w T i [e t−τ ; . . . ; e t ; . . . ; e t+τ ] e t−τ +i .<label>(2)</label></formula><p>where σ(·) is the sigmoid function, and 2τ + 1 is the local reordering window size. Notation [e t−τ ; . . . ; e t ; . . . ; e t+τ ] is the concatenation of vectors e t−τ , . . . , e t , . . . , e t+τ . For i = 0, . . . , 2τ , notation w i is the parameter for the gate function at position i of the input window. It decides the tasks (weeks for a moderate model size). In the meantime, we are actively looking into the algorithms that can significantly speed up SWAN. e t-2 e t-1 e t e t+1 e t+2 , i = 0, . . . , 4, are the gates that decides how much information ht should accept from those elements from this input window. Note that all information available in this input window helps decides each gate. (b) An illustration of the reordering layer that swaps information between e2 and e3 and contributes to h3 and h2, respectively.</p><formula xml:id="formula_3">σ t-2 σ t-1 σ t σ t+1 σ t+2 h t (a)</formula><p>weight of e t−τ +i through the gate σ w T i [e t−τ ; . . . ; e t ; . . . ; e t+τ ] . The final output h t is a weighted linear combination of the input elements, e t−τ , . . . , e t , . . . , e t+τ , in the window followed by a nonlinear transformation by the tanh(·) function. Here we want to (softly) select an input element from a window given all information available in this window. Suppose we have two adjacent windows, (e 1 , e 2 , e 3 ) and (e 2 , e 3 , e 4 ). If e 3 gets the largest weight (e 3 is picked) in the first window and e 2 gets the largest weight (e 2 is picked) in the second window, e 2 and e 3 are effectively reordered. Our layer is different from the attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Raffel et al., 2017;</ref><ref type="bibr" target="#b24">Vaswani et al., 2017)</ref> in following ways. First, we do not have a query to begin with as in standard attention mechanisms. Second, unlike standard attention, which is top-down from a decoder state to encoder states, the reordering operation is bottom-up. Third, the weights {w i } 2τ i=0 capture the relative positions of the input elements, whereas the weights are the same for different queries and encoder hidden states in the attention mechanism (no positional information). The reordering layer performs locally similar to a convolutional layer and the positional information is encoded by a different parameter w i for each relative position i in the window. Fourth, we do not normalize the weights for the input elements e t−τ , . . . , e t , . . . , e t+τ . This provides the reordering capability and can potentially turn off everything if needed. Finally, the gate of any position i in the reordering window is determined by all input elements e t−τ , . . . , e t , . . . , e t+τ in the window. We provide a visualizing example of the reordering layer gates that performs input swapping in Appendix A.</p><p>One related work to our proposed reordering layer is the Gated Linear Units (GLU)  which can control the information flow of the output of a traditional convolutional layer. But GLU does not have a mechanism to decide which input element from the convolutional window to choose. From our experiments, neither GLU nor traditional convolutional layer helped our NPMT. Another related work to the window size of the reordering layer is the distortion limit in traditional phrase-based statistical machine translation methods <ref type="bibr" target="#b2">(Brown et al., 1993)</ref>. Different window sizes restrict the context of each position to different numbers of neighbors. We provide an empirical comparison of different window sizes in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we evaluate our model on the IWSLT 2014 German-English <ref type="bibr" target="#b3">(Cettolo et al., 2014)</ref>, IWSLT 2014 English-German, and IWSLT 2015 English-Vietnamese <ref type="bibr" target="#b4">(Cettolo et al., 2015)</ref> machine translation tasks. We note that, in this paper, we limit the applications of our model to relatively small datasets to demonstrate the usefulness of our method. We plan to conduct more large scale experiments in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU Greedy Beam Search</head><p>MIXER <ref type="bibr" target="#b21">(Ranzato et al., 2015)</ref> 20.73 21.83 LL <ref type="bibr" target="#b27">(Wiseman &amp; Rush, 2016)</ref> 22.53 23.87 BSO <ref type="bibr" target="#b27">(Wiseman &amp; Rush, 2016)</ref> 23.83 25.48 LL <ref type="bibr" target="#b1">(Bahdanau et al., 2017)</ref> 25.82 27.56 LL * 26.17 27.61</p><p>RF-C+LL <ref type="bibr" target="#b1">(Bahdanau et al., 2017)</ref> 27.70 28.30 AC+LL <ref type="bibr" target="#b1">(Bahdanau et al., 2017)</ref> 27.49 28.53</p><p>NPMT (this paper) 28.57 29.92 NPMT+LM (this paper) -30.08 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IWSLT14 GERMAN-ENGLISH</head><p>We evaluate our model on the German-English machine translation track of the IWSLT 2014 evaluation campaign <ref type="bibr" target="#b3">(Cettolo et al., 2014)</ref>. The data comes from translated TED talks, and the dataset contains roughly 153K training sentences, 7K development sentences, and 7K test sentences. We use the same preprocessing and dataset splits as in Ranzato et al. <ref type="formula" target="#formula_1">(2015)</ref>; <ref type="bibr" target="#b27">Wiseman &amp; Rush (2016)</ref>; <ref type="bibr" target="#b1">Bahdanau et al. (2017)</ref>. The German and English vocabulary sizes are 32,010 and 22,823 respectively.</p><p>We report our IWSLT 2014 German-English experiments using one reordering layer with window size 7, two layers of bi-directional GRU encoder (Gated recurrent unit, <ref type="bibr" target="#b5">Chung et al. (2014)</ref>) with 256 hidden units, and two layers of unidirectional GRU decoder with 512 hidden units. We add dropout with a rate of 0.5 in the GRU layer. We choose GRU since baselines for comparisons were using GRU. The maximum segment length is set to 6. Batch size is set as 32 (per GPU) and the Adam algorithm <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2014)</ref> is used for optimization with an initial learning rate of 0.001. For decoding, we use greedy search and beam search with a beam size of 10. As reported in <ref type="bibr" target="#b18">Maas et al. (2014)</ref>; <ref type="bibr" target="#b1">Bahdanau et al. (2017)</ref>, we find that penalizing candidate sentences that are too short was required to obtain the best results. We add the middle term of Eq.</p><p>(3) to encourage longer candidate sentences. All hyperparameters are chosen based on the development set. NPMT takes about 2-3 days to run to convergence (40 epochs) on a machine with four M40 GPUs. The results are summarized in <ref type="table" target="#tab_1">Table 1</ref>. In addition to previous reported baselines in the literature, we also explored the best hyperparameter using the same model architecture (except the reordering layer) using sequence-to-sequence model with attention as reported as LL * of <ref type="table" target="#tab_1">Table 1</ref>.</p><p>NPMT achieves state-of-the-art results on this dataset as far as we know. Compared to the supervised sequence-to-sequence model, LL <ref type="bibr" target="#b1">(Bahdanau et al., 2017)</ref>, NPMT achieves 2.4 BLEU gain in the greedy setting and 2.25 BLEU gain using beam-search. Our results are also better than those from the actor-critic based methods in <ref type="bibr" target="#b1">Bahdanau et al. (2017)</ref>. But we note that our proposed method is orthogonal to the actor-critic method. So it is possible to further improve our results using the actor-critic method.</p><p>We also run the following two experiments to verify the sources of the gain. The first is to add a reordering layer to the original sequence-to-sequence model with attention, which gives us BLEU scores of 25.55 (greedy) and 26.91 (beam search). Since the attention mechanism and reordering layer capture similar information, adding the reordering layer to the sequence-to-sequence model with attention does not improve the performance. The second is to remove the reordering layer from NPMT, which gives us BLEU scores of 27.79 (greedy) and 29.28 (beam search). This shows that the reordering layer and SWAN are both important for the effectiveness of NPMT.  <ref type="figure">Figure 4</ref>: An example of NPMT greedy decoding output for German-English translation. The example corresponds to the first example of <ref type="table">Table 2</ref>. Note that for illustrating the input and output segments, we do not take into account of the behavior of the reordering layer and bi-directional RNN-the index mappings from source to target assumes monotonic alignments so some of them might be inaccurate.</p><p>source 1 danke 2 , 3 aber 4 das 5 beste 6 kommt 7 noch 8 . greedy decoding 1 thank you • 2 , • 3 but • 5 the best thing • 6 is still coming • 8 . target ground truth thanks . i haven 't come to the best part . source 1 sie 2 können 3 einen 4 schalter 5 dazwischen 6 einfügen 7 und 8 so 9 haben 10 sie 11 einen 12 kleinen 13 UNK 14 erstellt 15 . greedy decoding 1 you can put • 4 a switch • 5 in between • 7 and • 8 so • 10 they created • 12 a little • 13 UNK 14 . target ground truth you can put a knob in between and now you 've made a little UNK . source 1 sie 2 wollen 3 die 4 entscheidung 5 wirklich 6 richtig 7 treffen 8 , 9 wenn 10 es 11 für 12 alle 13 ewigkeit 14 ist 15 , 16 richtig 17 ? greedy decoding 1 you really want to make • 4 the decision . target ground truth there are tens of thousands of machines around the world that make small pieces of dna -30 to 50 letters -in length -and it 's a UNK process , so the longer you make the piece , the more errors there are . <ref type="table">Table 2</ref>: Examples of German-English translation outputs with their segmentations. We label the indexes of the words in the source sentence and we use those indexes to indicate where the output segment is emitted. For example, in greedy decoding results, " i word1, . . . , wordm" denotes i-th word in the source sentence emits words word1, . . . , wordm during decoding (assuming monotonic alignments). The "•" represents the segment boundary in the target output. See <ref type="figure">Figure 4</ref> for a visualization of row 1 in this table.</p><p>In greedy decoding, we can estimate the average segment length 4 for the output. The average segment length is around 1.4-1.6, indicating phrases with more than one word are being decoded. <ref type="figure">Figure 4</ref> shows an example of the input and decoding results with NPMT. We can observe phraselevel translation being captured by the model (e.g., "danke" → "thank you"). The model also knows when to sleep before outputting a phrase (e.g., "das" → "$"). We use the indexes of words in the source sentence to indicate where the output phrases are from. <ref type="table">Table 2</ref> shows some sampled exam-ples. We can observe there are many informative segments in the decoding results, e.g., "tens of thousands of", "the best thing", "a little", etc. There are also mappings from phrase to phrase, word to phrases, and phrase to word in the examples. Following the analysis, we show the most frequent phrase mappings in Appendix C.</p><p>We also explore an option of adding a language-model score during beam search as the traditional statistical machine translation does. This option might not make much sense in attention-based approaches, since the decoder itself is usually a neural network language model. In SWAN, however, there is no language models directly involved in the segmentation modeling, 5 and we find it useful to have an external language model during beam search. We use a 4th-order language model trained using the KenLM implementation <ref type="bibr" target="#b11">(Heafield et al., 2013)</ref> for English target training data. So the final beam search score we use is</p><formula xml:id="formula_4">Q(y) = log p(y|x) + λ 1 word_count(y) + λ 2 log p lm (y),<label>(3)</label></formula><p>where we empirically find that λ 1 = 1.2 and λ 2 = 0.2 give good performance, which are tuned on the development set. The results with the external language model are denoted by NPMT+LM in <ref type="table" target="#tab_1">Table 1</ref>. If no external language models are used, we set λ 2 = 0. This scoring function is similar to the one for speech recognition in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IWSLT14 ENGLISH-GERMAN</head><p>We also evaluate our model on the opposition direction, English-German, which translates from a more segmented text to a more inflectional one. Following the setup in Section 3.1, we use the same dataset with the opposite source and target languages. We use the same model architecture, optimization algorithm and beam search size as the German-English translation task. NPMT takes about 2-3 days to run to convergence (40 epochs) on a machine with four M40 GPUs.</p><p>Given there is no previous sequence-to-sequence attention model baseline for this setup, we create a strong one and tune hyperparameters on the development set. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Based on the development set, we set λ 1 = 1 and λ 2 = 0.15 in Eq.</p><p>(3). Our model outperforms sequence-to-sequence model with attention by 2.46 BLEU and 2.49 BLEU in greedy and beam search cases. We can also use a 4th-order language model trained using the KenLM implementation for German target training data, which further improves the performance. Some sampled examples are shown in <ref type="table" target="#tab_5">Table 4</ref>. Several informative segments/phrases can be found in the decoding results, e.g., "some time ago" → "vor enniger zeit".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU Greedy Beam Search</head><p>Sequence-to-sequence with attention 21.26 22.59 NPMT (this paper) 23.62 25.08 NPMT+LM (this paper) -25.36 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IWSLT15 ENGLISH-VIETNAMESE</head><p>In this section, we evaluate our model on the IWSLT 2015 English to Vietnamese machine translation task. The data is from translated TED talks, and the dataset contains roughly 133K training sentence pairs provided by the IWSLT 2015 Evaluation Campaign <ref type="bibr" target="#b4">(Cettolo et al., 2015)</ref>. Following the same preprocessing steps in ; <ref type="bibr" target="#b20">Raffel et al. (2017)</ref>, we use the TED tst2012 (1553 sentences) as a validation set for hyperparameter tuning and TED tst2013 (1268 sentences) as a test set. The Vietnamese and English vocabulary sizes are 7,709 and 17,191 respectively. source 1 how 2 would 3 you 4 guys 5 describe 6 your 7 brand 8 ? greedy decoding 1 wie • 2 würdet • 3 sie • 6 ihre marke • 8 beschreiben ? target ground truth wie würdet ihr eure marke beschreiben ? source 1 if 2 the 3 museum 4 has 5 given 6 us 7 the 8 image 9 , 10 you 11 click 12 on 13 it 14 . greedy decoding 1 wenn • 2 das museum • 6 uns • 7 das bild • 9 gegeben hat ,• 10 klicken sie • 13 darauf • 14 . target ground truth wenn das museum uns das bild gegeben hat , klicken sie darauf . source 1 they 2 are 3 frustrated 4 as 5 hell 6 with 7 it 8 , 9 but 10 they 11 're 12 not 13 complaining 14 about 15 it 16 , 17 they 18 're 19 fixing 20 it 21 . greedy decoding 1 sie sind • 3 frustriert • 8 , • 9 aber • 10 sie UNK sich • 12 nicht • 15 darüber • 16 , • 17 sie reparieren • 20 es • 21 . target ground truth sie sie sind fürchterlich frustriert mit ihr , aber sie beschweren sich nicht darüber , sie reparieren sie . ? source 1 now 2 some 3 time 4 ago 5 , 6 if 7 you 8 wanted 9 to 10 win 11 a 12 formula 13 1 14 race 15 , 16 you 17 take 18 a 19 budget 20 , 21 and 22 you 23 bet 24 your 25 budget 26 on 27 a 28 good 29 driver 30 and 31 a 32 good 33 car 34 . greedy decoding 2 vor einiger zeit • 6 wenn • 7 man • 11 eine formel • 15 gewinnen will , • 18 ein budget • 21 und • 23 , dass • 24 ihr budget • 27 auf einem guten • 29 fahrer • 30 und • 31 ein gutes • 33 auto • 34 . target ground truth vor einiger zeit war es so , dass wenn sie ein formel 1 rennen gewinnen wollten , dann nahmen sie ihr budget und setzten ihr geld auf einen guten fahrer und ein gutes auto . We use one reordering layer with window size 7, two layers of bi-directional LSTM (Long shortterm memory, <ref type="bibr" target="#b12">Hochreiter &amp; Schmidhuber (1997)</ref>) encoder with 512 hidden units, and three layers of unidirectional LSTM decoder with 512 hidden units. We add dropout with a rate of 0.4 in the LSTM layer. We choose LSTM since baselines for comparisons were using LSTM. The maximum segment length is set to 7. Batch size is set as 48 (per GPU) and the Adam algorithm <ref type="bibr" target="#b13">Kingma &amp; Ba (2014)</ref> is used for optimization with an initial learning rate of 0.001. For decoding, we use greedy decoding and beam search with a beam size of 10. The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. Based on the development set, we set λ 1 = 0.7 and λ 2 = 0.15 in Eq. <ref type="formula" target="#formula_4">(3)</ref>. NPMT takes about one day to run to convergence (15 epochs) on a machine with 4 M40 GPUs. Our model outperforms sequence-tosequence model with attention by 1.41 BLEU and 1.59 BLEU in greedy and beam search cases. We also use a 4th-order language model trained using the KenLM implementation for Vietnamese target training data, which further improves the BLEU score. Note that our reordering layer relaxes the monotonic assumption as in <ref type="bibr" target="#b20">Raffel et al. (2017)</ref> and is able to decode in linear time. Empirically we outperform models with monotonic attention. <ref type="table" target="#tab_8">Table 6</ref> shows some sampled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU Greedy Beam Search</head><p>Hard monotonic <ref type="bibr" target="#b20">(Raffel et al., 2017)</ref> 23.00 -  -23.30 Sequence-to-sequence model with attention 25.50 26.10 NPMT (this paper) 26.91 27.69 NPMT+LM (this paper) -28.07 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>We proposed NPMT, a neural phrase-based machine translation system that models phrase structures in the target language using SWAN. We also introduced a local reordering layer to mitigate the source 1 And 2 I 3 figured 4 , 5 this 6 has 7 to 8 stop 9 . greedy decoding 1 Và • 2 tôi • 3 nhận ra rằng • 4 , • 5 điều này • 6 phải• 8 dừng lại 9 . target ground truth Và tôi nhận ra rằng điều đó phải chấm dửt . source 1 So 2 great 3 progress 4 and 5 treatment 6 has 7 been 8 made 9 over 10 the 11 years 12 . greedy decoding 1 Vì vậy , • 2 tiến bộ• 4 và • 5 điều trị • 6 đã • 7 được • 8 tạo ra • 9 trong • 10 những • 11 năm • 12 . target ground truth Trong suốt những năm qua đã có sự tiến bộ to lớn trong quá trình điều trị .</p><p>source 1 The 2 passion 3 that 4 the 5 person 6 has 7 for 8 her 9 own 10 growth 11 is 12 the  monotonic alignment requirement in SWAN. Our experimental results showed promising results on IWSLT 2014 German-English, English-German, and IWSLT 2015 English-Vietnamese machine translation tasks. The results suggest that NPMT can potentially be extended to explore the structures in other challenging sequence-to-sequence problems. In future work, we will explore two directions: 1) speed up NPMT and apply it to larger datasets and more language pairs; 2) investigate how to learn input and output phrases simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A REORDERING LAYER ANALYSIS</head><p>To further understand the behavior of the reordering layer, we examine the values of the gate σ w T i [e t−τ ; . . . ; e t ; . . . ; e t+τ ] in Eq.</p><p>(2). We study the NPMT English-German model in Section 3.2. In <ref type="figure" target="#fig_3">Figure 5</ref>, we show an example that translates from "can you translate it ?" to "können man esübersetzen ?", where the mapping between words are as follows: "can → können", "you → man", "translate →übersetzen", "it → es" and "? → ?". Note that the example needs to be reordered from "translate it" to "esübersetzen". Each row of <ref type="figure" target="#fig_3">Figure 5</ref> represents a window of size 7 that is centered at a source sentence word. The values in the matrix represent the gate values for the corresponding words. The gate values will later be multiplied with the embedding e t−τ +i of Eq.</p><p>(2) and contribute to the hidden vector h t . The y-axis represents the word/phrases emitted from the corresponding position. We can observe that the gates mostly focus on the central word since the first part of the sentence only requires monotonic alignment. Interestingly, the model outputs "$" (empty) when the model has the word "translate" in the center of the window. Then, the model outputs "es" when the model encounters "it". Finally, in the last window (top row), the model not only has a large gate value to the center input "?", but the model also has a relatively large gate value to the word "translate" in order to output the translation "übersetzen ?". This shows an example of the reordering effect achieved by using the gating mechanism of the reordering layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EFFECT OF WINDOW SIZES IN THE REORDERING LAYER</head><p>In this section, we examine the effect of window sizes in the reordering layer. Following the setup in Section 3.2, we evaluate the performance of different window sizes on the IWSLT 2014 English-German translation task. <ref type="table" target="#tab_10">Table 7</ref> summarizes the results. We can observe that the performance reaches the peak with a windows size of 7. With a window size of 5, the performance drops 0.88 BLEU in greedy decoding and 0.72 BLEU using beam search. It suggests that the context window is not large enough to properly perform reordering. When the window sizes are 9 and 11, we do not observe further improvements. It might be because the translation between English and German mostly requires local word reordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PHRASE MAPPING EXAMPLES</head><p>Following the examples of <ref type="table">Table 2</ref>, we analyze the decoding results on the test set of the German-English translation task. Given we do not have explicit input segments in NPMT, we assume input words that emit "$" symbol are within the same group as the next non-'$' word. For example, in <ref type="figure">Figure 4</ref>, input words "das beste" are considered as an input segment. We then can aggregate all the input, output segments (phrases) and sort them based on the frequency. <ref type="table">Tables C and C show</ref>    <ref type="table">Table 8</ref>: German-English phrase mapping results. We show the top 10 input, output phrase mappings in five categories ("One" stands for single word and "Many" stands for multiple words.). In the last column, Many → Many * , we remove the phrases with the "UNK" word as the "UNK" appears often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrases with 3 words</head><p>Phrases with 4 words auf der ganzen → all over the auf der ganzen → a little bit of gibt eine menge → a lot of weiß nicht , was → what 's going to be dann hat er→ he doesn 't have tun , das wir → we can 't do , die man → you can do tat , das ich → i didn 't do das können wir → we can do that zu verbessern , die → that can be done <ref type="table">Table 9</ref>: German-English longer phrase mapping results. We show the top 5 input, output phrase mappings for two categories: input and output phrases with three words, and input and output phrases with four words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Courtesy to<ref type="bibr" target="#b25">Wang et al. (2017a)</ref>. Symbol $ indicates the end of a segment. Given a sequence of inputs x1, . . . , x5, which is from the outputs from the bi-directional RNN ofFigure 1(a), SWAN emits one particular segmentation of y1:3 = π(a1:5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Example of a local reordering layer of window size 5 (τ = 2) to compute ht. Here σt−2+i σ(w T i [et−2; et−1; et; et+1; et+2])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>(b) illustrates how local reordering works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualizing reordering gates in the NPMT English-German translation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Translation results on the IWSLT 2014 German-English test set. MIXER Ranzato et al. (2015) uses a convolutional encoder and simpler attention. LL (attention model with log likelihood) and BSO (beam search optimization) of Wiseman &amp; Rush (2016), and LL, RF-C+LL, and AC+LL of Bahdanau et al. (2017) use a</figDesc><table /><note>one-layer GRU encoder and decoder with attention. (RF-C+LL and AC+LL are different settings of actor-critic algorithms combined with LL.) LL* stands for a well-tuned attention model with log likelihood with the same word embedding size, and encoder and decoder size as NPMT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>• 6 right • 8 , • 9 if • 10 it 's • 11 for • 12 all • 13 eternity • 15 , • 16 right • 17 ? target ground truth you really want to get the decision right if it 's for all eternity , right ? source 1 es 2 gibt 3 zehntausende 4 maschinen 5 rund 6 um 7 die 8 welt 9 die 10 kleine 11 stücke 12 von 13 dna 14 herstellen 15 können 16 , 17 30 18 bis 19 50 20 buchstaben 21 lang 22 aber 23 es 24 ist 25 ein 26 UNK 27 prozess 28 , 29 also 30 je 31 länger 32 man 33 ein 34 stück 35 macht 36 , 37 umso 38 mehr 39 fehler 40 passieren 41 . greedy decoding 1 there are • 3 tens of thousands of • 4 machines • 6 around • 8 the world • 9 can make • 10 little • 11 pieces • 12 of • 13 dna • 16 , • 17 30 • 18 to • 19 50 • 20 letters • 21 long • 22 , but • 23 it 's • 26 a more UNK • 27 process • 28 , • 29 so • 31 the longer • 32 you make • 34 a piece • 36 , • 38 the more • 39 mistakes • 40 happen • 41</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Translation results on the IWSLT 2014 English-German test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Examples of English-German translation outputs with their segmentations. The meanings of the superscript indexes and the "•" symbol are the same as those inTable 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Translation results on the IWSLT 2015 English-Vietnamese tst2013 test set. The result of the sequence-to-sequence model with attention is obtained from an open source model provided by the authors. 7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>13 most 14 important 15 thing 16 . greedy decoding 1 Niềm đam mê • 3 rằng • 5 người • 6 có • 7 cho • 8 sự phát triển • 10 của cô ấy • 11 là • 13 điều • 14 quan trọng • 15 nhất • 16 . target ground truth Cái khát vọng của người phụ nữ có cho sự phát triển của bản thân là thứ quan trọng nhất . source 1 We 2 have 3 eight 4 species 5 of 6 UNK 7 that 8 occur 9 in 10 Kenya 11 , 12 of 13 which 14 six 15 are 16 highly 17 threatened 18 with 19 extinction 20 . greedy decoding 1 Chúng ta • 2 có • 3 8 • 4 loài • 6 UNK • 8 xảy ra • 9 ở • 10 Kenya • 11 , • 14 6 • 17 bị đe doạ • 19 tuyệt chủng • 20 . target ground truth Chúng ta có 8 loài kền kền xuất hiện tại Kenya , trong đó có 6 loài bị đe doạ với nguy cơ tuyệt chủng cao .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Examples of English-Vietnamese translation outputs with their segmentations. The meanings of the superscript indexes and the "•" symbol are the same as those inTable 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Analyze the effect of reordering layer window sizes in translation results on the IWSLT 2014 English-German test set. , → , es → it 's , dass → that die UNK → the UNK wissen sie → you know . → . UNK → the UNK in der → in der UNK → the UNK in diesem → in this und → and und → , and UNK . → . ein UNK → a UNK die welt → the world UNK → UNK das → this is UNK , → , das UNK → the UNK ist es → it 's aber , wissen sie → you know der welt → the world von → of dies → this is sind . → . in diesem → in this die frage → the question mit → with es → there 's , wenn → if dem UNK → the UNK haben wir → we have</figDesc><table><row><cell cols="5">One → One Many → Many  → but One → Many Many → One Many → Many das, → that 's , die → that eine UNK → a UNK " . → . "</cell></row><row><cell>" → "</cell><cell>UNK → a UNK</cell><cell>ist . → .</cell><cell>in UNK → in UNK</cell><cell>ein paar → a few</cell></row><row><cell>ist → is</cell><cell>ich → i think</cell><cell>in den → in</cell><cell>den UNK → the UNK</cell><cell>gibt es → there 's</cell></row><row><cell>der → of</cell><cell>es → it was</cell><cell>ist , →</cell><cell></cell><cell></cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If predefined phrase structure information is provided for the target language in advance, we can incorporate it into SWAN by restricting the size of Sy. We leave the exploration of this option as future work.3  The computational complexity of SWAN is still high even with the dynamic programming algorithm. This is the reason that it takes a longer time to train our method for larger datasets such as in WMT translation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The average segment length is defined as the length of the output (excluding end of segment symbol $) divided by the number of segments (not counting the ones only containing $).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In<ref type="bibr" target="#b25">Wang et al. (2017a)</ref>, SWAN does have an option to use a separate RNN that connects the segments, which can be seen as a language model. However, different from speech recognition experiments, we find in machine translation experiments, adding this separate RNN leads to a worse performance. We suspect this is because an RNN language model can be easier to learn than the segmentation structures and SWAN gets stuck in that local mode. This is further evidenced by the fact that the average segment length is much shorter with a separate RNN in SWAN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/tensorflow/nmt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ACKNOWLEDGMENTS</head><p>We thank Jacob Devlin, Adith Swaminathan, Frank Seide, Xiaodong He, and anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT evaluation campaign, IWSLT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
		<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The IWSLT 2015 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Spoken Language</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation leveraging phrase-based models in a hybrid search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Dahlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Petrushkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1422" to="1431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Philipp Koehn, Franz Josef Och, and Daniel Marcu</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1408.2873</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online and linear-time attention by enforcing monotonic alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1511.06732</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural machine translation with external phrase memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Lh</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01792</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence modeling via segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Translating phrases in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1432" to="1442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>abs/1606.02960</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online segment to segment neural transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1307" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The neural noisy channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep recurrent models with fast-forward connections for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
