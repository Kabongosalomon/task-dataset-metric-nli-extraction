<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human body pose estimation methods have become increasingly reliable. Powerful body part detectors <ref type="bibr" target="#b35">[36]</ref> in combination with tree-structured body models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b6">7]</ref> show impressive results on diverse datasets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref>. These benchmarks promote pose estimation of single pre-localized persons but exclude scenes with multiple people. This problem definition has been a driver for progress, but also falls short on representing a realistic sample of real-world images. Many photographs contain multiple people of interest (see <ref type="figure">Fig 1)</ref> and it is unclear whether single pose approaches generalize directly. We argue that the multi person case deserves more attention since it is an important real-world task.</p><p>Key challenges inherent to multi person pose estimation (a) (b) (c) <ref type="figure">Figure 1</ref>. Method overview: (a) initial detections (= part candidates) and pairwise terms (graph) between all detections that (b) are jointly clustered belonging to one person (one colored subgraph = one person) and each part is labeled corresponding to its part class (different colors and symbols correspond to different body parts); (c) shows the predicted pose sticks.</p><p>are the partial visibility of some people, significant overlap of bounding box regions of people, and the a-priori unknown number of people in an image. The problem thus is to infer the number of persons, assign part detections to person instances while respecting geometric and appearance constraints. Most strategies use a two-stage inference process <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref> to first detect and then independently estimate poses. This is unsuited for cases when people are in close proximity since they permit simultaneous assignment of the same body-part candidates to multiple people hypotheses. As a principled solution for multi person pose estimation a model is proposed that jointly estimates poses of all people present in an image by minimizing a joint objective. The formulation is based on partitioning and labeling an initial pool of body part candidates into subsets that correspond to sets of mutually consistent body-part candidates and abide to mutual consistency and exclusion constraints. The proposed method has a number of appealing properties. (1) The formulation is able to deal with an unknown number of people, and also infers this number by linking part hypotheses. <ref type="bibr" target="#b1">(2)</ref> The formulation allows to either deactivate or merge part hypotheses in the initial set of part candidates hence effectively performing non-maximum suppression (NMS). In contrast to NMS performed on individual part candidates, the model incorporates evidence from all other parts making the process more reliable. <ref type="bibr">(</ref>3) The problem is cast in the form of an Integer Linear Program (ILP). Although the problem is NP-hard, the ILP formulation facilitates the computation of bounds and feasible solutions with a certified optimality gap. This paper makes the following contributions. The main contribution is the derivation of a joint detection and pose estimation formulation cast as an integer linear program. Further, two CNN variants are proposed to generate representative sets of body part candidates. These, combined with the model, obtain state-of-the-art results for both single-person and multi-person pose estimation on different datasets. Related work. Most work on pose estimation targets the single person case. Methods progressed from simple part detectors and elaborate body models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19]</ref> to treestructured pictorial structures (PS) models with strong part detectors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>. Impressive results are obtained predicting locations of parts with convolutional neural networks (CNN) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref>. While body models are not a necessary component for effective part localization, constraints among parts allow to assemble independent detections into body configurations as demonstrated in <ref type="bibr" target="#b6">[7]</ref> by combining CNNbased body part detectors with a body model <ref type="bibr" target="#b41">[42]</ref>.</p><p>A popular approach to multi-person pose estimation is to detect people first and then estimate body pose independently <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="bibr" target="#b41">[42]</ref> proposes a flexible mixture-of-parts model for detection and pose estimation. <ref type="bibr" target="#b41">[42]</ref> obtains multiple pose hypotheses corresponding to different root part positions and then performing non-maximum suppression. <ref type="bibr" target="#b17">[18]</ref> detects people using a flexible configuration of poselets and the body pose is predicted as a weighted average of activated poselets. <ref type="bibr" target="#b28">[29]</ref> detects people and then predicts poses of each person using a PS model. <ref type="bibr" target="#b4">[5]</ref> estimates poses of multiple people in 3D by constructing a shared space of 3D body part hypotheses, but uses 2D person detections to establish the number of people in the scene. These approaches are limited to cases with people sufficiently far from each other that do not have overlapping body parts.</p><p>Our work is closely related to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> who also propose a joint objective to estimate poses of multiple people. <ref type="bibr" target="#b12">[13]</ref> proposes a multi-person PS model that explicitly models depth ordering and person-person occlusions. Our formulation is not limited by a number of occlusion states among people. <ref type="bibr" target="#b24">[25]</ref> proposes a joint model for pose estimation and body segmentation coupling pose estimates of individuals by image segmentation. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> uses a person detector to generate initial hypotheses for the joint model. <ref type="bibr" target="#b24">[25]</ref> resorts to a greedy approach of adding one person hypothesis at a time until the joint objective can be reduced, whereas our formulation can be solved with a certified optimality gap. In addition <ref type="bibr" target="#b24">[25]</ref> relies on expensive labeling of body part segmentation, which the proposed approach does not require.</p><p>Similarly to <ref type="bibr" target="#b7">[8]</ref> we aim to distinguish between visible and occluded body parts. <ref type="bibr" target="#b7">[8]</ref> primarily focuse on the singleperson case and handles multi-person scenes akin to <ref type="bibr" target="#b41">[42]</ref>. We consider the more difficult problem of full-body pose estimation, whereas <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref> focus on upper-body poses and consider a simplified case of people seen from the front.</p><p>Our work is related to early work on pose estimation that also relies on integer linear programming to assemble candidate body part hypotheses into valid configurations <ref type="bibr" target="#b18">[19]</ref>. Their single person method employs a tree graph augmented with weaker non-tree repulsive edges and expects the same number of parts. In contrast, our novel formulation relies on fully connected model to deal with unknown number of people per image and body parts per person.</p><p>The Minimum Cost Multicut Problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, known in machine learning as correlation clustering <ref type="bibr" target="#b3">[4]</ref>, has been used in computer vision for image segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref> but has not been used before in the context of pose estimation. It is known to be NP-hard <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>In this section, the problem of estimating articulated poses of an unknown number of people in an image is cast as an optimization problem. The goal of this formulation is to state three problems jointly: 1. The selection of a subset of body parts from a set D of body part candidates, estimated from an image as described in Section 4 and depicted as nodes of a graph in <ref type="figure">Fig. 1(a)</ref>. 2. The labeling of each selected body part with one of C body part classes, e.g., "arm", "leg", "torso", as depicted in <ref type="figure">Fig. 1(c)</ref>. 3. The partitioning of body parts that belong to the same person, as depicted in <ref type="figure">Fig. 1</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feasible Solutions</head><p>We encode labelings of the three problems jointly through triples (x, y, z) of binary random variables with domains</p><formula xml:id="formula_0">x ∈ {0, 1} D×C , y ∈ {0, 1} D 2 and z ∈ {0, 1} D 2 ×C 2 .</formula><p>Here, x dc = 1 indicates that body part candidate d is of class c, y dd = 1 indicates that the body part candidates d and d belong to the same person, and z dd cc are auxiliary variables to relate x and y through z dd cc = x dc x d c y dd . Thus, z dd cc = 1 indicates that body part candidate d is of class c (x dc = 1), body part candidate d is of class c (x d c = 1), and body part candidates d and d belong to the same person (y dd = 1).</p><p>In order to constrain the 01-labelings (x, y, z) to welldefined articulated poses of one or more people, we impose the linear inequalities (1)-(3) stated below. Here, the inequalities (1) guarantee that every body part is labeled with at most one body part class. (If it is labeled with no body part class, it is suppressed). The inequalities (2) guarantee that distinct body parts d and d belong to the same person only if neither d nor d is suppressed. The inequalities (3) guarantee, for any three pairwise distinct body parts, d, d and d , if d and d are the same person (as indicated by y dd = 1) and d and d are the same person (as indicated by y d d = 1), then also d and d are the same person (y dd = 1), that is, transitivity, cf. <ref type="bibr" target="#b8">[9]</ref>. Finally, the inequalities (4) guarantee, for any dd ∈ D 2 and any cc ∈ C 2 that z dd cc = x dc x d c y dd . These constraints allow us to write an objective function as a linear form in z that would otherwise be written as a cubic form in x and y. We denote by X DC the set of all (x, y, z) that satisfy all inequalities, i.e., the set of feasible solutions.</p><formula xml:id="formula_1">∀d ∈ D∀cc ∈ C 2 : x dc + x dc ≤ 1 (1) ∀dd ∈ D 2 : y dd ≤ c∈C x dc y dd ≤ c∈C x d c (2) ∀dd d ∈ D 3 : y dd + y d d − 1 ≤ y dd (3) ∀dd ∈ D 2 ∀cc ∈ C 2 : x dc + x d c + y dd − 2 ≤ z dd cc z dd cc ≤ x dc z dd cc ≤ x d c z dd cc ≤ y dd<label>(4)</label></formula><p>When at most one person is in an image, we further constrain the feasible solutions to a well-defined pose of a single person. This is achieved by an additional class of inequalities which guarantee, for any two distinct body parts that are not suppressed, that they must be clustered together:</p><formula xml:id="formula_2">∀dd ∈ D 2 ∀cc ∈ C 2 : x dc + x d c − 1 ≤ y dd<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Objective Function</head><p>For every pair (d, c) ∈ D × C, we will estimate a probability p dc ∈ [0, 1] of the body part d being of class c. In the context of CRFs, these probabilities are called part unaries and we will detail their estimation in Section 4.</p><p>For every dd ∈ D 2 and every cc ∈ C 2 , we consider a probability p dd cc ∈ (0, 1) of the conditional probability of d and d belonging to the same person, given that d and d are body parts of classes c and c , respectively. For c = c , these probabilities p dd cc are the pairwise terms in a graphical model of the human body. In contrast to the classic pictorial structures model, our model allows for a fully connected graph where each body part is connected to all other parts in the entire set D by a pairwise term. For c = c , p dd cc is the probability of the part candidates d and d representing the same part of the same person. This facilitates clustering of multiple part candidates of the same part of the same person and a repulsive property that prevents nearby part candidates of the same type to be associated to different people.</p><p>The optimization problem that we call the subset partition and labeling problem is the ILP that minimizes over the set of feasible solutions X DC :</p><formula xml:id="formula_3">min (x,y,z)∈X DC α, x + β, z ,<label>(6)</label></formula><p>where we used the short-hand notation</p><formula xml:id="formula_4">α dc := log 1 − p dc p dc (7) β dd cc := log 1 − p dd cc p dd cc (8) α, x := d∈D c∈C α dc x dc (9) β, z := dd ∈ D 2 c,c ∈C β dd cc z dd cc .<label>(10)</label></formula><p>The objective (6)-(10) is the MAP estimate of a probability measure of joint detections x and clusterings y, z of body parts, where prior probabilities p dc and p dd cc are estimated independently from data, and the likelihood is a positive constant if (x, y, z) satisfies (1)-(4), and is 0, otherwise. The exact form (6)-(10) is obtained when minimizing the negative logarithm of this probability measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Optimization</head><p>In order to obtain feasible solutions of the ILP (6) with guaranteed bounds, we separate the inequalities (1)-(5) in the branch-and-cut loop of the state-of-the-art ILP solver Gurobi. More precisely, we solve a sequence of relaxations of the problem (6), starting with the (trivial) unconstrained problem. Each problem is solved using the cuts proposed by Gurobi. Once an integer feasible solution is found, we identify violated inequalities (1)-(5), if any, by breadth-firstsearch, add these to the constraint pool and re-solve the tightened relaxation. Once an integer solution satisfying all inequalities is found, together with a lower bound that certifies an optimality gap below 1%, we terminate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pairwise Probabilities</head><p>Here we describe the estimation of the pairwise terms. We define pairwise features f dd for the variable z dd cc (Sec. 2). Each part detection d includes the probabilities f p dc (Sec. 4.4), its location (x d , y d ), scale h d and bounding box B d coordinates. Given two detections d and d , and the corresponding features (f p dc , x d , y d , h d , B d ) and (f p d c , x d , y d , h d , B d ), we define two sets of auxiliary variables for z dd cc , one set for c = c (same body part class clustering) and one for c = c (across two body part classes labeling). These features capture the proximity, kinematic relation and appearance similarity between body parts. The same body part class (c = c ). Two detections denoting the same body part of the same person should be in close proximity to each other. We introduce the following auxiliary variables that capture the spatial relations: Non-linear Mapping. We augment the feature representation by appending quadratic and exponential terms. The final pairwise feature f dd for the variable z dd cc is (∆x, ∆y, ∆h, IOU nion, IOM in, IOM ax, (∆x) 2 , . . . , (IOM ax) 2 , exp (−∆x), . . . , exp (−IOM ax)).</p><formula xml:id="formula_5">∆x = |x d −x d |/h, ∆y = |y d −y d |/h, ∆h = |h d −h d |/h,</formula><p>Two different body part classes (c = c ). We encode the kinematic body constraints into the pairwise feature by introducing auxiliary variables S dd and R dd , where S dd and R dd are the Euclidean distance and the angle between two detections, respectively. To capture the joint distribution of S dd and R dd , instead of using S dd and R dd directly, we employ the posterior probability p(z dd cc = 1|S dd , R dd ) as pairwise feature for z dd cc to encode the geometric relations between the body part class c and c . More specifically, assuming the prior probability p(z dd cc = 1) = p(z dd cc = 0) = 0.5, the posterior probability of detection d and d have the body part label c and c , namely z dd cc = 1, is</p><formula xml:id="formula_6">p(z dd cc = 1|S dd , R dd ) = p(S dd , R dd |z dd cc = 1) p(S dd , R dd |z dd cc = 1) + p(S dd , R dd |z dd cc = 0) ,</formula><p>where p(S dd , R dd |z dd cc = 1) is obtained by conducting a normalized 2D histogram of S dd and R dd from positive training examples, analogous to the negative likelihood p(S dd , R dd |z dd cc = 0). In Sec. 5.1 we also experiment with encoding the appearance into the pairwise feature by concatenating the feature f p dc from d and f p d c from d , as f p dc is the output of the CNN-based part detectors. The final pairwise feature is (p(z dd cc = 1|S dd , R dd ), f p dc , f p d c ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Probability Estimation</head><p>The coefficients α and β of the objective function (Eq. 6) are defined by the probability ratio in the log space (Eq. 7 and Eq. 8). Here we describe the estimation of the corresponding probability density: (1) For every pair of detection and part classes, namely for any (d, c) ∈ D × C, we estimate a probability p dc ∈ (0, 1) of the detection d being a body part of class c. (2) For every combination of two distinct detections and two body part classes, namely for any dd ∈ D 2 and any cc ∈ C 2 , we estimate a probability p dd cc ∈ (0, 1) of d and d belonging to the same person, meanwhile d and d are body parts of classes c and c , respectively. Learning. Given the features f dd and a Gaussian prior p(θ cc ) = N (0, σ 2 ) on the parameters, logistic model is</p><formula xml:id="formula_7">p(z dd cc = 1|f dd , θ cc ) = 1 1 + exp(− θ cc , f dd )</formula><p>. <ref type="formula">(11)</ref> (|C| × (|C| + 1))/2 parameters are estimated using ML. Inference Given two detections d and d , the coefficients α dc for x dc and α d c for x d c are obtained by Eq. 7, the coefficient β dd cc for z dd cc has the form</p><formula xml:id="formula_8">β dd cc = log 1 − p dd cc p dd cc = − f dd , θ cc .<label>(12)</label></formula><p>Model parameters θ cc are learned using logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Body Part Detectors</head><p>We first introduce our deep learning-based part detection models and then evaluate them on two prominent benchmarks thereby significantly outperforming state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adapted Fast R-CNN (AFR-CNN)</head><p>To obtain strong part detectors we adapt Fast R-CNN <ref type="bibr" target="#b15">[16]</ref>. FR-CNN takes as input an image and set of class-independent region proposals <ref type="bibr" target="#b38">[39]</ref> and outputs the softmax probabilities over all classes and refined bounding boxes. To adapt FR-CNN for part detection we alter it in two ways: 1) proposal generation and 2) detection region size. The adapted version is called AFR-CNN throughout the paper. Detection proposals. Generating object proposals is essential for FR-CNN, meanwhile detecting body parts is challenging due to their small size and high intra-class variability. We use DPM-based part detectors <ref type="bibr" target="#b27">[28]</ref> for proposal generation. We collect K top-scoring detections by each part detector in a common pool of N part-independent proposals and use these proposals as input to AFR-CNN. N is 2, 000 in case of single and 20, 000 in case of multiple people. Larger context. Increasing the size of DPM detections by upscaling every bounding box by a fixed factor allows to capture more context around each part. In Sec. 4.3 we evaluate the influence of upscaling and show that using larger context around parts is crucial for best performance. Details. Following standard FR-CNN training procedure Im-ageNet models are finetuned on pose estimation task. Center of a predicted bounding box is used for body part location prediction. See Appendix A for detailed parameter analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dense Architecture (Dense-CNN)</head><p>Using proposals for body part detection may be suboptimal. We thus develop a fully convolutional architecture for computing part probability scoremaps.</p><p>Stride. We build on VGG <ref type="bibr" target="#b33">[34]</ref>. Fully convolutional VGG has stride of 32 px -too coarse for precise part localization. We thus use hole algorithm <ref type="bibr" target="#b5">[6]</ref> to reduce the stride to 8 px.</p><p>Scale. Selecting image scale is crucial. We found that scaling to a standing height of 340 px performs best: VGG receptive field sees entire body to disambiguate body parts.</p><p>Loss function. We start with a softmax loss that outputs probabilities for each body part and background. The downside is inability to assign probabilities above 0.5 to several close-by body parts. We thus re-formulate the detection as multi-label classification, where at each location a separate set of probability distributions is estimated for each part. We use sigmoid activation function on the output neurons and cross entropy loss. We found this loss to perform better than softmax and converge much faster compared to MSE <ref type="bibr" target="#b36">[37]</ref>. Target training scoremap for each joint is constructed by assigning a positive label 1 at each location within 15 px to the ground truth, and negative label 0 otherwise.</p><p>Location refinement. In order to improve location precision we follow <ref type="bibr" target="#b15">[16]</ref>: we add a location refinement FC layer after the FC7 and use the relative offsets (∆x, ∆y) from a scoremap location to the ground truth as targets.</p><p>Regression to other parts. Similar to location refinement we add an extra term to the objective function where for each part we regress onto all other part locations. We found this auxiliary task to improve the performance (c.f. Sec. 4.3).</p><p>Training. We follow best practices and use SGD for CNN training. In each iteration we forward-pass a single image. After FC6 we select all positive and random negative samples to keep the pos/neg ratio as 25%/75%. We finetune VGG from Imagenet model to pose estimation task and use training data augmentation. We train for 430k iterations with the following learning rates (lr): 10k at lr=0.001, 180k at lr=0.002, 120k at lr=0.0002 and 120k at lr=0.0001. Pretraining at smaller lr prevents the gradients from diverging. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Part Detectors</head><p>Datasets. We train and evaluate on three public benchmarks: "Leeds Sports Poses" (LSP) <ref type="bibr" target="#b19">[20]</ref> (person-centric (PC)), "LSP Extended" (LSPET) [21] 2 , and "MPII Human Pose" ("Single Person") <ref type="bibr" target="#b2">[3]</ref>. The MPII training set (19185 people) is used as default. In some cases LSP training and LSPET are added to MPII (marked as MPII+LSPET in the experiments). Evaluation measures. We use the standard "PCK" metric <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37]</ref> and evaluation scripts available on the web page of <ref type="bibr" target="#b2">[3]</ref>. In addition, we report "Area under Curve" (AUC) computed for the entire range of PCK thresholds. AFR-CNN. Evaluation of AFR-CNN on LSP is shown in Tab. 1. Oracle selecting per part the closest from 2, 000 proposals achieves 97.8% PCK, as proposals cover majority of the ground truth locations. Choosing a single proposal per part using DPM score achieves 23.0% PCK -not surprising given the difficulty of the body part detection problem. Rescoring the proposals using AFR-CNN with AlexNet <ref type="bibr" target="#b23">[24]</ref> dramatically improves the performance to 56.9% PCK, as CNN learns richer image representations. Extending the regions by 4x (1x ≈ head size) achieves 65.1% PCK, as it incorporates more context including the information about symmetric parts and allows to implicitly encode higher-order part relations. Using data augmentation and slightly tuning training parameters improves the performance to 72.4% PCK. We refer to the Appendix A for detailed analysis.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Using Detections in DeepCut Models</head><p>The SPLP problem is NP-hard, to solve instances of it efficiently we select a subset of representative detections from the entire set produced by a model. In our experiments we use |D| = 100 as default detection set size. In case of the AFR-CNN we directly use the softmax output as unary probabilities: f p dc = (p d1 , . . . , p dc ), where p dc is the probability of the detection d being the part class c. For Dense-CNN detection model we use the sigmoid detection unary scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DeepCut Results</head><p>The aim of this paper is to tackle the multi person case. To that end, we evaluate the proposed DeepCut models on four diverse benchmarks. We confirm that both single person (SP) and multi person (MP) variants (Sec. 2) are effective on standard SP pose estimation datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref>. Then, we demonstrate superior performance of DeepCut MP on the multi person pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single Person Pose Estimation</head><p>We now evaluate single person (SP) and more general multi person (MP) DeepCut models on LSP and MPII SP benchmarks described in Sec. 4. Since this evaluation setting implicitly relies on the knowledge that all parts are present in the image we always output the full number of parts. Results on LSP. We report per-part PCK results (Tab. 3) and results for a variable distance threshold <ref type="figure" target="#fig_1">(Fig. 2 (a)</ref>).  DeepCut SP AFR-CNN model using 100 detections improves over unary only (83.0 vs. 82.8% PCK, 58.4 vs. 57% AUC), as pairwise connections filter out some of the high-scoring detections on the background. The improvement is clear in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref> for smaller thresholds. Using part appearance scores in addition to geometrical features in c = c pairwise terms only slightly improves AUC, as the appearance of neighboring parts is mostly captured by a relatively large region centered at each part. The performance of DeepCut MP AFR-CNN matches the SP and improves over AFR-CNN alone: DeepCut MP correctly handles the SP case. Performance of DeepCut SP Dense-CNN is almost identical to unary only, unlike the results for AFR-CNN. Dense-CNN performance is noticeably higher compared to AFR-CNN, and "easy" cases that could have been corrected by a spatial model are resolved by stronger part detectors alone.</p><p>Comparison to the state of the art (LSP). Tab. 3 compares results of DeepCut models to other deep learning methods specifically designed for single person pose estimation. All DeepCuts significantly outperform the state of the art, with DeepCut SP Dense-CNN model improving by 13.7% PCK over the best known result <ref type="bibr" target="#b6">[7]</ref>. The improvement is even more dramatic for lower thresholds <ref type="figure" target="#fig_1">(Fig. 2 (a)</ref>): for PCK @ 0.1 the best model improves by 19.9% over Tompson et al. <ref type="bibr" target="#b36">[37]</ref>, by 26.7% over Fan et al. <ref type="bibr" target="#b40">[41]</ref>, and by 32.4% PCK over Chen&amp;Yuille <ref type="bibr" target="#b6">[7]</ref>. The latter is interesting, as <ref type="bibr" target="#b6">[7]</ref> use a stronger spatial model that predicts the pairwise conditioned on the CNN features, whereas DeepCuts use geometric-only pairwise connectivity. Including body part orientation information into DeepCuts should further improve the results.</p><p>Results on MPII Single Person. Results are shown in Tab. 4 and <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. DeepCut SP AFR-CNN noticeably improves over AFR-CNN alone (79.8 vs. 78.8% PCK, 51.1 vs. 49.0% AUC). The improvement is stronger for smaller thresholds (c.f. <ref type="figure" target="#fig_1">Fig. 2</ref>), as spatial model improves part localization. Dense-CNN alone trained on MPII outperforms AFR-CNN (81.6 vs. 78.8% PCK), which shows the advantages of dense training and evaluation. As expected, Dense-CNN performs slightly better when trained on the larger MPII+LSPET. Finally, DeepCut Dense-CNN SP is slightly better than Dense-CNN alone leading to the best Comparison to the state of the art (MPII). We compare the performance of DeepCut models to the best deep learning approaches from the literature <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref>  <ref type="bibr" target="#b2">3</ref> . DeepCut SP Dense-CNN outperforms both <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref> (82.4 vs 79.6 and 82.0% PCK, respectively). Similar to them DeepCuts rely on dense training and evaluation of part detectors, but unlike them use single size receptive field and do not include multi-resolution context information. Also, appearance and spatial components of DeepCuts are trained piece-wise, unlike <ref type="bibr" target="#b36">[37]</ref>. We observe that performance differences are higher for smaller thresholds (c.f. <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>). This is remarkable, as a much simpler strategy for location refinement is used compared to <ref type="bibr" target="#b35">[36]</ref>. Using multi-resolution filters and joint training should improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multi Person Pose Estimation</head><p>We now evaluate DeepCut MP models on the challenging task of MP pose estimation with an unknown number of people per image and visible body parts per person. Datasets. For evaluation we use two public MP benchmarks: "We Are Family" (WAF) <ref type="bibr" target="#b12">[13]</ref> with 350 training and 175 testing group shots of people; "MPII Human Pose" ("Multi-Person") <ref type="bibr" target="#b2">[3]</ref> consisting of 3844 training and 1758 testing groups of multiple interacting individuals in highly articulated poses with variable number of parts. On MPII, we use a subset of 288 testing images for evaluation. We first prefinetune both AFR-CNN and Dense-CNN from ImageNet to MPII and MPII+LSPET, respectively, and further finetune each model to WAF and MPII Multi-Person. For WAF, we re-train the spatial model on WAF training set. WAF evaluation measure. Approaches are evaluated using the official toolkit <ref type="bibr" target="#b12">[13]</ref>, thus results are directly comparable to prior work. The toolkit implements occlusion-aware "Percentage of Correct Parts (mPCP)" metric. In addition, we report "Accuracy of Occlusion Prediction (AOP)" <ref type="bibr" target="#b7">[8]</ref>. MPII Multi-Person evaluation measure. PCK metric is suitable for SP pose estimation with known number of parts and does not penalize for false positives that are not a part of the ground truth. Thus, for MP pose estimation we use "Mean Average Precision (mAP)" measure, similar to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref>. In contrast to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref> evaluating the detection <ref type="bibr" target="#b2">3</ref>  <ref type="bibr" target="#b36">[37]</ref> was re-trained and evaluated on MPII dataset by the authors. of any part instance in the image disrespecting inconsistent pose predictions, we evaluate consistent part configurations. First, multiple body pose predictions are generated and then assigned to the ground truth (GT) based on the highest PCK h <ref type="bibr" target="#b2">[3]</ref>. Only single pose can be assigned to GT. Unassigned predictions are counted as false positives. Finally, AP for each body part is computed and mAP is reported. Baselines. To assess the performance of AFR-CNN and Dense-CNN we follow a traditional route from the literature based on two stage approach: first a set of regions of interest (ROI) is generated and then the SP pose estimation is performed in the ROIs.  <ref type="bibr" target="#b7">[8]</ref> primarily focuses on the single-person case and handles multi-person scenes akin to <ref type="bibr" target="#b41">[42]</ref>. In contrast to <ref type="bibr" target="#b7">[8]</ref>, DeepCuts are not limited by the number of possible occlusion patterns and cover person-person occlusions and other types as truncation and occlusion by objects in one formulation. DeepCuts significantly outperform <ref type="bibr" target="#b12">[13]</ref> while being more general: unlike <ref type="bibr" target="#b12">[13]</ref> DeepCuts do not require person detector and not limited by a number of occlusion states among people.</p><p>Qualitative comparison to <ref type="bibr" target="#b7">[8]</ref> is provided in <ref type="figure" target="#fig_2">Fig. 3</ref>. Results on MPII Multi-Person. Obtaining a strong detector of highly articulated people having strong occlusions and truncations is difficult. We employ a neck detector as a person detector as it turned out to be the most reliable part. Full body bounding box is created around a neck detection and used as det ROI. GT ROIs were provided by the authors <ref type="bibr" target="#b2">[3]</ref>.</p><p>As the MP approach <ref type="bibr" target="#b7">[8]</ref> is not public, we compare to SP state-of-the-art method <ref type="bibr" target="#b6">[7]</ref> applied to GT ROI image crops.</p><p>Results are shown in Tab. 6. DeepCut MP AFR-CNN improves over AFR-CNN det ROI by 4.3% achieving 51.4% AP. The largest differences are observed for the ankle, knee, elbow and wrist, as those parts benefit more from the connections to other parts. DeepCut MP UB AFR-CNN using upper body parts only slightly improves over the full body model when compared on common parts (60.5 vs 58.2% AP). Similar tendencies are observed for Dense-CNNs, though improvements of MP UB over MP are more significant.</p><p>All DeepCuts outperform Chen&amp;Yuille SP GT ROI, partially due to stronger part detectors compared to <ref type="bibr" target="#b6">[7]</ref> (c.f. Tab. 3). Another reason is that Chen&amp;Yuille SP GT ROI does not model body part occlusion and truncation always predicting the full set of parts, which is penalized by the AP measure. In contrast, our formulation allows to deactivate the part hypothesis in the initial set of part candidates thus effectively performing non-maximum suppression. In DeepCuts part hypotheses are suppressed based on the evidence from all other body parts making this process more reliable.  <ref type="table">Table 6</ref>. Pose estimation results (AP) on MPII Multi-Person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Articulated pose estimation of multiple people in uncontrolled real world images is challenging but of real world interest. In this work, we proposed a new formulation as a joint subset partitioning and labeling problem (SPLP). Different to previous two-stage strategies that separate the detection and pose estimation steps, the SPLP model jointly infers the number of people, their poses, spatial proximity, and part level occlusions. Empirical results on four diverse and challenging datasets show significant improvements over all previous methods not only for the multi person, but also for the single person pose estimation problem. On multi person WAF dataset we improve by 30% PCP over the traditional two-stage approach. This shows that a joint formulation is crucial to disambiguate multiple and potentially overlapping persons. Models and code available at http://pose.mpi-inf.mpg.de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results on LSP dataset</head><p>We provide additional quantitative results on LSP dataset using person-centric (PC) and observer-centric (OC) evaluation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. LSP Person-Centric (PC)</head><p>First, detailed performance analysis is performed when evaluating various parameters of AFR-CNN and results are reported using PCK <ref type="bibr" target="#b32">[33]</ref> evaluation measure. Then, performance of the proposed AFR-CNN and Dense-CNN part detection models is evaluated using strict PCP <ref type="bibr" target="#b13">[14]</ref> measure. Detailed AFR-CNN performance analysis (PCK). Detailed parameter analysis of AFR-CNN is provided in Tab. 7 and results are reported using PCK evaluation measure. Respecting parameters for each experiment are shown in the first column and parameter differences between the neighboring rows in the table are highlighted in bold. Re-scoring the 2000 DPM proposals using AFR-CNN with AlexNet <ref type="bibr" target="#b23">[24]</ref> leads to 56.9% PCK. This is achieved using basis scale 1 (≈ head size) of proposals and training with initial learning rate (lr) of 0.001 for 80k iterations, after which lr is reduced by 0.1, for a total number of 140k SGD iterations. In addition, bounding box regression and default IoU threshold of 0.5 for positive/negative label assignment <ref type="bibr" target="#b15">[16]</ref> have been used. Extending the regions by 4x increases the performance to 65.1% PCK, as it incorporates more context including the information about symmetric body parts and allows to implicitly encode higher-order body part relations into the part detector. No improvements observed for larger scales. Increasing lr to 0.003, lr reduction step to 160k and training for a larger number of iterations (240k) improves the results to 67.4, as higher lr allows for for more significant updates of model parameters when finetuned on the task of human body part detection. Increasing the number of training examples by reducing the training IoU threshold to 0.4 results into slight performance improvement (68.8 vs. 67.4% PCK). Further increasing the number of training samples by horizontally flipping each image and performing translation and scale jittering of the ground truth training samples improves the performance to 69.6% PCK and 42.3% AUC. The improvement is more pronounced for smaller distance thresholds (42.3 vs. 40.9% AUC): localization of body parts is improved due to the increased number of jittered samples that significantly overlap with the ground truth. Further increasing the lr, lr reduction step and total number of iterations altogether improves the performance to 72.4% PCK, and very minor improvements are observed when training longer. All results above are achieved by finetuning the AlexNet architecture from the ImageNet model on the MPII training set. Further finetuning the MPII-finetuned model on the LSP training set increases the performance to 77.9% PCK, as the network learns LSP-specific image representations. Using the deeper VGG <ref type="bibr" target="#b33">[34]</ref> architecture improves over more shallow AlexNet (77.9 vs. 72.4% PCK, 50.0 vs. 44.6% AUC). Funetuning VGG on LSP achieves remarkable 82.8% PCK and 57.0% AUC. Strong increase in AUC (57.0 vs. 50%) characterizes the improvement for smaller PCK evaluation thresholds. Switching off bounding box regression results into performance drop (81.3% PCK, 53.2% AUC) thus showing the importance of the bounding box regression for better part localization. Overall, we demonstrate that proper adaptation and tweaking of the state-of-the-art generic object detector FR-CNN <ref type="bibr" target="#b15">[16]</ref> leads to a strong body part detection model that dramatically improves over the vanilla FR-CNN (82.8 vs. 56.9% PCK, 57.8 vs. 35.9% AUC) and significantly outperforms the state of the art (+9.4% PCK over the best known PCK result <ref type="bibr" target="#b6">[7]</ref> and +9.7% AUC over the best known AUC result <ref type="bibr" target="#b36">[37]</ref>.</p><p>Overall performance using PCP evaluation measure. Performance when using the strict "Percentage of Correct Parts (PCP)" <ref type="bibr" target="#b13">[14]</ref> measure is reported in Tab. 8. In contrast to PCK measure evaluating the accuracy of predicting body joints, PCP evaluation metric measures the accuracy of predicting body part sticks. AFR-CNN achieves 78.3% PCP. Similar to PCK results, DeepCut SP AFR-CNN slightly improves over unary alone, as it enforces more consistent predictions of body part sticks. Using more general multiperson DeepCut MP AFR-CNN model results into similar performance, which shows the generality of DeepCut MP method. DeepCut SP Dense-CNN slightly improves over Dense-CNN alone (84.3 vs. 83.9% PCP) achieving the best PCP result on LSP dataset using PC annotations. This is in contrast to PCK results where performance differences DeepCut SP Dense-CNN vs. Dense-CNN alone are minor.</p><p>We now compare the PCP results to the state of the art. The DeepCut models outperform all other methods by a large margin. The best known PCP result by Chen&amp;Yuille <ref type="bibr" target="#b6">[7]</ref> is outperformed by 10.7% PCP. This is interesting, as their deep learning based method relies on the image conditioned pairwise terms while our approach uses more simple geometric only connectivity. Interestingly, AFR-CNN alone outperforms the approach of Fan et al. <ref type="bibr" target="#b40">[41]</ref> (78.3 vs. 70.1% PCP), who build on the previous version of the R-CNN detector <ref type="bibr" target="#b16">[17]</ref>. At the same time, the best performing dense architecture DeepCut SP Dense-CNN outperforms <ref type="bibr" target="#b40">[41]</ref> by +14.2% PCP. Surprisingly, DeepCut SP Dense-CNN dramatically outperforms the method of Tompson et al. <ref type="bibr" target="#b36">[37]</ref> (+17.7% PCP) that also produces dense score maps, but additionally includes multi-scale receptive fields and jointly trains appearance and spatial models in a single deep learning framework. We envision that both advances can further improve the performance of DeepCut models. Finally, all proposed approaches significantly outperform earlier non-deep  learning based methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref> relying on hand-crafted image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. LSP Observer-Centric (OC)</head><p>We now evaluate the performance of the proposed part detection models on LSP dataset using the observer-centric (OC) annotations <ref type="bibr" target="#b11">[12]</ref>. In contrast to the person-centric (PC) annotations used in all previous experiments, OC annotations do not penalize for the right/left body part prediction flips and count a body part to be the right body part, if it is on the right side of the line connecting pelvis and neck, and a body part to be the left body part otherwise.</p><p>Evaluation is performed using the official OC annotations provided by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref>. Prior to evaluation, we first finetune the AFR-CNN and Dense-CNN part detection models from ImageNet on MPII and MPII+LSPET training sets, respectively, (same as for PC evaluation), and then further finetuned the models on LSP OC training set. PCK evaluation measure. Results using OC annotations and PCK evaluation measure are shown in Tab. 9 and in <ref type="figure" target="#fig_4">Fig. 4</ref>. AFR-CNN achieves 84.2% PCK and 58.1% AUC. This result is only slightly better compared to AFR-CNN evaluated using PC annotations (84.2 vs 82.8% PCK, 58.1 vs. 57.0% AUC). Although PC annotations correspond to a harder task, only small drop in performance when using PC annotations shows that the network can learn to accurately predict person's viewpoint and correctly label left/right limbs in most cases. This is contrast to earlier approaches based on hand-crafted features whose perfor-   mance drops much stronger when evaluated in PC evaluation setting (e.g. <ref type="bibr" target="#b27">[28]</ref> drops from 71.0% PCK when using OC annotations to 58.0% PCK when using PC annotations). Similar to PC case, Dense-CNN detection model outperforms AFR-CNN (88.2 vs. 84.2% PCK and 65.0 vs. 58.1% AUC). The differences are more pronounced when examining the entire PCK curve for smaller distance thresholds (c.f. <ref type="figure" target="#fig_4">Fig. 4</ref>).</p><p>Comparing the performance by AFR-CNN and Dense-CNN to the state of the art, we observe that both proposed approaches significantly outperform other methods. Both deep learning based approaches of Chen&amp;Yuille <ref type="bibr" target="#b6">[7]</ref> and Ouyang et al. <ref type="bibr" target="#b25">[26]</ref> are outperformed by +10.7 and +18.2% PCK when compared to the best performing Dense-CNN. Analysis of PCK curve for the entire range of PCK distance thresholds reveals even larger performance differences (c.f. <ref type="figure" target="#fig_4">Fig. 4</ref>). The results using OC annotations confirm our findings from PC evaluation and clearly show the advantages of the proposed part detection models over the state-of-the-art deep learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, as well as over earlier pose estimation methods based on hand-crafted image features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. PCP evaluation measure. Results using OC annotations and PCP evaluation measure are shown in Tab. 10. Overall, the trend is similar to PC evaluation: both proposed approaches significantly outperform the state-of-the-art methods with Dense-CNN achieving the best result of 85.0% PCP thereby improving by +10% PCP over the best published result <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results on WAF dataset</head><p>Qualitative comparison of our joint formulation DeepCut MP Dense-CNN to the traditional two-stage approach Dense-CNN det ROI relying on person detector, and to the approach of Chen&amp;Yuille <ref type="bibr" target="#b7">[8]</ref> on WAF dataset is shown in <ref type="figure">Fig. 5</ref>. See figure caption for visual performance analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Results on MPII Multi-Person</head><p>Qualitative comparison of our joint formulation DeepCut MP Dense-CNN to the traditional two-stage approach Dense-CNN det ROI on MPII Multi-Person dataset is shown in <ref type="figure" target="#fig_6">Fig. 6 and 7</ref>. Dense-CNN det ROI works well when multiple fully visible individuals are sufficiently separated and thus their body parts can be partitioned based on the person detection bounding box. In this case the strong Dense-CNN body part detection model can correctly estimate most of the visible body parts (image <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19)</ref>. However, Dense-CNN det ROI cannot tell apart the body parts of multiple individuals located next to each other and possibly occluding each other, and often links the body parts across the individuals (images <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. In addition, Dense-CNN det ROI cannot reason about occlusions and truncations always providing a prediction for each body part (image 4, 6, 10). In contrast, DeepCut MP Dense-CNN is able to correctly partition and label an initial pool of body part candidates (each image, top row) into subsets that correspond to sets of mutually consistent body part candidates and abide to mutual consistency and exclusion constraints (each image, row 2), thereby outputting consistent body pose predictions (each image, row 3). c = c pairwise terms allow to partition the initial set of part detection candidates into valid pose configurations (each image, row 2: personclusters highlighted by dense colored connections). c = c pairwise terms facilitate clustering of multiple body part candidates of the same body part of the same person (each image, row 2: markers of the same type and color). In ad-dition, c = c pairwise terms facilitate a repulsive property that prevents nearby part candidates of the same type to be associated to different people (image 1: detections of the left shoulder are assigned to the front person only). Furthermore, DeepCut MP Dense-CNN allows to either merge or deactivate part hypotheses thus effectively performing non-maximum suppression and reasoning about body part occlusions and truncations (image 3, row 2: body part hypotheses on the background are deactivated (black crosses); image 6, row 2: body part hypotheses for the truncated body parts are deactivated (black crosses); image 1-6, 8-9, 13-14, row 3: only visible body parts of the partially occluded people are estimated, while non-visible body parts are correctly predicted to be occluded). These qualitative examples show that DeepCuts MP can successfully deal with the unknown number of people per image and the unknown number of visible body parts per person. 8 9 10    <ref type="figure">Fig. 1</ref> for the color-coding explanation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>IOU nion, IOM in, IOM ax. The latter three are intersections over union/minimum/maximum of the two detection boxes, respectively, andh = (h d + h d )/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>DeepCut SP Dense-CNN Tompson et al., NIPS'14 Tompson et al., CVPR'15 (a) LSP (PC) (b) MPII Single Person Pose estimation results over all PCK thresholds. outperforming AFR-CNN (c.f. Tab. 1, row 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparison of our joint formulation DeepCut MP Dense-CNN (middle) to the traditional two-stage approach Dense-CNN det ROI (top) and the approach of Chen&amp;Yuille [8] (bottom) on WAF dataset. In contrast to det ROI, DeepCut MP is able to disambiguate multiple and potentially overlapping persons and correctly assemble independent detections into plausible body part configurations. In contrast to [8], DeepCut MP can better predict occlusions (image 2 person 1 − 4 from the left, top row; image 4 person 1, 4; image 5, person 2) and better cope with strong articulations and foreshortenings (image 1, person 1, 3; image 2 person 1 bottom row; image 3, person 1-2). See Appendix B for more examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>et al., CVPR'14 Pishchulin et., ICCV'13 Ramakrishna et al., ECCV'14 Kiefel&amp;Gehler, ECCV'14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Pose estimation results over all PCK thresholds on LSP (OC) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Qualitative comparison of our joint formulation DeepCut MP Dense-CNN (rows 2, 5) to the traditional two-stage approach Dense-CNN det ROI (rows<ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4)</ref> and to the approach of Chen&amp;Yuille<ref type="bibr" target="#b7">[8]</ref> (rows 3, 6) on WAF dataset. det ROI does not reason about occlusion and often predicts inconsistent body part configurations by linking the parts across the nearby staying people (image 4, right shoulder and wrist of person 2 are linked to the right elbow of person 3; image 5, left elbow of person 4 is linked to the left wrist of person 3). In contrast, DeepCut MP predicts body part occlusions, disambiguates multiple and potentially overlapping people and correctly assembles independent detections into plausible body part configurations (image 4, left arms of people 1-3 are correctly predicted to be occluded; image 5, linking of body parts across people 3 and 4 is corrected; image 7, occlusion of body parts is correctly predicted and visible parts are accurately estimated). In contrast to Chen&amp;Yuille<ref type="bibr" target="#b7">[8]</ref>, DeepCut MP better predicts occlusions of person's body parts by the nearby staying people (images 1, 3-9), but also by other objects (image 2, left arm of person 1 is occluded by the chair). Furthermore, DeepCut MP is able to better cope with strong articulations and foreshortenings (image 1, person 6; image 3, person 2; image 5, person 4; image 7, person 4; image 8, person 1). Typical DeepCut MP failure case is shown in image 10: the right upper arm of person 3 and both arms of person 4 are not estimated due to missing part detection candidates. Qualitative comparison of our joint formulation DeepCut MP Dense-CNN (rows 1-3, 5-7) to the traditional two-stage approach Dense-CNN det ROI (rows 4, 8) on MPII SeeFig. 1for the color-coding explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison (contd.) of our joint formulation DeepCut MP Dense-CNN (rows 1-3, 5-7) to the traditional two-stage approach Dense-CNN det ROI (rows 4, 8) on MPII Multi-Person dataset. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Setting Head Sho Elb Wri Hip Knee Ank PCK AUC oracle 2000 98.8 98.8 97.4 96.4 97.4 98.3 97.7 97.8 84.0 DPM scale 1 48.8 25.1 14.4 10.2 13.6 21.8 27.1 23.0 13.6 AlexNet scale 1 82.2 67.0 49.6 45.4 53.1 52.9 48.2 56.9 35.9 AlexNet scale 4 85.7 74.4 61.3 53.2 64.1 63.1 53.8 65.1 39.0 + optimal params 88.1 79.3 68.9 62.6 73.5 69.3 64.7 72.4 44.6 VGG scale 4 optimal params 91.0 84.2 74.6 67.7 77.4 77.3 72.8 77.9 50.0 + finetune LSP 95.4 86.5 77.8 74.0 84.5 78.8 82.6 82.8 57.</figDesc><table /><note>0 Table 1. Unary only performance (PCK) of AFR-CNN on the LSP (Person-Centric) dataset. AFR-CNN is finetuned from ImageNet to MPII (lines 3-6), and then finetuned to LSP (line 7).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>SettingHeadSho Elb Wri Hip Knee Ank PCK AUC MPII softmax 91.5 85.3 78.0 72.4 81.7 80.7 75.7 80.8 51.9 + LSPET 94.6 86.8 79.9 75.4 83.5 82.8 77.9 83.0 54.7 + sigmoid 93.5 87.2 81.0 77.0 85.5 83.3 79.3 83.8 55.6 + location refinement 95.0 88.4 81.5 76.4 88.0 83.3 80.8 84.8 61.5 + auxiliary task 95.1 89.6 82.8 78.9 89.0 85.9 81.2 86.1 61.6 + finetune LSP 97.2 90.8 83.0 79.3 90.6 85.6 83.1 87.1 63.6 Unary only performance (PCK) of Dense-CNN VGG on LSP (PC) dataset. Dense-CNN is finetuned from ImageNet to MPII (line 1), to MPII+LSPET (lines 2-5), and finally to LSP (line 6).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Deeper VGG architecture improves over smaller AlexNet reaching 77.9% PCK. All results so far are achieved by finetuning the ImageNet models on MPII. Further finetuning to LSP leads to remarkable 82.8% PCK: CNN learns LSP-specific image representations. Strong increase in AUC (57.0 vs. 50%) is due to improvements for smaller PCK thresholds. Using no bounding box regression leads to performance drop (81.3% PCK, 53.2% AUC): location refinement is crucial for better localization. Overall AFR-CNN obtains very good results on LSP by far outperforming the state of the art (c.f. Tab. 3, rows 7 − 9). Evaluation on MPII shows competitive performance (Tab. 4, row 1). Dense-CNN. The results are in Tab. 2. Training with VGG on MPII with softmax loss achieves 80.8% PCK thereby</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). This shows the advantages of fully convolutional training and evaluation. Expectedly, training on larger MPII+LSPET dataset improves the results (83.0 vs. 80.8% PCK). Using crossentropy loss with sigmoid activations improves the results to 83.8% PCK, as it better models the appearance of close-by parts. Location refinement improves localization accuracy</figDesc><table /><note>(84.8% PCK), which becomes more clear when analyzing AUC (61.5 vs. 55.6%). Interestingly, regressing to other parts further improves PCK to 86.1% showing a value of training with the auxiliary task. Finally, finetuning to LSP achieves the best result of 87.1% PCK, which is significantly higher than the best published results (c.f. Tab. 3, rows 7−9). Unary-only evaluation on MPII reveals slightly higher AUC results compared to the state of the art (Tab. 4, row 3 − 4).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SettingHead Sho Elb Wri Hip Knee Ank PCK AUCAFR-CNN (unary) 95.4 86.5 77.8 74.0 84.5 82.6 78.8 82.8 57.0 + DeepCut SP 95.4 86.7 78.3 74.0 84.3 82.9 79.2 83.0 58.4 + appearance pairwise 95.4 87.2 78.6 73.7 84.7 82.8 78.8 83.0 58.5 + DeepCut MP 95.2 86.7 78.2 73.5 84.6 82.8 79.0 82.9 58.0 Dense-CNN (unary) 97.2 90.8 83.0 79.3 90.6 85.6 83.1 87.1 63.6 + DeepCut SP 97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1 63.5 + DeepCut MP 96.2 91.2 83.3 77.6 91.3 87.0 80.4 86.7 62.6 re-evaluated using the standard protocol, for details see project page of<ref type="bibr" target="#b40">[41]</ref> </figDesc><table><row><cell>Tompson et al. [37]</cell><cell>90.6 79.2 67.9 63.4 69.5 71.0 64.2 72.3 47.3</cell></row><row><cell>Chen&amp;Yuille [7]</cell><cell>91.8 78.2 71.8 65.5 73.3 70.2 63.4 73.4 40.1</cell></row><row><cell>Fan et al. [41]  *</cell><cell>92.4 75.2 65.3 64.0 75.7 68.3 70.4 73.0 43.2</cell></row><row><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Pose estimation results (PCK) on LSP (PC) dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Setting Head Sho Elb Wri Hip Knee Ank PCK h AUC AFR-CNN (unary) 91.5 89.7 80.5 74.4 76.9 69.6 63.1 78.8 49.0 + DeepCut SP 92.3 90.6 81.7 74.9 79.2 70.4 63.0 79.8 51.1 Dense-CNN (unary) 93.5 88.6 82.2 77.1 81.7 74.4 68.9 81.6 56.0 +LSPET 94.0 89.4 82.3 77.5 82.0 74.4 68.7 81.9 56.5 +DeepCut SP 94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4 56.5 Tompson et al. [37] 95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6 51.8 Tompson et al. [36] 96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0 54.9 Pose estimation results (PCK h ) on MPII Single Person. result on MPII dataset (82.4% PCK).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>DeepCut MP Dense-CNN 99.3 81.5 79.5 87.1 84.7 86.5 Pose estimation results (mPCP) on WAF dataset.</figDesc><table><row><cell>Setting</cell><cell cols="5">Head U Arms L Arms Torso mPCP AOP</cell></row><row><cell>AFR-CNN det ROI</cell><cell cols="2">69.8 46.0</cell><cell cols="3">36.7 83.7 53.1 73.9</cell></row><row><cell cols="3">DeepCut MP AFR-CNN 99.0 79.5</cell><cell cols="3">74.3 87.1 82.2 85.6</cell></row><row><cell>Dense-CNN det ROI</cell><cell cols="2">76.0 46.0</cell><cell cols="3">40.2 83.7 55.3 73.8</cell></row><row><cell>Ghiasi et. al. [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.6 74.0</cell></row><row><cell>Eichner&amp;Ferrari [13]</cell><cell cols="2">97.6 68.2</cell><cell cols="3">48.1 86.1 69.4 80.0</cell></row><row><cell>Chen&amp;Yuille [8]</cell><cell cols="2">98.5 77.2</cell><cell cols="3">71.3 88.5 80.7 84.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Results are shown in Tab.<ref type="bibr" target="#b4">5</ref>. det ROI is obtained by extending provided upper body detection boxes. AFR-CNN det ROI achieves 57.6% mPCP and 73.9% AOP. DeepCut MP AFR-CNN significantly improves over AFR-CNN det ROI achieving 82.2% mPCP. This improvement is stronger compared to LSP and MPII due to several reasons. First, mPCP requires consistent prediction of body sticks as opposite to body joints, and including spatial model enforces consistency. Second, mPCP metric is occlusionaware. DeepCuts can deactivate detections for the occluded parts thus effectively reasoning about occlusion. This is supported by strong increase in AOP (85.6 vs. 73.9%). Results by DeepCut MP Dense-CNN follow the same tendency achieving the best performance of 84.7% mPCP and 86.5% AOP. Both increase in mPCP and AOP show the advantages of DeepCuts over traditional det ROI approaches.Tab. 5 shows that DeepCuts outperform all prior methods. Deep learning method<ref type="bibr" target="#b7">[8]</ref> is outperformed both for mPCP (84.7 vs. 80.7%) and AOP (86.5 vs. 84.9%) measures. This is remarkable, as DeepCuts reason about part interactions across several people, whereas</figDesc><table /><note>This corresponds to unary only per- formance. ROI are either based on a ground truth (GT ROI) or on the people detector output (det ROI). Results on WAF.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Chen&amp;Yuille SP GT ROI 65.0 34.2 22.0 15.7 19.2 15.8 14.2 34.2 27.1</figDesc><table><row><cell>Setting</cell><cell cols="5">Head Sho Elb Wri Hip Knee Ank UBody FBody</cell></row><row><cell>AFR-CNN det ROI</cell><cell cols="5">71.1 65.8 49.8 34.0 47.7 36.6 20.6 55.2 47.1</cell></row><row><cell>AFR-CNN MP</cell><cell cols="5">71.8 67.8 54.9 38.1 52.0 41.2 30.4 58.2 51.4</cell></row><row><cell>AFR-CNN MP UB</cell><cell>75.2 71.0 56.4 39.6 -</cell><cell>-</cell><cell>-</cell><cell>60.5</cell><cell>-</cell></row><row><cell>Dense-CNN det ROI</cell><cell cols="5">77.2 71.8 55.9 42.1 53.8 39.9 27.4 61.8 53.2</cell></row><row><cell>Dense-CNN MP</cell><cell cols="5">73.4 71.8 57.9 39.9 56.7 44.0 32.0 60.7 54.1</cell></row><row><cell>Dense-CNN MP UB</cell><cell>81.5 77.3 65.8 50.0 -</cell><cell>-</cell><cell>-</cell><cell>68.7</cell><cell>-</cell></row><row><cell>AFR-CNN GT ROI</cell><cell cols="5">73.2 66.5 54.6 42.3 50.1 44.3 37.8 59.1 53.1</cell></row><row><cell>Dense-CNN GT ROI</cell><cell cols="5">78.1 74.1 62.2 52.0 56.9 48.7 46.1 66.6 60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Setting Head Sho Elb Wri Hip Knee Ank PCK AUC AlexNet scale 1, lr 0.001, lr step 80k, # iter 140k, IoU pos/neg 0.5 82.2 67.0 49.6 45.4 53.1 52.9 48.2 56.9 35.9 AlexNet scale 4, lr 0.001, lr step 80k, # iter 140k, IoU pos/neg 0.5 85.7 74.4 61.3 53.2 64.1 63.1 53.8 65.1 39.0 AlexNet scale 4, lr 0.003, lr step 160k, # iter 240k, IoU pos/neg 0.5 87.0 75.1 63.0 56.3 67.0 65.7 58.0 67.4 40.8 AlexNet scale 4, lr 0.003, lr step 160k, # iter 240k, IoU pos/neg 0.4 87.5 76.7 64.8 56.0 68.2 68.7 59.6 68.8 40.9 AlexNet scale 4, lr 0.003, lr step 160k, # iter 240k, IoU pos/neg 0.4, data augment 87.8 77.8 66.0 58.1 70.9 66.9 59.8 69.6 42.3 AlexNet scale 4, lr 0.004, lr step 320k, # iter 1M, IoU pos/neg 0.4, data augment 88.1 79.3 68.9 62.6 73.5 69.3 64.7 72.4 44.6 re-evaluated using the standard protocol, for details see project page of<ref type="bibr" target="#b40">[41]</ref> </figDesc><table><row><cell></cell><cell>+ finetune LSP, lr 0.0005, lr step 10k, # iter 40k</cell><cell>92.9 81.0 72.1 66.4 80.6 77.6 75.0 77.9 51.6</cell></row><row><cell cols="2">VGG scale 4, lr 0.003, lr step 160k, # iter 320k, IoU pos/neg 0.4, data augment</cell><cell>91.0 84.2 74.6 67.7 77.4 77.3 72.8 77.9 50.0</cell></row><row><cell cols="2">+ finetune LSP lr 0.0005, lr step 10k, # iter 40k</cell><cell>95.4 86.5 77.8 74.0 84.5 78.8 82.6 82.8 57.0</cell></row><row><cell cols="3">Table 7. PCK performance of AFR-CNN (unary) on LSP (PC) dataset. AFR-CNN is finetuned from ImageNet on MPII (lines 1-6, 8), and</cell></row><row><cell cols="2">then finetuned on LSP (lines 7, 9).</cell></row><row><cell></cell><cell>Torso Upper Lower Upper Fore-Head PCP</cell></row><row><cell></cell><cell>Leg Leg Arm arm</cell></row><row><cell>AFR-CNN (unary)</cell><cell>93.2 82.7 77.7 75.5 63.5 91.2 78.3</cell></row><row><cell>+ DeepCut SP</cell><cell>93.3 83.2 77.8 76.3 63.7 91.5 78.7</cell></row><row><cell cols="2">+ appearance pairwise 93.4 83.5 77.8 76.6 63.8 91.8 78.9</cell></row><row><cell>+ DeepCut MP</cell><cell>93.6 83.3 77.6 76.3 63.5 91.2 78.6</cell></row><row><cell>Dense-CNN (unary)</cell><cell>96.2 87.8 81.8 81.6 72.3 95.6 83.9</cell></row><row><cell>+ DeepCut SP</cell><cell>97.0 88.8 82.0 82.4 71.8 95.8 84.3</cell></row><row><cell>+ DeepCut MP</cell><cell>96.4 88.8 80.9 82.4 71.3 94.9 83.8</cell></row><row><cell>Tompson et al. [37]</cell><cell>90.3 70.4 61.1 63.0 51.2 83.7 66.6</cell></row><row><cell>Chen&amp;Yuille [7]</cell><cell>96.0 77.2 72.2 69.7 58.1 85.6 73.6</cell></row><row><cell>Fan et al. [41]  *</cell><cell>95.4 77.7 69.8 62.8 49.1 86.6 70.1</cell></row><row><cell>Pishchulin et al. [28]</cell><cell>88.7 63.6 58.4 46.0 35.2 85.1 58.0</cell></row><row><cell>Wang&amp;Li [40]</cell><cell>87.5 56.0 55.8 43.1 32.1 79.1 54.1</cell></row><row><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Pose estimation results (PCP) on LSP (PC) dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Pose estimation results (PCK) on LSP (OC) dataset.</figDesc><table><row><cell>Setting</cell><cell>Head Sho Elb Wri Hip Knee Ank PCK AUC</cell></row><row><cell>AFR-CNN (unary)</cell><cell>95.3 88.3 78.5 74.2 87.3 84.2 81.2 84.2 58.1</cell></row><row><cell>Dense-CNN (unary)</cell><cell>97.4 92.0 83.8 79.0 93.1 88.3 83.7 88.2 65.0</cell></row><row><cell>Chen&amp;Yuille [7]</cell><cell>91.5 84.7 70.3 63.2 82.7 78.1 72.0 77.5 44.8</cell></row><row><cell>Ouyang et al. [26]</cell><cell>86.5 78.2 61.7 49.3 76.9 70.0 67.6 70.0 43.1</cell></row><row><cell>Pishchulin et. [28]</cell><cell>87.5 77.6 61.4 47.6 79.0 75.2 68.4 71.0 45.0</cell></row><row><cell>Kiefel&amp;Gehler [22]</cell><cell>83.5 73.7 55.9 36.2 73.7 70.5 66.9 65.8 38.6</cell></row><row><cell cols="2">Ramakrishna et al. [30] 84.9 77.8 61.4 47.2 73.6 69.1 68.8 69.0 35.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Torso Upper Lower Upper Fore-Head PCP Leg Leg Arm arm AFR-CNN (unary) 92.9 86.3 79.8 77.0 64.2 91.8 79.9 Dense-CNN (unary) 96.0 91.0 83.5 82.8 71.8 96.2 85.0 Chen&amp;Yuille [7] 92.7 82.9 77.0 69.2 55.4 87.8 75.0 Ouyang et al. [26] 88.6 77.8 71.9 61.9 45.4 84.3 68.7 Ramakrishna et al. [30] 88.1 79.0 73.6 62.8 39.5 80.4 67.8 Pose estimation results (PCP) on LSP (OC) dataset.</figDesc><table><row><cell>Pishchulin et. [28]</cell><cell>88.7 78.9 73.2 61.8 45.0 85.1 69.2</cell></row><row><cell>Kiefel&amp;Gehler [22]</cell><cell>84.3 74.5 67.6 54.1 28.3 78.3 61.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To reduce labeling noise we re-annotated original high-resolution images and make the data available at http://datasets.d2.mpi-inf. mpg.de/hr-lspet/hr-lspet.zip</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ensemble segmentation using efficient integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1966" to="1977" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic image segmentation with closedness constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14. 1, 5, 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correlation clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="89" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3D pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<idno>CVPR&apos;14. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;14. 1</title>
		<meeting><address><addrLine>2, 6, 8, 9</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The partition problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correlation clustering in general weighted graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Demaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="172" to="187" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Geometry of Cuts and Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laurent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Appearance sharing for collective human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>ACCV&apos;12. 10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;10</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CVPR&apos;08. 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Parsing occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno>CVPR&apos;14. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;15</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>CVPR&apos;14. 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Global pose estimation using non-tree models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<idno>CVPR&apos;09. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC&apos;10</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Effective Human Pose Estimation from Inaccurate Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;11</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human pose estimation with fields of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;14</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image segmentation using higher-order correlation clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1761" to="1774" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;12</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>CVPR&apos;13. 10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;13. 2, 4</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormaehlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;12</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;14</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to parse images of articulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;06</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recovering human body configurations using pairwise constraints between parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;05</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;13. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR,14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;11. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;14. 1, 5, 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<idno>13. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>CVPR&apos;13. 10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI&apos;13</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast planar correlation clustering for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yarkony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
