<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Approach of Multi-scale Deep and Hand-crafted Features for Defocus Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsun</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Robotics and Computer Vision Lab., KAIST, Korea, Republic of ‡ Tencent YouTu Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Robotics and Computer Vision Lab., KAIST, Korea, Republic of ‡ Tencent YouTu Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Approach of Multi-scale Deep and Hand-crafted Features for Defocus Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce robust and synergetic handcrafted features and a simple but efficient deep feature from a convolutional neural network (CNN) architecture for defocus estimation. This paper systematically analyzes the effectiveness of different features, and shows how each feature can compensate for the weaknesses of other features when they are concatenated. For a full defocus map estimation, we extract image patches on strong edges sparsely, after which we use them for deep and hand-crafted feature extraction. In order to reduce the degree of patch-scale dependency, we also propose a multi-scale patch extraction strategy. A sparse defocus map is generated using a neural network classifier followed by a probability-joint bilateral filter. The final defocus map is obtained from the sparse defocus map with guidance from an edge-preserving filtered input image. Experimental results show that our algorithm is superior to state-of-the-art algorithms in terms of defocus estimation. Our work can be used for applications such as segmentation, blur magnification, all-in-focus image generation, and 3-D estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The amount of defocus represents priceless information can be obtained from a single image. If we know the amount of defocus at each pixel in an image, higher level information can be inferred based on defocus values such as depth <ref type="bibr" target="#b43">[43]</ref>, salient region <ref type="bibr" target="#b13">[13]</ref> and foreground and background of a scene <ref type="bibr" target="#b26">[26]</ref> and so on. Defocus estimation, however, is a highly challenging task, not only because the estimated defocus values vary spatially, but also because the estimated solution contains ambiguities <ref type="bibr" target="#b17">[17]</ref>, where the appearances of two regions with different amounts of defocus can be very similar. Conventional methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b42">42]</ref> rely on strong edges to estimate the amount of defocus. Determining the amount of defocus only based on the strength of strong edges, however, may lead to overconfidence and misestimations. Thus, we need a more reliable and robust defocus descriptor for defocus estimations.</p><p>In this paper, we present hand-crafted and deep features which assess various aspects of an image for defocus estimation, and a method to obtain a reliable full defocus map of a scene. Our hand-crafted features focus on three components of an image: the frequency domain power distribution, the gradient distribution and the singular values of an image. We also utilize a convolutional neural network (CNN) to extract high-dimensional deep features directly learnt from millions of in-focus and blurred image patches. All of the features are concatenated to construct our defocus feature vector and are fed into a fully connected neural network classifier to determine the amount of defocus.</p><p>One of the challenges associated with the defocus estimation is the vagueness of the amount of defocus in homogeneous regions, as such regions show almost no difference in appearance when they are in-focus or blurred. To avoid this problem, we first estimate the amount of defocus using multi-scale image patches from only strong edges, and then propagate the estimated values into homogeneous regions with the guidance of edge-preserving filtered input image. The full defocus map is obtained after the propagation step. We use the defocus map for various applications, such as segmentation, blur magnification, all-in-focus image generation, and 3-D estimation. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of our defocus estimation result and a refocusing application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Defocus Estimation Defocus estimation plays an important role in many applications in the computer vision community. It is used in digital refocusing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>, depth from defocus <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b43">43]</ref>, salient region detection <ref type="bibr" target="#b13">[13]</ref> and image matting <ref type="bibr" target="#b26">[26]</ref>, to name just a few.</p><p>Elder and Zucker <ref type="bibr" target="#b10">[10]</ref> estimate the minimum reliable scale for edge detection and defocus estimation. They utilize second derivative Gaussian filter responses, but this method is not robust due to errors which arise during the localization of edges. Bae and Durand <ref type="bibr" target="#b1">[2]</ref> also utilize second derivative Gaussian filter responses to magnify the amount of defocus on the background region. However, their strategy is time-consuming owing to its use of a brute-force scheme. Tai and Brown <ref type="bibr" target="#b35">[35]</ref> employ a measure called local contrast prior, which considers the relationships between local image gradients and local image contrasts, but the local contrast prior is not robust to noise. Zhuo and Sim <ref type="bibr" target="#b42">[42]</ref> use the ratio between the gradients of input and re-blurred images with a known Gaussian blur kernel. However, it easily fails with noise and edge mislocalization. Liu et al. <ref type="bibr" target="#b22">[22]</ref> inspect the power spectrum slope, gradient histogram, maximum saturation and autocorrelation congruency. Their segmentation result, however, cannot precisely localize blurry regions. Shi et al. <ref type="bibr" target="#b31">[31]</ref> use not only statistical measures such as peakedness and heavy-tailedness but also learnt filters from image data with labels. Homogeneous regions in the image are weak points in their algorithm. Shi et al. <ref type="bibr" target="#b32">[32]</ref> construct sparse dictionaries containing sharp and blurry bases and determine which dictionary can reconstruct an input image sparsely, but their algorithm is not robust to large blur, as it is tailored for just noticeable blur estimation.</p><p>Neural Networks Neural networks have proved their worth as algorithms superior to their conventional counterparts in many computer vision tasks, such as object and video classification <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b29">29]</ref>, image restoration <ref type="bibr" target="#b9">[9]</ref>, image matting <ref type="bibr" target="#b6">[7]</ref>, image deconvolution <ref type="bibr" target="#b38">[38]</ref>, motion blur estimation <ref type="bibr" target="#b34">[34]</ref>, blur classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">40]</ref>, super-resolution <ref type="bibr" target="#b8">[8]</ref>, salient region detection <ref type="bibr" target="#b16">[16]</ref> and edge-aware filtering <ref type="bibr" target="#b39">[39]</ref>.</p><p>Sun et al. <ref type="bibr" target="#b34">[34]</ref> focus on motion blur kernel estimation. They use a CNN to estimate pre-defined discretized motion blur kernels. However, their approach requires rotational input augmentation and takes a considerable amount of time during the MRF propagation step. Aizenberg et al. <ref type="bibr" target="#b0">[1]</ref> use a multilayer neural network based on multivalued neurons (MVN) for blur identification. The MVN learning step is computationally efficient. However, their neural network structure is quite simple. Yan and Shao <ref type="bibr" target="#b40">[40]</ref> adopt twostage deep belief networks (DBN) to classify blur types and to identify blur parameters, but they only rely on features from the frequency domain.</p><p>Our Work Compared with the previous works, instead of using only hand-crafted features, we demonstrate how we can apply deep features to the defocus estimation problem. The deep feature is learnt directly from training data with different amounts of defocus blur. Because each extracted deep feature is still a local feature, our hand-crafted features, which capture both local and global information of an image patch, demonstrate the synergetic effect of boosting the performance of our algorithm. Our work significantly outperforms previous works on defocus estimation in terms of both quality and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Extraction</head><p>We extract multi-scale image patches from an input image for feature extraction. In addition, we extract image patches on edges only because homogeneous regions are ambiguous in defocus estimation. For edge extraction, we first transform the input image from the RGB to the HSV color space and then use a V channel to extract image edges.</p><p>There have been numerous hand-crafted features for sharpness measurements <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>. In this work, three hand-crafted features related to the frequency domain power distribution, the gradient distribution, and the singular values of a grayscale image patch are proposed. The deep feature is extracted from a CNN which directly processes color image patches in the RGB space for feature extraction. All of the extracted features are then concatenated to form our final defocus feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-scale Patch Extraction</head><p>Because we extract hand-crafted and deep features based on image patches, it is important to determine a suitable patch size for each pixel in an image. Although there have been many works related to scale-space theories <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>, there is still some ambiguity with regard to the relationship between the patch scale and the hand-crafted features, as they utilize global information in an image patch. In other words, a sharp image patch can be regarded as blurry mistakenly depending on the size of the patch, and vice versa. In order to avoid patch scale dependency, we extract multi-scale patches depending on the strength of the edges. In natural images, strong edges are more likely to be in-focus than blurry ones ordinarily. Therefore, we assume that image patches from strong edges are in-focus and that weak edges are blurry during the patch extraction step. For sharp patches, we can determine their sharpness accurately with a small patch size, whereas blurry patches can be ambiguous with a small patch size because of their little change in the appearance when they are blurred or not. <ref type="figure" target="#fig_1">Figure 2</ref> shows multi-scale patches from strong and weak edges. <ref type="figure" target="#fig_1">Figure 2</ref>  edges still have abundant information for defocus estimation, while <ref type="figure" target="#fig_1">Figure 2</ref> (c) shows that small patches from weak edges severely lack defocus information and contain high degrees of ambiguity. Based on this observation, we extract small patches from strong edges and large patches from weak edges. Edges are simply extracted using the Canny edge detector <ref type="bibr" target="#b3">[4]</ref> with multi-threshold values. In general, the majority of the strong edges can be extracted in an infocus area. Weak edges can be extracted in both sharp and blurry areas because they can come from in-focus weak textures or out-of-focus sharp textures. Our multi-scale patch extraction scheme boosts the performance of a defocus estimation algorithm drastically (Section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DCT Feature</head><p>We transform a grayscale image patch P I to the frequency domain to analyze its power distribution. We utilize the discrete cosine transform (DCT) because the DCT offers strong energy compaction <ref type="bibr" target="#b28">[28]</ref>; i.e., most of the information pertaining to a typical signal tends to be concentrated in a few low-frequency bands. Hence, when an image is more detailed, more non-zero DCT coefficients are needed to preserve the information. Accordingly, we can examine highfrequency bands at a higher resolution with the DCT than with the discrete Fourier transform (DFT) or the discrete sine transform (DST). Because an in-focus image has more high-frequency components than an out-of-focus image, the ratio of high-frequency components in an image patch can be a good measure of the blurriness of an image. Our DCT feature f D is constructed using the power distribution ratio of frequency components as follows:</p><formula xml:id="formula_0">f D (k) = 1 W D log 1 + θ ρ k+1 ρ=ρ k |P(ρ, θ)| S k , k ∈ [1, n D ],</formula><p>(1) where | · |, P(ρ, θ), ρ k , S k , W D and n D denote the absolute operator, the discrete cosine transformed image patch with polar coordinates, the k-th boundary of the radial coordinate, the area enclosed by ρ k and ρ k+1 , a normalization factor to make sum of the feature unity, and the dimensions of the feature, respectively. <ref type="figure" target="#fig_2">Figure 3</ref>   tures from sharp and blurry patches after different transformations. The absolute difference between sharp and blurry features can be a measure of discriminative power. In this case, the DCT feature has the best discriminative power because its absolute difference between sharp and blurry features is greater than those of the other transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gradient Feature</head><p>We calculate the gradients of P I using Sobel filtering to obtain a gradient patch P G . Typically, there are more strong gradients in a sharp image patch than in a blurry image. Therefore, the ratio of the strong gradient components in an image patch can be another measure of the sharpness of the image. We use the normalized histogram of P G as a second component of our defocus feature. Our gradient feature f G is defined as follows:</p><formula xml:id="formula_1">f G (k) = 1 W G log (1 + H G (k)) , k ∈ [1, n G ],<label>(2)</label></formula><p>where H G , W G and n G denote the histogram of P G , the normalization factor and the dimensions of the feature, respectively. <ref type="figure" target="#fig_2">Figure 3</ref> (b) shows a comparison of sharp and blurry gradient features. Despite its simplicity, the gradient feature shows quite effective discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SVD Feature</head><p>Singular value decomposition (SVD) has many useful applications in signal processing. One such application of SVD is the low-rank matrix approximation <ref type="bibr" target="#b27">[27]</ref>   written as follows:</p><formula xml:id="formula_2">A = UΛV T = N k=1 A k = N k=1 λ k u k v T k ,<label>(3)</label></formula><p>where Λ, N , λ k , u k and v k denote the m × n diagonal matrix, the number of non-zero singular values of A, the k-th singular value, and the k-th column of the real unitary matrices U and V, respectively. If we construct a matrix A = n k=1 A k , where n ∈ [1, N ], we can approximate the given matrix A withÃ. In the case of image reconstruction, low-rank matrix approximation will discard small details in the image, and the amount of the loss of details is inversely proportional to n. <ref type="figure" target="#fig_4">Figure 4</ref> shows an example of the low-rank matrix approximation of an image. Note that the amount of preserved image detail is proportional to the number of preserved singular values.</p><p>We extract a SVD feature based on low-rank matrix approximation. Because more non-zero singular values are needed to preserve the details in an image, a sharp image patch tends to have more non-zero singular values than a blurry image patch; i.e., a non-zero λ k with large k is a clue to measure the amount of detail. The scaled singular values define our last hand-crafted feature as follows:</p><formula xml:id="formula_3">f S (k) = 1 W S log (1 + λ k ) , k ∈ [1, n S ],<label>(4)</label></formula><p>where W S denotes the normalization factor and n S denotes the dimensions of the feature. <ref type="figure" target="#fig_2">Figure 3 (c)</ref> shows a comparison of sharp and blurry SVD features. The long tail of the sharp feature implies that more details are preserved in an image patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Deep Feature</head><p>We extract the deep feature f C from a color image patch using a CNN. To deal with multi-scale patches, small-scale patches are zero-padded before they are fed into the CNN. Our feature extraction network consists of convolutional, ReLU and max pooling layers, as illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>. </p><formula xml:id="formula_4">f D (DCT) 38.15 f H (Hand-crafted) 71.49 f G (Gradient) 68.36 f C (Deep) 89.38 f S (SVD)</formula><p>61.76 f B (Concatenated) 94.16 <ref type="table">Table 1</ref>: Classification accuracies. Note that the accuracy of a random guess is 9.09 %.</p><p>(Section 5.1), it can accurately distinguish between sharp and blurry features. In addition, it compensates for the lack of color and cross-channel information in the hand-crafted features, which are important and valuable for our task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Our Defocus Feature</head><p>We concatenate all of the extracted features to construct our final defocus feature f B as follows:</p><formula xml:id="formula_5">f B = [f H , f C ] = [[f D , f G , f S ], f C ],<label>(5)</label></formula><p>where [·] denotes the concatenation. <ref type="table">Table 1</ref> shows the classification accuracy of each feature. We use a neural network classifier for the accuracy comparison. The classification tolerance is set to an absolute difference of 0.15 compared to the standard deviation value σ of the ground truth blur kernel. We train neural networks with the same architecture using those features individually and test on 576,000 features of 11 classes. The details of the classifier and the training process will be presented in Sections 4.1 and 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Defocus Map Estimation</head><p>The defocus features are classified using a neural network classifier to determine the amount of defocus at the center point of each patch. We obtain an initial sparse defocus map after the classification step and then filter out a number of outliers from the sparse defocus map using a sparse joint bilateral filter <ref type="bibr" target="#b42">[42]</ref> adjusted with the classification confidence values. A full defocus map is estimated from the filtered sparse defocus map using a matting Laplacian algorithm <ref type="bibr" target="#b18">[18]</ref>. The edge-preserving smoothing filtered <ref type="bibr" target="#b41">[41]</ref> color image is used as a guidance of propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Neural Network Classifier</head><p>We adopt a neural network classifier for classification because it can capture the highly non-linear relationship between our defocus feature components and the amount of defocus. Moreover, its outstanding performance has been demonstrated in various works <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33]</ref>. Our classifier network consists of three fully connected layers (300-150-11 neurons each) with ReLU and dropout layers. The softmax classifier is used for the last layer. Details about the classifier training process will be presented in Sections 5.1 and 5.2. Using this classification network, we obtain the labels and probabilities of features, after which the labels are converted to the corresponding σ values of the Gaussian kernel, which describe the amount of defocus. Subsequently, we construct the sparse defocus map I S using the σ values and the confidence map I C using the probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sparse Defocus Map</head><p>The sparse defocus map is filtered by the probabilityjoint bilateral filter to reject certain outliers and noise. In addition, we filter the input image with an edgepreserving smoothing filter to create a guidance image I G for probability-joint bilateral filtering and sparse defocus map propagation. A rolling guidance filter <ref type="bibr" target="#b41">[41]</ref> is chosen because it can effectively remove image noise together with the distracting small-scale image features and prevent erroneous guidances on some textured and noisy regions. Our probability-joint bilateral filter B(·) is defined as follows:</p><formula xml:id="formula_6">B(x) = 1 W (x) p∈Nx G σs (x, p) × G σr (I G (x), I G (p))G σc (1, I C (p))I S (p), (6) where G σ (u, v) = exp(− u − v 2</formula><p>F /2σ 2 ) and · F , W , N x , σ s , σ r and σ c denote the Frobenius norm, the normalization factor, the non-zero neighborhoods of x, the standard deviation of spatial, range and probability weight, respectively. The probability weight G σc is added to the conventional joint bilateral filter in order to reduce the unwanted effects of the outliers within neighborhood with low  probability values. Probability-joint bilateral filtering removes outliers and regularizes the sparse defocus map effectively as shown in <ref type="figure" target="#fig_9">Figure 6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Full Defocus Map</head><p>The full defocus map is obtained from the sparse defocus map using the matting Laplacian algorithm <ref type="bibr" target="#b18">[18]</ref> with the help of the guidance image I G . The matting Laplacian is defined as follows:</p><formula xml:id="formula_7">L(i, j) = k|(i,j)∈w k δ ij − 1 |w k | 1 + (I G (i) − µ k ) × (Σ k + |w k | I 3 ) −1 (I G (j) − µ k ) ,<label>(7)</label></formula><p>where i, j, k, w k , |w k |, δ ij , µ k , Σ k , I 3 and denote the linear indices of pixels, a small window centered at k, the size of w k , the Kronecker delta, the mean and variance of the w k , a 3×3 identity matrix and a regularization parameter, respectively. The full defocus map I F is obtained by solving the following least-squares problem:</p><formula xml:id="formula_8">I F = γ (L + D γ ) −1Î B ,<label>(8)</label></formula><p>whereÎ, γ and D γ denote the vector form of matrix I, a user parameter and a diagonal matrix whose element D(i, i) is γ when I B (i) is not zero, respectively. An example of a full defocus map is shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first describe our method to generate the training data and train the classifier, after which we compare the performance of our algorithm with the performances of state-ofthe-art algorithms using a blur detection dataset <ref type="bibr" target="#b31">[31]</ref>. Various applications such as blur magnification, all-in-focus image generation and 3-D estimation are also presented. Our codes and dataset are available on our project page. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Classifier Training</head><p>For the classification network training, 300 sharp images are randomly selected from the ILSVRC <ref type="bibr" target="#b29">[29]</ref> training data.  Similar to the feature extraction step, we extract approximately 1M multi-scale image patches on strong edges and regard these patches as sharp patches. After that, each sharp patch P S I is convolved with synthetic blur kernels to generate blurry patches P B I as follows:</p><formula xml:id="formula_9">P B l I = P S I * h σ l , l ∈ [1, L],<label>(9)</label></formula><p>where h σ , * and L denote the Gaussian blur kernel with a zero mean and variance σ 2 , the convolution operator and the number of labels, respectively. We set L = 11 and the σ values for each blur kernel are then calculated as follows:</p><formula xml:id="formula_10">σ l = σ min + (l − 1)σ inter ,<label>(10)</label></formula><p>where we set σ min = 0.5 and σ inter = 0.15.</p><p>For the training of the deep feature, f C , we directly connect the feature extraction network and the classifier network to train the deep feature and classifier simultaneously. The same method is applied when we use the concatenated feature, f H , for training. For the training of f B , we initially train the classifier connected to the feature extraction network only (i.e., with f C only), after which we fine-tune the classifier with the hand-crafted features, f H . We use the Caffe <ref type="bibr" target="#b12">[12]</ref> library for our network implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Scale Encoding</head><p>While we trained the classifier with features from small and large patches together, we found that the network does not converge. This occurs because some features from different scales with different labels can be similar. If we train classifiers for each scale individually, it is inefficient and risky, as the parameters to be trained are doubled and there is no assurance that individual networks are consistent.</p><p>In order to encode scale information to a feature itself, we assign specific positions for features from each scale, as shown in <ref type="figure" target="#fig_12">Figure 7</ref>. An entire feature consists of positions and constants for each scale. Additional constants are assigned to deal with biases for each scale.</p><p>In neural networks, the update rules for weights and the bias in a fully connected layer are as follows:</p><formula xml:id="formula_11">w t jk → w t jk − η(a t−1 k δ t j ),<label>(11)</label></formula><formula xml:id="formula_12">b t j → b t j − ηδ t j ,<label>(12)</label></formula><p>where w t jk , b t j , η, a and δ denote the k-th weight in the j-th neuron of the layer t, the bias of the j-th neuron of the layer t, the learning rate, the activation, and the error,  respectively. For the input layer (t = 1), if we set the kth dimension of the input feature to a constant C, Equation (11) becomes w 1 jk → w 1 jk − η(Cδ 1 j ), and surprisingly, if we let C = 1, the update rule takes the same form of the bias update rule. Therefore, by introducing additional constants into each scale, we are able not only effectively to separate the biases for each scale but also to apply different learning rates for each bias with different constants. After encoding scale information to a feature itself, the classifier network converges. In our experiments, we simply set C L = C S = 1.</p><p>Owing to this encoding scheme, we set the number of neurons in the first layer of the classifier network such that it is roughly two times greater than the dimensions of our descriptor to decode small-and large-scale information from encoded features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Blur Detection Dataset</head><p>We verify the reliability and robustness of our algorithm using a blur detection dataset <ref type="bibr" target="#b31">[31]</ref>. The dataset used contains 704 natural images containing blurry and sharp regions and corresponding binary blurry region masks manually segmented by a human. We extract 15 × 15 patches on strong edges and 27 × 27 patches on weak edges. We set n D = n G = n S = 25 for large patches, n D = n G = n S = 13 for small patches, σ s = σ r = 100.0, σ c = 1.0, = 1e −5 and γ = 0.005 for all experiments. We compare our algorithm to the results of Shi et al. <ref type="bibr" target="#b31">[31]</ref>, Shi et al. <ref type="bibr" target="#b32">[32]</ref> and Zhuo and Sim <ref type="bibr" target="#b42">[42]</ref>. Because the blur detection dataset contains only binary masks, quantitative results are obtained using binary blurry region masks from each algorithm. For binary segmentation, we apply a simple thresholding method to the full defocus map. The threshold value τ is determined as follows:</p><formula xml:id="formula_13">τ = αv max + (1 − α)v min ,<label>(13)</label></formula><p>where v max and v min denote the maximum and the minimum values of the full defocus map, and α is a user parameter. We set α = 0.3 for the experiments empirically and (a)   this value works reasonably well. <ref type="figure" target="#fig_14">Figure 8</ref> shows the segmentation accuracies and the precision-recall curves, and <ref type="figure" target="#fig_15">Figure 9</ref> shows the results of the different algorithms. The segmentation accuracies are obtained from the ratio of the number of pixels correctly classified to the total number of pixels. Precision-Recall curves can be calculated by adjusting τ from σ 1 to σ L .</p><formula xml:id="formula_14">(b) (c) (d) (e) (f) (g) (h)</formula><p>Our algorithm shows better results than the state-of-theart methods quantitatively and qualitatively. In the homogeneous regions of an image, sufficient textures for defocus estimation do not exist. The results of <ref type="bibr" target="#b31">[31]</ref> and <ref type="bibr" target="#b32">[32]</ref>, therefore, show some erroneous classification results in such regions. Our algorithm and <ref type="bibr" target="#b42">[42]</ref> can avoid this problem with the help of sparse patch extraction on strong edges. In contrast, sparse map propagation can also lead to uncertain results in textured regions because the prior used for the propagation is based on the color line model <ref type="bibr" target="#b18">[18]</ref>, as shown in <ref type="figure" target="#fig_15">Figure 9</ref> (e). An edge-preserving smoothed color image is adopted as a propagation prior in our algorithm, and it gives better results, as shown in <ref type="figure" target="#fig_15">Figures 9 (f) and (g)</ref>. In addition, we compare our algorithm with that of Shi et al. <ref type="bibr" target="#b30">[30]</ref>. We conducted a RelOrder <ref type="bibr" target="#b30">[30]</ref> evaluation and obtained a result of 0.1572 on their dataset, which is much better than their result of 0.1393 2 of <ref type="bibr" target="#b30">[30]</ref>. Moreover, the segmentation accuracy of <ref type="bibr" target="#b30">[30]</ref> (53.30%) is much lower than the accuracy of our algorithm because their method easily fails with a large amount of blur <ref type="figure" target="#fig_15">(Figure 9 (b)</ref>).</p><p>We also examined defocus maps from single and concatenated features qualitatively. As shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Applications</head><p>The estimated defocus maps can be used for various applications. We apply our defocus maps to the following applications and the results are quite pleasing.</p><p>Defocus Blur Magnification Our algorithm can be used for defocus blur magnification tasks. We can highlight the foreground by amplifying the blurriness of the background. <ref type="figure" target="#fig_0">Figure 11</ref> shows an example of the defocus blur magnification. The foreground objects appear more prominent in the blur magnified image.</p><p>All-in-focus Image Generation Contrary to defocus blur magnification, we can deblur blurry regions in an image to obtain an all-in-focus image. Using the σ values of each pixel in the defocus map, we generate corresponding Gaussian blur kernels and use them to deblur the image. We use the hyper-laplacian prior <ref type="bibr" target="#b15">[15]</ref> for non-blind deconvolution. <ref type="figure" target="#fig_0">Figure 12</ref> shows an example of the all-in-focus image generation. In accordance with the defocus map, we deblur the original image in a pixel-by-pixel manner. The blurry regions in the original image are restored considerably in the all-in-focus image <ref type="figure" target="#fig_0">(Figure 12 (b)</ref>).</p><p>3-D Estimation from a Single Image The amount of defocus is closely related to the depth of the corresponding point because the scaled defocus values can be regarded as pseudo-depth values if all of the objects are located on the same side of the focal plane. Because we need both defocused images and depth maps, we utilize light-field images, as there are numerous depth estimation algorithms and because digital refocusing can easily be done. We decode light-field images using <ref type="bibr" target="#b5">[6]</ref>, and then generate a refocused image using <ref type="bibr" target="#b36">[36]</ref>. Our algorithm is compared to algorithms for light-field depth estimation <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b36">36]</ref> and an algorithm for single image depth estimation <ref type="bibr" target="#b21">[21]</ref>. <ref type="figure" target="#fig_0">Figure 13</ref> shows an input image and depth map from each algorithm. Our depth map from a single image appears reasonable compared to those in the other works, which utilize correspondences between multiple images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced a unified approach to combine handcrafted and deep features and demonstrated their complementary effects for defocus estimation. A neural network classifier is shown to be able to capture highly non-linear relationships between each feature, resulting in high discriminative power. In order to reduce the patch scale dependency, multi-scale patches are extracted depending on the strength of the edges. The homogeneous regions in an image are well handled by a sparse defocus map, and the propagation process is guided by an edge-preserving smoothed image. The performance of our algorithm is compared to those of the state-of-the-art algorithms. In addition, the potential for use in various applications is demonstrated.</p><p>One limitation of our algorithm is that we occasionally obtain incorrect defocus values in a large homogeneous area. This is due to the fact that there is no strong edge within such regions for defocus estimation. A simple remedy to address this problem involves the random addition of classification seed points in large homogeneous regions. <ref type="figure" target="#fig_0">Figure 14</ref> shows the effect of additional seed points. Additional random seed points effectively guide the sparse defocus map, causing it to be propagated correctly into large homogeneous areas.</p><p>For future works, we expect to develop fully convolutional network architectures for our task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our defocus estimation result and a digital refocusing application example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>From strong edges (c) From weak edges Multi-scale patches from strong and weak edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(b)  shows that small patches from strong Sharp and blurry hand-crafted features. Average (a) DCT, DFT, DST, (b) gradient and (c) SVD features from sharp (dotted) and blurry (solid) patches. The number of samples exceeds more than 11K. The absolute difference between sharp and blurry features can be a measure of discriminative power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) shows fea-(a) Original (b) n = 60 (c) n = 30 (d) n = 15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Low-rank matrix approximation of an image. (a) Original and (b)-(d) approximated images with the number of preserved singular values. The more number of singular values are preserved, the more details are also preserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Our deep feature extraction network and average activations with sharp, intermediate and blurry patches. The output dimensions of each layer are shown together. The stride is set to 1 for convolution and to 3 for max pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Feature</head><label></label><figDesc>Successive convolutional, ReLU and max pooling layers are suitable to obtain highly non-linear features. Because the deep feature is learnt from a massive amount of training data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig- ure 5</head><label>5</label><figDesc>also shows the average outputs from our feature extraction network with sharp, intermediate and blurry image patches. The activations are proportional to the sharpness of the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Sparse defocus maps before (I S ) and after (I B ) probability-joint bilateral filtering. Solid boxes denote sharp areas (small value) and dashed boxes denote blurry areas (large value). Some outliers are filtered out effectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Our feature scale encoding scheme. If a feature is from large-scale, we fill the small-scale positions with zeros, and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Segmentation accuracies (top) and Precision-Recall comparison (bottom) of Shi et al. [31], Zhuo and Sim [42] and our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Defocus map estimation and binary blurry region segmentation results. (a) Input images. (b) Results of [30]. (c) Results of [31]. (d) Results of [32] (Inverted for visualization). (e) Results of [42]. (f) Our defocus maps and (g) corresponding binary masks. (h) Ground truth binary masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :</head><label>10</label><figDesc>Defocus maps from each feature. Features used for the estimation of each defocus map are annotated. The blue solid boxes and red dashed boxes in (e), (f) and (g) show the complementary roles of the hand-crafted and deep features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Figures 10 (b), (c) and (d), single hand-crafted features give unsatisfactory results compared to deep or concatenated features. Surprisingly, a deep feature alone (Figure 10 (e)) works quite well but gives a slightly moderate result compared to the concatenated features (See the solid blue boxes). The hand-crafted feature alone (Figure 10 (f)) also works nicely but there are several misclassifications (See the dashed red boxes). These features show complementary roles when they are concatenated. Certain misclassifications due to the hand-crafted feature are well handled by the deep feature, and the discriminative power of the deep feature was strengthened with the aid of the hand-crafted features, as shown in Figure 10 (g).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 11 :Figure 12 :Figure 13 :</head><label>111213</label><figDesc>Defocus blur magnification. The background blurriness is amplified to direct more attention to the foreground. All-in-focus image generation. Blurry regions (yellow boxes) in the original image look clearer in the allin-focus image. Depth estimation. (a) Input image. (b) Tao et al. [36]. (c) Jeon et al. [11]. (d) Liu et al. [21] (e) Ours. Our depth map looks reasonable compared to those in the other works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Effects of additional random seed points. (a) Input image. (b) I S . (c) I S + Seeds. (d) I F from (b). (e) I F from (c). Additional random seed points can effectively enhance the defocus accuracy in large homogeneous regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>of an image. The factorization of an m × n real matrix A can be</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Scaled for visualization</cell></row><row><cell>3x3 Conv.</cell><cell cols="2">25 3x3 Max Pool. 3x3 Conv. 25 ReLU +</cell><cell cols="2">50 ReLU + 3x3 Max Pool. 3x3 Conv. 50</cell><cell>50 ReLU + 1x1 Conv.</cell><cell>ReLU</cell><cell>25</cell><cell>Features ( )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Deep</cell></row><row><cell>Input RGB Patch (27 x 27 x 3)</cell><cell>Conv1 (25 x 25 x 25)</cell><cell>Pool1 (9 x 9 x 25)</cell><cell>Conv2 (7 x 7 x 50)</cell><cell>Pool2 (3 x 3 x 50)</cell><cell>Conv3 (1 x 1 x 50)</cell><cell cols="2">Conv4 (1 x 1 x 25)</cell><cell>Extracted</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. Our deep feature, f C , has the most discriminative power with regard to blurry and sharp features as compared to other individual hand-crafted features, {f D , f G , f S }, and their concatenation, f H . When all hand-crafted and deep features are concatenated (f B ), the performance is even more enhanced. Removing one of the hand-crafted features drops the performance by approximately 1-3%. For example, the classifica-</figDesc><table /><note>tion accuracies of [f D , f S , f C ] and [f G , f S , f C ] are 93.25% and 91.10%, respectively. In addition, the performance of f B with only single-scale patch extraction also decreases to 91.00%.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/zzangjinsun/DHDE_CVPR17</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Different from the reported value in<ref type="bibr" target="#b30">[30]</ref> because we utilize our own implementation. The evaluation codes of<ref type="bibr" target="#b30">[30]</ref> have not been released.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blur identification by multilayer neural network based on multivalued neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Aizenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Paliy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Astola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="883" to="898" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Defocus magnification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="571" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autofocusing and astigmatism correction in the scanning electron microscope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Batten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Digital multi-focusing from a single photograph taken with an uncalibrated conventional camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3703" to="3714" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling the calibration pipeline of the lytro camera for high quality lightfield image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local scale control for edge detection and blur estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="699" to="716" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate depth map estimation from a lenslet light field camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salient region detection by ufo: Uniqueness, focusness and objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1976" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyper-laplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1033" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Absolute depth estimation from a single defocused image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4545" to="4550" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature detection with automatic scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image partial blur detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The scale of edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="462" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blur determination in the compressed domain using dct information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Marichal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="386" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Defocus video matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="567" to="576" />
			<date type="published" when="2002" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Numerical recipes 3rd edition: The art of scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Discrete cosine transform: algorithms, advantages, applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Break ames room illusion: depth from general single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative blur detection features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Just noticeable defocus blur detection and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Single image defocus map estimation using local contrast prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1797" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth from combining defocus and correspondence using lightfield cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image sharpness measure using eigenvalues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paramesran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal Processing (ICSP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="840" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep edge-aware filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1669" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image blur classification and parameter identification using two-stage deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC). Citeseer</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rolling guidance filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="815" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Defocus map estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Depth from defocus estimation in spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deschênes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="165" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
