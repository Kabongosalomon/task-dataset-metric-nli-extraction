<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Science Department</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sydney 3 Research School of Computer Science</orgName>
								<orgName type="laboratory">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Postdoc Center of Information and Communication Engineering</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Science Department</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sydney 3 Research School of Computer Science</orgName>
								<orgName type="laboratory">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://github.com/zhunzhong07/ECN</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers the domain adaptive person reidentification (re-ID) problem: learning a re-ID model from a labeled source domain and an unlabeled target domain. Conventional methods are mainly to reduce feature distribution gap between the source and target domains. However, these studies largely neglect the intra-domain variations in the target domain, which contain critical factors influencing the testing performance on the target domain. In this work, we comprehensively investigate into the intra-domain variations of the target domain and propose to generalize the re-ID model w.r.t three types of the underlying invariance, i.e., exemplar-invariance, camerainvariance and neighborhood-invariance. To achieve this goal, an exemplar memory is introduced to store features of the target domain and accommodate the three invariance properties. The memory allows us to enforce the invariance constraints over global training batch without significantly increasing computation cost. Experiment demonstrates that the three invariance properties and the proposed memory are indispensable towards an effective domain adaptation system. Results on three re-ID domains show that our domain adaptation accuracy outperforms the state of the art by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (re-ID) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14]</ref> is a cross-camera image retrieval task, which aims to find matched persons of a given query from the database. In spite of the impressive achievement of supervised learning in the re-ID community, learning a re-ID model that generalizes well on a target domain remains a challenge <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>. Obtaining sufficient unlabeled data in the target domain is relatively easy, and yet it is difficult to learn a deep re-ID model without annotations. This work considers the problem of unsupervised domain adaptation (UDA), where we are provided with labeled source domain and unlabeled target domain. Our goal is to learn a discriminative representation for the target set.</p><p>In the traditional setting of UDA, most methods are developed under the closed-set scenario, assuming that the source and target domains share entirely the same classes <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10]</ref>. However, this assumption cannot be applied to UDA in person re-ID, because the classes from the two domains are completely different. UDA in person re-ID is an open set problem <ref type="bibr" target="#b2">[3]</ref> which is more challenging than closedset one. During UDA in person re-ID, it is improper to directly align the distributions of the source and target domains as in existing closed-set UDA methods. Instead, we should learn to well separate the unseen classes from the target domain.</p><p>Recent advanced methods address the UDA problem in person re-ID mostly by reducing the gap between the source and target domains on the image-level <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1]</ref> or the attribute feature-level <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16]</ref>. These methods only consider the overall inter-domain variations between the source and target domains, but ignore the intra-domain variations of the target domain. In fact, the target variations are critically influencing factors for person re-ID. In this study, we explicitly take into account the intra-domain variations of target domain and investigate three underlying invariances, i.e., exemplar-invariance, camera-invariance, and neighborhood-invariance, as described below.</p><p>First, given a deep re-ID model trained on a labeled set, we observe that the top-ranked retrieval results always are more likely to be visually correlated to the query. A similar phenomenon is observed in image classification <ref type="bibr" target="#b32">[33]</ref>. This  indicates that the deep re-ID model has learned the apparent similarity instead of semantic information from visual data. In reality, each person exemplar could differ significantly from other exemplars even belonged to the same identity. Thus, it is possible to enable the re-ID model to capture the apparent representation of person by learning to discriminate individual exemplars. Based on this, we introduce the exemplar-invariance to learn apparent similarity on unlabeled target data by enforcing each person exemplar to be close to itself and far away from others. Second, as a key influencing factor in person re-ID, camera-style variations <ref type="bibr" target="#b43">[44]</ref> might significantly change the appearance of person. Nevertheless, a person image generated by camerastyle transfer still belongs to the original identity. Taking this into account, we enforce the camera-invariance <ref type="bibr" target="#b42">[43]</ref> under the assumption that a person image and the corresponding camera-style transferred images should be close to each other. Third, suppose we are provided an appropriate re-ID model trained on the source and target domains. A target exemplar and its nearest-neighbors in the target set may probably have the same identity. Considering this trait, we present the neighborhood-invariance by encouraging an exemplar and its corresponding reliable neighbors to be close to each other. This helps us to learn a model that is more robust to overcome the image variations of the target domain, such as pose, view and background changes. Examples of these three invariances are shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>Based on the above aspects, we propose a novel unsupervised domain adaptation method for person re-ID. During the training process, an exemplar memory is introduced into the network to memorize the up-to-date representation of each exemplar of the target set. The memory enables us to enforce the invariance constraints over whole/global target training batch instead of the mini-batch. This helps us to effectively perform the invariance learning of the target domain during the network optimizing procedure.</p><p>In summary, the contribution of this work is three-fold:</p><p>• We comprehensively study three underlying invariances of the target domain. Experiments show that these properties are indispensable for improving the transferable ability of re-ID models.</p><p>• We propose a memory module to effectively enforce the three invariance properties into the system. The memory helps us to take advantage of sample similarity over the global training set. With the memory, accuracy can be significantly improved, requiring very limited extra computation cost and GPU memory.</p><p>• Our method outperforms the state-of-the-art UDA approaches by a large margin on three largescale datasets: Market-1501, DukeMTMC-reID and MSMT17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised domain adaptation. An effective approach for addressing UDA is by aligning the feature distributions between the two domains. This alignment can be achieved by reducing the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b10">[11]</ref> between domains <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>, or training an adversarial domain-classifier <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> to encourage the features of the source and target domains to be indistinguishable. The above mentioned methods are designed under the assumption of the closed-set scenario, where the classes of the source and target domains are entirely identical. However, in practice, there are many scenarios that exist unknown classes in the target domain. The unknown-class samples from the target domain should not be aligned with the source domain. This task is introduced by Busto and Grall <ref type="bibr" target="#b2">[3]</ref>, referred as open set domain adaptation. To tackle this problem, Busto and Grall <ref type="bibr" target="#b2">[3]</ref> develop a method to learn a mapping from the source domain to the target domain by discarding unknown-class target samples. Recently, an adversarial learning framework <ref type="bibr" target="#b21">[22]</ref> is proposed to separate target samples into known and unknown classes, and reject unknown classes during feature alignment. In this paper, we study the problem of UDA in person re-ID, where the classes are totally different between the source and target domains. This is a more challenging open set problem.</p><p>Unsupervised person re-identification. The art supervised methods have made great achievement in person re-ID <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25]</ref>, relying on rich-labeled data and the success of deep networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. However, the performance  <ref type="figure">Figure 2</ref>. The framework of the proposed approach. During training, labeled source data and unlabeled target data are fed-forward into the deep re-ID network to obtain up-to-date representations. Subsequently, two components are designed to optimize the network with source data and target data, respectively. The first component is a classification module that calculates the cross-entropy loss for labeled source data. The second component is an exemplar memory module that saves the up-to-date features for target data and computes the invariance learning loss for unlabeled target data. may drop significantly when tested on an unseen dataset. To address this problem, several methods use the labeled source domain to learn a deep re-ID model as an initialized feature extractor. Then, these methods learn a metric <ref type="bibr" target="#b35">[36]</ref> or refine the re-ID model by unsupervised clustering <ref type="bibr" target="#b8">[9]</ref> on the target domain. However, these methods do not take advantage of the labeled source data as a beneficial supervision during adapting procedure. To overcome previous drawbacks, many domain adaptation approaches are developed to adapt the model with both labeled source domain and unlabeled target domain. These methods are mainly to reduce the domain shifts between datasets on image-level <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1]</ref> and attribute feature-level <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16]</ref>. Despite their effectiveness, these methods largely ignore the intra-domain variations in target domain. Recently, Zhong et al. <ref type="bibr" target="#b42">[43]</ref> first propose a HHL method to learn camera-invariant network for the target domain. However, HHL overlooks the latent positive pairs in the target domain. This might lead the re-ID model to be sensitive to other variations in the target domain, such as pose and background variations.</p><p>Difference from previous works. Indeed, the three invariance properties and the memory module have been separately presented in existing works. However, our work is different from them. The exemplar-invariance and memory module have been presented in self-supervised learning <ref type="bibr" target="#b32">[33]</ref>, few-shot learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> and supervised learning <ref type="bibr" target="#b33">[34]</ref>. Yet, we explore the feasibility of this idea in unsupervised domain adaptation and overcoming the variations in the target domain. The neighborhood-invariance is similar to deep association learning (DAL) <ref type="bibr" target="#b3">[4]</ref>. A difference from DAL is that we design a soft classification loss to align the top-k neighbors instead of calculating the triplet loss between the mutual top-1 neighbors. Importantly, comparing with HHL <ref type="bibr" target="#b42">[43]</ref> and DAL <ref type="bibr" target="#b3">[4]</ref>, we comprehensively consider three invariance constraints. It is worthy of discovering the mutual benefit among the three invariance properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>Preliminary. In the context of unsupervised domain adaptation (UDA) in person re-ID, we are provided with a fully labeled source domain {X s , Y s }, including N s person images. Each person image x s,i is associated with an identity y s,i . The number of identities is M for source domain. In addition, we are provided with an unlabeled target domain X t , containing N t person images. The identity annotation of the target domain is not available. Our goal is to learn a transferable deep re-ID model using both labeled source domain and unlabeled target domain, which generalizes well on the target testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of Framework</head><p>The framework of our method is shown in <ref type="figure">Fig. 2</ref>. In our model, the ResNet-50 <ref type="bibr" target="#b11">[12]</ref> pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref> is utilized as the backbone. Specifically, we keep the layers of ResNet-50 till the Pooling-5 layer as the base network and add a 4096-dimensional fully convolutional (FC) layer after Pooling-5 layer. The new FC layer is named FC-4096, followed by batch normalization <ref type="bibr" target="#b12">[13]</ref>, ReLU <ref type="bibr" target="#b18">[19]</ref>, Dropout <ref type="bibr" target="#b23">[24]</ref> and two components. The first component is a classification module for supervised learning with the labeled source data. It has an M -dimensional FC layer (named as FC-#id) and a softmax activation function. We use the cross-entropy loss to calculate the loss for the source domain. The other component is an exemplar memory module for invariance learning with the unlabeled target data. The exemplar memory is served as a feature-storage that saves the up-to-date output of FC-4096 layer for each target image. We calculate the invariance learning loss of the target domain by estimating the similarities between the target samples within mini-batch and whole target samples saved in the exemplar memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised Learning for Source Domain</head><p>Due to the identities of source images are available, we treat the training process of the source domain as a classification problem <ref type="bibr" target="#b37">[38]</ref>. The cross-entropy loss is used to optimize the network, formulated as,</p><formula xml:id="formula_0">L src = − 1 ns ns i=1 log p(y s,i |x s,i ),<label>(1)</label></formula><p>where n s is the number of source images in a training batch. p(y s,i |x s,i ) is the predicted probability that the source image x s,i belongs to identity y s,i , which is obtained by the classification module. The model trained using labeled source data produces a high accuracy on the same distributed testing set. However, the performance will deteriorate seriously when the testing set has a different distribution to the source domain. Next, we will introduce an exemplar memory based method to overcome this problem by considering the intra-domain variations of target domain in the training of network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Exemplar Memory</head><p>In order to improve the generalization ability of the network on the target testing set, we propose to enforce invariance learning into the network by estimating the similarities between target images. To achieve this goal, we first construct an exemplar memory for storing the up-todate features of all target images. The exemplar memory is a key-value structure <ref type="bibr" target="#b33">[34]</ref>, which has the key memory (K) and the value memory (V). In the exemplar memory, each slot stores the L2-normalized feature of FC-4096 in the key part, while storing the label in the value part. Given an unlabeled target data including N t images, we regard each image instance as an individual category. Thus, the exemplar memory contains N t slots, in which each slot storing the feature and label of a target image. In the initialization, we initialize the values of all the features in the key memory to zeros. For simplicity, we assign the corresponding indexes as the labels of target samples and store them in the value memory. For example, the class of i-th target image in value memory is assigned to V[i] = i. The labels in the value memory are fixed throughout training process. During each training iteration, for a target training sample x t,i , we forward it through the deep reID network and obtain the L2-normalized feature of FC-4096, f (x t,i ). During the back-propagation, we update the feature in the key memory for the training sample x t,i through,</p><formula xml:id="formula_1">K[i] ← αK[i] + (1 − α)f (x t,i ),<label>(2)</label></formula><p>where K[i] is the key memory of image x t,i in i-th slot. The hyper-parameter α ∈ [0, 1] controls the updating rate.</p><formula xml:id="formula_2">K[i] is then L2-normalized via K[i] ← K[i] 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Invariance Learning for Target Domain</head><p>The deep re-ID model trained with only source domain is usually sensitive to the intra-domain variations of the target domain. The variations are critical influencing factors for the performance. Therefore, it is necessary to consider the image variations of the target domain during transferring the knowledge from source domain to target domain. In this study, we investigate three underlying invariances of target data for UDA in person re-ID, i.e., exemplar-invariance, camera-invariance and neighborhood-invariance.</p><p>Exemplar-invariance. The appearance of each person image may be very different from others even shared the same identity. In other words, each person image can be close to itself while far away from others. Therefore, we enforce exemplar-invariance into the re-ID model by learning to distinguish individual person images. This allows the re-ID model to capture the apparent representation of person. To achieve this goal, we regard the N t target images as N t different classes, and classify each image into its own class. Given a target image x t,i , we first compute the cosine similarities between the feature of x t,i and features saved in the key memory. Then, the predicted probability that x t,i belongs to class i is calculated using softmax function,</p><formula xml:id="formula_3">p(i|x t,i ) = exp(K[i] T f (xt,i)/β) N t j=1 exp(K[j] T f (xt,i)/β) ,<label>(3)</label></formula><p>where β ∈ (0, 1] is temperature fact that balances the scale of distribution. The objective of exemplar-invariance is to minimize the negative log-likelihood over target training image, as</p><formula xml:id="formula_4">L ei = − log p(i|x t,i ).<label>(4)</label></formula><p>Camera-invariance. Camera style variation is an important factor in person re-ID. A person image may encounter with significant changes in appearance under different cameras. The re-ID model trained using labeled source data can capture the camera-invariance for source domain, but may suffer from the image variations caused by target cameras. Since the camera settings of the two domains will be very different. To overcome this problem, we propose to equip the network with camera-invariance <ref type="bibr" target="#b42">[43]</ref> of target domain, based on the assumption that an image and its camera-style transferred counterparts should be close to each other. In this paper, we suppose the camera-ID of each image is known, since the camera-ID can be easily obtained when collecting person images from video sequences. Given the unlabeled target data, we consider each camera as a style domain and adopt StarGAN <ref type="bibr" target="#b4">[5]</ref> to train a camera style (CamStyle) transfer model <ref type="bibr" target="#b43">[44]</ref> for the target domain. With the learned CamStyle transfer model, each real target image collected from camera c is augmented with C − 1 images in the styles of other cameras while remain-ing the original identity. C is the number of cameras in the target domain.</p><p>To introduce the camera-invariance into the model, we regard that each real image and its style-transferred counterparts share the same identity. Thus, the loss function of camera-invariance is explained as,</p><formula xml:id="formula_5">L ci = − log p(i|x t,i ),<label>(5)</label></formula><p>wherex t,i is a target sample randomly selected from the style-transferred images of x t,i . In this way, images in different camera styles of the same sample are forced to be close to each other. Neighborhood-invariance. For each target image, there may exist a number of positive samples in the target data. If we could exploit these positive samples in the training process, we are able to further improve the robustness of re-ID model in overcoming the variations of target domain. To achieve this objective, we first calculate the cosine similarities between f (x t,i ) and the features stored in the key memory K. Then, we find the k-nearest neighbors of x t,i in K and define the indexes of them as M(x t,i , k). k is the size of M(x t,i , k). The nearest one in M(x t,i , k) is i.</p><p>We endow the neighborhood-invariance into the network under the assumption that the target image x t,i should belong to the classes of candidates in M(x t,i , k). Thus, we assign the weight of the probability that x t,i belongs to the class j as,</p><formula xml:id="formula_6">w i,j = 1 k , j = i 1, j = i , ∀j ∈ M(x t,i , k).<label>(6)</label></formula><p>The objective of neighborhood-invariance is formulated as a soft-label loss,</p><formula xml:id="formula_7">L ni = − j =i w i,j log p(j|x t,i ), ∀j ∈ M(x t,i , k).<label>(7)</label></formula><p>Note that, to distinguish between exemplar-invariance and neighborhood-invariance, x t,i is not classified to its own class in Eq. 7.</p><p>Overall loss of invariance learning. By jointly considering the exemplar-invariance, camera-invariance and neighborhood-invariance, the overall loss of invariance learning over target training images can be written as,</p><formula xml:id="formula_8">L tgt = − 1 nt nt i=1 j w i,j log p(j|x * t,i ),<label>(8)</label></formula><p>where j ∈ M(x * t,i , k). x * t,i is an image randomly sampled from the union set of x t,i and its camera styletransferred images. n t is the number of target images in the training batch. In Eq. 8, when i = j, we optimize the network with the exemplar-invariance learning and camera-invariance learning by classifying x * t,i into its own class. When i = j, the network is optimized with the neighborhood-invariance learning by leading x * t,i to be close to its neighbors in M(x * t,i , k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Final Loss for Network</head><p>By combining the losses of source and target domains, the final loss for the network is formulated as,</p><formula xml:id="formula_9">L = (1 − λ)L src + λL tgt ,<label>(9)</label></formula><p>where λ ∈ [0, 1] controls the importance of the source loss and the target loss. To this end, we introduce a loss function for UDA person re-ID, in which, the loss of source domain aims to maintain a basic representation for person. As well as, the loss of target domain attempts to take the knowledge from labeled source domain and incorporate the invariance properties of target domain into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Discussion on the Three Invariance Properties</head><p>We analyze the advantage and disadvantage for each invariance. The exemplar-invariance enforces each exemplar away from each other. It is beneficial to enlarge the distance between exemplars from different identities. However, exemplars of the same identity will also be far apart, which is harmful to the system. On the contrast, neighborhoodinvariance encourages each exemplar and its neighbors to be close to each other. It is beneficial to reduce the distance between exemplars of the same identity. However, neighborhood-invariance might also pull closer images of different identities, because we could not guarantee that each neighbor shares the same identity with the query exemplar. Therefore, there exists a trade off between exemplarinvariance and neighborhood-invariance, where the former aims to lead the exemplars from different identities to be far away while the latter attempts to encourage exemplars of the same identity to be close to each other. Camera-invariance has the similar effect as the exemplar-invariance and also leads the exemplar and its camera-style transferred samples to share the same representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We evaluate the proposed method on three large-scale person re-identification (re-ID) benchmarks: Market-1501 <ref type="bibr" target="#b36">[37]</ref>, DukeMTMC-reID <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref> and MSMT17 <ref type="bibr" target="#b29">[30]</ref>. Performance is evaluated by the cumulative matching characteristic (CMC) and mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Setting</head><p>Deep re-ID model. We adopt ResNet-50 <ref type="bibr" target="#b11">[12]</ref> as the backbone of our model and initialize the model with the parameters pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref>. We fix the first two residual layers to save GPU memory. The input image is resized to 256 ×128. During training, we perform random flipping, random cropping and random erasing <ref type="bibr" target="#b41">[42]</ref> for data augmentation. The probability of dropout is set to 0.5. We train the model with a learning rate of 0.01 for ResNet-50 base layers and of 0.1 for the others in the first 40 epochs. The learning rate is divided by 10 for the next 20 epochs. The SGD optimizer is used to train the model. We set the mini-batch size to 128 for both source images and target images. We initialize the updating rate of key memory α to 0.01 and increase α linearly with the number of epochs, i.e., α = 0.01 × epoch. Without specification, we set the temperature fact β = 0.05, number of candidate positive samples k = 6 and weight of losses λ = 0.3. We train the model with exemplar-invariance and camera-invariance learning at the first 5 epochs and add the neighborhoodinvariance learning for the rest epochs. In testing, we extract the L2-normalized output of Pooling-5 layer as the image feature and adopt the Euclidean distance to measure the similarities between query and gallery images. Baseline. We set the model as the baseline when trained the network using only the classification component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter Analysis</head><p>We first analyze the sensitivities of our approach to three important hyper-parameters, i.e., the temperature fact β, the weight of losses λ, and the number of candidate positive samples k. By default, we vary the value of one parameter and keep the others fixed.</p><p>Temperature fact β. In <ref type="table" target="#tab_0">Table 1</ref>, we investigate the effect of the temperature fact β in Eq. 3. Using a lower value for β leads to a lower entropy, which commonly achieves better results. However, the network does not converge if the temperature fact is too low, e.g., β = 0.01. The best results are produced when β is around 0.05.</p><p>The weight of source and target losses λ. In <ref type="figure" target="#fig_3">Fig. 3</ref> we compare different values of λ in Eq. 9. When λ = 0, our method reduces to the baseline that trained the model only with labeled source data. It is clearly shown that, when considering invariance learning for target domain (λ &gt; 0), our approach significantly improves the baseline at all values. It is worth noting that our approach outperforms the baseline by a large margin even trained the model using only unlabeled target data (λ = 1). This demonstrates the effectiveness of our approach and the importance of overcoming the variations in target domain. When λ is between 0.3 to   k <ref type="figure">Figure 4</ref>. Evaluation with different number of candidate positive samples in neighborhood-invariance learning. 0.8, our result is impacted just marginally and the best results are obtained. This shows that our method is insensitive to λ in an appropriate range. Number of positive samples k. In <ref type="figure">Fig. 4</ref>, we show the results of using different number of positive samples in neighborhood-invariance learning. When k = 1, our approach reduces to the model trained with exemplarinvariance and camera-invariance learning. When adding neighborhood-invariance learning into the system (k &gt; 1), our results achieve consistent improvement. The rank-1 accuracy and mAP first improve with the increase of k and achieve best results when k is between 6 to 8. Assigning a too large value to k reduces the results. This is because an excess of false positive samples may include during neighborhood-invariance learning, which could have deleterious effects on performance.</p><p>According to the analysis above, we set β = 0.05, λ = 0.3 and k = 6 in the following experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation</head><p>Performance of baseline. Ablation experiment on invariance learning. To investigate the effectiveness of the proposed invariance learning for target domain, we conduct ablation studies in <ref type="table">Table 2</ref>. First, we show the effect of exemplar-invariance learning by adding exemplar-invariance learning into the baseline. As shown in <ref type="table">Table 2</ref>, "Ours w/ E" consistently improves the results over baseline (Source Only). Specifically, the rank-1 accuracy improves from 43.1% to 48.7% and 28.9% to 34.2% when tested on Market-1501 and DukeMTMC-reID, respectively. This demonstrates that exemplar-invariance learning is an effective way to improve the discrimination of person descriptors for the target domain.</p><p>Next, we validate the effectiveness of camera-invariance learning over the model trained with exemplar-invariance learning (Ours w/ E). In <ref type="table">Table 2</ref>, we observe significant improvement when adding camera-invariance learning into the system. For example, "Ours w/ E+C" achieves a rank-1 accuracy of 63.1% when regarding DukeMTMC-reID as source domain and tested on Market-1501. This is higher than "Ours w/ E" by 14.4% in rank-1 accuracy. The improvement demonstrates that the image variations caused by target cameras severely impact the performance in testing set. Injecting camera-invariance learning into the model could effectively improve the robustness of the system to camera style variations.</p><p>We also evaluate the effect of neighborhood-invariance learning. As reported in <ref type="table">Table 2</ref>, "Ours w/ E+N" consistently improves the results of "Ours w/ E". Using exemplarinvariance and neighborhood-invariance during training, the model (Ours w/ E+N) has 39.7% rank-1 accuracy and 23.6% mAP when using Market-1501 as source domain and tested on DukeMTMC-reID. This increases the results of "Ours w/ E" by 5.5% in rank-1 accuracy and by 4.9% in mAP, respectively. Furthermore, when integrating neighborhood-invariance learning into a better model (Ours w/ E+C), our approach would gain more improvement in performance. For example, "Ours w/ E+C+N" achieves rank-1 accuracy of 75.1% when regarding DukeMTMC-reID as source domain and tested on Market-1501, improving the rank-1 accuracy of "Ours w/ E+C" by 12%. Similar improvement is observed when tested on DukeMTMC-reID. This is because that more reliable positive samples would be mined from unlabeled target set by integrating neighborhood-invariance learning into a more discriminative model. The benefit of the exemplar memory. We use the proposed exemplar memory and the mini-batch to implement the proposed invariance learning, respectively. For minibatch based method, input samples are composed of the target samples, corresponding CamStyle samples and corresponding k-nearest neighbors. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the exemplar memory based method clearly outperforms the mini-batch based method. It is noteworthy that using the exemplar memory introduces limited additional training time (≈ + 1.3 mins) and GPU memory (≈ + 260 MB) compared to using the mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-art Methods</head><p>We compare our approach with state-of-the-art unsupervised learning methods when tested on Market-1501, DukeMTMC-reID and MSMT17. <ref type="table" target="#tab_4">Table 4</ref> reports the comparisons when tested on Market-1501 and DukeMTMC-reID. We use DukeMTMC-reID as the source set when tested on Market-1501 and vice versa. We compare with two hand-crafted feature based methods without transfer learning: LOMO <ref type="bibr" target="#b14">[15]</ref> and BOW <ref type="bibr" target="#b36">[37]</ref>, three unsupervised methods that use a labeled source data to initialize the model but ignore the labeled source data during learning feature for target domain: CAMEL <ref type="bibr" target="#b35">[36]</ref>, UMDL <ref type="bibr" target="#b19">[20]</ref> and PUL <ref type="bibr" target="#b8">[9]</ref>, and six unsupervised domain adaptation approaches: PTGAN <ref type="bibr" target="#b29">[30]</ref>, SPGAN <ref type="bibr" target="#b6">[7]</ref>, MMFA <ref type="bibr" target="#b15">[16]</ref>, TJ-AIDL <ref type="bibr" target="#b28">[29]</ref>, CamStyle <ref type="bibr" target="#b44">[45]</ref>, HHL <ref type="bibr" target="#b42">[43]</ref>. We first compare with hand-crafted feature based methods which do not require learning on neither labeled source set nor unlabeled target set. These two hand-crated features have demonstrated the effectiveness on small datasets, but fail to produce competitive results on large-scale datasets. For example, the rank-1 accuracy of LOMO is 12.3% when tested on DukeMTMC-reID. This is much lower than transferring learning based methods. Next, we compare with three unsupervised methods. Benefit from initializing model from the labeled source data and learning with unlabeled target data, the results of these three unsupervised approaches are commonly superior to hand-crafted methods. For example, PUL obtains rank-1 accuracy of 45.5% when using DukeMTMC-reID as source set and tested on Market-1501, surpassing BOW by 9.7% in rank-1 accuracy. Compare to state-of-theart domain adaptation approaches, our approach clearly outperforms them by a large margin on both datasets. Specifically, our method achieves rank-1 accuracy = 75.1% and mAP = 43.0% when using DukeMTMC-reID as source set and tested on Market-1501, and, obtains rank-1 accuracy = 63.3% and mAP = 40.4% when using Market-1501 as source set and tested on DukeMTMC-reID. The rank-1 accuracy is 12.9% higher and 16.4% higher than current best results (HHL <ref type="bibr" target="#b42">[43]</ref>) when tested on Market-1501 and DukeMTMC-reID, respectively. We also evaluate our approach on a larger and more challenging dataset, i.e., MSMT17. Since it is a newly released dataset, there is only one unsupervised method (PTGAN <ref type="bibr" target="#b29">[30]</ref>) reported on MSMT17. As shown in method produces rank-1 accuracy = 30.2% and mAP = 10.2% when using DukeMTMC-reID as source set. This is higher than PTGAN by 18.4% in rank-1 accuracy and by 6.9% in mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an exemplar memory based unsupervised domain adaptation (UDA) method for person re-ID task. With the exemplar memory, we can directly evaluate the relationships between target samples. And thus we could effectively enforce the underlying invariance constraints of the target domain into the network training process. Experiment demonstrates the effectiveness of the invariance learning for improving the transferable ability of deep re-ID model. Our approach produces a new state of the art in UDA accuracy on three large-scale domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>first exemplar CamStyle images of the third exemplar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Examples of three underlying invariances. (a) Exemplar-invariance: an exemplar is enforced to be apart from others. (b) Camerainvariance: an exemplar and its camera-style transferred (CamStyle) images are encouraged to be close to each other, as well as CamStyle images should be far away from others. (c) Neighborhood-invariance: an exemplar and its neighbors are forced to be close to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Evaluation with different values of λ in Eq. 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>63</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation with different values of β in Eq. 3.</figDesc><table><row><cell>β</cell><cell cols="2">Duke → Market-1501 Rank-1 mAP</cell><cell cols="2">Market-1501 → Duke Rank-1 mAP</cell></row><row><cell>0.01</cell><cell>47.3</cell><cell>20.0</cell><cell>29.1</cell><cell>13.2</cell></row><row><cell>0.03</cell><cell>72.3</cell><cell>40.3</cell><cell>59.7</cell><cell>35.7</cell></row><row><cell>0.05</cell><cell>75.1</cell><cell>43.0</cell><cell>63.3</cell><cell>40.4</cell></row><row><cell>0.1</cell><cell>71.4</cell><cell>36.8</cell><cell>59.3</cell><cell>35.8</cell></row><row><cell>0.5</cell><cell>52.3</cell><cell>23.1</cell><cell>45.4</cell><cell>24.2</cell></row><row><cell>1.0</cell><cell>47.8</cell><cell>20.8</cell><cell>40.2</cell><cell>19.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 Table 2 .</head><label>22</label><figDesc>reports the results of the baseline. When trained with labeled target training set and tested on the target testing set, the baseline (called Supervised Learning) achieves high accuracy. However, we observe a serious drop in performance when the baseline is trained using labeled source set only (called Source Only) and directly applied to the target testing set.For example, when tested on Market-1501, the baseline trained on Methods comparison when tested on Market-1501 and DukeMTMC-reID. Supervised Learning: Baseline model trained with labeled target data. Source Only: Baseline model trained with only labeled source data. E: Exemplar-invariance. C: Camera-invariance. N: Neighborhood-invariance. Src.: Source domain.</figDesc><table><row><cell>Methods</cell><cell>Src.</cell><cell>R-1</cell><cell cols="4">Market-1501 R-5 R-10 R-20 mAP</cell><cell>Src.</cell><cell>R-1</cell><cell cols="4">DukeMTMC-reID R-5 R-10 R-20 mAP</cell></row><row><cell>Supervised Learning</cell><cell>N/A</cell><cell>87.6</cell><cell>95.5</cell><cell>97.2</cell><cell>98.3</cell><cell>69.4</cell><cell>N/A</cell><cell>75.6</cell><cell>87.3</cell><cell>90.6</cell><cell>92.9</cell><cell>57.8</cell></row><row><cell>Source Only Ours w/ E Ours w/ E+C Ours w/ E+N Ours w/ E+C+N</cell><cell>DukeMTMC</cell><cell>43.1 48.7 63.1 58.0 75.1</cell><cell>58.8 67.4 79.1 69.9 87.6</cell><cell>67.3 74.0 84.6 75.6 91.6</cell><cell>74.3 80.2 89.1 80.4 94.5</cell><cell>17.7 21.0 28.4 27.7 43.0</cell><cell>Market-1501</cell><cell>28.9 34.2 53.9 39.7 63.3</cell><cell>44.0 51.3 70.8 53.0 75.8</cell><cell>50.9 58 76.1 58.1 80.4</cell><cell>57.5 64.2 80.7 62.9 84.2</cell><cell>14.8 18.7 29.7 23.6 40.4</cell></row><row><cell cols="5">labeled Market-1501 training set achieves a rank-1 accu-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">racy of 87.6%. However, the rank-1 accuracy declines to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">43.1% when trained the baseline on labeled DukeMTMC-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">reID training set. A similar drop can be observed when</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">tested on DukeMTMC-reID. This decline in accuracy is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">mainly caused by the domain shifts between datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Computational cost analysis of the exemplar memory.</figDesc><table><row><cell>Method</cell><cell cols="3">DukeMTMC-reID → Market-1501 R-1 Time (mins) Memory (MB)</cell></row><row><cell cols="2">Ours w/ mini-batch 67.2</cell><cell>≈ 59.3</cell><cell>≈5,000</cell></row><row><cell>Ours w/ memory</cell><cell>75.1</cell><cell>≈ 60.6</cell><cell>≈5,260</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Unsupervised person re-ID performance comparison with state-of-the-art methods on Market-1501 and DukeMTMC-reID.</figDesc><table><row><cell>Methods</cell><cell>R-1</cell><cell cols="2">Market-1501 R-5 R-10</cell><cell>mAP</cell><cell>R-1</cell><cell cols="2">DukeMTMC-reID R-5 R-10</cell><cell>mAP</cell></row><row><cell>LOMO [15]</cell><cell>27.2</cell><cell>41.6</cell><cell>49.1</cell><cell>8.0</cell><cell>12.3</cell><cell>21.3</cell><cell>26.6</cell><cell>4.8</cell></row><row><cell>Bow [37]</cell><cell>35.8</cell><cell>52.4</cell><cell>60.3</cell><cell>14.8</cell><cell>17.1</cell><cell>28.8</cell><cell>34.9</cell><cell>8.3</cell></row><row><cell>UMDL [20]</cell><cell>34.5</cell><cell>52.6</cell><cell>59.6</cell><cell>12.4</cell><cell>18.5</cell><cell>31.4</cell><cell>37.6</cell><cell>7.3</cell></row><row><cell>PTGAN [30]</cell><cell>38.6</cell><cell>-</cell><cell>66.1</cell><cell>-</cell><cell>27.4</cell><cell>-</cell><cell>50.7</cell><cell>-</cell></row><row><cell>PUL [9]</cell><cell>45.5</cell><cell>60.7</cell><cell>66.7</cell><cell>20.5</cell><cell>30.0</cell><cell>43.4</cell><cell>48.5</cell><cell>16.4</cell></row><row><cell>SPGAN [7]</cell><cell>51.5</cell><cell>70.1</cell><cell>76.8</cell><cell>22.8</cell><cell>41.1</cell><cell>56.6</cell><cell>63.0</cell><cell>22.3</cell></row><row><cell>CAMEL [36]</cell><cell>54.5</cell><cell>-</cell><cell>-</cell><cell>26.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMFA [16]</cell><cell>56.7</cell><cell>75.0</cell><cell>81.8</cell><cell>27.4</cell><cell>45.3</cell><cell>59.8</cell><cell>66.3</cell><cell>24.7</cell></row><row><cell>SPGAN+LMP [7]</cell><cell>57.7</cell><cell>75.8</cell><cell>82.4</cell><cell>26.7</cell><cell>46.4</cell><cell>62.3</cell><cell>68.0</cell><cell>26.2</cell></row><row><cell>TJ-AIDL [29]</cell><cell>58.2</cell><cell>74.8</cell><cell>81.1</cell><cell>26.5</cell><cell>44.3</cell><cell>59.6</cell><cell>65.0</cell><cell>23.0</cell></row><row><cell>CamStyle [45]</cell><cell>58.8</cell><cell>78.2</cell><cell>84.3</cell><cell>27.4</cell><cell>48.4</cell><cell>62.5</cell><cell>68.9</cell><cell>25.1</cell></row><row><cell>HHL [43]</cell><cell>62.2</cell><cell>78.8</cell><cell>84.0</cell><cell>31.4</cell><cell>46.9</cell><cell>61.0</cell><cell>66.7</cell><cell>27.2</cell></row><row><cell>Ours (ECN)</cell><cell>75.1</cell><cell>87.6</cell><cell>91.6</cell><cell>43.0</cell><cell>63.3</cell><cell>75.8</cell><cell>80.4</cell><cell>40.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 ,Table 5 .</head><label>55</label><figDesc>our approach clearly surpasses PTGAN when using Market-1501 and DukeMTMC-reID as source domains. For example, our Performance evaluation when tested on MSMT17.</figDesc><table><row><cell>Methods</cell><cell>Src.</cell><cell>R-1</cell><cell cols="2">MSMT17 R-5 R-10</cell><cell>mAP</cell></row><row><cell cols="3">PTGAN [30] Market 10.2</cell><cell>-</cell><cell>24.4</cell><cell>2.9</cell></row><row><cell>Ours (ECN)</cell><cell cols="2">Market 25.3</cell><cell>36.3</cell><cell>42.1</cell><cell>8.5</cell></row><row><cell>PTGAN [30]</cell><cell>Duke</cell><cell>11.8</cell><cell>-</cell><cell>27.4</cell><cell>3.3</cell></row><row><cell>Ours (ECN)</cell><cell>Duke</cell><cell>30.2</cell><cell>41.5</cell><cell>46.8</cell><cell>10.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation through synthesis for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep association learning for unsupervised video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo Ha2 Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Chichung</forename><surname>Kot</surname></persName>
		</author>
		<editor>Prco. BMVC</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shaogang Gong, Tiejun Huang, and Yonghong Tian</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCVW</title>
		<meeting>ECCVW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dissecting person reidentification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive learning for person re-identification with one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crossview asymmetric metric learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<title level="m">Person re-identification: Past, present and future</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Random erasing data augmentation. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Camstyle: A novel data augmentation method for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
