<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective Aesthetics Prediction with Multi-level Spatially Pooled Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Hosu</surname></persName>
							<email>vlad.hosu@uni-konstanz.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Goldlücke</surname></persName>
							<email>bastian.goldluecke@uni-konstanz.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
							<email>dietmar.saupe@uni-konstanz.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective Aesthetics Prediction with Multi-level Spatially Pooled Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an effective deep learning approach to aesthetics quality assessment that relies on a new type of pretrained features, and apply it to the AVA data set, the currently largest aesthetics database. While all previous approaches miss some of the information in the original images, due to taking small crops, down-scaling or warping the originals during training, we propose the first method that efficiently supports full resolution images as an input, and can be trained on variable input sizes. This allows us to significantly improve upon the state of the art, increasing the Spearman rank-order correlation coefficient (SRCC) of ground-truth mean opinion scores (MOS) from the existing best reported of 0.612 to 0.756. To achieve this performance, we extract multi-level spatially pooled (MLSP) features from all convolutional blocks of a pretrained InceptionResNet-v2 network, and train a custom shallow Convolutional Neural Network (CNN) architecture on these new features. 1 dpchallenge.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aesthetics quality assessment (AQA) is an interesting, yet very difficult task to perform algorithmically. It involves predicting subjective opinions, i.e., aesthetics ratings, or distributions thereof. The complexity of the task is due to the many influencing factors for human judgments. An image's aesthetic quality, in addition to photography rules-ofthumb, is influenced by affective and personal preferences, for example for different content types or style. Earlier studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> have tried to encode photography rules as features that were used for training AQA models. Later works use different hand-crafted features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref> or more general purpose ones <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>A limitation of these traditional machine learning approaches is the representational power of the extracted features, and they are now outperformed by deep learning, which has been shown to be very suitable for AQA <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6]</ref>. The de facto standard for training and evaluation is the large-scale database for Aesthetic Vi-sual Analysis (AVA) <ref type="bibr" target="#b16">[17]</ref>. It consists of about 250 thousand images, collected from a photography community 1 , each rated by an average of 210 users.</p><p>Unfortunately, approaches based on deep learning typically need to perform a combination of rescaling and/or cropping to fit to the input shape of the chosen Deep Neural Network (DNN). The reason is that for efficient computation, GPUs need to process fixed sized blocks of data at a time, such as batches of images at the same resolution. However, AVA contains thousands of unique resolutions. Even for DNN architectures which can accept arbitrary input image resolutions, training would be extremely inefficient due to the small batch sizes required. Not only would processing be much slower, but learning performance would also suffer for models that employ batch normalization, such as Inception-v3. Moreover, the AVA database contains high resolution images, with an average width × height of 629 × 497 and maximum of 800 × 800 pixels. Training DNN models at this size is very resource intensive, requiring images in the database to be resampled.</p><p>An obvious solution is to rescale all images to the DNN input resolution when training, which either removes highfrequency information when down-scaling, or introduces artificial blurs when up-scaling. Thus, this approach removes or masks some information that the models could learn from, leading to a lower performance <ref type="bibr" target="#b14">[15]</ref>. Even if we proportionally down-scale large images to at least keep their aspect ratio, some information is still lost, and it would not alleviate the problem of different input resolution. Consequently, several existing works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> have already noted the drawbacks of learning to predict the aesthetics scores from images that are not of the same resolution as the original. Images that are downsized, stretched, or cropped do not contain the same information as the higher resolution image that was originally assessed by human observers. Thus, predictive performance is negatively affected.</p><p>On the other hand, previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref> have shown the effectiveness of multi-level pre-trained features (models trained on ImageNet) to predict perceptual judgments, either for image similarity or image quality assessment (IQA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Training pipeline of our framework. We extract and store features from variable sized high resolution images with Inception-type networks, and introduce a new type of features: Multi-level Spatially Pooled activation blocks (MLSP). Training aesthetic assessment from features is then done separately, enabling much larger batch sizes. This approach bypasses the resolution bottleneck, and very efficiently trains on the original images in the AVA data set (up to 800 × 800 pixels).</p><p>These have employed earlier DNN models such as VGG16. In <ref type="bibr" target="#b3">[4]</ref>, multi-level features were extracted from the more modern Inception architecture. The authors did not use pretrained features directly, but fine-tuned the network and then used the updated features.</p><p>Contributions. In this work, we devise an efficient staged training approach for AQA relying on a new type of perceptual features (MLSP). We show that we can efficiently learn from these very high-dimensional features (5 × 5 × 16, 928), far surpassing the state of the art with respect to correlation metrics. To achieve this, we extract and store fixed sized features from variable resolution images with augmentation, <ref type="figure">Fig. 1</ref>. We propose multiple head architectures that give better results either for narrow <ref type="bibr">(</ref></p><formula xml:id="formula_0">1 × 1 × b) or wide (5 × 5 × b) MLSP features.</formula><p>Variable, large resolution inputs have been the main stumbling block of all existing approaches on AVA, limiting training to low resolution images or small batch sizes. Compared to end-to-end training, staged training significantly reduces GPU memory requirements, standardizes variable resolution inputs enabling larger batch sizes, which leads to much faster training speeds (20x-200x speedup). This has several benefits:</p><p>• Effectiveness: we significantly improve upon the stateof-the-art in AQA on AVA, particularly for correlation metrics: previous 0.612 SRCC <ref type="bibr" target="#b21">[22]</ref> vs ours 0.756 SRCC.</p><p>• Reduced GPU memory: MLSP features are compact, and independent of the input image resolution. The head networks required by our approach are shallow making training much more memory-efficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>State-of-the-art Convolutional Neural Network (CNN) architectures for image classification are designed for small images below 300 × 300 pixels, all at the same resolution. Thus, deep learning for AQA has faced two major challenges. First, the AVA database contains over 25 thousand unique image resolutions, second, images are large, up to 800 × 800 pixels. Since users rate the original images, any transformation applied to an original does not preserve the available information. Many existing DNN AQA methods have tried to overcome these two main limitations by designing multi-column architectures that take several small resolution patches or fixed size rescaled images (usually 224 × 224) as inputs and aggregate their contributions to the aesthetics score.</p><p>Lu et al. <ref type="bibr" target="#b10">[11]</ref> employ a double-column DNN architecture to support the integration of both a global rescaled view, 224 × 224 randomly cropped from the 256 × 256 downsized original, and a local crop of 224 × 224 extracted from the original. In <ref type="bibr" target="#b11">[12]</ref>, they extend the initial work to use multiple columns that take random 256 × 256 patches from the original image. Both networks employ custom shallow shared-weights CNNs for each column. Ma et al. <ref type="bibr" target="#b13">[14]</ref> further extend Lu et al.'s work <ref type="bibr" target="#b11">[12]</ref> by selecting 224 × 224 patches non-randomly based on image saliency and aggregating them while modeling their relative layout. Instead of taking patches, Mai et al. <ref type="bibr" target="#b14">[15]</ref> use a multi-column architecture to integrate five spatial pooling sizes, which allows for testing with variable resolution images. Their base column architecture is VGG-16. Our main improvement over these approaches is to significantly simplify the architecture and bypass the requirement for taking small patches altogether. We train on features extracted directly from the original images, alleviating the limitations of previous works.</p><p>Talebi et al. <ref type="bibr" target="#b21">[22]</ref> have shown that simple extensions to existing image classification architectures can work surprisingly well. They rely on VGG16, MobileNet, and Inception-v2 base architectures, and replace the classification with a fully-connected regression head that predicts the distributions of ratings for each image. While training on 224 × 224 crops from 256 × 256 rescaled images in AVA, they showed promising results. We take the same approach to augmentation when fine-tuning base networks with our proposed regression head. However, when training on features from the original images, we extract crops that are proportional to each image, at 87.5% the width and height.</p><p>Another less strongly related branch of methods relies on ranking-type losses to encode the relationships between the aesthetic quality of images. Kong et al. <ref type="bibr" target="#b8">[9]</ref> propose to rank the aesthetics of photos by training on AVA with a rankbased loss function. They introduced a two column architecture (AlexNet-type base CNN), learning the difference of the aesthetic scores between two input images. They trained on low resolution 227 × 227 random crops from 256 × 256 rescaled originals. Schwarz et al. <ref type="bibr" target="#b17">[18]</ref> use a triplet loss, with a ResNet base architecture taking inputs at 224 × 224 pixels. Kong et al. <ref type="bibr" target="#b8">[9]</ref> are the first to report the SRCC metric on AVA, which is a natural way to evaluate ranking loss. Both works suggest that optimizing for ranking is important.</p><p>Aesthetic quality involves both low level factors such as image degradations (blur, sharpness, noise, artifacts) as well as higher level content-related factors. The authors of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> therefore propose to employ concatenated features from multiple levels of a network. Karayev et al. <ref type="bibr" target="#b5">[6]</ref> compute content features from the last two levels of a DeCAF network (DeCAF5 and DeCAF6) and have shown that these features extracted from a pre-trained network on ImageNet perform well when applied to recognizing image style. Hii et al. <ref type="bibr" target="#b3">[4]</ref> concatenate global average pooled (GAP) activations from the last 5 blocks of an Inception <ref type="bibr" target="#b20">[21]</ref> network and fine-tune the architecture for binary classification. Both of these approaches suggest that multi-level features can be helpful in predicting aesthetics. However, in contrast to our approach, they only consider some of the latter levels in the network, and extract a small set of globally pooled features.</p><p>In addition to these aesthetics-related works, others have considered using multi-level features for perceptual similarity <ref type="bibr" target="#b23">[24]</ref> or image quality assessment <ref type="bibr" target="#b2">[3]</ref>. They use Im-ageNet pre-trained features directly and a form of global pooling by means of one or more statistics, e.g., mean, max, min. In our work, we study spatially pooled features from multiple levels in Inception-v3 and InceptionResNet-v2 DNNs in-depth, and investigate how to best use them for assessing aesthetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network architecture and training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature extraction</head><p>Modern deep neural networks for object classification are structured in multiple sequential operation blocks that each have a similar structure. For inception networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>, these are often called inception modules. We study the suitability of the Inception-v3 and InceptionResNet-v2 blocks for AQA more in-depth by extracting both types of proposed MLSP features.</p><p>In <ref type="figure">Fig. 1</ref>, we show a general description of the approach for an Inception-type network. The pooling operation is slightly different between the two feature types: one resizes each activation block output of an inception module to a fixed spatial resolution: 1 × 1 global average pooling (GAP) for narrow features, and 5 × 5 spatial average pool-ing for wide MLSP features, <ref type="figure" target="#fig_0">Fig. 2</ref>. We compute the spatial pooling by resizing feature blocks using area interpolation (the same as INTER AREA in OpenCV). The resized features are concatenated along their kernel axis. There are 11 blocks in Inception-v3 (10,048 kernels), and 43 in InceptionResNet-v2 (16,928 kernels).</p><p>Multiple augmentations are considered for each image in the AVA data set, and their corresponding features are stored. We augment images with proportional crops at 87.5% of width and height, e.g., 224 × 224 from 256 × 256, covering the four corners of each image, together with a mirroring augmentation (on and off). The 8 sets of features are stored in a single HDF5 file for all images. In our tests there was no significant decrease in performance (train and test) when converting features from 32 to 16 bit floating point numbers, so we use the smaller type to save memory. During training AQA on features, a single random augmentation per epoch is drawn for each image from among the 8 stored (4 crops, 2 flips). At test time, the final score for an image is calculated by averaging the predictions over all stored augmentations. This is different to when we do fine-tuning of the base network, where we use crops at completely random locations instead and random flips. At test time in this latter situation, we average over 20 random augmentations obtained in this way. We noticed there is generally no improvement in performance if we use more than 20 augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">AQA from deep image features</head><p>In this section, we design different architectures for learning AQA from deep features. In principle, they can be stacked on top of the base network for feature extraction and trained end-to-end. However, our experiments at low image resolution show that there is no significant performance gain in doing so, see <ref type="table">Table 7</ref> later on, while at high image resolutions this approach would be too demanding on GPU memory. It turns out that narrow MLSP features do well with simpler fully-connected architectures, whereas  we need another approach to handle the spatial component of the wide MLSP features.</p><p>A common component of many of our proposed networks is a custom fully-connected head (3FC) that is tied directly to predicting the mean opinion score (MOS) for each image. Throughout the work we use a Mean Squared Error (MSE) loss in our architectures. The 3FC head, <ref type="figure" target="#fig_1">Fig. 3</ref>, uses dropout to reduce over-fitting and batch normalization to improve the smoothness of the convergence profile, i.e., the loss curve. It is assigned a special symbol, see <ref type="figure" target="#fig_1">Fig. 3</ref>, which is used in the following figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Narrow MLSP feature architectures</head><p>We present three architectures that we designed to work best as regression heads on top of narrow MLSP features:</p><p>Single-1FC: The most simple network is similar to what Hii et al. <ref type="bibr" target="#b3">[4]</ref> introduced: adding a single fully-connected layer on top of the concatenated Global Average Pooling (GAP). We modify it by using a single score (MOS) predictor, see <ref type="figure" target="#fig_2">Fig. 4(a)</ref>. This model performs fairly well given its simplicity, but it can be easily improved.</p><p>Single-3FC: Here, we incorporate the 3FC component in the head architecture, as shown in <ref type="figure" target="#fig_2">Fig. 4(b)</ref>. This approach results in a further performance improvement.</p><p>Multi-3FC: Inspired by the BLINDER approach <ref type="bibr" target="#b2">[3]</ref>, where the authors train independent Support Vector Regression models on top of each block of pooled features from all levels of a VGG network, we propose a fully-connected architecture that first learns from individual feature blocks and then linearly combines the results into the final prediction, see <ref type="figure" target="#fig_3">Fig. 5</ref>. Feature blocks are extracted from the output  of each Inception module. This architecture, even though having more capacity and more granular information than Single-3FC, performs about the same.</p><p>We have considered other variations on the presented models, including changes in number of neurons, fully connected structure, use of batch normalization, dropout rates, etc. The reported parameters worked best for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Wide MLSP feature architecture</head><p>Pool-3FC: for wide MLSP features, we take a slightly different approach, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. First we need to narrow down the features to a size that can be handled efficiently by fully connected components. We do this using a modified Inception module, where we only have three columns: 1 × 1, 3 × 3 convolutions, and average pooling with the corresponding 1 × 1 convolution to reduce the output size. After trying different numbers of kernels, we got the best results for 1024 per column.  <ref type="table" target="#tab_4">Multi-3FC  5  Single-3FC for each GAP block  Pool-3FC  6</ref> Inception-type module + Single-3FC <ref type="table">Table 1</ref>: Summary of proposed architectures. The Inception-type module is modified from Inception-v3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>All our networks are trained on the AVA dataset <ref type="bibr" target="#b16">[17]</ref>, and tested on the same random subset as previously used in the literature. This test set consists of 20,000 images, however we were only able to read correctly 19,928 of them. The remaining 235,574 images of AVA are further randomly split into a training (95%) and validation set (5%). The validation set is used to choose the best performing model, i.e. the one with minimum loss, via early stopping. The loss of the network is the MSE between the predicted and the ground-truth MOS for each image.</p><p>The initial learning rate used in training is 10 −4 , which is divided by 10 every 20 epochs, or when the loss does not decrease within 5 epochs, whichever comes first. The lowest learning rate we used was 10 −6 . At each change in learning rate the previous best model is loaded. The Adam optimizer <ref type="bibr" target="#b7">[8]</ref> is used, with all hyper-parameters except learning rate set to their defaults. When learning AQA from our pretrained features, we use a batch size of 128, and when finetuning models we use 32 images per batch. Experiments are performed on a single Nvidia Titan Xp GPU, using Keras with the Tensorflow backend to implement our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Rescaled input images</head><p>In a first step, we only evaluate the performance of the different head architectures from the previous section, and train on 224 × 224 crops from images which have been rescaled to the fixed resolution of 256 × 256. The purpose is to compare several options for learning AQA from features, while having images which are small enough so it is still possible to fine-tune the entire architecture end-toend without having to extract and store features as a first step. Results and more details can be observed in <ref type="table" target="#tab_2">Table 2</ref>. Notably, the optimal performance obtained with either endto-end fine-tuning or learning just from pre-trained features is practically the same. We only start to see a difference between these two if we use fewer features, for instance, Single-1FC (5) performs better when fine-tuned than when doing feature learning. This indicates that learning from comprehensive features may generally be as good as finetuning, even when generalizing to larger resolution images.</p><p>Note that images are augmented during fine-tuning by taking random crops and flips, whereas for learning from pretrained features we only use four fixed position crops and flips, potentially putting the latter at a slight disadvantage. When testing on feature based models, we aggregate predictions over all stored augmentations, while for fine-tuned models we aggregate over 20 random augmentations.</p><p>The best model for learning from pre-trained features is Single-3FC, which is simpler than Multi-3FC. For this reason, later on, we only consider Single-3FC when comparing different approaches to feature extraction.   <ref type="formula">(5)</ref> which mimics the approach taken by multiGAP <ref type="bibr" target="#b3">[4]</ref>, modified for single score prediction, using only the last 5 blocks. For Single-3FC (+do) the dropout rates are increased to (0.5, 0.5, 0.75). Single-3FC (-aug) does not use augmentation, but is simply trained on features from 256 × 256 rescaled originals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head architecture Finetune SRCC PLCC Accuracy</head><p>In some of the previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref> only the last few levels of the network have been used, 2 and 5 respectively. However, in <ref type="table" target="#tab_2">Table 2</ref>, we have also shown that Single-1FC performs worse when using only the last 5 levels of Inception-v3 compared to all 11. To further investigate this dependency, we evaluate the performance of the Single-3FC model when increasing the number of levels considered, from last (content) to first (low level features) in the network, see <ref type="figure" target="#fig_5">Fig. 7</ref>. The last 8 layers have a stronger effect on the training performance, however, the first 3 still bring some benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Arbitrary input image shape</head><p>Moving on to features extracted from the original images, the results become much better. In <ref type="table" target="#tab_4">Table 3</ref> we can see that the wider MLSP features perform the best (Pool-3FC). Narrow features, extracted from InceptionResNet-v2 based architecture, which is deeper and has a better performance at classification than Inception-v3, perform about as well as the spatially pooled features from Inception-v3. This may also be partially due to the increased granularity that more kernels offer: there are 16,928 kernels extracted from 43 levels in InceptionResNet-v2, compared to 10,048 kernels from 11 levels in Inception-v3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to previous methods</head><p>The main goal of all existing AQA methods trained on AVA is to improve binary classification accuracy. As we will discuss more in-depth later in Sec. 5.1, binary accuracy makes an arbitrary and limiting choice to separate low from high quality images at a score of 5 (of 10). Thus, methods that reach an optimal performance with respect to accuracy may not perform as well for the whole range of scores. This is the case with both existing works that report SRCC as well: for Talebi at al. <ref type="bibr" target="#b21">[22]</ref> and Kong et al. <ref type="bibr" target="#b8">[9]</ref> the reported SRCC is much lower relative to the accuracy, see <ref type="table">Table 4</ref>. They report 0.612 SRCC with 0.8151 accuracy (0.75 ratio), and 0.558 SRCC with 0.7733 accuracy (0.72 ratio) respec-tively. Our feature-based model Single-1FC has an SRCC of 0.644 at a similar accuracy to Kong et al. <ref type="bibr" target="#b8">[9]</ref> of 0.779 (ratio 0.82), whereas our best method Pool-3FC has an SRCC of 0.756 at a similar accuracy to Talebi at al. <ref type="bibr" target="#b21">[22]</ref> of 0.8161 (ratio 0.92). The higher ratios show that our feature based methods are generalizing better to the entire range of scores.</p><p>The reported results for the NIMA architecture (V2) introduced by Talebi et al. <ref type="bibr" target="#b21">[22]</ref> are using Inception-v2 as the base architecture. We re-trained their model with Inception-v3 to have a fairer comparison with our work (Talebi et al. <ref type="bibr" target="#b21">[22]</ref> V3 in <ref type="table">Table 4</ref>). The re-trained V3 model has a lower accuracy, however its performance on the correlation metrics increases. The best Earth Mover's Distance (EMD) loss we obtained was 0.07, compared to the reported 0.05 in the original work. This may be one reason for the difference in performance, however it is possible that the Inception-v3 architecture is more suited for correlation metrics.</p><p>Some existing works such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> use additional information about each image. We only compare methods that use purely information derived from image content. For instance, <ref type="bibr" target="#b3">[4]</ref> use user comments to enrich the available information for each image. While this latter source is clearly incompatible with image content, Ma et al. <ref type="bibr" target="#b13">[14]</ref> indirectly use information about the actual resolution of each image by including it in attribute graphs. Thus <ref type="bibr" target="#b13">[14]</ref> reports a higher accuracy of 82.5% when using extra information, while their performance based on image content alone is 81.70%.</p><p>We confirm that the resolution of images is indeed informative on our metrics. For instance, the SRCC between the number of pixels of each image (width × height) and the MOS is 0.188. This is not entirely unexpected, as higher resolution images of the same quality could lead to a better quality of experience, e.g., due to a wider field of view. We also went ahead and trained a small fully-connected 4-layer network (11 × 9 × 7 × 2 neurons) with a cross entropy loss, to predict high (MOS &gt; 5) and low quality images based on two inputs: the width and height of each image, normalized by dividing with the maximum value for each. Due to the imbalance of the test set the baseline for classification is 71.08%, i.e., assign all images to the high quality class. Our resolution based network slightly improves this performance, reaching an accuracy of 72.40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance evaluation</head><p>The predominant performance measure used in all methods trained on the AVA data set is the binary classification accuracy, as originally defined by Murray et al. <ref type="bibr" target="#b16">[17]</ref>. While easy to compute, this measure is not the most representative for the entire data set. An important use case for an aesthetics assessment model is its ability to establish ranking relationships between the quality of images. This can help Model SRCC PLCC Accuracy Murray et al. <ref type="bibr" target="#b16">[17]</ref> --66.70% Kao et al. <ref type="bibr" target="#b4">[5]</ref> --71.42% Lu et al. <ref type="bibr" target="#b11">[12]</ref> --74.46% Lu et al. <ref type="bibr" target="#b10">[11]</ref> --75.42% Hii et al. <ref type="bibr" target="#b3">[4]</ref> 75.76% Mai et al. <ref type="bibr" target="#b14">[15]</ref> --77.40% Kong et al. <ref type="bibr" target="#b8">[9]</ref> 0.558 -77.33% Talebi et al. <ref type="bibr" target="#b21">[22]</ref> V2 0.612 0.636 81.51% Talebi et al. <ref type="bibr" target="#b21">[22]</ref> V3 0.639* 0.645* 72.30%* Ma et al. <ref type="bibr" target="#b13">[14]</ref> --81.70%</p><p>Ours (Pool-3FC) 0.756 0.757 81.72% <ref type="table">Table 4</ref>: Performance comparison for existing methods. All numbers are reproduced as reported in the respective works, except for those marked with (*), which we have retrained. We report the performance of the best methods introduced in their respective works which use image content information exclusively.</p><p>to automatically guide enhancement <ref type="bibr" target="#b21">[22]</ref> or sort collections of images based on aesthetics. We argue that correlation performance metrics such as Spearman Rank-order Correlation Coefficient (SRCC) and Pearson Linear Correlation Coefficient (PLCC) are better suited for the purpose, compared to binary classification accuracy: 1. High quality images in the binary classification as defined in the original work <ref type="bibr" target="#b16">[17]</ref> and all later works are those that have a MOS greater than 5. Considering that the average MOS in AVA is 5.5, this choice is entirely arbitrary. The resulting accuracy varies greatly with different choices of the binary split. Optimizing classification performance for any particular threshold is thus not representative for the entire range of scores.</p><p>2. Random test sets, including the official one used in most previous works, have an unbalanced number of high quality (≈70%) and low quality (≈30%) instances. To the best of our knowledge, the binary classification accuracy measure as reported in the related works is not accounting for imbalanced classes. This implies that the minimum accuracy of any method should be around 70% , since this is the accuracy of a naive classifier that assigns all test images to the high quality class.</p><p>We are aware of only two works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> that deal with AQA and report correlation-based performance metrics. Nonetheless, these metrics have been widely applied for Image Quality Assessment (IQA), and do not have the downsides of the binary accuracy measure. They are representative of the entire range of quality scores, do not make arbitrary choices, and for SRCC account for the ranking errors of the predicted and ground-truth scores. Our approach is optimized to achieve the highest performance on correlation metrics. Nonetheless, we report binary accuracy, achieving comparable results with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Failure cases</head><p>In <ref type="figure">Figures 8 and 9</ref> we show several examples of images for which their predicted MOS values are close to the ground-truth ( <ref type="figure">Fig. 8</ref>) and some that have large absolute errors, see <ref type="figure">Fig. 9</ref>. Generally, we observe that images that have a low prediction error have both a high technical quality and show an interesting subject. In comparison, images that exhibit large prediction errors tend to have some obvious technical faults, however the subject matter is still interesting, thus the high user ratings. It appears our model has an easier time learning to differentiate images based on technical quality aspects, thus over-emphasizing their importance in some unusual cases.</p><p>We selected the images as follows. Images in the test set were sorted in decreasing order of their ground-truth MOS. The top 5,000 (out 19,928) were further sorted by the absolute error between the ground-truth MOS and the predicted one. The lowest and highest 40 images were reviewed. The images shown in figures 8 and 9 have been selected as representative samples. Among low aesthetic quality images, w.r.t. the ground-truth MOS, we could not find a clear pattern that differentiates low and high error cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Aesthetics quality assessment (AQA) is a challenging and useful perceptual problem. Our proposed approach substantially outperforms the state of the art w.r.t. correlation evaluation metrics, and matches it for a less discriminating, but popular metric: classification accuracy. We argued for the use of correlation metrics, which are well established in the broader field of image quality assessment.</p><p>Our proposed approach is general, but show-cased on AQA. We introduced a transfer learning strategy that uses features extracted from pretrained ImageNet CNNs (MLSP features, a new type of perceptual features) to surpass the performance of end-to-end training approaches. The power of perceptual features has been suggested before by <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b23">[24]</ref> in other domains and approaches. We show that our proposed features perform the best for AQA, while enabling much faster training and requiring less GPU memory.</p><p>For some problems, the speed gains and low resource usage of employing pre-trained networks can make it possible to learn from larger collections of images, at very high resolutions, and iterate quickly on limited GPU memory. The memory limitation is offset by storing large sets of features on disk. Moreover, using spatially wide and multi-level features leads to further performance gains, suggesting that we are yet to reach the full potential of pre-trained features when applied to perceptual assessment. <ref type="figure">Figure 8</ref>: High quality images from the test set, for which our best model's assessment errors are lowest. Our predicted score is the number on the left in each image while the ground-truth MOS is shown in brackets. <ref type="figure">Figure 9</ref>: High quality images from the test set, for which our best model's assessment errors are some of the highest. Our predicted score is the number on the left in each image while the ground-truth MOS is shown in brackets.</p><p>While our approach matches the binary classification accuracy of existing works (81.7%), it is substantially better over the entire score range: our SRCC is 0.756 compared to the existing reported 0.612. The largest performance improvement for our method is achieved by using information extracted from the original resolution images, without rescaling or cropping. This is likely both due to the masking effect of rescaling on technical quality aspects, e.g., noise, blur, sharpness, etc. and the changes in the aesthetics of the composition when cropping small parts of an image.</p><p>MLSP features from InceptionResNet-v2 do consistently better than those from Inception-v3. Both of these were trained on a subset of ImageNet (1000 classes). This sug-gest that deeper architectures, and maybe those trained on larger data sets such as the entire ImageNet, might provide further performance gains when using pre-trained features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>a) 3 of 11 activation blocks from an Inception-v3 CNN with an input image size of 800 × 600 pixels. b), c) MLSP features, inspired by ASP<ref type="bibr" target="#b14">[15]</ref> (originally used VGG-16), and Multi-GAP<ref type="bibr" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Three layer fully-connected component (3FC) that is used in multiple of our proposed models. The size X of the first layer depends on the individual model it is used in, ranging from 256 to 2048 neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>a) Single-1FC architecture, b) Single-3FC with X = 2048. Complete feature learning architectures used to train on narrow MLSP features (1 ×1 × b). For each architecture we predict the mean opinion scores (MOS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Multi-3FC: alternative architecture for learning from narrow MLSE features. Features extracted from different levels of the network are used separately. The joint prediction is their weighted sum. For each 3FC head (orange-blue triangle), X = b i , i = 1..N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Pool-3FC: Architecture for wide MLSP features. It first reduces the dimension of the features using a modified Inception module (without the 5×5 convolution), pools the concatenated features and adds a 3FC head at the end with X = 3072. The sizes are b = 10, 048 for Inception-v3 and b = 16, 928 for InceptionResNet-v2 features. Operations use padding to keep the spatial dimensions unchanged. fc + batch norm + dropout</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Performance of learning from pre-trained features (Single-3FC model) when increasing the number of inception blocks that are being considered, from the last to the first block (Inception-v3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Speed: training for an epoch on AVA for wide MLSP features takes about 10 minutes, while for narrow MLSP features it is less than a minute. Fine-tuning on average sized images (629 × 497 pixels) takes about 4.5 hours on one GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance of different architectures, trained on 224×224 crops from 256×256 rescaled originals, for direct learning from features (Finetune = no) or with fine-tuning. All are trained on narrow MLSP features (11 blocks) from Inception-v3, except for Single-1FC</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Training on features from the AVA original images,</cell></row><row><cell>extracted from Inception-v3 an InceptionResNet-v2. The</cell></row><row><cell>latter is marked with (*). The performance without crop/flip</cell></row><row><cell>augmentation is lower (-aug = no augmentation). Training</cell></row><row><cell>Pool-3FC on the wide MLSP features gives the best perfor-</cell></row><row><cell>mance of all methods.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) in the SFB Transregio 161 "Quantitative Methods for Visual Computing" (Projects A05 and B05), and the ERC Starting Grant "Light Field Imaging and Analysis" (336978-LIA).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind image quality prediction by exploiting multi-level deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="432" to="442" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multigap: Multi-pooled inception network with text augmentation for aesthetic prediction of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Hii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kairanbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>IEEE. 1, 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual aesthetic quality assessment with a regression model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1583" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.3715</idno>
		<title level="m">Recognizing image style</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="662" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rapid: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia (ACM MM)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rating image aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Multimedia (ToM)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2021" to="2034" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep multipatch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="386" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A-lamp: Adaptive layoutaware multi-patch deep convolutional neural network for photo aesthetic assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Composition-preserving deep photo aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ava: A largescale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Will people like your image? learning the aesthetic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scenic photo quality assessment with bag of aesthetics-preserving features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia (ACM MM)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nima</surname></persName>
		</author>
		<title level="m">Neural image assessment. Transactions on Image Processing (TIP)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3998" to="4011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Multimedia (ToM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
