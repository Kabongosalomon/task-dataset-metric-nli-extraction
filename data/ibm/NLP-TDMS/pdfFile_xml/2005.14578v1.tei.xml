<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Milde</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Informatics</orgName>
								<orgName type="laboratory">Language Technology Group</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Informatics</orgName>
								<orgName type="laboratory">Language Technology Group</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: unsupervised learning</term>
					<term>unsupervised acoustic models</term>
					<term>sparse autoencoders</term>
					<term>acoustic unit discovery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Sparsespeech model is an unsupervised acoustic model that can generate discrete pseudo-labels for untranscribed speech. We extend the Sparsespeech model to allow for sampling over a random discrete variable, yielding pseudoposteriorgrams. The degree of sparsity in this posteriorgram can be fully controlled after the model has been trained. We use the Gumbel-Softmax trick to approximately sample from a discrete distribution in the neural network and this allows us to train the network efficiently with standard backpropagation.</p><p>The new and improved model is trained and evaluated on the Libri-Light corpus, a benchmark for ASR with limited or no supervision. The model is trained on 600h and 6000h of English read speech. We evaluate the improved model using the ABX error measure and a semi-supervised setting with 10h of transcribed speech. We observe a relative improvement of up to 31.4% on ABX error rates across speakers on the test set with the improved Sparsespeech model on 600h of speech data and further improvements when we scale the model to 6000h. Index Terms: unsupervised learning, unsupervised acoustic models, sparse autoencoders, acoustic unit discovery</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transcribed and labeled speech data is usually needed to train supervised speech recognition systems, yet it is costly to obtain and transcribe. In contrast, unlabeled speech data can be obtained in vast quantities, even for languages for which much less resources are available as compared to e.g. English.</p><p>In recent years, unsupervised acoustic modelling has been gaining traction as viable models emerge to leverage and make use of a treasure trove of unlabelled speech data. The task of acoustic unit discovery has gained significant popularity in unsupervised or zero resource speech processing <ref type="bibr" target="#b0">[1]</ref>. Unsupervised unit discovery in isolation can provide insights into datasets, phoneme modelling choices and ultimately provide representations that enables working with raw speech when transcriptions are completely absent.</p><p>However, using what unsupervised acoustic models learn and transferring that knowledge in semi-supervised and transfer learning settings is of considerable practical interest. These learning settings hold the promise to boost performance of supervised systems, especially in low-resource settings. In this work, we extend and evaluate an unsupervised acoustic model originally proposed for acoustic unit discovery also in a semisupervised setting. A large amount of untranscribed speech data (600h-6000h) and only a small amount (10h) of transcribed speech training data is available in this learning setting.</p><p>Using the masking technique for unsupervised modelling and then fine-tuning is reminiscent of transformer models such as BERT <ref type="bibr" target="#b1">[2]</ref>, that are currently very popular on text data. As speech is continuous, using the idea of masking becomes a bit more difficult to transfer directly. In the following we present our approach that is based on a memory component addressed by Gumbel-Softmax, as part of a larger recurrent encoder/decoder network. We then use the masking technique on the internal representation, i.e. the pseudo-posteriograms used for reconstruction and generated at each time step can be randomly masked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The ZeroSpeech challenges <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4]</ref> target speech processing in a zero resource setting, i.e. models that learn from raw speech without transcription. The challenges have also established the use of the ABX discriminability <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> to intrinsically evaluate how well semantically relevant sounds are mapped by a discovered representation in the acoustic unit discovery task.</p><p>Models trained on untranscribed speech are recently becoming relevant since they can also boost performance in supervised systems:</p><p>Schneider et al. <ref type="bibr" target="#b6">[7]</ref> showed with Wav2vec that pre-training a model on raw speech with a similar binary contrastive loss as Word2Vec <ref type="bibr" target="#b7">[8]</ref> can be effective to improve supervised end-toend acoustic models. The discrete variant vq-wav2vec <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> of this model with vector quantization into several thousand of units has also been successfully used to pretrain BERT [2] on a sequence masking task, followed by using BERT representations in a wav2letter <ref type="bibr" target="#b10">[11]</ref> acoustic model for speech recognition.</p><p>Vector quantized variational autoencoders <ref type="bibr" target="#b11">[12]</ref> can also be used to learn discrete representations of speech, as demonstrated by the end-to-end system involving attention based ASR and TTS <ref type="bibr" target="#b12">[13]</ref> to encode and decode. Wang et al. proposed input masking <ref type="bibr" target="#b13">[14]</ref> in recurrent auto-encoders.</p><p>Contrastive Predictive Coding (CPC) <ref type="bibr" target="#b14">[15]</ref> is a representation learning model trained by predicting future hidden states, which can be applied to raw speech in the time domain. In <ref type="bibr" target="#b15">[16]</ref>, Kahn et al. created a benchmark for ASR with no or limited supervision, based on English audio books and Librispeech data, also providing results for using CPC. We use the Libri-Light corpus for training and evaluating our models, as it provides a good benchmark for unsupervised acoustic models that can scale well to large amounts of (untranscribed) speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparsespeech model</head><p>In <ref type="bibr" target="#b17">[17]</ref>, we previously proposed an approach to train unsupervised bi-directional recurrent neural network (RNN) acoustic models that learn discrete representations, with a memoryaugmented auto encoder. The Sparsespeech model also uses sequence masking (sequence dropout) on a quasi-symbolic representation that the network generates. The model consists of an encoder that generates the quasi-sparse representation of speech and a decoder that reconstructs the input features from embed-dings of a memory component addressed with this quasi-sparse representation. Encoder and decoder are each a bi-directional stacked Long Short-Term Memory (LSTM). A continuous context vector is also an additional input to the decoder, with the idea to capture and entangle variability of utterance global factors such as speaker identity or the environment. It can be an explicit context representation <ref type="bibr" target="#b18">[18]</ref> or speaker vector <ref type="bibr" target="#b19">[19]</ref>; in this paper we use an implicit context vector that is the mean of all encoder states, as also evaluated in <ref type="bibr" target="#b17">[17]</ref>. This also has the advantage that no separate model needs to be trained. When Sparsespeech representations are generated, we use output of the encoder's softmax. A sparsity constraint and diversity constraint on the encoders softmax output values σ is used in the original model to train the model on continuous approximations of one-hot vectors:</p><formula xml:id="formula_0">Sparsity-L = 1 − sup n σi</formula><p>(1)</p><formula xml:id="formula_1">Diversity-L = 1 m m j=1 DKL(σj||U )<label>(2)</label></formula><p>where m are time steps of an utterance with n softmax outputs per timestep using Kullback-Leibler (KL) divergence <ref type="bibr" target="#b20">[20]</ref> and U (x) = 1 n . A sparsity weight is multiplied with Sparsity-L and a diversity weight is multiplied with Diversity-L, these terms are then added to the loss function.</p><p>One drawback of this sparsity constraint is that it cannot be changed at generation time, as it is a hyper-parameter at training time. In this paper, we extend Sparsespeech to model symbolic self-labeling as an (approximated) discrete distribution, introducing an additional parameter that can be used to control the sparseness of the pseudo-posteriorgram representations that our model generates after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Categorical reparameterization</head><p>Discrete variables are difficult to train directly in a neural network, as the backpropagation algorithm cannot by applied to a non-differentable layer.</p><p>We use categorical reparameterization <ref type="bibr" target="#b21">[21]</ref> by Gumbel-Softmax <ref type="bibr" target="#b22">[22]</ref> to implement approximate discrete inference within the network while training it. This uses the softmax function as a differentiable approximation to argmax as follows:</p><p>We sample a noise vector g = g1 . . . g k from a Gumbel distribution with a uniform random sampler U :</p><formula xml:id="formula_2">g = −log(−log(U (0, 1)))<label>(3)</label></formula><p>Where k is the number of elements in the softmax. We then compute the Gumbel-Softmax as:</p><formula xml:id="formula_3">sof tmax( logits + ω · g τ )<label>(4)</label></formula><p>Where ω is a noise weight parameter. We set ω = 1 while training the network and ω = 0 after the training is completed to disable the Gumbel noise.</p><p>The temperature parameter τ controls the amount of sparsity of the sample drawn from the distribution provided by the (unscaled) input logits. We illustrate this in <ref type="figure">Figure 1</ref> with example samples drawn from the same distribution with varying τ . Lower temperatures (0.05, 0.1, 0.2) tend to make the drawn samples sparse, approximating a one hot vector, while higher temperatures (2.0, 5.0) increase denseness and approximate a uniform distribution. While training the network we use annealing, starting with a higher temperature and slowly decreasing it to a cutoff value below 0.5, for example τ = 2 → 0.1. In the Sparsespeech model, the Gumbel-Softmax replaces the regular softmax with the sparsity constraint. <ref type="figure">Figure 2</ref> illustrates the complete Sparsespeech model with the added Gumbel-Softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Setup</head><p>We use the newest version of Sparsespeech 1 , Tensorflow 1.8 and Python 3.6.9. The relevant changes necessary for the categorical reparameterization have been added to the original model and repository. For evaluation we use the supplied auxiliary scripts of the Libri-Light corpus 2 , with minor enhancements, such as the possibility to use Kullback-Leibler (KL) <ref type="bibr" target="#b20">[20]</ref> as a distance function in the ABX evaluation. <ref type="bibr" target="#b2">3</ref> KL is a better metric to compare pseudo-spectograms such as the ones our model generates, while the default cosine distance function of the Libri-Light scripts are better suited for comparing embedding representations.</p><p>We use 4-layer stacked biLSTM decoder/encoders in our Sparsespeech models, with a width of 256 neurons for all experiments. Perceptual linear predictive (PLP) <ref type="bibr" target="#b23">[23]</ref> input features are computed with Kaldi <ref type="bibr" target="#b24">[24]</ref> using the standard settings of 13 dimensions and 100 frames per second on (downsampled if necessary) 16kHz audio. The sparsity constraint of Spars-espeech is disabled (sparsity weight set to 0) for all experiments with the new model, while the diversity constraint of original model is kept and the diversity weight set to 100. We keep the 2-stage training approach of the original model where the model is pre-trained without the memory component. For the second training stage, we use temperature annealing while training the network: the τ parameter for Gumbel-Softmax is set to 2.0 and then slowly decreases by multiplying with an annealing factor (0.9999) every x batches. A cut off parameter, 0.1 or 0.2 in most experiments is set after which the annealing scheme stops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>We evaluate on two proposed evaluation tasks in Libri-Light <ref type="bibr" target="#b15">[16]</ref>: completely unsupervised and semi-supervised with limited supervision on English audio book read speech. We currently focus on the 600h (small) and 6000h (medium) subsets of untranscribed speech to train our models. In the unsupervised evaluation, we measure ABX error rates <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. This provides an error rate that measures how well the trained unsupervised representation can differentiate between same/different tri-phones within and across speakers, for example "bit" vs. "bat". It is also agnostic to the representation and can be evaluated on pseudo-labels as well as dense representations. We use the dev sets to calibrate parameters and test the best performing models on the test set. The ABX error measure uses DTW to compare two segments of different length, we use symmetric Kullback-Leibler divergence as local comparison function. This is the recommended distance function for posteriorgram-like representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. In the semi-supervised setting, we first train a Sparsespeech model on the unannoated data from Libri-Light. We then follow <ref type="bibr" target="#b15">[16]</ref> and evaluate with a simple phoneme classifier with Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b25">[25]</ref> that is trained on the representation with 10h of limited-resource phone labels. In <ref type="table" target="#tab_0">Table 1</ref> we generate pseudo-posteriorgrams with different temperatures τ from the same model. This model has been trained on 6000h, with 42 components in the memory bank and output representation (n42) and a temperature annealing training scheme of τ 2 → 0.1. The sparseness of the output can also be controlled with τ after training. The ABX error measure is sensitive to too sparse representations, as a different scaling with a higher τ significantly reduces the error measure. Temperatures 2.0 and 3.0 produced the lowest ABX error rates.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we mainly evaluate different n in the memory component of Sparsespeech, trained on the Libri-Light small subset of 600h. Using n=100 or n=128 components produced good within and across speakers results, n=42 also performed well on the across speakers ABX error. All models have been In <ref type="table" target="#tab_2">Table 3</ref> we compare ABX error rates on the Libri-Light test set. We use the best Sparsespeech models with 600h and 6000h as determined on the dev set. We compare against baseline PLP features, a baseline Sparsespeech model trained with the original sparsity loss method without Gumbel-Softmax and representations from Contrastive Predictive Coding (CPC) <ref type="bibr" target="#b14">[15]</ref> as reported in <ref type="bibr" target="#b15">[16]</ref>. While the Sparsespeech models are trained on PLP features (n=13) as input, the CPC are trained on raw 16kHz speech in the time domain. Like on the dev set, there is a large reduction in error rates when training with Gumbel-Softmax. However, the dense embedding representations trained with the CPC model show lower error rates than the best Sparsespeech model that we trained on the 6000h medium subset.</p><p>In <ref type="table" target="#tab_3">Table 4</ref> we compare the representations of our models in terms of how well a simple phoneme recognizer can classify phonemes with it as input. The phoneme recognizers are  trained on 10h of representations with phone labels. They are trained without explicit alignments using the CTC loss. Using only a linear 1D-convolution on posteriorgram-like representations as in <ref type="bibr" target="#b15">[16]</ref> proved to be challenging, as the most frequent emission symbol per timestep with the CTC loss is the blank label. Adding a simple 1-layer LSTM makes sure that the network can learn when to emit a label other than the blank label and also keep track of context. The 1D convolution has a kernel size of 8 (default in the Libri-Light evaluation script) and the number of output channels of the convolution is set to match the number of phones in the transcription plus the blank label (45). The 1-Layer LSTM has a fixed hidden size of 100. A simple decoder with beam search generates the hypothesized phone sequence. Phoneme error rate (PER) is then computed by comparing the sequence to the Libri-Light transcriptions on the dev and test set (these sets are the same as the ones in Librispeech <ref type="bibr" target="#b26">[26]</ref>). With the original Sparsespeech model we were getting similar PER results as the PLP baseline, but with the improved Sparsespeech model the phoneme recognizer can improve PER by 8.5% relative over the PLP baseline on the Libri-Light test-clean and by 5.6% relative on test-other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Using Gumbel-Softmax in the Sparsespeech model is an effective improvement. It yields a relative reduction of 23.9% in ABX error rates on the test set (with clean speech) accross speakers compared to the original model in <ref type="bibr" target="#b17">[17]</ref> on n = 20. The dimensionality of the learned representations of the new Sparsespeech model can now also be scaled up to representations with bigger n, while also improving ABX error rates. So far n = 100 and n = 128 yielded the best results for within  <ref type="figure" target="#fig_1">Figure 3</ref>) with better phoneme discriminability as measured by ABX and PER than PLP features. However when we compare ABX and PER error to unsupervised dense embedding representations such as the ones generated by CPC (n=256), there still a relatively large gap in error rates on the Libri-Light test set.</p><p>One difference is the type of input features; CPC uses raw wavforms in the time domain while we have used PLP features. We also plan to try out end-to-end learning on raw wavforms, as this could show how much of the performance gap can be attributed to this difference. CPC representations could potentially also be used as input features to the Sparsespeech model or the models could be combined. We are currently also working on scaling the Sparsespeech model to the full 60 thousand hours of speech data available in Libri-Light. Based on the training time on the smaller sets, we would expect an estimated training time of roughly 10 weeks on a single GPU with Sparsespeech for the full Libri-Light dataset.</p><p>Another major difference is strucural in the type of the generated representations. There might be a trade-off in ABX error rates between low-bitrate sparse representations and higher bitrate dense representations. The results from last years Zero Resource Challenge <ref type="bibr" target="#b3">[4]</ref> support this hypothesis with systems with higher ABX errors having lower bit rate representations. The organizers concluded that "this suggests that discretizing learned speech embeddings well is hard".</p><p>The pseudo-posteriorgrams that our Sparsespeech model can generate have the advantage over embeddings that they can directly be interpreted as a (soft) clustering of phoneme-like units. They can also be easily discretized and translated to symbolic pseudo transcriptions, where the ABX discriminability is still largely preserved (see <ref type="bibr" target="#b17">[17]</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>distribution t=0.05 t=0.1 t=0.2 t=0.5 t=1.0 t=2.0 t=5.0 samples: Drawing samples with different temperatures with the Gumbel-Softmax from a discrete distribution. The Sparsespeech unsupervised acoustic model with Gumbel-Softmax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Example feature representations generated by the Sparsespeech model "S6000h-n42-τ 2 → 0.1" with varying temperature.trained for 3 epochs, with a training time ranging from 23.8h to 30.43h for the second stage of the Sparsespeech training (with the memory component) on a single Nvidia Titan XP GPU. We have experimented with different annealing schemes, but settled on τ = 2.0 → 0.2 for most experiments. We have also trained two Sparsespeech models with n = 20 and n = 100 using original sparsity loss training method without Gumbel-Softmax. This did not show good ABX error rates on the Libri-Light dev set, in fact only accross speaker ABX error improved over the PLP features baseline. The new models trained with Gumbel-Softmax show significant relative error rate improvements over the original Sparsespeech model, with nearly all tested representations better than PLP features in all tested settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ABX error rates on features/posteriograms generated by our model for the Libri-Light dev set with different temperatures τ .</figDesc><table><row><cell>Model or features</cell><cell cols="4">Temp. within speaker across speaker</cell></row><row><cell></cell><cell>τ</cell><cell>clean</cell><cell>other clean</cell><cell>other</cell></row><row><cell>PLP Features</cell><cell>-</cell><cell>11.12</cell><cell>15.08 25.87</cell><cell>33.74</cell></row><row><cell cols="2">S6000h-n42-τ 2 → 0.1 0.2</cell><cell>12.66</cell><cell>15.52 18.86</cell><cell>24.84</cell></row><row><cell>"</cell><cell>0.8</cell><cell>11.04</cell><cell>13.65 17.02</cell><cell>23.01</cell></row><row><cell>"</cell><cell>1.0</cell><cell>10.66</cell><cell>13.25 16.34</cell><cell>22.55</cell></row><row><cell>"</cell><cell>2.0</cell><cell>9.57</cell><cell>12.15 14.73</cell><cell>20.68</cell></row><row><cell>"</cell><cell>3.0</cell><cell>9.51</cell><cell>12.15 14.41</cell><cell>20.25</cell></row><row><cell>"</cell><cell>5.0</cell><cell>10.48</cell><cell>12.94 15.28</cell><cell>20.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ABX error on features/posteriograms generated by our model for the Libri-Light dev set with different n (components in the memory bank).</figDesc><table><row><cell>Model or features</cell><cell cols="4">Temp. within speaker across speaker</cell></row><row><cell></cell><cell>τ</cell><cell>clean</cell><cell>other clean</cell><cell>other</cell></row><row><cell>PLP Features (n=13)</cell><cell>-</cell><cell>11.12</cell><cell>15.08 25.87</cell><cell>33.74</cell></row><row><cell>S600h-n20-sparsityloss-2.0</cell><cell>-</cell><cell>14.65</cell><cell>17.37 27.09</cell><cell>32.43</cell></row><row><cell cols="2">S600h-n100-sparsityloss-2.0 -</cell><cell>14.03</cell><cell>16.63 24.80</cell><cell>29.67</cell></row><row><cell>S600h-n20-τ 2 → 0.5</cell><cell>2.0</cell><cell>11.56</cell><cell>13.75 21.18</cell><cell>26.66</cell></row><row><cell>S600h-n42-τ 2 → 0.2</cell><cell>3.0</cell><cell>11.38</cell><cell>13.49 17.64</cell><cell>22.46</cell></row><row><cell>S600h-n100-τ 2 → 0.2</cell><cell>2.0</cell><cell>10.43</cell><cell>13.00 18.78</cell><cell>24.00</cell></row><row><cell>S600h-n100-τ 2 → 0.2</cell><cell>3.0</cell><cell>10.43</cell><cell>13.00 18.86</cell><cell>24.08</cell></row><row><cell>S600h-n128-τ 2 → 0.2</cell><cell>3.0</cell><cell>10.00</cell><cell>12.46 17.97</cell><cell>23.16</cell></row><row><cell>S600h-n256-τ 2 → 0.2</cell><cell>3.0</cell><cell>11.41</cell><cell>14.02 22.73</cell><cell>27.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>ABX error on features/posteriograms generated by our model for the Libri-Light test set. CPC results are taken from<ref type="bibr" target="#b15">[16]</ref>.</figDesc><table><row><cell>Model or features</cell><cell cols="5">Temp. within speaker across speaker</cell></row><row><cell></cell><cell>τ</cell><cell>clean</cell><cell cols="2">other clean</cell><cell>other</cell></row><row><cell>PLP Features (n=13)</cell><cell>-</cell><cell>10.46</cell><cell cols="2">14.69 23.78</cell><cell>34.15</cell></row><row><cell>S600h-n20-sparsityloss-2.0</cell><cell>-</cell><cell>13.98</cell><cell cols="2">16.95 24.80</cell><cell>32.66</cell></row><row><cell cols="2">S600h-n100-sparsityloss-2.0 -</cell><cell>14.12</cell><cell cols="2">16.97 22.86</cell><cell>30.53</cell></row><row><cell>S600h-n20-τ 2 → 0.5</cell><cell>2.0</cell><cell>10.92</cell><cell cols="2">14.06 18.86</cell><cell>27.16</cell></row><row><cell>S600h-n128-τ 2 → 0.2</cell><cell>3.0</cell><cell>10.59</cell><cell cols="2">13.78 15.68</cell><cell>23.16</cell></row><row><cell>S6000h-n42-τ 2 → 0.1</cell><cell>3.0</cell><cell>9.33</cell><cell cols="2">12.05 13.53</cell><cell>20.60</cell></row><row><cell>CPC-600h (n=256)</cell><cell>-</cell><cell>6.90</cell><cell>9.59</cell><cell>9.00</cell><cell>15.10</cell></row><row><cell>CPC-6000h (n=256)</cell><cell>-</cell><cell>6.22</cell><cell>8.55</cell><cell>8.05</cell><cell>13.81</cell></row><row><cell>CPC-60000h (n=256)</cell><cell>-</cell><cell>5.83</cell><cell>8.14</cell><cell>7.56</cell><cell>13.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>PER error for training a very simple phoneme recognizer with 10h of data on: PLP features, CPC model features or Sparsespeech model features. 4% on ABX error rates across speakers on the clean test set compared to the original model. A first result on training on the 6000h medium subset of the Libri-Light corpus further improved error rates and shows that the model is scaling. Currently, tuning the temperature parameter after a Sparsespeech model has been trained seems to be important to reduce ABX error rates, but higher temperatures when generating Sparsespeech features such as 2.0 and 3.0 seem to work well across models with different hyperparameters.PER error rates also show an 8.5% improvement over a PLP baseline with the new model when a simple phoneme recognizer is trained on the representations. The generated representations from the new model are still relatively compact and sparse (see also</figDesc><table><row><cell>Model or features</cell><cell>Temp.</cell><cell cols="2">dev PER</cell><cell>test PER</cell></row><row><cell></cell><cell>τ</cell><cell>clean</cell><cell cols="2">other clean</cell><cell>other</cell></row><row><cell>PLP Features (n=13)</cell><cell>-</cell><cell cols="3">52.44 62.36 50.96 63.13</cell></row><row><cell cols="2">S600h-n100-sparsityloss-2.0 -</cell><cell cols="3">52.48 61.65 50.48 63.23</cell></row><row><cell>CPC-600h (n=256)</cell><cell>-</cell><cell cols="3">40.21 51.80 38.18 53.85</cell></row><row><cell>CPC-6000h (n=256)</cell><cell>-</cell><cell cols="3">34.40 47.60 34.44 49.40</cell></row><row><cell>CPC-60000h (n=256)</cell><cell>-</cell><cell cols="3">31.16 46.67 32.67 48.93</cell></row><row><cell>S600h-n100-τ 2 → 0.2</cell><cell>3.0</cell><cell cols="3">50.39 59.82 48.29 61.75</cell></row><row><cell>S600h-n128-τ 2 → 0.2</cell><cell>3.0</cell><cell cols="3">50.56 60.20 48.05 61.69</cell></row><row><cell>S6000h-n42-τ 2 → 0.1</cell><cell>3.0</cell><cell cols="3">47.77 57.77 46.61 59.61</cell></row><row><cell cols="5">speaker ABX when trained on 600h of untranscribed speech.</cell></row><row><cell cols="5">Representations with bigger n also show a relative improve-</cell></row><row><cell>ment of over 31.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://gitlab.com/milde/sparsespeech/ 2 https://github.com/facebookresearch/libri-light<ref type="bibr" target="#b2">3</ref> These changes have been made available to the authors of Libri-Light as a pull request.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Zero Resource Speech Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjumea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="323" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Zero Resource Speech Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thiolliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3169" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Algayres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjumea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miskic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11469</idno>
		<title level="m">The zero resource speech challenge 2019: TTS without T</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating speech features with the minimal-pair ABX task: Analysis of the classical MFC/PLP pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1781" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ABX-discriminability measures and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Université Paris 6 (UPMC</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Selfsupervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effectiveness of self-supervised pre-training for asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7694" to="7698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Wav2letter: an endto-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">VQVAE unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11449</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised pre-training of bidirectional speech encoders via masked reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10603</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fuegen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07875</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparsespeech: Unsupervised acoustic unit discovery with memory-augmented sequence autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2019</title>
		<meeting>Interspeech 2019<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="256" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unspeech: Unsupervised speech context embeddings</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2693" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Government Printing Office</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<meeting>Automatic Speech Recognition and Understanding Workshop (ASRU)<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brisbane, QL, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
