<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransPose: Towards Explainable Human Pose Estimation by Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">TransPose: Towards Explainable Human Pose Estimation by Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Convolutional Neural Networks (CNNs) have made remarkable progress on human pose estimation task. However, there is no explicit understanding of how the locations of body keypoints are predicted by CNN, and it is also unknown what spatial dependency relationships between structural variables are learned in the model. To explore these questions, we construct an explainable model named TransPose based on Transformer architecture and low-level convolutional blocks. Given an image, the attention layers built in Transformer can capture long-range spatial relationships between keypoints and explain what dependencies the predicted keypoints locations highly rely on. We analyze the rationality of using attention as the explanation to reveal the spatial dependencies in this task. The revealed dependencies are image-specific and variable for different keypoint types, layer depths, or trained models. The experiments show that TransPose can accurately predict the positions of keypoints. It achieves state-of-the-art performance on COCO dataset, while being more interpretable, lightweight, and efficient than mainstream fully convolutional architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Convolutional Neural Networks have dominated the field of human pose estimation, with DeepPose <ref type="bibr" target="#b41">[42]</ref> being the early classic method. Afterwards, fully convolutional networks such as <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b37">38]</ref> have become the mainstream by predicting keypoints heatmaps, which implicitly learn spatial relationships between body parts. Most prior works take deep CNN as a powerful black box predictor and focus on improving the network structure to boost the performances, but what exactly happens inside the model or how these convolutional architectures capture the dependencies between body parts remains unexplained. From the scientific and practical perspectives, such interpretability of the model is imperative. It can provide insights into why the models behave the way they do, increase the safety and transparency of the model, help model users for decision-making, and help researchers understand how the structural variables, e.g., body parts in this task, are jointly related to each other to reach the final prediction. <ref type="figure" target="#fig_8">Figure 1</ref>: A schematic diagram of TransPose. Below: The inference pipeline of the model. Above: Dependencies areas for all predicted keypoints by inspecting the attention map from the predicted locations, which are annotated by white pentagrams. In this example, the person's left-ankle is occluded by a dog, but the model can still accurately infer its location. Why? the attention map (red box) may give a meaningful explanation: the predicted location of the left ankle relies on the image clues provided by the left knee and some joints on the other side of the symmetry.</p><p>We empirically summarize some reasons that might hinder the explainability of human pose estimation models.</p><p>• Deepness. Since the used convolutional neural networks are usually very deep such as <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b37">38]</ref>, one cannot easily figure out what role each layer plays, especially the high-level layers.</p><p>• Relationships and features are coupled. The global spatial relationships between body parts are not only implicitly encoded in the parameters of convolution kernels, but also expressed in the activations of neurons. Only looking at the intermediate feature activations like <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b51">52]</ref> cannot reveal the spatial relationship. It is also challenging to decouple and understand these coupled relationships solely from numerical values.</p><p>• Limited memory and expressiveness for large numbers of images. The explanations we expect should be variable, image-specific, and fine-grained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stacking Many Convolutional Layers to Achieve a Large Receptive Field</head><p>Attend to Any Pairwise Locations by One Attention Layer When CNN does inference, however, the static convolution kernels parameters are limited in the ability to represent variables due to their limited working memory <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, thereby making it difficult for CNN to explicitly express the variability of the image-dependent spatial dependencies as the postures of the human body vary in a large joint configuration space.</p><p>• Lack of tools. Although there are already visualization techniques like Maximizing Activations <ref type="bibr" target="#b10">[11]</ref>, DeconvNet <ref type="bibr" target="#b48">[49]</ref>, Saliency Map <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b12">13]</ref>, Feature Visualization <ref type="bibr" target="#b27">[28]</ref>, or Class Activation Mapping <ref type="bibr" target="#b53">[54]</ref>, etc., most of them aim to find out class-specific input patterns or saliency maps rather than to explain the fine-grained dependencies between related and structural predicted variables. By far, how to construct or develop explainable pose estimation models is still an open question.</p><p>In this work, we aim to build a human pose estimation model that can explain its predictions and directly reveal the image-dependent spatial relationships between keypoints, as illustrated in <ref type="figure" target="#fig_8">Fig. 1</ref>. We argue that convolution has advantages in extracting low-level features, but deeply stacking convolutional layers at high-level to enlarge the receptive field is not efficient to capture global dependencies. And such deepness increases the difficulty in interpreting CNN predictions. The Transformer architecture <ref type="bibr" target="#b43">[44]</ref> commonly used in NLP tasks, has a natural advantage over convolutional architectures in terms of drawing pairwise or higher-order interactions. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, one attention layer can capture interactions between any pairwise locations, and the attention map acts as an immediate memory to store these dependencies. Thus Transformer would be eligible for human pose estimation since this task requires the model to be able to capture long-range dependencies.</p><p>Based on these considerations, we design a novel model called TransPose, using convolutions to extract features at low-level and Transformer encoder layers to capture global dependencies at high-level. More importantly, the attention maps store the pairwise dependencies measured by the similarities of query-keys transformed from feature vectors at pairs of locations, the model can explain which regions the predicted keypoints locations highly rely on. Given a specific image, we can explicitly understand what image clues significantly contribute to the predictions (maximum activation position) and which parts are affected by the other ones, by unfolding and visualizing the attention map in each attention layer.</p><p>TransPose model is simple and lightweight. It can precisely predict keypoints positions based on heatmaps and obtain excellent performances. It is on par with state-of-the-art -SimpleBaseline <ref type="bibr" target="#b45">[46]</ref>, HRNet <ref type="bibr" target="#b37">[38]</ref>, and DarkPose <ref type="bibr" target="#b49">[50]</ref> baseline via fewer model parameters and faster speeds. In summary, our contributions are as follow:</p><p>• To the best of our knowledge, we are the first to use Transformer architecture to capture the spatial relationships between structural human body parts.</p><p>• TransPose can explain what spatial dependencies the predicted keypoints rely on. Qualitative analysis reveals the fine-grained spatial dependencies, which are image-specific and variable for different keypoint types, layer depths, and trained models.</p><p>• TransPose achieves 75.8 AP and 75.0 AP on COCO validation set and testdev set, with 73% fewer parameters and 1.4× faster than HRNet-W48.</p><p>• We find that position embedding is important for accurately predicting the 2D positions of keypoints and helps model generalize better on unseen input resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Explainability</head><p>Explainability means a better understanding for human of how the model makes predictions. Many works define the goal for explanation is often to determine what inputs are the most relevant to the prediction, as surveyed by <ref type="bibr" target="#b34">[35]</ref>. Activation Maximizing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> performs gradient descent in the input space to find out what input patterns can maximize (explain) a given unit. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b11">12]</ref> further consider generating the image-specific class saliency maps. <ref type="bibr" target="#b48">[49]</ref> uses DeConvNet to generate feature activities to show what has been learned in each convolutional layer. <ref type="bibr" target="#b21">[22]</ref> visualizes the feature maps and finds that some maximally activated neurons work like body parts patterns/templates detector. <ref type="bibr" target="#b51">[52]</ref> uses channel-wise attention mechanism to find out the channels of feature activations associated with the occlusion patterns for pedestrian detection. There are also works like Network Dissection <ref type="bibr" target="#b1">[2]</ref>, Feature Visualization <ref type="bibr" target="#b27">[28]</ref>, Excitation Backprop <ref type="bibr" target="#b50">[51]</ref>, LRP <ref type="bibr" target="#b0">[1]</ref>, CAM <ref type="bibr" target="#b53">[54]</ref>, and Grad-CAM <ref type="bibr" target="#b35">[36]</ref>, which aim to explain the prediction of CNN classifier or visualize the saliency area significantly affecting the class. Different from most prior works, we aim to explain the fine-grained dependencies between body joints variables in the structural skeleton. And our model can directly exploit the ingredient of itself to explain predictions without the help of external tools. It is worth noting that there are some works to explain how the neural network predicts the positions and stores the position information by designing proxy tasks such as Coord-Conv <ref type="bibr" target="#b23">[24]</ref> and Zero Padding <ref type="bibr" target="#b18">[19]</ref>. Our work is carried out on real-world task, explores the importance of position embedding for predicting the locations and its generalization on unseen input scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Human Pose Estimation</head><p>Deep CNNs have achieved great success in human pose estimation. The inductive biases of vanilla convolution kernel <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref> are locality and translation equivariance. It proves to be efficient to extract low-level image feature. For human pose estimation, capturing global dependencies is crucial <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b28">29]</ref>, but the locality nature of convolution makes it impossible to capture long-range interactions. To address this issue, an effective but brute solution is to enlarge the receptive field, e.g. by downsampling feature map size, increasing the model depth or expanding the kernel size, etc. Further, more sophisticated design strategies are proposed such as multi-scale feature fusion <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref>, stacking modules and multiple stages to refine the prediction <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27]</ref>, or high-resolution representation <ref type="bibr" target="#b37">[38]</ref>, etc; meanwhile, many successful architectures have emerged such as CPM <ref type="bibr" target="#b44">[45]</ref>, Stacked Hourglass Network <ref type="bibr" target="#b26">[27]</ref>, FPN <ref type="bibr" target="#b47">[48]</ref>, CPN <ref type="bibr" target="#b5">[6]</ref>, SimpleBaseline <ref type="bibr" target="#b45">[46]</ref>, HRNet <ref type="bibr" target="#b37">[38]</ref>, RSN <ref type="bibr" target="#b3">[4]</ref>, even automated architectures <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53]</ref>. But as the model architecture design becomes more complex, it is more challenging but imperative than ever to seek the transparency and interpretability of the human pose estimation model. Our model, in contrast, can capture the spatial relationship in an efficient yet interpretable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformer</head><p>Transformer architecture was proposed by Vaswani et al. <ref type="bibr" target="#b43">[44]</ref> for neural machine translation (NMT) task <ref type="bibr" target="#b39">[40]</ref>. Recently, Transformer or attention-augmented layers have merged as new choices for vision tasks such as image classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43]</ref> and object detection <ref type="bibr" target="#b4">[5]</ref>. Unlike DETR <ref type="bibr" target="#b4">[5]</ref>, ViT <ref type="bibr" target="#b9">[10]</ref>, and DeiT <ref type="bibr" target="#b42">[43]</ref> applying Transformer to predict the object instances set or image categories, we use Transformer to predict the heatmaps represented with 2D spatial distributions of keypoints, and use attention maps as the explanations for this 2D-structural prediction task.  Firstly, the feature maps are extracted by a CNN backbone and flattened into a sequence. Next, the Transformer encode layers iteratively capture dependencies from the current sequence by query-key-value attention. Then, a simple head is used to predict the keypoints heatmaps. The model can explain what regions or joints significantly contribute to the predicted locations by the attention maps in Transformer encoder layers, and the spatial dependencies can be further revealed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to build a model that can capture the spatial dependencies between human body parts and explain its predictions. In this section, we describe the architecture of the proposed model, and then we recall the attention mechanism and further analyze how it explains what the predicted keypoints depend on when given a specific image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, TransPose model consists of three components: a CNN backbone to extract low-level image feature; a Transformer Encoder to capture long-range spatial interactions between feature vectors across the locations; a head to predict the keypoints heatmaps.</p><p>Backbone. Many common CNNs can be taken as the backbone. For better comparisons, we choose two typical CNN architectures: ResNet <ref type="bibr" target="#b16">[17]</ref> and HR-Net <ref type="bibr" target="#b37">[38]</ref>. To keep the simplicity of the model, we only retain the initial part of the original ImageNet pretrained CNN as the initial layers to extract low-level feature from input image. We name them ResNet-S and HRNet-S (including HRNet-S-W32 and HRNet-S-W48). The numbers of parameters of candidate CNNs are only 1.4M, 7.3M and 16.7M, which are 5.5%, 25.6% and 24.3% of the original ResNet-50 (25.6M), HRNet-W32 (28.5M), and HRNet-W48 (68.6M).</p><p>Transformer. We follow the standard Transformer architecture <ref type="bibr" target="#b43">[44]</ref> as closely as possible. And only the encoder is employed, as we believe that pure heatmaps prediction task is simply an encoding task, which compresses the original im-age information into a compact positions representation of human body keypoints. Given an input image I ∈ R 3×H I ×W I , we assume that the CNN backbone outputs a 2D spatial structure image feature X f ∈ R d×H×W whose feature dimension has been transformed to d by a 1×1 convolution. Then, the image feature map is flattened into a sequence X ∈ R L×d , i.e., L d-dimensional feature vectors where L = H × W . It enters Transformer Encoder and goes through N attention layers and feed-forward networks (FFN). We provide details in Section 3.4.</p><p>Head. A simple and lightweight head is attached to Transformer Encoder output E ∈ R L×d to predict K types of keypoints heatmaps P ∈ R K×H * ×W * where H * , W * = H I /4, W I /4 by default. We firstly reshape E back to R C×H×W shape. If H, W equal H * , W * , only a 1×1 convolution reduces the channel dimension of E from d to K; if H, W equal H * /2, W * /2, we use a bilinear interpolation or a 4×4 deconvolution to do 2× upsampling before 1×1 convolution 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Resolution Settings.</head><p>The computational complexity of per self-attention layer is O (HW ) 2 · d . Considering the trade-off between the memory footprint for attention layers and the loss in detailed information, we restrict the attention layers to operate at a resolution with r× downsampling rate w.r.t. the original input, i.e., H, W = H I /r, W I /r; we adopt r = 8 and r = 4 setting for ResNet-S and HRNet-S backbone. In the common human pose estimation architectures <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b37">38]</ref>, 32× downsampling is usually adopted as a standard setting to obtain a very low resolution map containing global information. By contrast, our model can directly capture long-range interactions at a higher resolution, while preserving the fine-grained local feature information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Position Embedding</head><p>Without the position information embedded in the input sequence, the Transformer Encoder is a permutation-equivariant architecture:</p><formula xml:id="formula_0">Encoder (ρ (X)) = ρ (Encoder (X)) ,<label>(1)</label></formula><p>where ρ is any permutation for the pixel locations or the order of sequence. To make the order of sequence or the spatial structure of the image feature map matter, the original Transformer adds sine and cosine positional encodings to the input embeddings. 2D Sine position embedding. Likewise, we follow the sine positional encodings <ref type="bibr" target="#b43">[44]</ref> but further hypothesize that the position information is independent at x (horizontal) and y (vertical) direction of an image. Thus we adopt both x-direction and y-direction position embedding for image feature, like <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5]</ref>. It is injected into the input sequence before self-attention layer.  We use 2D sine position embedding by default for all TransPose models. See Appendix for how to construct 2D sine position embedding. Learnable position embedding and w/o position embedding. In experiments, we also use a learnable position embedding LPE ∈ R L×d to explore whether the model can implicitly learn the position information. In addition, we also conduct the experiment without adding any position information to explore the importance of the position information for predicting 2D-structure heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Explaining by Attention</head><p>Attention mechanism. The core mechanism of Transformer <ref type="bibr" target="#b43">[44]</ref> is multi-head self-attention. It first projects an input sequence X ∈ R L×d into queries Q ∈ R L×d , keys K ∈ R L×d and values V ∈ R L×d by three matrices W q , W k , W v ∈ R d×d . Note that in this work the position embedding will be added into the input sequences except for computing the values V. Then, the attention scores</p><formula xml:id="formula_1">matrix 2 A ∈ R N ×N is computed by: A = softmax QK √ d .<label>(2)</label></formula><p>Each query q i ∈ R d belonging to the token (feature vector) x i = X i computes similarities with all the keys to achieve a weight vector w i = A i,: ∈ R 1×L , which determines how much dependency is needed from each token in the previous sequence for a new token. Then an incremental update (residual connection) for x i is achieved by linearly combining each value in Value matrix Q with the corresponding weight in w i . By doing this, the attention maps can be seen as dynamic weights that store the similarities dependent on the current input context or feature activations, and weight the distributions in the forward propagation. This mechanism plays a crucial role in capturing and explaining how much contribution the final predictions aggregate from the context token at each location of the sequence. Note that the pairwise and global interactions mostly occur at the attention layers 3 . The subsequent layers in feed-forward network (FFN) and head only serve as position-wise transformation. They are unable to capture global interactions but approximately linearly transform the contributions from all positions by shared weights. Specifically, the last attention layer in Transformer Encoder, whose attention scores are seen as the dynamic (image-dependent) weights, has the most direct effect on the predictions. Although the weights in FFN or head cannot be ignored, they are static (image-independent) during inference and shared across all locations. We make gradient analysis in Appendix G to analyze the rationality of using the attention scores of the last attention layer as the explanations for this task.</p><p>Explaining the predicted locations by attention maps. Similar with the ideas in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref>, the interpretability behind TransPose lies in: the regions which can maximize a given prediction (location) can explain what this prediction is looking for or depend on.</p><p>In this task, the learning target is to expect the output value h i * in the heatmap to be maximally activated where i * represents the groundtruth location of a keypoint:</p><formula xml:id="formula_2">θ * = arg max θ h i * (θ, I).<label>(3)</label></formula><p>Assuming the model has been optimized with parameters θ * and it predicts the location of a particular keypoint as i (maximally activated in heatmaps), why the model predicts such prediction can be explained by the fact that those locations J, whose element j has higher attention score (≥ δ) with i, are the dependencies that significantly contribute to the prediction. These locations can be found by:</p><formula xml:id="formula_3">J = {j|A i,j (θ * , I) ≥ δ} ,<label>(4)</label></formula><p>(a) TP-R: predictions and dependencies of each keypoints for input A.        where A ∈ R L×L is the attention map of the last attention layer and also a function w.r.t θ * and I. Given an image I and a query location i, A i,: can reveal what dependencies a predicted location i highly relies on, we named it dependency area; A :,j can reveal what positions a specified location j mostly affects, we name it affected area. Different from Activation Maximizing methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49]</ref> or Saliency Map Visualization <ref type="bibr" target="#b36">[37]</ref>, we do not need external tools or extra training costs to learn some explainable patterns, but only need to inspect the ingredients of the trained models to reveal what spatial relationships the TransPose has learned. In addition, unlike the visualizations on the fixed patterns that the convolutional kernels prefer to look for, the dynamic characteristic of attention map can show changeable dependencies w.r.t input images, layer depths, types of keypoints or trained models. We make further analysis in the Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset. We evaluate our models on COCO dataset <ref type="bibr" target="#b22">[23]</ref>, which contains more than 200k images in the wild and 250k person instances. We only use COCO train2017 for training, which consists of 57k images and 150k person instances. Val2017 set contains 5k images and test-dev2017 consists of 20k images. Object keypoint similarity (OKS) is the evaluation metric for keypoints locating accuracy <ref type="bibr" target="#b3">4</ref> .</p><p>Technical details. We follow the top-down human pose estimation paradigm. The training samples are the cropped images with single person. We resize all input images into 256×192 resolution. We use the same training strategies, data augmentation and person detected results as <ref type="bibr" target="#b37">[38]</ref>. We also adopt the coordinate decoding strategy proposed by <ref type="bibr" target="#b49">[50]</ref> to reduce the quantisation error when decoding from downscaled heatmaps. The feed forward layers are trained with 0.1 dropout and ReLU activate function. Next, we name the models based on ResNet-S and HRNet-S TransPose-R and TransPose-H, abbreviated as TP-R and TP-H. The architecture details are reported in the Tab. 1. We use Adam optimizer for all models. Training epochs are 230 for TP-R and 240 for TP-H. The cosine annealing learning rate decay is used. The learning rates for TP-R-A4 and TP-H-A6 models decay from 0.0001 to 0.00001, we recommend to use such a schedule for all models. Considering the compatibility with backbone and the memory consumption, we adjust the hyperparameters of Transformer encoder to make the model capacity not very large.</p><p>Comparisons with state-of-the-art methods. We compare TransPose based on ResNet-S and HRNet-S with SimpleBaseline, HRNet and the stronger models proposed by <ref type="bibr" target="#b49">[50]</ref>. Specially, we trained the SimpleBaseline-ResNet50-DarkPose on our machines according to the official code. The others results showed in Tab. 2 are come from the papers. We test all models on a single NVIDIA 2080Ti GPU with the same experimental conditions to compute the average FPS. As shown in Tab. 2, TransPose-R-A4 and TransPose-H-A6 have obviously overperformed Simple-Res50-DarkPose (+0.6AP), HRNet-W32-Dark (+0.2AP), and HRNet-W48 (+0.7AP) baseline models, with significantly fewer model parameters and faster speeds. Tab. <ref type="bibr" target="#b3">4</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablations</head><p>The importance of position embedding. Without position embedding, the 2D spatial structure information loses in Transformer. To explore the importance of position embedding, we conduct experiments on TransPose-R-A3 models with three position embedding strategies: 2D sine position embedding, learnable position embedding, and no position embedding. As expected, the models with position embedding have better performances, particularly for 2D sine position embedding, as shown in Tab. 3. But note that TransPose without position embedding can also perform well with only losing 1.3 AP. The reason perhaps is that the sparse position encoding in the target heatmaps makes the 2D spatial structure less important. We also observe what has been learned in the learnable position embedding, by computing the cosine similarities between the vectors in any paired locations of the learnable position embedding. The results (see Appendix) indicate that each vector in the learnable embedding has similar values with its neighbours in the 2D grid, which means the coarse 2D position information is learned and exploited in the model. Position embedding helps generalize better on unseen input resolutions. The top-down paradigm scales all the cropped images containing a single person to a fixed size. But for some cases even with a fixed input size or the bottom-up multi-person pose estimation paradigm, the scale of the human body in the input might be various, the robustness of handling with different scales is also important. To test the generalization of models, we design an extreme experiment: we test SimpleBaseline-ResNet50-DarkPose and TransPose-R-A3 models on unseen 128×96, 384×288, 512×388 input resolutions, all of    which only have been trained with 256×192 input images. Interestingly, the results in <ref type="figure" target="#fig_8">Fig. 8</ref> demonstrate that SimpleBaseline and TransPose-R without position embedding have obvious performance collapses on these unseen input resolutions, particularly on 128×96, but TransPose-R with learnable or 2D Sine position embedding have significantly better generalization, especially for the Sine position embedding with minimal performance degradation. We conjecture that 1) it is hard for the models with a fixed receptive field to adapt the changes in the feature scale; 2) building associations with relative position information encoded in Sine position embedding <ref type="bibr" target="#b43">[44]</ref> may help the model generalize on different sizes of inputs or feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Explainability Analysis</head><p>In this section, we show the dependencies exhibited by the attention maps are dynamic across different trained models, types of predicted keypoints, depths of attention layers, and input images. We choose cropped images by GT boxes   <ref type="figure" target="#fig_8">Fig. 4(a)</ref> with <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, although TP-R and TP-H predict exactly the same locations of keypoints in the input A, TP-H obviously exploits the image cues from the longer-distance joints to predict most keypoints. In contrast, TP-R prefers to attend to the short-range (local) image cues around the target keypoint. This characteristic can be further confirmed by the affected areas illustrated in <ref type="figure" target="#fig_8">Fig. 7</ref>, in which the keypoints have larger affected areas in TP-H. The larger receptive field and information fusion of HRNet-S might account for this.</p><p>Dependencies and influences vary for different types of keypoints. Overall, the upper limb and the keypoints belonging to it, especially the head, have a greater impact on predicting positions of keypoints in the lower limb. Such influences are more obvious in TP-H, mainly due to its stronger long-distance capturing ability than TP-R model. As shown in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, <ref type="figure" target="#fig_5">Fig. 4(d)</ref>, <ref type="figure" target="#fig_8">Fig. 4(h)</ref>, <ref type="figure" target="#fig_8">Fig. 4(f)</ref>, and <ref type="figure" target="#fig_8">Fig. 6</ref>, we can further observe that a well-performed model might gather more significant clues from more other parts to predict the target keypoint. We thus suppose that learning strong associations between accurate keypoints locations can reinforce the model to localize any one of them more accurately. This can explain why the model still can predict the location of an occluded keypoint accurately, and the occluded keypoint with ambiguity feature would have less impact on the other predictions, such as the left-ankle joint shown in <ref type="figure" target="#fig_8">Fig. 4(g)</ref> and <ref type="figure" target="#fig_8">Fig. 4(h)</ref>. Moreover, the joints with a high degree of freedom, e.g., elbows, wrists, knees, and ankles, also depend on the nearby limbs and joints on the symmetrical side.</p><p>Attentions gradually focus on the more fine-grained dependencies with the depth of layer increasing. Observing all attention layers, as shown in the 1,2,3-th rows of <ref type="figure" target="#fig_8">Fig. 6</ref>, we surprisingly find that even without the intermediate GT locations supervision, TP-H model can still attend to the accurate locations of joints in the early layers. For both models, with the depth of layer increasing, the predictions gradually depend on the more fine-grained body part image clues or keypoints positions, as shown in <ref type="figure" target="#fig_5">Fig. 5 and Fig. 6</ref>.</p><p>The dependencies slightly change for different input image context. Different from the static dependencies encoded in the weights and bias of CNN after training, the attention maps are dynamic to inputs. By choosing a model alone and testing different inputs for it, e.g., input A, B, C or D for TP-R in <ref type="figure" target="#fig_8">Fig. 10</ref>, we can observe that despite the statistical commonalities on the dependency relationships for the predicted keypoints, the fine-grained dependencies would slightly change according to the image context. With the existence of occlusion or invisibility in a given image such as input D <ref type="figure" target="#fig_8">(Fig. 4(h)</ref>), the model can still localize the position of the partially obscured keypoint by looking for more significant image clues and reduces reliance on the invisible keypoint to predict the other ones. Such a dynamic characteristic can depict the unique dependencies existing in an unique image when inferring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we explored using Transformer encoder with low-level convolutional blocks to construct an explainable human pose estimation model. We analyzed and showed that the attention mechanism is very promising to capture and explain the image-dependent spatial relationships in this structure prediction task. With lightweight architectures, TransPose models match the state-of-the-art on COCO Keypoint Detection task that has been dominated by deep fully convolutional architectures, and there seems to have further space to improve the upper limit of model performance by expanding the size of TransPose. Furthermore, we validate the importance of position embedding, and our qualitative analysis reveals the image-specific and variable spatial dependencies for different layer depths, keypoints types, or trained models. We believe that these ideas could help researchers or users have a deeper understanding of how neural networks make structural predictions for human pose estimation task, and further improve the model design. Cosine Similarity <ref type="figure" target="#fig_8">Figure 9</ref>: The cosine similarities between the learned position embedding vectors, which have been reshaped into 2D grid and interpolated with 0.25 scale factor for a better illustration (the original shape is <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b31">32)</ref>). Each map in x-row and y-col of the figure represents the cosine similarities between the embedding vector in position (x, y) and the embedding vectors at other locations. of the paper. As shown in <ref type="figure" target="#fig_8">Fig. 9</ref>, we visualize the similarities by calculating the cosine similarity between vectors at any pair of locations of the learnable position embedding and reshaping it into a 2D grid-like map. We find that the embedding in each location of learnable position embedding has a unique vector value in the d-dim vector space, but it has relatively higher cosine similarity values with the neighbour locations in 2D-grid and lower values with those far away from it. The results indicate the coarse 2D position information has been implicitly learned in the learnable position embedding. We suppose that the learning sources of the position information might be the 2D-structure groundtruth heatmaps and the similar features existing in the 1D-structure sequences. The model learns to build associations between position embedding and input sequences, as a result it can predict the target heatmaps with 2D Gaussian peaking at groundtruth keypoints locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B 2D Sine Position Embedding</head><p>We follow the sine positional encodings but further hypothesize that the position information is independent at x (horizontal) and y (vertical) direction of an image. Concretely, we keep the original 2D-structure respectively with d/2</p><p>channels for x, y-direction: </p><formula xml:id="formula_4">P E (2i,</formula><p>where i = 0, 1, ..., d/2 − 1, p x or p y is the position index along x or y-direction.</p><p>Then they are stacked and flattened into a shape R L×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Transformer Encoder Layer</head><p>The Transformer Encoder layer <ref type="bibr" target="#b43">[44]</ref> we used can be formulated as:</p><formula xml:id="formula_6">Z = LayerNorm (MultiheadSelfAttention (X) + X) , X * = LayerNorm (FFN (Z) + Z) ,<label>(6)</label></formula><p>where X is the original input sequence that has not yet been added with position embedding. The position embedding will be added to X for computing querys and keys. X * is the output sequence of the current Transformer Encoder layer, as the input sequence of next encoder layer. The formulations of Multihead Self-Attention and FFN are defined in <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Detailed Results on COCO test-dev2017 Set</head><p>In Tab. 7, we show the results on COCO test-dev2017 set on AP (Average Precision) and AR (Average Recall) metrics, which are reported by the COCO official server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Architecture Details</head><p>We report the architecture details of ResNet-S and HRNet-S-W32(48) in Tab. <ref type="bibr" target="#b4">5</ref> and Tab. 6. The ResNet-S* only differs from ResNet-S in that ResNet-S* has 10 Bottleneck-c128 blocks. More details about HRNet-W32 and HRNet-W48 are described in <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ablation Study on the Size of Transformer Encoder</head><p>We analyze the effect of the size of encoder layers on the model performances, as shown in Tab. 8. For TransPose-R models, with the number of layers increas-   <ref type="bibr" target="#b47">(48)</ref>. More detailed information about the transition layer and stage blocks are described in the HR-Net paper <ref type="bibr" target="#b37">[38]</ref>.   (a) TP-R: predictions and dependencies of each keypoint for input 1.</p><formula xml:id="formula_7">ResNet-S Stem Conv-k7-s2-c64, BN, ReLU Pooling-k3-s2 Blocks 3×Bottleneck-c64 Conv-k3-s2-c128, BN 4×Bottleneck-c128 Conv-k1-s1-c256</formula><p>(b) TP-H: predictions and dependencies of each keypoint for input 1.</p><p>(c) TP-R: predictions and dependencies of each keypoint for input 2.</p><p>(d) TP-H: predictions and dependencies of each keypoints for input 2.</p><p>(e) TP-R: predictions and dependencies of each keypoint for input 3.  ing to 6, the performance improvement gradually tends to saturate or degenerate. But we have not observed such a phenomenon on TransPose-H models (base on HRNet-S-W48). We partially attribute this to the limited model capacity of TransPose-R based on the very lightweight backbone ResNet-S. And there seems to have a room to improve the performance of TransPose-H by expanding the model sizes, e.g., increasing the number of attention layers, the dimensions d, or hidden units in FFN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Gradient Analysis</head><p>As revealed by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36]</ref>, the gradient information indicates how much importance a token x j ∈ R d (feature vector) at location j affects the final prediction in the heatmap. That assumption is based on that tiny change in the input token with the most important feature value causes a large change in what the output of the model would be.</p><p>Assuming that h i ∈ R K is the scores for all K types of keypoints at location i; z i ∈ R d is the intermediate feature outputted by the last attention    layer before being fed into FFN. There is only a ReLU excluding the linear and convolutions (head) layers after the last attention layer. ReLU (rectified linear unit) activation function in FFN can be empirically regarded as the negative contribution filter, which only retains positive contributions. Next we choose numerator layout for computing the derivative of a vector with respect to a vector. We thus assume the mapping from z i to h i can be approximated as a linear function f with learned weights W f ∈ R K×d and bias b ∈ R K by computing the first-order Taylor expansion at a given point z 0 i , i.e., h i ≈ W f z i + b, W f = ∂hi ∂zi z 0 i . Then we compute the derivative of h i w.r.t the token x j at location j of the input sequence of the last attention layer:</p><formula xml:id="formula_8">∂h i ∂x j = ∂h i ∂z i ∂z i ∂x j = ∂f (z i ) ∂z i (1 + ∂w i V ∂x j ) ≈ W f (1 + ∂w i,0 v 0 + ... + w i,j v j + ... + w i,L−1 v L−1 ∂x j ) = W f (1 + ∂w i,j v j ∂x j ) = W f (1 + ∂A i,j W v x j ∂x j )<label>(7)</label></formula><p>where v j ∈ R d is the value vector transformed from v j = W v x j . A i,j is a scalar value that is computed by the dot-product between q i and k j . We assume G := ∂hi ∂xj as a function w.r.t. a given attention score A i,j , then: </p><formula xml:id="formula_9">G (A i,j ) = W f (1 + ∂A i,j W v x j ∂x j ) = W f 1 + A i,j W v = A i,j W f W v + W f = A i,</formula><formula xml:id="formula_10">= A i,j · K + B<label>(8)</label></formula><p>where K, B ∈ R K×d are static weights shared across all positions. We can see that how x j affects h i could be approximated as a linear combination of dynamic weights -attention score A i,j and learned static weights, i.e., the subsequent layers almost linearly and equally transform the attention scores from tokens across all the positions. The last attention layer in Transformer Encoder, whose attention scores are seen as the image-dependent weights, has the most direct effect on the predictions. Though the weights in FFN or head cannot be ignored, they are image-independent during inference and shared across all locations. Note that Q = (X + P) W q , K = (X + P) W k , V = XW v where P is the position embedding. Because A i,j ∝ Q i K j , the position embedding values also affect the attention score to some extent. This section is associated with Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H More Attention Maps Visualizations</head><p>In this section, we show more visualization results of the attention maps from TP-R (TransPose-R) and TP-H (TransPose-H-S) models. The attention maps of the last attention layers of two models are shown in <ref type="figure" target="#fig_8">Fig. 10</ref>. The attention maps in different attention layers of two models are shown in <ref type="figure" target="#fig_8">Fig. 11</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>CNN vs. Attention. Left: The receptive filed enlarges in the deeper convolutional layer. Right: One self-attention layer can capture the pairwise relationship between any pair of locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The TransPose model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( b )</head><label>b</label><figDesc>TP-H: predictions and dependencies of each keypoint for input A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( c )</head><label>c</label><figDesc>TP-R: predictions and dependencies of each keypoint for input B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( d )</head><label>d</label><figDesc>TP-H: predictions and dependencies of each keypoint for input B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(e) TP-R: predictions and dependencies of each keypoint for input C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(f) TP-H: predictions and dependencies of each keypoints for input C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( g )</head><label>g</label><figDesc>TP-R: predictions and dependencies of each keypoint for input D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( h )</head><label>h</label><figDesc>TP-H: predictions and dependencies of each keypoints for input D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>In each sub-figure, the first one is the original input plotted with predicted skeleton. The other maps are the unfolded attention maps in the final layer of TP-R and TP-H inspected by the predicted locations of keypoints, each position of which is annotated by a white pentagram. All the maps are visualized by the unnormalized attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random Dependency areas for the particular positions in the different layers of TP-R for input E. eye(l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random Dependency areas for the particular positions in the different layers of TP-H for input E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) TP-H: predictions and affected areas for Input F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Affected areas for the particular positions in the different depths of attention layer.MethodInput size Params FLOPs AP AP0.5 AP0.75 AP M AP L AR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Generalization performances on unseen input resolutions. Simple-Baseline and TransPose without Position Embedding (PE) generalize worse than TranPose with PE obviously. from COCO val dataset and use the trained models TP-R (TP-R-A4 model) and TP-H (TransPose-H-S model) with 75.1AP and 76.1 AP performances as exemplars, to make qualitative analysis by controlling variables.The ranges of dependencies for predictions are longer in the model with higher performance. The attention map in the final attention layer directly reflects the dependencies for predicting the keypoints locations. For example, comparing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>py,:) = sin 2π * p y /(H * 10000 2i/ d 2 ) , P E (2i+1,py,:) = cos 2π * p y /(H * 10000 2i/ d 2 ) , P E (2i,:,px) = sin 2π * p x /(W * 10000 2i/ d 2 ) , P E (2i+1,:,px) = cos 2π * p x /(W * 10000 2i/ d 2 ) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Method</head><label></label><figDesc>Input size Params FLOPs AP AP 0.5 AP 0.75 AP M AP L AR AR 0.5 AR 0.75 AR M AR L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>( f )</head><label>f</label><figDesc>TP-H: predictions and dependencies of each keypoints for input 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 10 :</head><label>10</label><figDesc>In each sub-figure, the first one is the original input plotted with predicted skeleton. The other maps are the unfolded attention maps in the last layer of TP-R and TP-H inspected by the predicted locations of keypoints, each position of which is annotated by a white pentagram. All the maps are visualized by the unnormalized attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) TP-R: predictions and dependency areas of each keypoint in different attention layers. eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random TP-H: predictions and dependency areas of each keypoint in different attention layers. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) TP-R: predictions and dependency areas of each keypoint in different attention layers. eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random TP-H: predictions and dependency areas of each keypoint in different attention layers. eye(l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random 0.5 1.0 (e) TP-R: predictions and dependency areas of each keypoint in different attention layers. eye(l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random 0.5 1.0 (f) TP-H: predictions and dependency areas of each keypoint in different attention layers. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) TP-R: predictions and affected areas of each keypoint in different attention layers. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) TP-H: predictions and affected areas of each keypoint in different attention layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 11 :</head><label>11</label><figDesc>Dependency areas and Affected areas for different input images. Model #Layers d h Params FLOPs FPS AP AR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Architecture configurations for different TransPose models. More details about the backbones are described in Appendix.</figDesc><table><row><cell>Model Name</cell><cell cols="2">Backbone Resolution for Attention</cell><cell>Upsampling</cell><cell cols="3">#layers heads d h Params</cell></row><row><cell>TransPose-R*</cell><cell>ResNet-S*</cell><cell>1/8</cell><cell>Bilinear Interpolation</cell><cell>3</cell><cell cols="2">8 256 512 5.0M</cell></row><row><cell>TransPose-R-A3</cell><cell>ResNet-S</cell><cell>1/8</cell><cell>Deconvolution</cell><cell>3</cell><cell cols="2">8 256 1024 5.2M</cell></row><row><cell>TransPose-R-A4</cell><cell>ResNet-S</cell><cell>1/8</cell><cell>Deconvolution</cell><cell>4</cell><cell cols="2">8 256 1024 6.0M</cell></row><row><cell cols="2">TransPose-H-S HRNet-S-W32</cell><cell>1/4</cell><cell>None</cell><cell>4</cell><cell>1</cell><cell>64 128 8.0M</cell></row><row><cell cols="2">TransPose-H-A4 HRNet-S-W48</cell><cell>1/4</cell><cell>None</cell><cell>4</cell><cell>1</cell><cell>96 192 17.3M</cell></row><row><cell cols="2">TransPose-H-A6 HRNet-S-W48</cell><cell>1/4</cell><cell>None</cell><cell>6</cell><cell>1</cell><cell>96 192 17.5M</cell></row><row><cell>Method</cell><cell></cell><cell cols="3">Input Size AP AR Params</cell><cell cols="2">FLOPs</cell><cell>FPS</cell></row><row><cell cols="2">SimpleBaseline-Res50 [46]</cell><cell cols="2">256×192 70.4 76.3 34.0M</cell><cell></cell><cell cols="2">8.9G</cell><cell>114</cell></row><row><cell cols="4">SimpleBaseline-Res50 + DarkPose 256×192 72.0 77.6 34.0M</cell><cell></cell><cell cols="2">8.9G</cell><cell>114</cell></row><row><cell>TransPose-R*</cell><cell></cell><cell cols="5">256×192 71.5 76.9 5.0M (↓85%) 5.4G 137 (↑20%)</cell></row><row><cell cols="2">TransPose-R-A3</cell><cell cols="5">256×192 71.7 77.1 5.2M (↓85%) 8.0G 141 (↑23%)</cell></row><row><cell cols="2">TransPose-R-A4</cell><cell cols="5">256×192 72.6 78.0 6.0M (↓82%) 8.9G 138 (↑21%)</cell></row><row><cell cols="2">HRNet-W32 [38]</cell><cell cols="2">256×192 74.4 73.7 28.5M</cell><cell></cell><cell cols="2">7.2G</cell><cell>28</cell></row><row><cell cols="2">HRNet-W32 + DarkPose [50]</cell><cell cols="2">256×192 75.6 76.7 28.5M</cell><cell></cell><cell cols="2">7.2G</cell><cell>28</cell></row><row><cell cols="2">HRNet-W48 [38]</cell><cell cols="2">256×192 75.1 80.4 63.6M</cell><cell></cell><cell cols="2">14.6G</cell><cell>27</cell></row><row><cell>TransPose-H-S</cell><cell></cell><cell cols="5">256×192 74.2 78.0 8.0M (↓72%) 10.2G 45 (↑61%)</cell></row><row><cell cols="2">TransPose-H-A4</cell><cell cols="5">256×192 75.3 80.3 17.3M (↓73%) 17.5G 41 (↑52%)</cell></row><row><cell cols="2">TransPose-H-A6</cell><cell cols="5">256×192 75.8 80.8 17.5M (↓73%) 21.8G 38 (↑41%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on COCO validation set, provided with the same detected human boxes. TransPose-R, TransPose-H-S, and TransPose-H-A* achieve competitive results to SimpleBaseline, HRNet, and HRNet-DarkPose, with fewer parameters and faster speeds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>shows the results on COCO test set.</figDesc><table><row><cell cols="2">Position Embedding Params FLOPs AP</cell></row><row><cell></cell><cell>4.999M 7.975G 70.4</cell></row><row><cell>Learnable</cell><cell>5.195M 7.976G 70.9</cell></row><row><cell>2D Sine (Fixed)</cell><cell>5.195M 7.976G 71.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results for different position embedding schemes for TransPose models. The input size is 256 × 192.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with state-of-the-art on COCO test-dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">: The detailed configurations for ResNet-S. Conv-k7-s2-c64 means</cell></row><row><cell cols="2">a convolutional layer with 7×7 kernel size, 2 stride, and 64 output chan-</cell></row><row><cell cols="2">nels, followed by a BN and ReLU; the same below. The Bottleneck-c64</cell></row><row><cell cols="2">includes Conv-k1-s1-c64-BN-ReLU, Conv-k3-s1-c64-BN-ReLU, and Conv-k1-</cell></row><row><cell cols="2">s1-c256-BN. Bottleneck-c128 includes Conv-k1-s1-c128-BN-ReLU, Conv-k3-s1-</cell></row><row><cell cols="2">c128-BN-ReLU, and Conv-k1-s1-c512-BN. See details in [17].</cell></row><row><cell>Backbone</cell><cell>HRNet-S-W32(48)</cell></row><row><cell></cell><cell>Conv-k3-s2-c64, BN, ReLU</cell></row><row><cell>Stem</cell><cell>Conv-k3-s2-c64, BN, ReLU</cell></row><row><cell></cell><cell>4×Bottleneck-c64</cell></row><row><cell></cell><cell>transition1∼stage2</cell></row><row><cell>Blocks</cell><cell>transition2∼stage3</cell></row><row><cell></cell><cell>Conv-k1-s1-c64(92)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The detailed configurations for HRNet-S-W32</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparisons with state-of-the-art on COCO test-dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on the hyperparameters of Transformer Encoder. #Layers, d and h are the number of encoder layers, the dimensions d, and the number of hidden units of FFN.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In fact, a 1×1 convolution is completely equivalent to a position-wise linear layer that reduces the dimension of vector in each position of the output sequence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here we consider single-head self attention. For multi-head self-attention, the attention matrix is the average of attention maps in all heads.<ref type="bibr" target="#b2">3</ref> We assume that the used convolutions are responsible to extract feature in a limited local patch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://cocodataset.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A What position information has been learned in the TransPose model with learnable position embedding?</head><p>We show what position information has been learned in the TransPose (TransPose-R) with learnable position embedding. It has been discussed in the Section 4.2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scalenas: One-shot learning of scale-aware representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Pai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14584,2020.5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5669" to="5678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1341</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3449" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Autopose: Searching multi-scale branch aggregation for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07018</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<idno>ICLR, 2020. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9605" to="9616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Evopose2d: Pushing the boundaries of 2d human pose estimation using neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08446,2020.5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Feature visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<ptr target="https://distill.pub/2017/feature-visualization.3" />
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anselm Levskaya, and Jonathon Shlens. Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Explainable AI: interpreting, explaining and visualizing deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer Nature</publisher>
			<biblScope unit="volume">11700</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop Poster)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07068</idno>
		<title level="m">Pose neural fabrics search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Efficientpose: Efficient human pose estimation with neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07086</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
