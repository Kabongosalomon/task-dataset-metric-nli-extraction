<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image Depth Estimation Trained via Depth from Defocus Cues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
							<email>shir.gur@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Lior Wolf Facebook AI Research</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single Image Depth Estimation Trained via Depth from Defocus Cues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating depth from a single RGB images is a fundamental task in computer vision, which is most directly solved using supervised deep learning. In the field of unsupervised learning of depth from a single RGB image, depth is not given explicitly. Existing work in the field receives either a stereo pair, a monocular video, or multiple views, and, using losses that are based on structure-from-motion, trains a depth estimation network. In this work, we rely, instead of different views, on depth from focus cues. Learning is based on a novel Point Spread Function convolutional layer, which applies location specific kernels that arise from the Circle-Of-Confusion in each image location. We evaluate our method on data derived from five common datasets for depth estimation and lightfield images, and present results that are on par with supervised methods on KITTI and Make3D datasets and outperform unsupervised learning approaches. Since the phenomenon of depth from defocus is not dataset specific, we hypothesize that learning based on it would overfit less to the specific content in each dataset. Our experiments show that this is indeed the case, and an estimator learned on one dataset using our method provides better results on other datasets, than the directly supervised methods.</p><p>Our method relies on a novel Point Spread Function (PSF) layer, which preforms a local operation over an image, with a location dependent kernel which is computed "on-the-fly", according to the estimated parameters of the PSF at each location. More specifically, the layer receives three inputs: an all-in-focus image, estimated depth-map and camera parameters, and outputs an image at one specific focus. This image is then compared to the training images to compute a loss. Both the forward and backward operations of the layer are efficiently computed using a dedicated CUDA kernel. This layer is then used as part of a novel architecture, combining the successful ASPP architecture <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>. To improve the ASPP block, we add dense connections [16], followed by self-attention <ref type="bibr" target="#b41">[42]</ref>.</p><p>We evaluate our method on all relevant benchmarks we were able to obtain. These include the flower lightfield dataset and the multifocus indoor and outdoor scene dataset, for which we compare the ability to generate unseen focus arXiv:2001.05036v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In classical computer vision, many depth cues were used in order to recover depth from a given set of images. These shape from X methods include structure-frommotion, which is based on multi-view geometry, shape from structured light, in which the known light source plays the role of an additional view, shape from shadow, and most relevant to our work, shape from defocus. In machine learning based computer vision, the interest has mostly shifted into depth from a single image, treating the problem as a multivariant image-to-depth regression problem, with an additional emphasis on using deep learning.</p><p>Learning depth from a single image consists of two forms. There are supervised methods, in which the target in-formation (the depth) is explicitly given, and unsupervised methods, in which the depth information is given implicitly. The most common approach in unsupervised learning is to provide the learning algorithm with stereo pairs or other forms of multiple views <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref>. In these methods, the training set consists of multiple scenes, where for each scene, we are given a set of views. The output of the method, similar to the supervised case, is a function that given a single image, estimates depth at every point.</p><p>In this work, we rely, instead of multiple view geometry, on shape from defocus. The input to our method, during training, is an all-in-focus image and one or more focused images of the same scene from the same viewing point. The algorithm then learns a regression function, which, given an all-in-focus image, estimates depth by reconstructing the given focused images. In classical computer vision, research in this area led to a variety of applications <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32]</ref>, such as estimating depth from mobile phone images <ref type="bibr" target="#b32">[33]</ref>. A deep learning based approach was presented by Anwar et al. <ref type="bibr" target="#b0">[1]</ref> who employ synthetic focus images in supervised depth learning, and an aperture supervision depth learning by Srinivasan et al. <ref type="bibr" target="#b30">[31]</ref>, who employ lightfield images in the same way we use defocus images.</p><p>images with other methods. We also evaluate on the KITTI, NYU, and Make3D, which are monocular depth estimation datasets. In all cases, we show an improved performance in comparison to methods with a similar level of supervision, and performance that is on par with the best directly supervised methods on KITTI and Make3D datasets. We note that our method uses focus cues for depth estimation, hence the task of defocusing for itself is not evaluated.</p><p>When learning depth from a single image, the most dominant cue is often the content of the image. For example, in street view images one can obtain a good estimate of the depth based on the type of object (sidewalk, road, building, car) and its location in the image. We hypothesize that when learning from focus data, the role of local image statistics becomes more dominant, and that these image statistics are more global between different visual domains. We therefore conduct experiments in which a depth estimator trained on one dataset is evaluated on another. Our experiments show a clear advantage to our method, in comparison to the stateof-the-art supervised monocular method of <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning based monocular depth estimation In monocular depth estimation, a single image is given as input, and the output is the predicted depth associated with that image. Supervised training methods learn from the ground truth depth directly and the so-called unsupervised methods employ other data cues, such as stereo image pairs. One of the first methods in the field was presented by Saxena et al. <ref type="bibr" target="#b26">[27]</ref>, applying supervised learning and proposed a patchbased model and Markov Random Field (MRF). Following this work, a variety of approaches had been presented using hand crafted representations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11]</ref>. Recent methods use convolutional neural networks (CNN), starting from learning features for a conditional random field (CRF) model as in Liu et al. <ref type="bibr" target="#b21">[22]</ref>, to learning end-to-end CNN models refined by CRFs, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Many models employ an autoencoder structure <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9]</ref>, with an added advantage to very deep networks that employ ResNets <ref type="bibr" target="#b14">[15]</ref>. Eigen et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> showed that using multi-scaled depth predictions helps with the decrease in spatial resolution, which happened in the encoder model, and improves depth estimation. Other work uses different loss for regression, such as the reversed Huber <ref type="bibr" target="#b23">[24]</ref> used by Laina et al. <ref type="bibr" target="#b18">[19]</ref> to lower the smoothness effect of the L 2 norm, and the recent work by Fu et al. <ref type="bibr" target="#b8">[9]</ref> who uses ordinal regression for each pixel with their spacingincreasing discretization (SID) strategy to discretize depth. Unsupervised depth estimation Modern methods for unsupervised depth estimation have relied on the geometry of the scene, Garg et al. <ref type="bibr" target="#b11">[12]</ref> for example, proposed using stereo pairs for learning, introducing the differentiable inverse warping. Godard et al. <ref type="bibr" target="#b13">[14]</ref> added the Left-Right consistency constraint to the loss function, exploiting another geometrical cue. Zhou et al. <ref type="bibr" target="#b42">[43]</ref> learned, in addition the ego-motion of the scene, and GeoNet <ref type="bibr" target="#b40">[41]</ref> also used the optical flow of the scene. Wang et al. <ref type="bibr" target="#b36">[37]</ref> recently showed that using direct visual odometry along with depth normalization substantially improves performance on prediction.</p><p>Depth from focus/defocus The difference between depth from focus and depth from defocus is that, in the first case, camera parameters can be changed during the depth estimation process. In the second case, this is not allowed. Unlike the motion based methods above, these methods obtain depth using the structure of the optical geometry of the lens and light ray, as described in Sec. 3.1. Work in this field mainly focuses on analytical techniques. Zhuo et al. <ref type="bibr" target="#b43">[44]</ref> for example, estimated the amount of spatially varying defocus blur at edge locations. The use of Coded Aperture had been proposed by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30]</ref> to improve depth estimation. Later work in this field, such as Suwajanakorn et al. <ref type="bibr" target="#b32">[33]</ref>, Tang et al. <ref type="bibr" target="#b34">[35]</ref> and Surh et al. <ref type="bibr" target="#b31">[32]</ref> employed focal stacks -sets of images of the same scene with different focus distances -and estimated depth based on a variety of blurring models, such as the Ring Difference Filter <ref type="bibr" target="#b31">[32]</ref>. These methods first reconstruct an all-in-focus image and then optimize a depth map that best explains the re-rendering of the focal stack images out of the all-in-focus image.</p><p>There are not many deep learning works in the field. Srinivasan et al. <ref type="bibr" target="#b30">[31]</ref> presented a new lightfield dataset of flower images. They used the ground truth lightfield images to render focused images and employed a regression model to estimate depth from defocus by reconstruction of the rendered focused images.While Srinivasan et al. <ref type="bibr" target="#b30">[31]</ref> did not compare to other RGB-D datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23]</ref>, their method can take as input any all-in-focus image. We evaluate <ref type="bibr" target="#b30">[31]</ref> rendering process using our network on the KITTI dataset. Anwar et al. <ref type="bibr" target="#b0">[1]</ref> utilized the provided depth of those datasets to integrate focus rendering within a fully supervised depth learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Differentiable Optical Model</head><p>We review the relevant optical geometry on which our PSF layer relies and then move to the layer itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Depth From Defocus</head><p>Depth from focus methods are mostly based on the thinlens model and geometry, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The figure illustrates light rays trajectories and the blurring effect made by out-of-focus objects. The plane of focus is defined such that light rays emerging from it towards the lens fall at the same point on the camera sensor plane. An object is said to be in focus, if its distance from the lens falls inside the camera's depth-of-field (DoF), which is the distance about the plane of focus where objects appear acceptably sharp (a) Lens illustration <ref type="bibr" target="#b15">16</ref> 32  by the human eye. Objects outside the DoF appear blurred on the image plane, an effect caused by the spread of light rays coming from the unfocused objects and forming what is called the "Circle-Of-Confusion" (CoC), as marked by C in <ref type="figure" target="#fig_0">Fig. 1</ref>(a). In this paper, we will use the following terminology: an all-in-focus image is an image where all objects appear in focus, and a focused image is one where blurring effects caused by the lens configuration are observed.</p><p>In this model, we consider the following parameters to describe a specific camera: focal-length F , which is the distance between the lens plane and the point where initially parallel rays are brought to a focus, aperture A, which is the diameter of the lens (or an opening through which light travels), and the plane of focus D f (or focus distance), which is the distance between the lens plane and the plane where all points are in focus. Following the thin-lens model, we define the size of blur, i.e., the diameter of the CoC, which we denote as C mm , according to the following equation:</p><formula xml:id="formula_0">C mm = A |D o − D f | D o F D f − F<label>(1)</label></formula><p>where D o is the distance between an object to the lens plane, and A = F/N where N is what is known as the f-number of the camera. While CoC is usually measured in millimeters (C mm ), we transform its size to pixels by considering a camera pixel-size of p = 5.6µm as in <ref type="bibr" target="#b2">[3]</ref>, and a camera output scale s, which is the ratio between sensor size and output image size. The final CoC size in pixels C is computed as follows:</p><formula xml:id="formula_1">C = C mm p · s .<label>(2)</label></formula><p>The CoC is directly related to the depth, as illustrated in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, where each line represents a different focus distance D f . As can be seen, the relation is not one-to-one and will cause ambiguity in depth estimation. Moreover, different camera settings are required for different scenes in terms of the scene's maximum depth, i.e. for KITTI, we consider maximum depth of 80 meters, and 10 meters for NYU. We also consider a constant f-number of N = 2.8 and a different focal-length for all datasets, in order to lower depth ambiguity by lowering the DoF range (see Sec. 5.2 for more details).</p><p>We now refer to one more measurement named CoClimit, defined as the largest blur spot that will still be perceived by the human eye as a point, when viewed on a final image from a standard viewing distance. The CoC-limit also limits the kernel size used for rendering and is, therefore, highly influential on the run time (bigger kernels lead to more computations). We employ a kernel of size 7 × 7, which reflects a standard CoC-limit of 0.061mm.</p><p>In this work, following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, we consider the blur model to be a disc-shaped point spread function (PSF), modeled by a Gaussian kernel with radius r = C/2 and kernel's location indices u, v:</p><formula xml:id="formula_2">G(u, v, r) = 1 2πr 2 exp − u 2 + v 2 2r 2<label>(3)</label></formula><p>Because we work in pixel space, if the diameter is less then one pixel (C &lt; 1), we ignore the blurring effect. According to the above formulation, a focused image can be generated from an all-in-focus image and depth-map, as commonly done in graphics rendering. Let I be an all-infocus image and J be a rendered focused image derived from depth-map D o , CoC-map C, camera parameters A, F and D f , we define J as follows:</p><formula xml:id="formula_3">F x,y (u, v) = 2 πC 2 x,y exp − 2 u 2 + v 2 C 2 x,y (4) J x,y : = (I F ) (5) = u,v∈Ω I x−u,y−v F x−u,y−v (u, v)dudv u ,v ∈Ω F x−u ,y−v (u , v )du dv ,</formula><p>where Ω is an offsets set related to a kernel of size m × m:</p><formula xml:id="formula_4">Ω := (u, v) : u, v ∈ − m 2 , . . . , 0, . . . , m 2 ∈ N (6)</formula><p>We denote by the convolution operation with a functional kernel F, by (x, y) the image location indices, and by (u, v) the offset indices bounded by the kernel size. Based on Eq. 5, given a set of focused images of the same scene, one may optimize a model to predict the all-in-focus image and the depth map. Alternatively, given a focused image and its correspondent all-in-focus image, we predict the scene depth by reconstructing the focused image.</p><p>While <ref type="bibr" target="#b30">[31]</ref> uses a weighted sum of disk kernels to render blur, our blur kernel is a Gaussian composition of different blur contributions from all neighbors (Eq. 5) where each kernel coefficient is calculated by a Gaussian function w.r.t. a different estimated CoC, as illustrated in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The PSF Convolutional layer</head><p>The PSF layer we employ can be seen as a particular case of the locally connected layers of <ref type="bibr" target="#b33">[34]</ref>, with a few differences: first, in the PSF layer, the same operator is applied across all channels, while in the locally-connected layer, as well as in conventional layers (excluding depthconvolution <ref type="bibr" target="#b5">[6]</ref>), the local operator varies between the input channels. Additionally, The PSF layer does not sum the outcomes, and returns the same number of channels in the output tensor as in the input tensor.</p><p>The PSF convolutional layer, designed for the task of Depth from Defocus (DfD), is based on Eq. 5, where kernels vary between locations and are calculated "on-the-fly", according to function F, which is defined in Eq. 4. The kernel is, therefore, a local function of the object's distance, with a blur kernel applied to out-of-focus pixels. The layer takes as input an all-in-focus image I, depth-map D o and the camera parameters vector ρ, which contains the aperture A, the focal length F and the focal depth D f . The layer then outputs a focused image J. As mentioned before, we fix the near and far distance limits to fit each dataset and use the fixed pixel size mentioned above. The rendering process begins by first calculating the CoC-map C according to Eq. 1, and then applying the functional kernel convolution defined in Eq. 5. We implement the following operation in CUDA and compute its derivative as follows:</p><formula xml:id="formula_5">∂J s,t ∂I x,y = F x,y (u, v) u ,v ∈Ω F s−u ,t−v (u , v )du dv (7) ∂J s,t ∂C x,y = ξ x,y (u, v)(I x,y − J s,t )F x,y (u, v) u ,v ∈Ω F s−u ,t−v (u , v )du dv (8) ξ x,y (u, v) : = 4(u 2 + v 2 ) − 2C 2 x,y C 3 x,y<label>(9)</label></formula><p>A detailed explanation of the forward and backward pass is provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>In this section, we describe the training method and the model architecture, which extends the ASPP architecture to include both self-attention and dense connections. We then describe the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">General Architecture and the Training Loss</head><p>Let J be a (real-world) focused version of I, andJ be a predicted focused version of I. We train a regression model to minimize the reconstruction loss of J andJ.</p><p>We define two networks, f and g, for depth estimation and focus rendering respectively. While f is learned, g implements Eq. 4 and 5. Both networks take part in the loss, and backpropagation through g is performed using Eq. 7, 8.D The learned network f is applied to an all-in-focus image I and returns a predicted depthD o = f (I). The fixed network g consists of the PSF layer, as described in Sec. 3.2. It takes as input an all-in-focus I, a depth (estimated or not) D o and the camera parameters vector ρ. It outputs J = g(I, D o , ρ), which is a focused version of I according to depth D o and camera parameters ρ. We distinguish between a rendered focus image from ground truth depth D o which we denote as J (also used for real focused imaged), and rendered focused image from predicted depth D o , which we denote asJ = g(I,D o , ρ).</p><p>The training procedure has two cases, training with real data or on generated data, depending on the training dataset at hand. In both cases, training is performed end-to-end by running f and g sequentially. First, f is applied to an all-in-focus image I and outputs the predicted depth-map D o . Using this map, the all-in-focus image and camera parameters ρ, g renders the predicted focused imageJ. A reconstruction error is then applied with J andJ, where for the case of depth-based datasets, we render the training focused images J, according to ground truth depth-map D o and camera specifications ρ. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the training scheme, where the blue dashed rectangle illustrates the second case, where J is rendered from the ground truth depth.</p><p>In the first case, since we compare with the work of <ref type="bibr" target="#b30">[31]</ref>, we use a single focused image during training, although more can be used. In the second case, we compare with fully supervised methods, that benefit from a direct access to the depth information, and we report results for 1, 2, 6 and 10 rendered focused images. Training loss We first consider the reconstruction loss and the depth smoothness <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14]</ref> w.r.t. the input image I, the predicted focused imageJ, the focused image J, and the estimated depth mapD o :</p><formula xml:id="formula_6">L rec = 1 N α 1 − SSIM (J, J) 2 + (1 − α) J − J 1<label>(10)</label></formula><formula xml:id="formula_7">L smooth = 1 N |∂ xDo |e −|∂xI| + |∂ yDo |e −|∂yI| (11)</formula><p>where SSIM is the Structural Similarity measure <ref type="bibr" target="#b37">[38]</ref>, and α controls the balance w.r.t. to L 1 loss.  The reconstruction loss above does not take into account the blurriness in some parts of image J, which arise from regions that are out of focus. We, therefore, add a sharpness measure S(I) similar to <ref type="bibr" target="#b24">[25]</ref>, which considers the sharpness of each pixel. It contains three parts: (i) the image Laplacian ∆I := ∂ 2</p><p>x I + ∂ </p><formula xml:id="formula_8">L sharp = S(Ĵ) − S(J) 1 .<label>(12)</label></formula><p>The final loss term is then:</p><formula xml:id="formula_9">Loss = λ 1 L rec + λ 2 L smooth + λ 3 L sharp<label>(13)</label></formula><p>For all experiments, we set λ 1 = 1, λ 2 = 10 −3 , λ 3 = 10 −1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Architecture</head><p>Our network f is illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. It consists of an encoder-decoder architecture, where we rely on the DeepLabV3+ <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> model, which was found to be effective for semantic segmentation and depth estimation tasks <ref type="bibr" target="#b8">[9]</ref>. The encoder has two parts: a ResNet <ref type="bibr" target="#b14">[15]</ref> backbone and a subsequent Atrous Spatial Pyramid Pooling (ASPP) module. Unlike <ref type="bibr" target="#b8">[9]</ref>, we do not employ a pretrained ResNet and learn it end-to-end.</p><p>The Atrous convolutions (also called dilated convolutions) add padding between kernel cells to enlarge the receptive field from earlier layers, while keeping the weight size constant. ASPP contains several parallel Atrous convolutions with different dilations. As advised in <ref type="bibr" target="#b4">[5]</ref>, we also replace all pooling layers of the encoder with convolution layers with an appropriate stride.</p><p>The loss is computed in the highest resolution, to support higher quality outputs. However, to comply with GPU memory constraints, the network takes as an input, a downsampled image of half the original size. The network's output is then upsampled to the original image size.</p><p>Dense ASPP with Self-Attention The original ASPP consists of three or more independent layers -average pooling followed by 1 × 1 convolution, 1 × 1 convolution, and four Atrous layers. Each convolution layer has 256 channels and the four outputs of these layers, along with the pool+conv layer are concatenated together to form a tensor with channel size C = 1280. We propose two additional modifications from different parts of the literature: dense connections <ref type="bibr" target="#b15">[16]</ref> and self attention <ref type="bibr" target="#b41">[42]</ref>.</p><p>We add dense connections between the 1×1 convolution and all Atrous convolution layers of the ASPP module, sequentially connecting all layers from smallest to the largest dilation layer. Each layer, therefore, receives as the input tensor not just the output of the previous layer, but the concatenation of the output tensors of all preceding layers. This is illustrated as the skip connection arrows in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>Self-Attention aims to integrate local features with their global dependencies, and as shown in previous work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b9">10]</ref>, it improve results in image segmentation and generation. Our implementation is based on <ref type="bibr" target="#b9">[10]</ref> dual-attention.</p><p>The decoder part of f consists of three upsampling blocks, each having three convolution layers followed by bilinear upsampling. A skip connection from a low level layer of the backbone is concatenated with the input of the second block. The output of decoder is the predicted depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We divide our experiments into two types, DoF supervision and DoF supervision from rendered data, as mentioned in the previous section. We further experiment with cross domain evaluation, where we evaluate our method in comparison to the state-of-the-art supervised method <ref type="bibr" target="#b8">[9]</ref>. Here the models are trained on domain A and tested on domain B, denoted as A → B. We show that learning depth from focus cues, though not achieving better results than the supervised methods -but comparable with top methods in KITTI and Make3D datasets, achieves better generalization expressed by higher results in cross domain evaluation.</p><p>The network is trained on a single Titan-X Pascal GPUs with batch size of 3, using Adam for optimization with a learning rate of 2 · 10 −5 and weight decay of 4 · 10 −5 . The dedicated CUDA implementation of the PSF layer runs x80 faster than the optimized pytorch implementation.</p><p>The following five benchmarks are used: Lightfield dataset <ref type="bibr" target="#b30">[31]</ref> The dataset contains lightfield flowers and plants images, taken with a Lytro Illum camera. From the lightfield images, we follow the procedure of <ref type="bibr" target="#b30">[31]</ref> to generate the all-in-focus and shallow DoF images, and split the dataset into 3143 and 300 images for train and test. DSLR dataset <ref type="bibr" target="#b2">[3]</ref> This dataset contains 110 images and ground truth depth from indoor scenes, with 81 images for training and 29 images for testing, and 34 images from outdoor scenes without ground truth depth. Each scene is ac- Make3D <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> The Make3D benchmark contains 534 RGB-depth pairs, split into 400 pairs for training and 134 for testing. The input images are provided at a high resolution, while the depth-maps are at low resolution. Therefore, data is resized to 460 × 345, as proposed by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Following <ref type="bibr" target="#b26">[27]</ref>, results are evaluated in two settings: C1 for depth cap of 0-70, and C2 for depth cap 0-80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DoF supervision</head><p>We first report results on the Lightfield dataset dataset, which provides focused and all-in-focus image pairs with no ground truth depth. The performance is evaluated using the PSNR and SSIM measures. Our results are shown in Tab. 1. As can be seen, we significantly outperform the literature baselines provided by <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rendered DoF supervision</head><p>For rendered DoF supervision, we consider four datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b2">3]</ref> with ground truth depth, where we render focused images with different focus distances. We denote by F1, F2, F6, F10 the four training setups, which differ by the number of rendered focused images used in training. The order in which focal distances are selected, is defined by the following focal sequence [0.2, 0.8, 0.1, 0.9, 0.3, 0.7, 0.4, 0.6, 0.5, 0.35], where each number represents the percent of the maximum depth used for each dataset. For example, F2 employs focal distances of 0.2 and 0.8 times the maximal depth.</p><p>We perform two types of evaluations. First, we evalu-ate our method for each dataset with different numbers of focused images during training, and compare our results with other unsupervised methods, as well as with supervised ones. The evaluation measures are those commonly used in the literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> and include various RMSE measures and a thresholded error rate. Tab. 2 and 3 show that our method outperforms monocular and stereo supervision methods on the KITTI and Make3D dataset. This also holds when the previous methods are trained with additional data obtained from the Cityscapes dataset. In comparison to the depth supervised methods, we outperform all methods on KITTI, with the exception of <ref type="bibr" target="#b8">[9]</ref>, and outperform <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref> on Make3D. In <ref type="figure">Fig. 4</ref>, we present qualitative results of our method compared to the state-of-the-art unsupervised method <ref type="bibr" target="#b36">[37]</ref> on the KITTI dataset. As can be seen in Tab. 4, there are no literature unsupervised methods reported for the NYU dataset, where we are slightly outperformed by the supervised methods. We next preform cross domain evaluation compared to the published models of the state-of-the-art supervised method <ref type="bibr" target="#b8">[9]</ref>, where training is performed on KITTI or NYU, and tested on different datasets. These tests are meant to evaluate the specificity of the learned network to a particular dataset. Since the absolute depth differs between datasets, we evaluate the methods by computing the Pearson correlation metric. Results are shown in Tab. 5. As can be seen, when transferring from both KITTI and NYU, we outperform the directly supervised method. The gap is especially visible for the NYU network.</p><p>We also provide cross-domain results for the outdoor images of the DSLR dataset, where no ground truth depth is provided, using the PSNR and SSIM metrics. Tab. 6 shows in this case that our method transfers better from NYU and only slightly better from KITTI in comparison to <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>The Effect of Focal Distance Because the focus distance D f and DoF range are positively correlated, training with a far focus distance increases the DoF and puts a large range of distances in focus. As a result, focus cues are lowered, causing performance to decrease. In <ref type="figure" target="#fig_5">Fig. 5</ref> we present, for the Make3D dataset, the accuracy of F1 training with different focus distances, where a clear decrease in performance is seen at mid-range D f and an increase afterward, as a result of the dataset maximum depth, capping the far DoF distance, i.e. lowering the DoF range, and increasing focus cues for closer objects. Dense ASPP with Self-Attention We evaluate our dense ASPP with self-attention in comparison to three versions of the original ASPP model: vanilla ASPP, ASPP with dense connections and ASPP with self-attention. In order to differentiate between different ambiguity scenarios, training is preformed with the F1, F2, F6 and F10 methods. As can be <ref type="figure">Figure 4</ref>: KITTI: Qualitative results on the KITTI Eigen Split. All images are cropped to the valid depth region as proposed in <ref type="bibr" target="#b7">[8]</ref>. From left to right, reference image and ground truth, Wang et al. <ref type="bibr" target="#b36">[37]</ref> and ours.      <ref type="bibr" target="#b30">[31]</ref>). From Tab. 8, the compositional method of <ref type="bibr" target="#b30">[31]</ref> preforms poorly on KITTI in the F1 and F2 setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a method for learning to estimate depth from a single image, based on focus cues. Our method outperforms the similarly supervised method <ref type="bibr" target="#b30">[31]</ref> and all other unsupervised literature methods. In most cases, it matches the performance of directly supervised methods, when evaluated on test images from the training domain. Since focus cues are more generic than content cues, our method outperforms the state-of-the-art supervised method in cross domain evaluation on all available literature datasets.</p><p>We introduce a differentiable PSF convolutional layer, which propagates image based losses back to the estimated depth. We also contribute a new architecture that introduces dense connection and Self-Attention to the ASPP module. Our code is available as part of the supplementary material, and on GitHub https://github.com/      <ref type="figure">Figure 6</ref>: Illustration of the PSF convolution layer for focus rendering. each kernel is spatially computed from the CoC map and multiplied with the corespondent image patch both channel and element wise.</p><p>In this appendix we show the mathematical development of our PSF layer for backward and forward pass. For easy notation we define:</p><formula xml:id="formula_10">M x−i,y−j := M [ x − i, y − j]</formula><p>where M is any matrix or function, (x, y) are location indices and (u, v) are offsets. An illustration of the the PSF convolution is shown in <ref type="figure">Fig. 6</ref>. Forward Pass: For a PSF kernel of size m × m where m is odd, we consider an offsets set Ω of the form:</p><formula xml:id="formula_11">Ω := (u, v) : u, v ∈ [− m 2 , . . . , 0, . . . , m 2 ] ∈ N</formula><p>The forward pass is calculate by the following equations, where I is the input image, J is the output image and C is the CoC map:   </p><p>Note that we consider only C where (s−u , t−v ) = (x, y), where in these locations the offset for F is the opposite of (u, v) meaning (−u, −v), but because we only calculate their second power we can consider F x,y (u, v):</p><p>∂φ ∂C x,y = −2 · 2I x,y πC 3</p><p>x,y exp − 2 u 2 + v 2 C 2</p><p>x,y</p><formula xml:id="formula_13">+ 2 · 2(u 2 + v 2 ) C 3 xy 2I x,y πC 2 xy exp − 2 u 2 + v 2 C 2<label>(9)</label></formula><p>x,y = I x,y exp − 2 u 2 + v 2 C 2</p><p>x,y 2 πC 2</p><p>x,y · 4(u 2 + v 2 ) − 2C 2</p><p>x,y C 3</p><p>x,y</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Illustration of lens principles. Blue beams represent an object in focus. Red beams represent an object further away and out of focus. See text for symbol definitions. (b) CoC diameter w.r.t. object distance as seen in KITTI. Camera settings are: N = 2.8, F = 35, and s = 2. (c) Sample blur kernel. Green line represents depth edge, Blue colors represent the relative blur contribution w.r.t. CoC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Training scheme. Blue region represents the rendering branch, which is used for depth-based datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Dense ASPP with an added attention block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2 y I, (ii) the image Contrast Visibility C(I) := I−µ I µ I , and (iii) the image Variance V (I) := (I − µ I ) 2 , where µ I is the average pixel value in a window of size 7 × 7 pixels. The sharpness measure is given by S(I) = −∆I − C(I) − V (I), and the loss term is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(a) δ&lt;1.25, higher is better, for training F1 with different focus distance. (b) RMSE, lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>F</head><label></label><figDesc>I x−u,y−v F x−u,y−v (u, v)dudv u ,v ∈Ω F x−u ,y−v (u , v )du dv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F</head><label></label><figDesc>s−u ,t−v (u , v )du dv (4)Secondly compute derivative of loss L w.r.t. CoC map C at location (x, y):I s−u ,t−v F s−u ,t−v (u , v )du dv (7) ψ : = 1 u ,v ∈Ω F s−u ,t−v (u , v )du dv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AlgorithmSupervision Abs Rel Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>Godard et al. [14]</cell><cell>S</cell><cell>0.148</cell><cell>1.344</cell><cell>5.927</cell><cell>0.247</cell><cell>0.803</cell><cell>0.922</cell><cell>0.964</cell></row><row><cell>Geonet-ResNet [41]</cell><cell>M</cell><cell>0.155</cell><cell>1.296</cell><cell>5.857</cell><cell>0.233</cell><cell>0.793</cell><cell>0.931</cell><cell>0.973</cell></row><row><cell>Wang et al. [37]</cell><cell>M</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell>0.810</cell><cell>0.936</cell><cell>0.974</cell></row><row><cell>Godard et al. [14]</cell><cell>S(K+CS)</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>Ours F1</cell><cell>DoF</cell><cell>0.141</cell><cell>1.473</cell><cell>5.187</cell><cell>0.221</cell><cell>0.846</cell><cell>0.953</cell><cell>0.981</cell></row><row><cell>Ours F2</cell><cell>DoF</cell><cell>0.129</cell><cell>0.722</cell><cell>4.233</cell><cell>0.183</cell><cell>0.856</cell><cell>0.960</cell><cell>0.985</cell></row><row><cell>Ours F6</cell><cell>DoF</cell><cell>0.114</cell><cell>0.671</cell><cell>4.144</cell><cell>0.172</cell><cell>0.867</cell><cell>0.963</cell><cell>0.987</cell></row><row><cell>Ours F10</cell><cell>DoF</cell><cell>0.110</cell><cell>0.666</cell><cell>4.186</cell><cell>0.168</cell><cell>0.880</cell><cell>0.966</cell><cell>0.988</cell></row><row><cell>Liu et al. [22]</cell><cell>Depth</cell><cell>0.202</cell><cell>1.614</cell><cell>6.523</cell><cell>0.275</cell><cell>0.678</cell><cell>0.895</cell><cell>0.965</cell></row><row><cell>Kuznietsov et al. [17]</cell><cell>Depth</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell>DORN et al. [9]</cell><cell>Depth</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>KITTI: Quantitative results on the KITTI Eigen split. Top -Unsupervised methods where 'S' and 'M' stands for stereo and video (monocular) supervision, and 'K+CS' stands for training with the added data from the CityScapes dataset. Middle -Our method. Bottom -Supervised methods.</figDesc><table><row><cell>Algorithm</cell><cell>Supervision</cell><cell cols="6">C1 Abs Rel RMSE log10 RMSE Abs Rel RMSE log10 RMSE C2</cell></row><row><cell>Godard et al. [14]</cell><cell>S</cell><cell>0.443</cell><cell>0.156</cell><cell>11.513</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhou et al. [43]</cell><cell>MS</cell><cell>0.383</cell><cell>0.478</cell><cell>10.470</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et al. [37]</cell><cell>MS</cell><cell>0.387</cell><cell>0.204</cell><cell>8.090</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours F1</cell><cell>DoF</cell><cell>0.568</cell><cell>0.192</cell><cell>8.822</cell><cell>0.575</cell><cell>0.195</cell><cell>10.147</cell></row><row><cell>Ours F2</cell><cell>DoF</cell><cell>0.287</cell><cell>0.116</cell><cell>7.710</cell><cell>0.294</cell><cell>0.121</cell><cell>9.387</cell></row><row><cell>Ours F6</cell><cell>DoF</cell><cell>0.262</cell><cell>0.109</cell><cell>7.474</cell><cell>0.269</cell><cell>0.115</cell><cell>9.248</cell></row><row><cell>Ours F10</cell><cell>DoF</cell><cell>0.246</cell><cell>0.110</cell><cell>7.671</cell><cell>0.254</cell><cell>0.116</cell><cell>9.494</cell></row><row><cell>Li et al. [21]</cell><cell>Depth</cell><cell>0.278</cell><cell>0.092</cell><cell>7.120</cell><cell>0.279</cell><cell>0.102</cell><cell>10.27</cell></row><row><cell>MS-CRF [40]</cell><cell>Depth</cell><cell>0.184</cell><cell>0.065</cell><cell>4.380</cell><cell>0.198</cell><cell>-</cell><cell>8.56</cell></row><row><cell>DORN [9]</cell><cell>Depth</cell><cell>0.157</cell><cell>0.062</cell><cell>3.970</cell><cell>0.162</cell><cell>0.067</cell><cell>7.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Make3D: Quantitative results on Make3D<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> dataset. Top -Unsupervised methods where 'S' and 'M' stands for stereo and video (monocular) supervision. Middle -Our method. Bottom -Supervised methods.</figDesc><table><row><cell>Algorithm</cell><cell cols="7">Supervision Abs Rel RMSE log10 RMSE δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</cell></row><row><cell>Ours F1</cell><cell>DoF</cell><cell>0.254</cell><cell>0.092</cell><cell>0.766</cell><cell>0.691</cell><cell>0.880</cell><cell>0.944</cell></row><row><cell>Ours F2</cell><cell>DoF</cell><cell>0.162</cell><cell>0.068</cell><cell>0.574</cell><cell>0.774</cell><cell>0.941</cell><cell>0.984</cell></row><row><cell>Ours F6</cell><cell>DoF</cell><cell>0.149</cell><cell>0.063</cell><cell>0.546</cell><cell>0.797</cell><cell>0.951</cell><cell>0.987</cell></row><row><cell>Ours F10</cell><cell>DoF</cell><cell>0.162</cell><cell>0.068</cell><cell>0.575</cell><cell>0.772</cell><cell>0.942</cell><cell>0.984</cell></row><row><cell>Li et al. [21]</cell><cell>Depth</cell><cell>0.143</cell><cell>0.063</cell><cell>0.635</cell><cell>0.788</cell><cell>0.958</cell><cell>0.991</cell></row><row><cell>MS-CRF [40]</cell><cell>Depth</cell><cell>0.121</cell><cell>0.052</cell><cell>0.586</cell><cell>0.811</cell><cell>0.954</cell><cell>0.987</cell></row><row><cell>DORN [9]</cell><cell>Depth</cell><cell>0.115</cell><cell>0.051</cell><cell>0.509</cell><cell>0.828</cell><cell>0.965</cell><cell>0.992</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Transition</cell><cell>Algorithm</cell><cell>Correlation</cell></row><row><cell></cell><cell>DORN [9]</cell><cell>0.423 ± 0.010</cell></row><row><cell>KITTI → NYU</cell><cell>Ours F1</cell><cell>0.121 ± 0.006</cell></row><row><cell></cell><cell>Ours F10</cell><cell>0.429 ± 0.009</cell></row><row><cell></cell><cell>DORN [9]</cell><cell>0.616 ± 0.011</cell></row><row><cell>KITTI → Make3D</cell><cell>Ours F1</cell><cell>0.484 ± 0.019</cell></row><row><cell></cell><cell>Ours F10</cell><cell>0.642 ± 0.014</cell></row><row><cell></cell><cell>DORN [9]</cell><cell>0.145 ± 0.048</cell></row><row><cell>KITTI → D3Net</cell><cell>Ours F1</cell><cell>0.148 ± 0.032</cell></row><row><cell></cell><cell>Ours F10</cell><cell>0.275 ± 0.054</cell></row><row><cell>NYU → KITTI</cell><cell>DORN [9] Ours F1</cell><cell>0.456 ± 0.006 0.567 ± 0.006</cell></row><row><cell></cell><cell>Ours F10</cell><cell>0.634 ± 0.005</cell></row><row><cell>NYU → Make3D</cell><cell>DORN [9] Ours F1</cell><cell>0.250 ± 0.019 0.249 ± 0.032</cell></row><row><cell></cell><cell>Ours F10</cell><cell>0.456 ± 0.022</cell></row><row><cell></cell><cell>DORN [9]</cell><cell>0.260 ± 0.054</cell></row><row><cell>NYU → D3Net</cell><cell>Ours F1</cell><cell>0.530 ± 0.048</cell></row><row><cell></cell><cell>Ours F10</cell><cell>0.434 ± 0.052</cell></row></table><note>NYU: Quantitative results on NYU V2 [23] dataset. Top -Our method. Bottom -Supervised methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results for cross domain evaluation. Models are trained on domain A and tested on domain B. Reported numbers are mean ± standard error. seen in Tab 7, our model outperform the different ASPP versions. However, as the number of focused images increases, the gaps are reduced.</figDesc><table><row><cell>Different rendering methods To further compare</cell></row><row><cell>with [31], we have conducted a test on the KITTI dataset,</cell></row><row><cell>where we replaced our rendering network g with their</cell></row><row><cell>compositional rendering, and modified our depth network</cell></row><row><cell>f 's last layer to output 80 depth probabilities (similar</cell></row><row><cell>to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results on the outdoor DSLR<ref type="bibr" target="#b2">[3]</ref> test set, reported as mean value of PSNR and SSIM of the reconstructed focused image.</figDesc><table><row><cell>Model</cell><cell>F1</cell><cell>F2</cell><cell>F6</cell><cell>F10</cell></row><row><cell>ASPP</cell><cell>5.412</cell><cell>4.422</cell><cell>4.311</cell><cell>4.194</cell></row><row><cell>ASPP + D</cell><cell>5.285</cell><cell>4.351</cell><cell>4.170</cell><cell>4.190</cell></row><row><cell>ASPP + SA</cell><cell>5.387</cell><cell>4.402</cell><cell>4.232</cell><cell>4.188</cell></row><row><cell>Our</cell><cell>5.187</cell><cell>4.233</cell><cell>4.144</cell><cell>4.186</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>A comparison on KITTI between the original ASPP and our dense ASPP with self-attention. We denote 'D' for Dense connections and 'SA' for Self-Attention. RMSE is shown for focused image stacks of different sizes. RMSE δ&lt;1.25 Abs Rel RMSE δ&lt;1.<ref type="bibr" target="#b24">25</ref> </figDesc><table><row><cell cols="2">F1 0.489 12.395 0.293 Abs Rel [31] Rendering</cell><cell>F2 0.636 11.177 0.230</cell></row><row><cell>[31]+BF</cell><cell>0.379 11.921 0.354</cell><cell>0.339 11.612 0.418</cell></row><row><cell>Ours</cell><cell>0.141 5.187 0.846</cell><cell>0.129 4.233 0.856</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>A comparison on KITTI dataset between different blur methods on top of our network. BF= bilateral filtering.</figDesc><table><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt; 1.25</cell><cell>0.78 0.80 0.82</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.74</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.72</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.70</cell><cell>0</cell><cell>8 16 24 32 40 48 56 64 72 80 Df Distance [meter]</cell><cell>0</cell><cell>8</cell><cell>16 24 32 40 48 56 64 72 80 Df Distance [meter]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Backward Pass:We first compute derivative of loss L w.r.t. image I at location (x, y):</figDesc><table><row><cell>∂L ∂I x,y</cell><cell>=</cell><cell>∂L ∂J s,t</cell><cell>∂J s,t ∂I x,y</cell><cell>dudv</cell><cell>(3)</cell></row><row><cell></cell><cell>u,v∈Ω</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>s=x−u</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>t=y−v</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>∂J s,t ∂I x,y</cell><cell>=</cell><cell cols="2">F x,y (u, v)</cell><cell></cell><cell></cell></row></table><note>u ,v ∈Ω</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">u ,v ∈Ω F s−u ,t−v (u , v )du dv</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974). The contribution of the first author is part of a Ph.D. thesis research conducted at Tel Aviv University.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Now that we have each individual derivative we can compute according to the product rule:</p><p>Finally we get the complete answer: </p><p>= ξ x,y (u, v)(I x,y − J s,t )F x,y (u, v)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth estimation and blur removal from a single out-of-focus image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep depth from defocus: how can defocus blur improve 3D estimation using dense neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trouvé-Peloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Champagnat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>3DRW ECCV Workshop</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Depth estimation using structured light flow-analysis of projected pattern flow on an object&apos;s surface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawasaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00513</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A robust hybrid of lasso and ridge regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">443</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="59" to="72" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An all approach for multifocus image fusion using neural network. Artificial Intelligent Systems and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pagidimarry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="732" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dense monocular depth estimation in complex dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4058" to="4066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning 3-d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Which side of the focal plane are you on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sellent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on computational photography (ICCP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Barron. Aperture supervision for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6393" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Noise robust depth from focus using a ring difference filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Surh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Depth from focus with your mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Kutulakos. Depth from defocus in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multiscale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Geonet: Unsupervised learning of dense depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Selfattention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Defocus map estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1852" to="1858" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>= I X</surname></persName>
		</author>
		<imprint>
			<pubPlace>y F x,y (u, v</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
