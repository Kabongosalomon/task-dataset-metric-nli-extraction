<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Video Object Segmentation from Static Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Video Object Segmentation from Static Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by recent advances of deep learning in instance segmentation and object tracking, we introduce video object segmentation problem as a concept of guided instance segmentation. Our model proceeds on a per-frame basis, guided by the output of the previous frame towards the object of interest in the next frame. We demonstrate that highly accurate object segmentation in videos can be enabled by using a convnet trained with static images only. The key ingredient of our approach is a combination of offline and online learning strategies, where the former serves to produce a refined mask from the previous' frame estimate and the latter allows to capture the appearance of the specific object instance. Our method can handle different types of input annotations: bounding boxes and segments, as well as incorporate multiple annotated frames, making the system suitable for diverse applications. We obtain competitive results on three different datasets, independently from the type of input annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (convnets) have shown outstanding performance in many fundamental areas in computer vision, enabled by the availability of large-scale annotated datasets (e.g., ImageNet classification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>). However, some important challenges in video processing can be difficult to approach using convnets, since creating a sufficiently large body of densely, pixel-wise annotated video data for training is usually prohibitive.</p><p>One example domain is video object segmentation. Given only one or a few frames with segmentation mask annotations of a particular object instance, the task is to accurately segment the same instance in all other frames of the video. Current top performing approaches either interleave box tracking and segmentation <ref type="bibr" target="#b47">[48]</ref>, or propagate the * The first two authors contributed equally first frame segment annotation in space-time via CRF-like and GrabCut-like techniques <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>One of the key insights and contributions of this paper is that fully annotated video data is not necessary. We demonstrate that highly accurate video object segmentation can be enabled using a convnet trained with static images only.</p><p>We approach video object segmentation from a new angle. We show that a convnet designed for semantic image segmentation <ref type="bibr" target="#b7">[8]</ref> can be utilized to perform per-frame instance segmentation, i.e., segmentation of generic objects while distinguishing different instances of the same class. For each new video frame the network is guided towards the object of interest by feeding in the previous' frame mask estimate. We therefore refer to our approach as guided instance segmentation. To the best of our knowledge, it represents the first fully trained approach to video object segmentation.</p><p>Our system is efficient due to its feed-forward architecture and can generate high quality results in a single pass over the video, without the need for considering more than one frame at a time. This is in stark contrast to many other video segmentation approaches, which usually require global connections over multiple frames or even the whole video sequence in order to achieve coherent results. The method can handle different types of annotations and in the extreme case, even simple bounding boxes as input are sufficient, achieving competitive results, rendering our method flexible with respect to various practical applications.</p><p>Key to the video segmentation quality of our approach is a combined offline / online learning strategy. In the offline phase, we use deformation and coarsening on the image masks in order to train the network to produce accurate output masks from their rough estimates. An online training phase extends ideas from previous works on object tracking <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> to video segmentation and enables the method to be easily optimized with respect to an object of interest in a novel input video.</p><p>The result is a single, generic system that compares favourably to most classical approaches on three extremely heterogeneous video segmentation benchmarks, despite using the same model and parameters across all videos. We provide a detailed ablation study and explore the impact of varying number and types of annotations, and moreover discuss extensions of the proposed model, allowing to improve the quality even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The idea of performing video object segmentation via tracking at the pixel level is at least a decade old <ref type="bibr" target="#b35">[36]</ref>. Recent approaches interweave box tracking with box-driven segmentation (e.g. TRS <ref type="bibr" target="#b47">[48]</ref>), or propagate the first frame segmentation via graph labeling approaches.</p><p>Local propagation JOTS <ref type="bibr" target="#b46">[47]</ref> builds a graph over neigbhour frames connecting superpixels and (generic) object parts to solve the video labeling task. ObjFlow <ref type="bibr" target="#b43">[44]</ref> builds a graph over pixels and superpixels, uses convnet based appearance terms, and interleaves labeling with optical flow estimation. Instead of using superpixels or proposals, BVS <ref type="bibr" target="#b25">[26]</ref> formulates a fully-connected pixel-level graph between frames and efficiently infer the labeling over the vertices of a spatio-temporal bilateral grid <ref type="bibr" target="#b6">[7]</ref>. Because these methods propagate information only across neighbor frames they have difficulties capturing long range relationships and ensuring globally consistent segmentations.</p><p>Global propagation In order to overcome these limitations, some methods have proposed to use long-range connections between video frames <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50]</ref>. In particular, we compare to FCP <ref type="bibr" target="#b30">[31]</ref>, Z15 <ref type="bibr" target="#b50">[51]</ref> and W16 <ref type="bibr" target="#b44">[45]</ref> which build a global graph structure over object proposal segments, and then infer a consistent segmentation. A limitation of methods utilizing long-range connections is that they have to operate on larger image regions such as superpixels or object proposals for acceptable speed and memory usage, compromising on their ability to handle fine image detail.</p><p>Unsupervised segmentation Another family of work does general moving object segmentation (over all parts of the image), and selects post-hoc the space-time tube that best match the annotation, e.g. NLC <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>In contrast, our approach sides-steps the use of any intermediate tracked box, superpixels, or object proposals and proceeds on a per-frame basis, therefore efficiently handling even long sequences at full detail. We focus on propagating the first frame segmentation forward onto future frames, using an online fine-tuned convnet as appearance model for segmenting the object of interest in the next frames.</p><p>Box tracking Some previous works have investigated approaches that improve segmentation quality by leveraging object tracking and vice versa <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref>. More recent, state-of-the-art tracking methods are based on discriminative correlation filters over handcrafted features (e.g. HOG) and over frozen deep learned features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, or are convnet based trackers on their own right <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Our approach is most closely related to the latter group. GOTURN <ref type="bibr" target="#b17">[18]</ref> proposes to train offline a convnet so as to directly regress the bounding box in the current frame based on the object position and appearance in the previous frame. MDNet <ref type="bibr" target="#b28">[29]</ref> proposes to use online fine-tuning of a convnet to model the object appearance.</p><p>Our training strategy is inspired by GOTURN for the offline part, and MDNet for the online stage. Compared to the aforementioned methods our approach operates at pixel level masks instead of boxes. Differently from MDNet, we do not replace the domain-specific layers, instead finetuning all the layers on the available annotations for each individual video sequence.</p><p>Instance segmentation At each frame, video object segmentation outputs a single instance segmentation. Given an estimate of the object location and size, bottom-up segment proposals <ref type="bibr" target="#b33">[34]</ref> or GrabCut <ref type="bibr" target="#b37">[38]</ref> variants can be used as shape guesses. Also specific convnet architectures have been proposed for instance segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref>. Our approach outputs per-frame instance segmentation using a convnet architecture, inspired by works from other domains like <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>. A concurrent work <ref type="bibr" target="#b4">[5]</ref> also exploits convnets for video object segmentation. Differently from our approach their segmentation is not guided, which might result in performance decay over time. Furthermore, the offline training exploits full video sequence annotations that are notoriously difficult to obtain.</p><p>Interactive video segmentation Applications such as video editing for movie production often require a level of accuracy beyond the current state-of-the-art. Thus several works have also considered video segmentation with variable annotation effort, enabling human interaction using clicks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>, or strokes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b51">52]</ref>. In this work we consider instead box or (full) segment annotations on multiple frames. In §5 we report results when varying the amount of annotation effort (from one frame per video to all frames).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MaskTrack method</head><p>We approach the video object segmentation problem from a new angle we refer as guided instance segmentation. For each new frame we wish to label pixels as object/nonobject of interest, for this we build upon the architecture of the existing pixel labelling convnet and train it to generate per-frame instance segments. We pick DeepLabv2 <ref type="bibr" target="#b7">[8]</ref>, but our approach is agnostic of the specific architecture selected.</p><p>The challenge is then: how to inform the network which instance to segment? We solve this by using two complementary strategies. One is guiding the network towards the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MaskTrack ConvNet</head><p>Input frame t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask estimate t-1</head><p>Refined mask t <ref type="figure">Figure 1</ref>: Given a rough mask estimate from the previous frame t − 1 , we train a convnet to provide a refined mask output for the current frame t.</p><p>instance of interest by feeding in the previous' frame mask estimate during offline training ( §3.1). And a second is employing online training ( §3.2) to fine-tune the model to become more specialized for the specific instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning to segment instances offline</head><p>In order to guide the pixel labeling network to segment the object of interest, we begin by expanding the convnet input from RGB to RGB+mask channel (4 channels). The extra mask channel is meant to provide an estimate of the visible area of the object in the current frame, its approximate location and shape. We can then train the labelling convnet to provide as output an accurate segmentation of the object, given as input the current image and a rough estimate of the object mask. Our tracking network is de-facto a "mask refinement" network.</p><p>There are two key observations that make this approach practical. First, very rough input masks are enough for our trained network to provide sensible output segments. Even a large bounding box as input will result in a reasonable output (see §5.2). The input mask's main role is to point the convnet towards the correct object instance to segment.</p><p>Second, this particular approach does not require us to use video as training data, such as done in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>. Because we only use a mask as additional input, instead of an image crop as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>, we can easily synthesize training samples from single frame instance segmentation annotations. This allows to train from a large set of diverse images and avoids having to use existing (scarce and small) video segmentation benchmarks for training. <ref type="figure">Figure 1</ref> shows our overall architecture. To simulate the noise in the previous frame output masks, during offline training we generate input masks by deforming the annotated masks via affine transformation as well as non-rigid deformations via thin-plate splines <ref type="bibr" target="#b3">[4]</ref>, followed by a coarsening step (dilation morphological operation) to remove details of the object contour. We apply this data generation procedure over a dataset of ∼ The generated masks mimic plausible object shapes on the preceding frame. time, given the mask estimate at time t − 1, we apply the dilation operation and use the resulting rough mask as input for object segmentation in frame t.</p><p>The affine transformations and non-rigid deformations aim at modelling the expected motion of an object between two frames. The coarsening permits us to generate training samples that resembles the test time data, simulating the blobby shape of the output mask given from the previous frame by the convnet. These two ingredients make the estimation more robust and help to avoid accumulation of errors from the preceding frames.</p><p>After training the resulting convnet has learnt to do guided instance segmentation, similar to networks like DeepMask <ref type="bibr" target="#b31">[32]</ref> and Hypercolumns <ref type="bibr" target="#b16">[17]</ref>, but instead of taking a bounding box as guidance, we can use an arbitrary input mask. The training details are described in §4.</p><p>When using offline training only, the segmentation procedure consists of two steps: the previous frame mask is coarsened and then fed into the trained network to estimate the current frame mask. Since objects have a tendency to move smoothly through space, the object mask in the preceding frame will provide a good guess in the current frame and simply copying the coarse mask from the previous frame is enough. This approach is fast and already provides good results. We also experimented using optical flow to propagate the mask from one frame to the next, but found the optical flow errors to offset the gains.</p><p>With only the offline trained network, the proposed approach allows to achieve competitive performance compared to previously reported results (see §5.2). However the performance can be further improved by integrating online training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning to segment instances online</head><p>For further boosting the video segmentation quality, we borrow and extend ideas that were originally proposed for tracking. Current top performing tracking techniques <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> all use some form of online training. We thus consider improving results by adding this as a second strategy.</p><p>The idea is to use, at test time, the segment annotation of the first video frame as additional training data. Using aug-mented versions of this single frame annotation, we proceed to fine-tune the model to become more specialized for the specific object instance at hand.</p><p>We use a similar data augmentation as for offline training. On top of affine and non-rigid deformations for the input mask, we also add image flipping and rotations to generate multiple training samples from one frame. We generate ∼10 3 training samples from this single annotation, and proceed to fine-tune the model previously trained offline.</p><p>With online fine-tuning, the network weights partially capture the appearance of the specific object being tracked. The model aims to strike a balance between general instance segmentation (so as to generalize to the object changes), and specific instance segmentation (so as to leverage the common appearance across video frames). The details of the online fine-tuning are provided in §4. In our experiments we only do fine-tuning using the annotated frame(s).</p><p>To the best of our knowledge our approach is the first to use a pixel labelling network (like DeepLabv2 <ref type="bibr" target="#b7">[8]</ref>) for the task of video object segmentation. We name our full approach (using both offline and online training) MaskTrack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Variants</head><p>Additionally we consider variations of the proposed model. First, we want to show that our approach is flexible and could handle different types of input annotations, using less supervision in the first frame annotation. Second, motion information could be easily integrated in the system, improving the quality of the object segments. Box annotation Here we discuss a variant named MaskTrack Box , that takes a bounding box annotation in the first frame as an input supervision instead of a segmentation mask. To handle this variant we use on the first frame a second convnet model trained with bounding box rectangles as input masks. From the next frame onwards we use the standard MaskTrack model. Optical flow On top of MaskTrack, we consider to employ optical flow as a source of additional information to guide the segmentation. Given a video sequence, we compute the optical flow using EpicFlow <ref type="bibr" target="#b36">[37]</ref> with Flow Fields matches <ref type="bibr" target="#b1">[2]</ref> and convolutional boundaries <ref type="bibr" target="#b26">[27]</ref>. In parallel to the vanilla MaskTrack, we proceed to compute a second output mask using the magnitude of the optical flow field as input image (replicated into a three channel image). The model is used as-is, without retraining. Although it has been trained on RGB images, this strategy works because object flow magnitude roughly looks like a gray-scale object, and still captures useful object shape information, see examples in <ref type="figure" target="#fig_1">Figure 3</ref>. Using the RGB model allows to avoid training the convnet on a video dataset with segmentation annotations.</p><p>We then fuse by averaging the output scores given by the two parallel networks (using RGB image and optical flow magnitude as inputs). We name this variant </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network implementation and training</head><p>Following, we describe the implementation details of our approach, namely the offline and online training strategy and the data augmentation. Network For all our experiments we use the training and test parameters of DeepLabv2-VGG network <ref type="bibr" target="#b7">[8]</ref>. The model is initialized from a VGG16 network pre-trained on ImageNet <ref type="bibr" target="#b41">[42]</ref>. For the extra mask channel of filters in the first convolutional layer we use gaussian initialization. We also tried zero initialization, but observed no difference. Offline training The advantage of our method is that it does not require expensive pixel-label annotations on videos for training the convnet. Thus we can employ images and annotations from existing saliency segmentation datasets. We consider images and segmentation masks from ECSSD <ref type="bibr" target="#b40">[41]</ref>, MSRA10K <ref type="bibr" target="#b8">[9]</ref>, SOD <ref type="bibr" target="#b27">[28]</ref>, and PASCAL-S <ref type="bibr" target="#b24">[25]</ref>. This results in an aggregate set of 11 282 training images.</p><p>The input masks for an extra channel are generated by deforming the binary segmentation masks via affine transformation and non-rigid deformations, as discussed in §3.1. For affine transformation we consider random scaling (±5% of object size) and translation (±10% shift). Nonrigid deformations are done via thin-plate splines <ref type="bibr" target="#b3">[4]</ref> using 5 control points and randomly shifting the points in x and y directions within ±10% margin of the original segmentation mask width and height. Then the mask is coarsened using dilation operation with 5 pixel radius. This mask deformation procedure is applied over all object instances in the training set. For each image two different masks are generated, see examples in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>For training we follow <ref type="bibr" target="#b7">[8]</ref> and use SGD with minibatches of 10 images and a polynomial learning policy with initial learning rate of 0.001. The momentum and weight decay are set to 0.9 and 0.0005, respectively. The network is trained for 20k iterations.</p><p>Online training For online adaptation we finetune the model previously trained offline on the first frame for 200 iterations with training samples generated from the first frame annotation. We augment the first frame by image flipping and rotations as well as by deforming the annotated masks for an extra channel via affine and non-rigid deformations with the same parameters as for the offline training. This results in an augmented set of ∼10 3 training images.</p><p>The network is trained with the same learning parameters as for offline training, finetuning all convolutional and fully connected layers.</p><p>At test time our base MaskTrack system runs at about 12 seconds per frame (average over DAVIS dataset, amortizing the online fine-tuning time over all video frames), which is a magnitude faster compared to ObjFlow <ref type="bibr" target="#b43">[44]</ref> (takes 2 minutes per frame, averaged over DAVIS dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section we describe our evaluation protocol ( §5.1), study the quantitative importance of the different components of our system ( §5.2), and report results comparing to state of art techniques over three datasets (190 videos total, §5.3), as well as comparing the effects of different amounts of annotation on final quality ( §5.4). Additional quantitative and qualitative results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>Datasets We evaluate the proposed approach on three different video object segmentation datasets: DAVIS <ref type="bibr" target="#b29">[30]</ref>, YoutubeObjects <ref type="bibr" target="#b34">[35]</ref>, and SegTrack-v2 <ref type="bibr" target="#b23">[24]</ref>. These datasets include assorted challenges such as appearance change, occlusion, motion blur and shape deformation.</p><p>DAVIS <ref type="bibr" target="#b29">[30]</ref> consists of 50 high quality videos, totalling 3 455 frames. Pixel-level segmentation annotations are provided for each frame, where one single object or two connected objects are separated from the background.</p><p>YoutubeObjects <ref type="bibr" target="#b34">[35]</ref> includes videos with 10 object categories. We consider the subset of 126 videos with more than 20 000 frames, for which the pixel-level ground truth segmentation masks are provided by <ref type="bibr" target="#b18">[19]</ref>.</p><p>SegTrack-v2 <ref type="bibr" target="#b23">[24]</ref> contains 14 video sequences with 24 objects and 947 frames. Every frame is annotated with a pixel-level object mask. As instance-level annotations are provided for sequences with multiple objects, each specific instance segmentation is treated as separate problem. Evaluation We evaluate using the standard mIoU metric: intersection-over-union of the estimated segmentation and the ground truth binary mask, also known as Jaccard Index, averaged across videos. For DAVIS we use the provided benchmark code <ref type="bibr" target="#b29">[30]</ref>, which excludes the first and the last frames from the evaluation. For YoutubeObjects and SegTrack-v2 only the first frame is excluded. Previous work used diverse evaluations procedure. To ensure a consistent comparison between methods, when needed, we re-computed scores for using shared results maps, or by generating them using available open source code. In particular, we collected new results for ObjFlow <ref type="bibr" target="#b43">[44]</ref> and BVS <ref type="bibr" target="#b25">[26]</ref> in order to present other methods with results across the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>We first study different ingredients of our method. We experiment on the DAVIS dataset and measure the performance using the mean intersection-over-union metric (mIoU). <ref type="table">Table 1</ref> shows the importance of each of the ingredients described in §3 and reports the improvement of adding extra components to the MaskTrack model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Add-ons</head><p>We first study the effect of adding a couple of ingredients on top of our base MaskTrack system, which are specifically fine-tuned for DAVIS. We see that optical flow provides complementary information to the appearance, boosting further the results (74.8 → 78.4). Adding on top a well-tuned post-processing CRF <ref type="bibr" target="#b20">[21]</ref> can gain a couple of mIoU points, reaching 80% mIoU on DAVIS, the best known result on this dataset.</p><p>Albeit optical flow can provide interesting gains, we found it to be brittle when going across different datasets. Different strategies to handle optical flow provide 1 ∼ 4% on each dataset, but none provide consistent gains across all datasets; mainly due to failure modes of the optical flow algorithms. For the sake of presenting a single model with fix parameters across all datasets, we refrain from using a per-dataset tuned optical flow in the results of §5.3.</p><p>Training We next study the effect of offline/online training of the network. By disabling online fine-tuning, and only relying on offline training we see a ∼ 5 IoU percent points drop, showing that online fine-tuning indeed expand the tracking capabilities. If instead we skip offline training and only rely on online fine-tuning performance drop drastically, albeit the absolute quality (57.6 mIoU) is surprisingly high for a system trained on ImageNet+single frame.</p><p>By reducing the amount of training data from 11k to 5k we only see a minor decrease in mIoU; this indicates that the amount of training data is not critical to get reasonable performance. That being said, further increase of the training data volume would lead to improved results.</p><p>Additionally, we explore the effect of the offline training on video data instead of using static images. We train the model on the annotated frames of two combined datasets, SegTrack-v2 and YoutubeObjects. By switching to train on video data we observe a minor decrease in mIoU; this could be explained by lack of diversity in the video training data due to the small scale of the existing datasets, as well as the effect of the domain shift between different benchmarks. This shows that employing static images in our approach does not result in any performance drop. Mask deformation We also study the influence of mask deformations. We see that coarsening the mask via dilation provides a small gain, as well as adding non-rigid deformations. All-and-all, <ref type="table">Table 1</ref> shows that the main factor affecting the quality is using any form of mask deformations when creating the training samples (both for offline and online training). This ingredient is critical for our overall approach, making the segmentation estimation more robust at test time to the noise in the input mask. Input channel Next we experiment with different variants of the extra channel input. Even by changing the input from segments to boxes, a model trained for this modality still provides reasonable results.</p><p>Most interestingly we also evaluated a model that does not use any mask input. Without the additional input channel, this pixel labelling convnet was trained for saliency offline and fine-tuned online to capture the appearance of the object of interest. This model obtains competitive results (72.5 mIoU), showing the power of modelling the video segmentation task as an instance segmentation problem. <ref type="table" target="#tab_1">Table 2</ref> presents results when the first frame is annotated with an object segmentation mask. This is the protocol commonly used on DAVIS, SegTrack-v2, and YoutubeObjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Single frame annotations</head><p>We see that MaskTrack obtains competitive performance across all three datasets. This is achieved using our purely frame-by-frame feed-forward system, using the exact same model and parameters across all datasets. Our MaskTrack results are obtained in a single pass, do not use any global optimization, not even optical flow. We believe this shows the promise of formulating video object segmentation from the instance segmentation perspective.</p><p>On SegTrack-v2, JOTS <ref type="bibr" target="#b46">[47]</ref> reported higher numbers (71.3 mIoU), however, they report tuning their method pa-  rameters' per video, and thus it is not comparable to our setup with fix-parameters. <ref type="table" target="#tab_1">Table 2</ref> also reports results for the MaskTrack Box variant described in §3.3. Starting only from box annotations on the first frame, our system still generates comparably good results (see <ref type="figure" target="#fig_2">Figure 4)</ref>, remaining on the top three best results in all the datasets covered.</p><p>By adding additional ingredients specifically tuned for different datasets, such as optical flow (see §3.3) and CRF post-processing, we can push the results even further, reaching 80.3 mIoU on DAVIS, 72.6 on YoutubeObjects and 70.3 on SegTrack-v2. The dataset specific tuning is described in the supplementary material.  Attribute-based analysis <ref type="figure" target="#fig_3">Figure 5</ref> presents a more detailed evaluation on DAVIS <ref type="bibr" target="#b29">[30]</ref> using video attributes. The attribute based analysis shows that our generic model, MaskTrack, is robust to various video challenges present in DAVIS. It compares favourably on any subset of videos sharing the same attribute, except camera-shake, where ObjFlow <ref type="bibr" target="#b43">[44]</ref> marginally outperforms our approach. We observe that MaskTrack handles fast-motion and motionblur well, which are typical failure cases for methods relying on spatio-temporal connections <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Due to the online fine-tuning on the first frame annotation of a new video, our system is able to capture the appearance of the specific object of interest. This allows it to better recover from occlusions, out-of-view scenarios and appearance changes, which usually affect methods that strongly rely on propagating segmentations on a per-frame basis.</p><p>Incorporating optical flow information into MaskTrack substantially increases robustness on all categories. As one could expect, MaskTrack+Flow+CRF better discriminates cases involving color ambiguity and salient motion. However, we also observed less-obvious improvements in cases with scale-variation and low-resolution objects.</p><p>Conclusion With our simple, generic system for video object segmentation we are able to achieve competitive results with existing techniques, on three different datasets. These results are obtained with fixed parameters, from a forwardpass only, using only static images during offline training. We also reach good results even when using only a box annotation as starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Multiple frames annotations</head><p>In some applications, e.g. video editing for movie production, one may want to consider more than a single frame annotation on videos. <ref type="figure">Figure 7</ref> shows the video object segmentation quality result when considering different number of annotated frames, on the DAVIS dataset. We show results for both pixel accurate segmentation per frame, or bounding box annotation per frame.</p><p>For these experiments, we run our method twice, forward and backwards; and for each frame pick the result closest in  <ref type="figure">Figure 7</ref>: Percent of annotated frames versus video object segmentation quality. We report mean IoU, and quantiles at 5, 10, 20, 30, 70, 80, 90, and 95%. Result on DAVIS dataset, using segment or box annotations. The baseline simply copies the annotations to adjacent frames. Discussion in §5.4. time to the annotated frame (either from forward or backwards propagation). Here, the online fine-tuning uses all annotated frames instead of only the first one. For the experiments with box annotations <ref type="figure">(Figure 7b</ref>), we use a similar procedure to MaskTrack Box . Box annotations are first converted to segments, and then apply MaskTrack as-is, treating these as the original segment annotations.</p><p>The evaluation reports the mean IoU results when annotating one frame only (same as table 2), and every 40th, 30th, 20th, 10th, 5th, 3rd, and 2nd frame. Since DAVIS videos have length ∼100 frames, 1 annotated frame corresponds to ∼1%, otherwise annotations every 20th is 5% of annotated frames, 10th 10%, 5th 20%, etc. We follow the same DAVIS evaluation protocol as §5.3, ignoring first and last frame, and choosing to include the annotated frames in the evaluation (this is particularly relevant for the box anno-DAVIS SegTrack YouTube 1st frame, GT segment Results with MaskTrack, the frames are chosen equally distant based on the video sequence length <ref type="figure" target="#fig_4">Figure 6</ref>: Qualitative results of three different datasets. Our algorithm is robust to challenging situations such as occlussions, fast motion, multiple instances of the same semantic class, object shape deformation, camera view change and motion blur. tation results).</p><p>Other than mean IoU we also show the quantile curves indicating the cutting line for the 5%, 10%, 20%, etc. lowest quality video frame results. This gives a hint of how much targeted additional annotations might be needed. The higher mean IoU of these quantiles are, the better.</p><p>The baseline for these experiments consists in directly copying the ground truth annotations from the nearest annotated neighbour. This baseline indicates "level zero" for our results. For visual clarity, we only include the mean value for the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>We can see that <ref type="figure">Figures 7a and 7b</ref> show slightly different trends. When using segment annotations <ref type="figure">(Figure  7a)</ref>, the baseline quality increases steadily until reaching IoU 1 when all frames are annotated. Our MaskTrack approach provides large gains with 30% of annotated frames or less. For instance when annotating 10% of the frames we reach mIoU 0.86, notice that also the 20% quantile is at 0.81 mIoU. This means with only 10% of annotated frames, 80% of all video frames will have a mean IoU above 0.8, which is good enough to be used for many applications , or can serve as initialization for a refinement process. With 10% of annotated frames the baseline only reaches 0.64 mIoU. When using box annotations ( <ref type="figure">Figure 7b</ref>) the quality of the baseline and our method saturates. There is only so much information our instance segmenter can esti-mate from boxes. After 10% of annotated frames, not much additional gain is obtained. Interestingly, the mean IoU and 30% quantile here both reach ∼0.8 mIoU range. Additionally, 70% of the frames have IoU above 0.89.</p><p>Conclusion Results indicate that with only 10% of annotated frames we can reach satisfactory quality, even when using only bounding box annotations. We see that with moving from one annotation per video to two or three frames (1% → 3% → 4%) quality increases sharply, showing that our system can adequately leverage a few extra annotations per video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a novel approach to video object segmentation. By treating video object segmentation as a guided instance segmentation problem, we have proposed to use a pixel labelling convnet for frame-by-frame segmentation. By exploiting both offline and online training with image annotations only our approach is able to produce highly accurate video object segmentation. The proposed system is generic and reaches competitive performance on three extremely heterogeneous video segmentation benchmarks, using the same model and parameters across all videos. The method can handle different types of input annotations and our results are competitive even when using only bounding box annotations (instead of segmentation masks).</p><p>We provided a detailed ablation study, and explored the effect of varying the amount of annotations per video. Our results show that with only one annotation every 10th frame we can reach 85% mIoU quality. Considering we only do per-frame instance segmentation without any form of global optimization, we deem these results encouraging to achieve high quality via additional post-processing.</p><p>We believe the use of labelling convnets for video object segmentation is a promising strategy. Future work should consider exploring more sophisticated network architectures, incorporating temporal dimension and adding global optimization strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material A. Content</head><p>This supplementary material provides both additional quantitative and qualitative results, as well as an attached video.</p><p>• Section B provides additional quantitative results for DAVIS, YoutubeObjects, and SegTrackv-2 (see <ref type="table" target="#tab_4">Tables  S5 -S4</ref>).</p><p>• Detailed attribute-based evaluation is reported in Section C and <ref type="table" target="#tab_8">Table S6</ref>.</p><p>• The dataset specific tuning for additional ingredients is described in Section D.</p><p>• Additional qualitative results with first frame box and segment supervision are presented in Section E and <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>• Examples of mask generation for the extra input channel are shown in Section F and <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>• Examples of optical flow magnitude images are presented in Section G and <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional quantitative results</head><p>In this section we present additional quantitative results for three different datasets: DAVIS <ref type="bibr" target="#b29">[30]</ref>, YoutubeObjects <ref type="bibr" target="#b34">[35]</ref>, and SegTrackv-2 <ref type="bibr" target="#b23">[24]</ref>. This section complements §5.3 in the main paper. DAVIS We present the per-sequence comparison with other state-of-the-art methods on DAVIS in <ref type="table" target="#tab_3">Table S3</ref>. SegTrack-v2 <ref type="table" target="#tab_4">Table S4</ref> reports the per-sequence comparison with other state-of-the-art methods on SegTrack-v2. YoutubeObjects The per-category comparison with other state-of-the-art methods on YoutubeObjects is shown in Table S5. <ref type="table" target="#tab_8">Table S6</ref> presents a more detailed evaluation on DAVIS using video attributes and complements <ref type="figure" target="#fig_3">Figure 5</ref> in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attribute-based evaluation</head><p>The attribute based evaluation shows that our generic model, MaskTrack, is robust to various video challenges present in DAVIS. It compares favourably on any subset of videos sharing the same attribute, except camera-shake, where ObjFlow <ref type="bibr" target="#b43">[44]</ref> marginally outperforms our approach.</p><p>We observe that MaskTrack handles well fast-motion, appearance change and out-of-view, where competitive methods are failing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset specific tuning</head><p>As mentioned in §5.3 in the main paper by adding additional ingredients specifically tuned for different datasets, such as optical flow and CRF post-processing, we can push the results even further, reaching 80.3 mIoU on DAVIS, 72.6 on YoutubeObjects and 70.3 on SegTrackv-2. In this section we discuss the dataset specific tuning. Optical flow Although optical flow can provide interesting gains, we found it to be brittle when going across different datasets. Therefore we explored different strategies to handle optical flow.</p><p>As discussed in §3.3 of the main paper given a video sequence, we compute the optical flow using EpicFlow <ref type="bibr" target="#b36">[37]</ref> with Flow Fields matches <ref type="bibr" target="#b1">[2]</ref> and convolutional boundaries <ref type="bibr" target="#b26">[27]</ref>. In parallel to the MaskTrack with RGB images, we  proceed to compute a second output mask using the magnitude of the optical flow field as input image (replicated into a three channel image). We then fuse by averaging the output scores given by the two parallel networks (using RGB image and optical flow magnitude as inputs). For DAVIS we use the original MaskTrack model (trained with RGB images) as-is, without retraining. However, this strategy fails on YoutubeObjects and SegTrackv-2, mainly due to the failure modes of the optical flow algorithm and its sensitivity to the video data quality. To overcome this limitation we additionally trained the MaskTrack model using optical flow magnitude images on video data instead of RGB images. Training on optical flow magnitude images helps the network to be robust to the optical flow errors during the test time and provides a marginal improvement on YoutubeObjects and SegTrackv-2.</p><p>Overall integrating optical flow on top of MaskTrack provides 1∼4% on each dataset. CRF post-processing As have been shown in <ref type="bibr" target="#b7">[8]</ref> adding on top a well-tuned post-processing CRF <ref type="bibr" target="#b20">[21]</ref> can gain a couple of mIoU points. Therefore following <ref type="bibr" target="#b7">[8]</ref> we crossvalidate the parameters of the fully connected CRF per each dataset based on the available first frame segment annotations of all video sequences. We employ coarse-to-fine search scheme for tuning CRF parameters and fix the number of mean field iterations to 10. We apply the CRF on a temporal window of 3 frames to improve the temporal stability of the results. The color (RGB) and the spatiotemporal (XYT) standard deviation of the appearance kernel are set, respectively, to 10 and 5. The pairwise term weight is set to 5. We employ an additional smoothness kernel to remove small isolated regions. Both its weight and the spatial (XY) standard deviation are set to 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional qualitative results</head><p>In this section we provide additional qualitative results for the MaskTrack Box and MaskTrack systems, described in §3 in the man paper. <ref type="figure">Figure S8</ref> shows the video object segmentation results when considering different types of annotations on DAVIS. Starting from segment annotations or even only from box annotations on the first frame, our model generates high quality segmentations, making the system suitable for diverse applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Examples of training mask generation</head><p>In §5.2 of the main paper we show that the main factor affecting the quality is using any form of mask deformations when creating the training samples (both for offline and online training). The mask deformation ingredient is crucial for our MaskTrack approach, making the segmentation estimation more robust at test time to the noise in the input mask. <ref type="figure">Figure S9</ref> complements <ref type="figure" target="#fig_0">Figure 2</ref> in the main paper and shows examples of generated masks using affine transformation as well as non-rigid deformations via thin-plate splines (see §4 in the main paper for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Examples of optical flow magnitude images</head><p>In §3.3 of the paper we propose to employ optical flow magnitude as a source of additional information to guide the segmentation. The flow magnitude roughly looks like a gray-scale object and captures useful object shape information, therefore complementing the MaskTrack model with RGB images as inputs. Examples of optical flow magnitude images are shown in <ref type="figure">Figure S10</ref>. <ref type="figure">Figure S10</ref> complements <ref type="figure" target="#fig_1">Figure 3</ref> in the main paper.  <ref type="figure">Figure S8</ref>: Qualitative results of MaskTrack Box and MaskTrack on Davis using 1st frame annotation supervision (box or segment). By propagating annotation from the 1st frame, either from segment or just bounding box annotations, our system generates results comparable to ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotated image</head><p>Example training masks <ref type="figure">Figure S9</ref>: Examples of training mask generation. From one annotated image, multiple training masks are generated. The generated masks mimic plausible object shapes on the preceding frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optical Flow</head><p>SegTrack-v2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Image</head><p>Optical Flow <ref type="figure">Figure S10</ref>: Examples of optical flow magnitude images for different datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>10 4 images containing diverse object instances, see examples in Figure 2. At test (a) Annotated image (b) Example training masks Examples of training mask generation. From one annotated image, multiple training masks are generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples of optical flow magnitude images.MaskTrack+Flow. Optical flow provides complementary information to the MaskTrack with RGB images, improving the overall performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>By propagating annotation from the 1st frame, either from segment or just bounding box annotations, our system generates results comparable to ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>BFigure 5 :</head><label>5</label><figDesc>a c k g ro u n d c lu tt e r D e fo rm a ti o n In te ra c ti n g o b je c ts O c c lu s io n C a m e ra -s h a k e H e te ro g e n e u s o b je c t S h a p e c o m p le x it y D y n a m ic b a c k g ro u n d E d g e a m b ig u it y F a s t m o ti o n M o ti o n b lu r O u t-o f-v ie w A p p e a ra n c e c h a n g e S c a le -v a ri a ti o Attribute based evaluation on DAVIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6</head><label>6</label><figDesc>presents qualitative results of the proposed MaskTrack model across three different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Results with MaskTrack Box and MaskTrack, the frames are chosen equally distant based on the video sequence length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Video object segmentation results on three datasets. Compared to related state of the art, our ap- proach provides consistently good results. On DAVIS the extended version of our system MaskTrack+Flow+CRF reaches 80.3 mIoU. See §5.3 for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table S3 :</head><label>S3</label><figDesc>Per-sequence results on the DAVIS dataset.</figDesc><table><row><cell>Sequence</cell><cell cols="4">Method, mIoU BVS [26] ObjFlow [44] TRS [48] MaskTrack</cell></row><row><cell>bird of paradise</cell><cell>89.7</cell><cell>87.1</cell><cell>90.0</cell><cell>84.0</cell></row><row><cell>birdfall</cell><cell>65.3</cell><cell>52.9</cell><cell>72.5</cell><cell>56.6</cell></row><row><cell>bmx#1</cell><cell>67.1</cell><cell>87.9</cell><cell>86.1</cell><cell>81.9</cell></row><row><cell>bmx#2</cell><cell>3.2</cell><cell>4.0</cell><cell>40.3</cell><cell>0.1</cell></row><row><cell>cheetah#1</cell><cell>5.4</cell><cell>25.9</cell><cell>61.2</cell><cell>69.3</cell></row><row><cell>cheetah#2</cell><cell>9.2</cell><cell>37.2</cell><cell>39.4</cell><cell>17.4</cell></row><row><cell>drift#1</cell><cell>68.5</cell><cell>77.9</cell><cell>70.7</cell><cell>47.4</cell></row><row><cell>drift#2</cell><cell>32.7</cell><cell>27.4</cell><cell>70.7</cell><cell>70.9</cell></row><row><cell>frog</cell><cell>76.1</cell><cell>78.4</cell><cell>80.2</cell><cell>85.3</cell></row><row><cell>girl</cell><cell>86.5</cell><cell>84.2</cell><cell>86.4</cell><cell>86.8</cell></row><row><cell cols="2">hummingbird#1 53.2</cell><cell>67.2</cell><cell>53.0</cell><cell>39.0</cell></row><row><cell cols="2">hummingbird#2 28.7</cell><cell>68.5</cell><cell>70.5</cell><cell>49.6</cell></row><row><cell>monkey</cell><cell>85.7</cell><cell>87.8</cell><cell>83.1</cell><cell>89.3</cell></row><row><cell>monkeydog#1</cell><cell>40.5</cell><cell>47.1</cell><cell>74.0</cell><cell>25.3</cell></row><row><cell>monkeydog#2</cell><cell>17.1</cell><cell>21.0</cell><cell>39.6</cell><cell>31.7</cell></row><row><cell>parachute</cell><cell>93.7</cell><cell>93.3</cell><cell>95.9</cell><cell>93.7</cell></row><row><cell>penguin#1</cell><cell>81.6</cell><cell>80.4</cell><cell>53.2</cell><cell>93.7</cell></row><row><cell>penguin#2</cell><cell>82.0</cell><cell>83.5</cell><cell>72.9</cell><cell>85.2</cell></row><row><cell>penguin#3</cell><cell>78.5</cell><cell>83.9</cell><cell>74.4</cell><cell>90.1</cell></row><row><cell>penguin#4</cell><cell>76.4</cell><cell>86.2</cell><cell>57.2</cell><cell>90.5</cell></row><row><cell>penguin#5</cell><cell>47.8</cell><cell>82.3</cell><cell>63.5</cell><cell>78.4</cell></row><row><cell>penguin#6</cell><cell>84.3</cell><cell>87.3</cell><cell>65.7</cell><cell>89.3</cell></row><row><cell>soldier</cell><cell>55.3</cell><cell>86.8</cell><cell>76.3</cell><cell>82.0</cell></row><row><cell>worm</cell><cell>65.4</cell><cell>83.2</cell><cell>82.4</cell><cell>80.4</cell></row><row><cell>Mean</cell><cell>58.4</cell><cell>67.5</cell><cell>69.1</cell><cell>67.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S4 :</head><label>S4</label><figDesc>Per-sequence results on the SegTrack-v2 dataset.</figDesc><table><row><cell>Furthermore, incorporating optical flow information</cell></row><row><cell>and CRF post-processing into MaskTrack substan-</cell></row><row><cell>tially increases robustness on all categories, reaching</cell></row><row><cell>over 70% mIoU on each subcategory. In particular,</cell></row><row><cell>MaskTrack+Flow+CRF better discriminates cases of low</cell></row><row><cell>resolution, scale-variation and appearance change.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S5 :</head><label>S5</label><figDesc>Per-category results on the YoutubeObjects dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Attribute Method, mIoU BVS [26] ObjFlow [44] MaskTrack MaskTrack+Flow MaskTrack+Flow+CRF</figDesc><table><row><cell>Appearance change</cell><cell>0.46</cell><cell>0.54</cell><cell>0.65</cell><cell>0.75</cell><cell>0.76</cell></row><row><cell>Background clutter</cell><cell>0.63</cell><cell>0.68</cell><cell>0.77</cell><cell>0.78</cell><cell>0.79</cell></row><row><cell>Camera-shake</cell><cell>0.62</cell><cell>0.72</cell><cell>0.71</cell><cell>0.77</cell><cell>0.78</cell></row><row><cell>Deformation</cell><cell>0.7</cell><cell>0.77</cell><cell>0.77</cell><cell>0.78</cell><cell>0.8</cell></row><row><cell>Dynamic background</cell><cell>0.6</cell><cell>0.67</cell><cell>0.69</cell><cell>0.75</cell><cell>0.76</cell></row><row><cell>Edge ambiguity</cell><cell>0.58</cell><cell>0.65</cell><cell>0.68</cell><cell>0.74</cell><cell>0.74</cell></row><row><cell>Fast-motion</cell><cell>0.53</cell><cell>0.55</cell><cell>0.66</cell><cell>0.74</cell><cell>0.75</cell></row><row><cell>Heterogeneous object</cell><cell>0.63</cell><cell>0.66</cell><cell>0.71</cell><cell>0.77</cell><cell>0.79</cell></row><row><cell>Interacting objects</cell><cell>0.63</cell><cell>0.68</cell><cell>0.74</cell><cell>0.75</cell><cell>0.77</cell></row><row><cell>Low resolution</cell><cell>0.59</cell><cell>0.58</cell><cell>0.6</cell><cell>0.75</cell><cell>0.77</cell></row><row><cell>Motion blur</cell><cell>0.58</cell><cell>0.6</cell><cell>0.66</cell><cell>0.72</cell><cell>0.74</cell></row><row><cell>Occlusion</cell><cell>0.68</cell><cell>0.66</cell><cell>0.74</cell><cell>0.75</cell><cell>0.77</cell></row><row><cell>Out-of-view</cell><cell>0.43</cell><cell>0.53</cell><cell>0.66</cell><cell>0.71</cell><cell>0.71</cell></row><row><cell>Scale variation</cell><cell>0.49</cell><cell>0.56</cell><cell>0.62</cell><cell>0.72</cell><cell>0.73</cell></row><row><cell>Shape complexity</cell><cell>0.67</cell><cell>0.69</cell><cell>0.71</cell><cell>0.72</cell><cell>0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S6 :</head><label>S6</label><figDesc>Attribute based evaluation on DAVIS.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video snapcut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TOG</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09549</idno>
		<title level="m">Fully-convolutional siamese networks for object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bookstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixãl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05198</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time edge-aware image processing with the bilateral grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive fragments-based tracking of non-rigid objects using level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pixeltrack: A fast adaptive algorithm for tracking non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jumpcut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Click carving: Segmenting objects in video with point clicks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HCOMP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maerki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking as repeated figure/ground segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic portrait segmentation for image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hierarchical image saliency detection on extended CSSD. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fomtrace: Interactive video segmentation by image graphs and fuzzy object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03369</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation for weakly labeled semantic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lensu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Touchcut: Fast image and video segmentation using single-touch interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Jots: Joint online tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic object segmentation via detection in weakly labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discontinuityaware video object cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
