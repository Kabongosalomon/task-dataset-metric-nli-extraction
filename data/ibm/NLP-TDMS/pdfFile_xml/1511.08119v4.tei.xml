<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Higher Order Conditional Random Fields in Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Higher Order Conditional Random Fields in Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Segmentation</term>
					<term>Conditional Random Fields</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image's visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation involves assigning a visual object class label to every pixel in an image, resulting in a segmentation with a semantic meaning for each segment. While a strong pixel-level classifier is critical for obtaining high accuracy in this task, it is also important to enforce the consistency of the semantic segmentation output with visual features of the image. For example, segmentation boundaries should usually coincide with strong edges in the image, and regions in the image with similar appearance should have the same label.</p><p>Recent advances in deep learning have enabled researchers to create stronger classifiers, with automatically learned features, within a Convolutional Neural Network (CNN) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. This has resulted in large improvements in semantic segmentation accuracy on widely used benchmarks such as PASCAL VOC <ref type="bibr" target="#b3">[4]</ref>. CNN classifiers are now considered the standard choice for pixel-level classifiers used in semantic segmentation.</p><p>On the other hand, probabilistic graphical models have long been popular for structured prediction of labels, with constraints enforcing label consistency. Conditional Random Fields (CRFs) have been the most common framework, and various rich and  <ref type="figure">Fig. 1</ref>: Overview of our system We train a Higher Order CRF end-to-end with a pixelwise CNN classifier. Our higher order detection and superpixel potentials improve significantly over our baseline containing only pairwise potentials.</p><p>expressive models <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, based on higher order clique potentials, have been developed to improve segmentation performance.</p><p>Whilst some deep learning methods showed impressive performance in semantic segmentation without incorporating graphical models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>, current state-of-the-art methods <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> have all incorporated graphical models into the deep learning framework in some form. However, we observe that the CRFs that have been incorporated into deep learning techniques are still rather rudimentary as they consist of only unary and pairwise potentials <ref type="bibr" target="#b9">[10]</ref>. In this paper, we show that CRFs with carefully designed higher order potentials (potentials defined over cliques consisting of more than two nodes) can also be modelled as CNN layers when using mean field inference <ref type="bibr" target="#b12">[13]</ref>. The advantage of performing CRF inference within a CNN is that it enables joint optimisation of CNN classifier weights and CRF parameters during the end-to-end training of the complete system. Intuitively, the classifier and the graphical model learn to optimally co-operate with each other during the joint training.</p><p>We introduce two types of higher order potential into the CRF embedded in our deep network: object-detection based potentials and superpixel-based potentials. The primary idea of using object-detection potentials is to use the outputs of an off-the-shelf object detector as additional semantic cues for finding the segmentation of an image. Intuitively, an object detector with a high recall can help the semantic segmentation algorithm by finding objects appearing in an image. As shown in <ref type="figure">Fig. 1</ref>, our method is able to recover from poor segmentation unaries when we have a confident detector response. However, our method is robust to false positives identified by the object detector since CRF inference identifies and rejects false detections that do not agree with other types of energies present in the CRF.</p><p>Superpixel-based higher order potentials encourage label consistency over superpixels obtained by oversegmentation. This is motivated by the fact that regions defined by superpixels are likely to contain pixels from the same visual object. Once again, our formulation is robust to the violations of this assumption and errors in the initial superpixel generation step. In practice, we noted that this potential is effective for getting rid of small regions of spurious labels that are inconsistent with the correct labels of their neighbours.</p><p>We evaluate our higher order potentials on the PASCAL VOC 2012 semantic segmentation benchmark as well as the PASCAL Context dataset, to show significant improvements over our baseline and achieve state-of-the art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Before deep learning became prominent, semantic segmentation was performed with dense hand-crafted features which were fed into a per-pixel or region classifier <ref type="bibr" target="#b13">[14]</ref>. The individual predictions made by these classifiers were often noisy as they lacked global context, and were thus post-processed with a CRF, making use of prior knowledge such as the fact that nearby pixels, as well as pixels of similar appearance, are likely to share the same class label <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>The CRF model of <ref type="bibr" target="#b13">[14]</ref> initially contained only unary and pairwise terms in an 8-neighbourhood, which <ref type="bibr" target="#b15">[16]</ref> showed can result in shrinkage bias. Numerous improvements to this model were subsequently proposed including: densely connected pairwise potentials facilitating interactions between all pairs of image pixels <ref type="bibr" target="#b16">[17]</ref>, formulating higher order potentials defined over cliques larger than two nodes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> in order to capture more context, modelling co-occurrence of object classes <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>, and utilising the results of object detectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Recent advances in deep learning have allowed us to replace hand-crafted features with features learned specifically for semantic segmentation. The strength of these representations was illustrated by <ref type="bibr" target="#b2">[3]</ref> who achieved significant improvements over previous hand-crafted methods without using any CRF post-processing. Chen et al. <ref type="bibr" target="#b11">[12]</ref> showed further improvements by post-processing the results of a CNN with a CRF. Subsequent works <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b22">23]</ref> have taken this idea further by incorporating a CRF as layers within a deep network and then learning parameters of both the CRF and CNN together via backpropagation.</p><p>In terms of enhancements to conventional CRF models, Ladicky et al. <ref type="bibr" target="#b5">[6]</ref> proposed using an off-the-shelf object detector to provide additional cues for semantic segmentation. Unlike other approaches that refine a bounding-box detection to produce a segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>, this method used detector outputs as a soft constraint and can thus recover from object detection errors. Their formulation, however, used graph-cut inference, which was only tractable due to the absence of dense pairwise potentials. Object detectors have also been used by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, who also modelled variables that describe the degree to which an object hypothesis is accepted.</p><p>We formulate the detection potential in a different manner to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> so that it is amenable to mean field inference. Mean field permits inference with dense pairwise connections, which results in substantial accuracy improvements <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>. Furthermore, mean field updates related to our potentials are differentiable and its parameters can thus be learned in our end-to-end trainable architecture.</p><p>We also note that while the semantic segmentation problem has mostly been formulated in terms of pixels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>, some have expressed it in terms of superpixels <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. Superpixels can capture more context than a single pixel and computational costs can also be reduced if one considers pairwise interactions between superpixels rather than individual pixels <ref type="bibr" target="#b20">[21]</ref>. However, such superpixel representations assume that the segments share boundaries with objects in an image, which is not always true. As a result, several authors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> have employed higher order potentials defined over superpixels that encourage label consistency over regions, but do not strictly enforce it. This approach also allows multiple, non-hierarchical layers of superpixels to be integrated. Our formulation uses this kind of higher order potential, but in an end-to-end trainable CNN.</p><p>Graphical models have been used with CNNs in other areas besides semantic segmentation, such as in pose-estimation <ref type="bibr" target="#b28">[29]</ref> and group activity recognition <ref type="bibr" target="#b29">[30]</ref>. Alternatively, Ionescu et al. <ref type="bibr" target="#b30">[31]</ref> incorporated structure into a deep network with structured matrix layers and matrix backpropagation. However, the nature of models used in these works is substantially different to ours. Some early works that advocated gradient backpropagation through graphical model inference for parameter optimisation include <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> and <ref type="bibr" target="#b33">[34]</ref>.</p><p>Our work differentiates from the above works since, to our knowledge, we are the first to propose and conduct a thorough experimental investigation of higher order potentials that are based on detection outputs and superpixel segmentation in a CRF which is learned end-to-end in a deep network. Note that although <ref type="bibr" target="#b6">[7]</ref> formulated mean field inference with higher order potentials, they did not consider object detection potentials at all, nor were the parameters learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conditional Random Fields</head><p>We now review conditional random fields used in semantic segmentation and introduce the notation used in the paper. Take an image I with N pixels, indexed 1, 2, . . . , N . In semantic segmentation, we attempt to assign every pixel a label from a predefined set of labels L = {l 1 , l 2 , . . . , l L }. Define a set of random variables X 1 , X 2 , . . . , X N , one for each pixel, where each X i ∈ L. Let X = [X 1 X 2 . . . X N ] T . Any particular assignment x to X is thus a solution to the semantic segmentation problem.</p><p>We use notations {V}, and V (i) to represent the set of elements of a vector V, and the i th element of V, respectively. Given a graph G where the vertices are from {X} and the edges define connections among these variables, the pair (I, X) is modelled as a CRF characterised by Pr(X = x|I) = (1/Z(I)) exp(−E(x|I)), where E(x|I) is the energy of the assignment x and Z(I) is the normalisation factor known as the partition function. We drop the conditioning on I hereafter to keep the notation uncluttered. The energy E(x) of an assignment is defined using the set of cliques C in the graph G. More specifically, E(x) = c∈C ψ c (x c ), where x c is a vector formed by selecting elements of x that correspond to random variables belonging to the clique c, and ψ c (.) is the cost function for the clique c. The function, ψ c (.), usually uses prior knowledge about a good segmentation, as well as information from the image, the observation the CRF is conditioned on.</p><p>Minimising the energy yields the maximum a posteriori (MAP) labelling of the image i.e. the most probable label assignment given the observation (image). When dense pairwise potentials are used in the CRF to obtain higher accuracy, exact inference is impracticable, and one has to resort to an approximate inference method such as mean field inference <ref type="bibr" target="#b16">[17]</ref>. Mean field inference is particularly appealing in a deep learning setting since it is possible to formulate it as a Recurrent Neural Network <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CRF with Higher Order Potentials</head><p>Many CRF models that have been incorporated into deep learning frameworks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> have so far used only unary and pairwise potentials. However, potentials defined on higher order cliques have been shown to be useful in previous works such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>. The key contribution of this paper is to show that a number of explicit higher order potentials can be added to CRFs to improve image segmentation, while staying compatible with deep learning. We formulate these higher order potentials in a manner that mean field inference can still be used to solve the CRF. Advantages of mean field inference are twofold: First, it enables efficient inference when using densely-connected pairwise potentials. Multiple works, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref> have shown that dense pairwise connections result in substantial accuracy improvements, particularly at image boundaries <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. Secondly, we keep all our mean field updates differentiable with respect to their inputs as well as the CRF parameters introduced. This design enables us to use backpropagation to automatically learn all the parameters in the introduced potentials.</p><p>We use two types of higher order potential, one based on object detections and the other based on superpixels. These are detailed in Sections 4.1 and 4.2 respectively. Our complete CRF model is represented by</p><formula xml:id="formula_0">E(x) = i ψ U i (x i ) + i &lt; j ψ P ij (x i , x j ) + d ψ Det d (x d ) + s ψ SP s (x s ),<label>(1)</label></formula><p>where the first two terms ψ U i (.) and ψ P ij (., .) are the usual unary and densely-connected pairwise energies <ref type="bibr" target="#b16">[17]</ref> and the last two terms are the newly introduced higher order energies. Energies from the object detection take the form ψ Det</p><formula xml:id="formula_1">d (x d ),</formula><p>where vector x d is formed by elements of x that correspond to the foreground pixels of the d th object detection. Superpixel label consistency based energies take the form ψ SP s (x s ), where x s is formed by elements of x that correspond to the pixels belonging to the s th superpixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Object Detection Based Potentials</head><p>Semantic segmentation errors can be classified into two broad categories <ref type="bibr" target="#b34">[35]</ref>: recognition and boundary errors. Boundary errors occur when semantic labels are incorrect at the edges of objects, and it has been shown that densely connected CRFs with appearanceconsistency terms are effective at combating this problem <ref type="bibr" target="#b16">[17]</ref>. On the other hand, recognition errors occur when object categories are recognised incorrectly or not at all. A CRF with only unary and pairwise potentials cannot effectively correct these errors since they are caused by poor unary classification. However, we propose that a state-of-the-art object detector <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> capable of recognising and localising objects, can provide important information in this situation and help reduce the recognition error, as shown in <ref type="figure">Fig. 2</ref>.</p><p>A key challenge in feeding-in object-detection potentials to semantic segmentation are false detections. A naïve approach of adding an object detector's output to a CRF formulated to solve the problem of semantic segmentation would confuse the CRF due to the presence of the false positives in the detector's output. Therefore, a robust formulation, which can automatically reject object detection false positives when they <ref type="figure">Fig. 2</ref>: Utility of object detections as another cue for semantic segmentation For every pair, segmentation on the left was produced with only unary and pairwise potentials. Detection based potentials were added to produce the result on the right. Note how we are able to improve our segmentations for the bus, table and bird over their respective baselines. Furthermore, our system is able to reject erroneous detections such as the person in (b) and the bottle and chair in (d). Images were taken from the PASCAL VOC 2012 reduced validation set. Baseline results were produced using the public code and model of <ref type="bibr" target="#b9">[10]</ref>.</p><formula xml:id="formula_2">(a) (b) (c) (d) (e) (f)</formula><p>do not agree with other types of potentials in the CRF, is desired. Furthermore, since we are aiming for an end-to-end trainable CRF which can be incorporated into a deep neural network, the energy formulation should permit a fully differentiable inference procedure. We now propose a formulation which has both of these desired properties. Assume that we have D object detections for a given image, and that the d th detection is of the form (l d , s d , F d ), where l d ∈ L is the class label of the detected object, s d is the confidence score of the detection, and F d ⊆ {1, 2, . . . , N }, is the set of indices of the pixels belonging to the foreground of the detection. The foreground within a detection bounding box could be obtained using a foreground/background segmentation method (i.e. GrabCut <ref type="bibr" target="#b37">[38]</ref>), and represents a crude segmentation of the detected object. Using our detection potentials, we aim to encourage the set of pixels represented by F d , to take the label l d . However, this should not be a hard constraint since the foreground segmentation could be inaccurate and the detection itself could be a false detection. We therefore seek a soft constraint that assigns a penalty if a pixel in F d takes a label other than l d . Moreover, if other energies used in the CRF strongly suggest that many pixels in F d do not belong to the class l d , the detection d should be identified as invalid.</p><p>An approach to accomplish this is described in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b20">[21]</ref>. However, in both cases, dense pairwise connections were absent and different inference methods were used. In contrast, we would like to use the mean field approximation to enable efficient inference with dense pairwise connections <ref type="bibr" target="#b16">[17]</ref>, and also because its inference procedure is fully differentiable. We therefore use a detection potential formulation quite different to the ones used in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b20">[21]</ref>.</p><p>In our formulation, as done in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b20">[21]</ref>, we first introduce latent binary random variables Y 1 , Y 2 , . . . Y D , one for each detection. The interpretation for the random variable Y d that corresponds to the d th detection is as follows: If the d th detection has been found to be valid after inference, Y d will be set to 1, it will be 0 otherwise. Mean field inference probabilistically decides the final value of Y d . Note that, through this formulation, we can account for the fact that the initial detection could have been a false positive: some of the detections obtained from the object detector may be identified to be false following CRF inference.</p><p>All Y d variables are added to the CRF which previously contained only X i variables.</p><formula xml:id="formula_3">Let each (X d , Y d ), where {X d } = {X i ∈ {X}|i ∈ F d },</formula><p>form a clique c d in the CRF. We define the detection-based higher order energy associated with a particular assignment (x d , y d ) to the clique (X d , Y d ) as follows:</p><formula xml:id="formula_4">ψ Det d (X d = x d , Y d = y d ) =      w Det s d n d n d i=1 [x (i) d = l d ] if y d = 0, w Det s d n d n d i=1 [x (i) d = l d ] if y d = 1,<label>(2)</label></formula><p>where n d = |F d | is the number of foreground pixels in the d th detection, x (i) d is the i th element of the vector x d , w Det is a learnable weight parameter, and [ . ] is the Iverson bracket. Note that this potential encourages X (i) d s to take the value l d when Y d is 1, and at the same time encourages Y d to be 0 when many X </p><formula xml:id="formula_5">d for i = 1, 2, . . . , n d . That is, ψ Det d (X d = x d , Y d = y d ) = n d i=1 f d (x (i) d , y d ), where, f d (x (i) d , y d ) =    w Det s d n d [x (i) d = l d ] if y d = 0, w Det s d n d [x (i) d = l d ] if y d = 1.<label>(3)</label></formula><p>We make use of this simplification in Section 5 when deriving the mean field updates associated with this potential.</p><p>For the latent Y variables, in addition to the joint potentials with X variables, described in Eq. <ref type="formula" target="#formula_4">(2)</ref> and <ref type="formula" target="#formula_5">(3)</ref>, we also include unary potentials, which are initialised from the score s d of the object detection. The underlying idea is that if the object detector detects an object with high confidence, the CRF in turn starts with a high initial confidence about the validity of that detection. This confidence can, of course, change during the CRF inference depending on other information (e.g. segmentation unary potentials) available to the CRF.</p><p>Examples of input images with multiple detections and GrabCut foreground masks are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Note how false detections are ignored and erroneous parts of the foreground mask are also largely ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Superpixel Based Potentials</head><p>The next type of higher order potential we use is based on the idea that superpixels obtained from oversegmentation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> quite often contain pixels from the same visual object. It is therefore natural to encourage pixels inside a superpixel to have the same semantic label. Once again, this should not be a hard constraint in order to keep the Incorrect parts of the foreground segmentation of the main aeroplane, and entire TV detection have been ignored by CRF inference as they did not agree with the other energy terms. The person is a failure case though as the detection has caused part of the sofa to be erroneously labelled. algorithm robust to initial superpixel segmentation errors and to violations of this key assumption.</p><p>We use two types of energies in the CRF to encourage superpixel consistency in semantic segmentation. Firstly, we use the P n -Potts model type energy <ref type="bibr" target="#b40">[41]</ref>, which is described by,</p><formula xml:id="formula_6">ψ SP s (X s = x s ) =    w Low (l) if all x (i) s = l, w High otherwise,<label>(4)</label></formula><p>where w Low (l) &lt; w High for all l, and {X s } ⊂ {X} is a clique defined by a superpixel. The primary idea is that assigning different labels to pixels in the same superpixel incurs a higher cost, whereas one obtains a lower cost if the labelling is consistent throughout the superpixel. Costs w Low (l) and w High are learnable during the end-to-end training of the network. Secondly, to make this potential stronger, we average initial unary potentials from the classifier (the CNN in our case), across all pixels in the superpixel and use the average as an additional unary potential for those pixels. During experiments, we observed that superpixel based higher order energy helps in getting rid of small spurious regions of wrong labels in the segmentation output, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mean Field Updates and Their Differentials</head><p>This section discusses the mean field updates for the higher order potentials previously introduced. These update operations are differentiable with respect to the Q i (X i ) distribution inputs at each iteration, as well as the parameters of our higher order potentials. This allows us to train our CRF end-to-end as another layer of a neural network.</p><p>Take a CRF with random variables V 1 , V 2 , . . . , V N and a set of cliques C, which includes unary, pairwise and higher order cliques. Mean field inference approximates the joint distribution Pr(V = v) with the product of marginals i Q(V i = v i ). We use Q(V c = v c ) to denote the marginal probability mass for a subset {V c } of these variables. Where there is no ambiguity, we use the short-hand notation Q(v c ) to represent Q(V c = v c ). General mean field updates of such a CRF take the form <ref type="bibr" target="#b12">[13]</ref> </p><formula xml:id="formula_7">Q t+1 (V i = v) = 1 Z i exp   − c∈C {v c |v i =v} Q t (v c−i ) ψ c (v c )   ,<label>(5)</label></formula><p>where Q t is the marginal after the t th iteration, v c an assignment to all variables in clique c, v c−i an assignment to all variables in c except for V i , ψ c (v c ) is the cost of assigning v c to the clique c, and Z i is the normalisation constant that makes Q(V i = v) a probability mass function after the update.</p><p>Updates from Detection Based Potentials Following Eq. (3) above, we now use Eq. (5) to derive the mean field updates related to ψ Det d . The contribution from ψ Det d to the update of Q(X</p><formula xml:id="formula_8">(i) d = l) takes the form {(x d ,y d )|x (i) d =l} Q(x d−i , y d ) ψ Det d (x d , y d ) =    w Det s d n d Q(Y d = 0) if l = l d , w Det s d n d Q(Y d = 1) otherwise,<label>(6)</label></formula><p>where x d−i is an assignment to X d with the i th element deleted. Using the same equations, we derive the contribution from the energy ψ Det d to the update of Q(Y d = b) to take the form</p><formula xml:id="formula_9">{(x d ,y d )|y d =b} Q(x d ) ψ Det d (x d , y d ) =    w Det s d n d n d i=1 Q(X (i) d = l d ) if b = 0, w Det s d n d n d i=1 (1 − Q(X (i) d = l d )) otherwise.<label>(7)</label></formula><p>It is possible to increase the number of parameters in ψ Det d (.). Since we use backpropagation to learn these parameters automatically during end-to-end training, it is desirable to have a high number of parameters to increase the flexibility of the model. Following this idea, we made the weight w Det class specific, that is, a function w Det (l d ) is used instead of w Det in Eqs. (2), (6) and <ref type="bibr" target="#b6">(7)</ref>. The underlying assumption is that detector outputs can be very helpful for certain classes, while being not so useful for classes that the detector performs poorly on, or classes for which foreground segmentation is often inaccurate.</p><p>Note that due to the presence of detection potentials in the CRF, error differentials calculated with respect to the X variable unary potentials and pairwise parameters will no longer be valid in the forms described in <ref type="bibr" target="#b9">[10]</ref>. The error differentials with respect to the X and Y variables, as well as class-specific detection potential weights w Det (l) are included in the supplementary material.</p><p>Updates for Superpixel Based Potentials The contribution from the P n -Potts type potential to the mean field update of Q(x i = l), where pixel i is in the superpixel clique s, was derived in <ref type="bibr" target="#b6">[7]</ref> as</p><formula xml:id="formula_10">{x s |x (i) s =l} Q(x s−i ) ψ SP s (x s ) = w Low (l) j∈c,j =i Q(X j = l)+w High   1 − j∈c−i Q(X j = l)   .</formula><p>(8) This update operation is differentiable with respect to the parameters w Low (l) and w High , allowing us to optimise them via backpropagation, and also with respect to the Q(X) values enabling us to optimise previous layers in the network.</p><p>Convergence of parallel mean field updates Mean field with parallel updates, as proposed in <ref type="bibr" target="#b16">[17]</ref> for speed, does not have any convergence guarantees in the general case. However, we usually empirically observed convergence with higher order potentials, without damping the mean field update as described in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42]</ref>. This may be explained by the fact that the unaries from the initial pixelwise-prediction part of our network provide a good initialisation. In cases where the mean field energy did not converge, we still empirically observed good final segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate our new CRF formulation on two different datasets using the CRF-RNN network <ref type="bibr" target="#b9">[10]</ref> as the main baseline, since we are essentially enriching the CRF model of <ref type="bibr" target="#b9">[10]</ref>. We then present ablation studies on our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental set-up and results</head><p>Our deep network consists of two conceptually different, but jointly trained stages. The first, "unary" part of our network is formed by the FCN-8s architecture <ref type="bibr" target="#b2">[3]</ref>. It is initialised from the Imagenet-trained VGG-16 network <ref type="bibr" target="#b1">[2]</ref>, and then fine-tuned with data from the VOC 2012 training set <ref type="bibr" target="#b3">[4]</ref>, extra VOC annotations of <ref type="bibr" target="#b42">[43]</ref> and the MS COCO <ref type="bibr" target="#b43">[44]</ref> dataset.  Ours 77.9 DPN <ref type="bibr" target="#b8">[9]</ref> 77.5 Centrale Super Boundaries <ref type="bibr" target="#b44">[45]</ref> 75.7 Dilated Convolutions <ref type="bibr" target="#b45">[46]</ref> 75.3 BoxSup <ref type="bibr" target="#b34">[35]</ref> 75.2 DeepLab Attention <ref type="bibr" target="#b46">[47]</ref> 75.1 CRF-RNN (baseline) <ref type="bibr" target="#b9">[10]</ref> 74.7 DeepLab WSSL <ref type="bibr" target="#b47">[48]</ref> 73.9 DeepLab <ref type="bibr" target="#b11">[12]</ref> 72.7 The output of the first stage is fed into our CRF inference network. This is implemented using the mean field update operations and their differentials described in Section 5. Five iterations of mean field inference were performed during training. Our CRF network has two additional inputs in addition to segmentation unaries obtained from the FCN-8s network: data from the object detector and superpixel oversegmentations of the image.</p><p>We used the publicly available code and model of the Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> object detector. The fully automated version of GrabCut <ref type="bibr" target="#b37">[38]</ref> was then used to obtain foregrounds from the detection bounding boxes. These choices were made after conducting preliminary experiments with alternate detection and foreground segmentation algorithms.</p><p>Four levels of superpixel oversegmentations were used, with increasing superpixel size to define the cliques used in this potential. Four levels were used since performance on the VOC validation set stopped increasing after this number. We used the superpixel method of <ref type="bibr" target="#b38">[39]</ref> as it was shown to adhere to object boundaries the best <ref type="bibr" target="#b39">[40]</ref>, but our method generalises to any oversegmentation algorithm.</p><p>We trained the full network end-to-end, optimising the weights of the CNN classifier (FCN-8s) and CRF parameters jointly. We initialised our network using the publicly available weights of <ref type="bibr" target="#b9">[10]</ref>, and trained with a learning rate of 10 −10 and momentum of 0.99. The learning rate is low because the loss was not normalised by the number of pixels in the training image. This is to have a larger loss for images with more pixels. When training our CRF, we only used VOC 2012 data <ref type="bibr" target="#b3">[4]</ref> as it has the most accurate labelling, particularly around boundaries.</p><p>PASCAL VOC 2012 Dataset The improvement obtained by each higher order potential was evaluated on the same reduced validation set <ref type="bibr" target="#b2">[3]</ref> used by our baseline <ref type="bibr" target="#b9">[10]</ref>. As <ref type="table" target="#tab_1">Table  1</ref> shows, each new higher order potential improves the mean IoU over the baseline. We only report test set results for our best method since the VOC guidelines discourage the use of the test set for ablation studies. On the test set <ref type="table" target="#tab_2">(Table 2)</ref>, we outperform our baseline by 3.2% which equates to a 12.6% reduction in the error rate. This sets a new state-of-the-art on the VOC dataset. Qualitative results highlighting success and failure cases of our algorithm, as well as more detailed results, are shown in our supplementary material. <ref type="table" target="#tab_3">Table 3</ref> shows our state-of-the-art results on the recently released PASCAL Context dataset <ref type="bibr" target="#b49">[50]</ref>. We trained on the provided training set of 4998 images, and evaluated on the validation set of 5105 images. This dataset augments VOC with annotations for all objects in the scene. As a result, there are 59 classes as opposed to the 20 in the VOC dataset. Many of these new labels are "stuff" classes such as "grass" and "sky". Our object detectors are therefore only trained for 20 of the 59 labels in this dataset. Nevertheless, we improve by 0.8% over the previous state-of-the-art <ref type="bibr" target="#b34">[35]</ref> and 2% over our baseline <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Studies</head><p>We perform additional experiments to determine the errors made by our system, show the benefits of end-to-end training and compare our detection potentials to a simpler baseline. Unless otherwise stated, these experiments are performed on the VOC 2012 reduced validation set.</p><p>Error Analysis To analyse the improvements made by our higher order potentials, we separately evaluate the performance on the "boundary" and "interior" regions in a similar manner to <ref type="bibr" target="#b34">[35]</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 5 c)</ref> and d), we consider a narrow band (trimap <ref type="bibr" target="#b15">[16]</ref>) around the "void" labels annotated in the VOC 2012 reduced validation set. The mean IoU of pixels lying within this band is termed the "Boundary IoU" whilst the "Interior IoU" is evaluated outside this region. <ref type="figure" target="#fig_3">Fig. 5</ref> shows our results as the trimap width is varied. Adding the detection potentials improves the Interior IoU over our baseline (only pairwise potentials <ref type="bibr" target="#b9">[10]</ref>) as the object detector may recognise objects in the image which the pixelwise classification stage of our network may have missed out. However, the detection potentials also improve the Boundary IoU for all tested trimap widths as well. Improving the recognition of pixels in the interior of an object also helps with delineating the boundaries since the strength of the pairwise potentials exerted by the Q distributions at each of the correctly-detected pixels increase.</p><p>Our superpixel priors also increase the Interior IoU with respect to the baseline. Encouraging consistency over regions helps to get rid of spurious regions of wrong labels (as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>). <ref type="figure" target="#fig_3">Fig. 5</ref> suggests that most of this improvement occurs in the interior of an object. The Boundary IoU is slightly lower than the baseline, and this may be due to the fact that superpixels do not always align correctly with the edges of an object (the "boundary recall" of various superpixel methods are evaluated in <ref type="bibr" target="#b39">[40]</ref>).</p><p>We can see that the combination of detection and superpixel potentials results in a substantial improvement in our Interior IoU. This is the primary reason our overall IoU on the VOC benchmark increases with higher order potentials. <ref type="table" target="#tab_4">Table 4</ref> shows how end-to-end training outperforms piecewise training. We trained the CRF piecewise by freezing the weights of the unary part of the network, and only learning the CRF parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of end-to-end training</head><p>Our results in <ref type="table" target="#tab_2">Table 2</ref> used the FCN-8s <ref type="bibr" target="#b2">[3]</ref> architecture to generate unaries. To show that our higher order potentials improve performance regardless of the underlying CNN used for producing unaries, we also perform an experiment using our reimplementation of the "front-end" module proposed in the Dilated Convolution Network (DCN) of <ref type="bibr" target="#b45">[46]</ref> instead of FCN-8s. <ref type="table" target="#tab_4">Table 4</ref> shows that end-to-end training of the CRF yields considerable improvements over piecewise training. This was the case when using either FCN-8s or DCN for obtaining the initial unaries before performing CRF inference with higher order potentials. This suggests that our CRF network module can be plugged into different architectures and achieve performance improvements.</p><p>Baseline for detections To evaluate the efficacy of our detection potentials, we formulate a simpler baseline since no other methods use detection information at inference time (BoxSup <ref type="bibr" target="#b34">[35]</ref> derives ground truth for training using ground-truth bounding boxes).</p><p>Our baseline is similar to CRF-RNN <ref type="bibr" target="#b9">[10]</ref>, but prior to CRF inference, we take the segmentation mask from the object detection and add a unary potential proportional to the detector's confidence to the unary potentials for those pixels. We then perform meanfield inference (with only pairwise terms) on these "augmented" unaries. Using this method, the mean IoU increases from 72.9% to 73.6%, which is significantly less than the 74.9% which we obtained using only our detection potentials without superpixels <ref type="table" target="#tab_1">(Table 1)</ref>.</p><p>Our detection potentials perform better since our latent Y detection variables model whether the detection hypothesis is accepted or not. Our CRF inference is able to evaluate object detection inputs in light of other potentials. Inference increases the relative score of detections which agree with the segmentation, and decreases the score of detections which do not agree with other energies in the CRF. <ref type="figure">Figures 2 b) and d)</ref> show examples of false-positive detections that have been ignored and correct detections that have been used to refine our segmentation. Our baseline, on the other hand, is far more sensitive to erroneous detections as it cannot adjust the weight given to them during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a CRF model with two different higher order potentials to tackle the semantic segmentation problem. The first potential is based on the intuitive idea that object detection can provide useful cues for semantic segmentation. Our formulation is capable of automatically rejecting false object detections that do not agree at all with the semantic segmentation. Secondly, we used a potential that encourages superpixels to have consistent labelling. These two new potentials can co-exist with the usual unary and pairwise potentials in a CRF.</p><p>Importantly, we showed that efficient mean field inference is still possible in the presence of the new higher order potentials and derived the explicit forms of the mean field updates and their differentials. This enabled us to implement the new CRF model as a stack of CNN layers and to train it end-to-end in a unified deep network with a pixelwise CNN classifier. We experimentally showed that the addition of higher order potentials results in a significant increase in semantic segmentation accuracy allowing us to reach state-of-the-art performance. This work was supported by ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1, EPSRC/MURI grant EP/N019474/1 and the Clarendon Fund.</p><formula xml:id="formula_11">Q 0 (Y d = b) ← s b d (1 − s d ) (1−b) , ∀d, b Initialisation for t = 0 : T − 1 do E t (X i = l) ← UnaryUpdate + PairwiseUpdate + DetectionUpdate + SuperpixelUpdate, ∀i, l E t (Y d = b) ← Y UnaryUpdate + Y DetectionUpdate Mean field updates Q t+1 (X i = l) ← 1 Z i exp −E t (X i = l) , ∀i, l Q t+1 (Y d = b) ← 1 Z d exp −E t (Y d = b) , ∀d, b Normalising end for</formula><p>For the explicit forms of the UnaryUpdate and PairwiseUpdate above, and their differentials, we refer the reader to <ref type="bibr" target="#b9">[10]</ref> and discuss the terms DetectionUpdate and SuperpixelUpdate in detail below.</p><p>Let us assume that only one object detection of the form (l d , s d , F d ) is available for the image under consideration. When multiple detections are present, simply a summation of the updates and differentials discussed below apply. Therefore, no generality is lost with this assumption. Similarly, we can assume that only one superpixel clique {X s } is present, without a loss of generality.</p><p>Assuming that pixel i in Algorithm 1 belongs to F d , Eq. (6) in the main paper described the exact form of DetectionUpdate. Similarly, assuming that pixel i belongs to {X s } Eq. (8) described the form of SuperpixelUpdate.</p><p>Let L denote the value of the loss function calculated at the output of the deep network. This could be the softmax loss or any other appropriate loss function. During backpropagation, we get the error signal ∂L ∂Q T at the output of the mean field inference. Using this error information, we need to compute the derivative of the loss L with respect to the X unaries and various CRF parameters. Note that, if we compute the relevent differentials for only one iteration of the mean field algorithm, it is possible to calculate them for multiple iterations using the recurrent behaviour of the iterations.</p><p>Note that, by looking at Normalising step of Algorithm 1, it is trivial to calculate ∂Q t+1 ∂E t . Therefore, we can then calculate ∂L ∂E t using the chain rule. This is same as backpropagation of the usual softmax operation in a deep network (up to a negative sign). Using this observation we can calculate the necessary differentials to take the forms shown below:</p><formula xml:id="formula_12">∂L ∂w Det = s d n d n d i=1 ∂L E t (X (i) d = l d ) Q t (Y d = 1) + (9) l =l d ∂L ∂E t (X (i) d = l ) Q t (Y d = 1) + ∂L ∂E t (Y d = 0) s d n d n d i=1 Q t (X (i) d = l d ) + ∂L ∂E t (Y d = 1) s d n d n d i=1 1 − Q t (X (i) d = l d ) ∂L ∂Q t (X (i) d = l d ) = w Det ∂L ∂E t (Y d = 0) − w Det ∂L ∂E t (Y d = 1)<label>(10)</label></formula><formula xml:id="formula_13">∂L ∂Q t (Y d = 0) = w Det s d n d n d i=1 ∂L E t (X (i) d = l d )<label>(11)</label></formula><formula xml:id="formula_14">∂L ∂Q t (Y d = 1) = w Det s d n d n d i=1 l =l d ∂L ∂E t (X (i) d = l )<label>(12)</label></formula><formula xml:id="formula_15">∂L ∂w Low (l) = i∈s   ∂L ∂E t (X (i) s = l) j∈c,j =i Q t (X j = l)  <label>(13)</label></formula><formula xml:id="formula_16">∂L ∂w High = i∈s l∈L   ∂L ∂E t (X (i) s = l)   1 − j∈c,j =i Q t (X j = l)    <label>(14)</label></formula><p>Effect of the superpixel potentials on the derivatives ∂L ∂Q t (X i =l) were negligible. Therefore, we ignored them in our calculations. <ref type="table" target="#tab_5">Table 5</ref> presents more detailed results of our method, and that of other state-of-the-art techniques, on the PASCAL VOC 2012 test set. In particular, we present the accuracy for every class in the VOC test set. Note that our per-class accuracy improves over our baseline, CRF-RNN <ref type="bibr" target="#b9">[10]</ref>, for all of the 20 classes in PASCAL VOC. <ref type="figure">Figure 6</ref> shows more sample results of our system, compared to our baseline, CRFas-RNN <ref type="bibr" target="#b9">[10]</ref>. <ref type="figure">Figure 7</ref> shows examples of failure cases of our method. <ref type="figure" target="#fig_4">Figure 8</ref> examines the effect of each of our potentials. Finally, <ref type="figure">Figure 9</ref> shows a qualitative comparison between the output of our system and other current methods on the PASCAL VOC 2012 test set. Input image CRF-as-RNN <ref type="bibr" target="#b9">[10]</ref> Our method Ground truth <ref type="figure">Fig. 6</ref>: Examples of images where our method has improved over our baseline, CRF-as-RNN <ref type="bibr" target="#b9">[10]</ref>. The input images have the detection bounding boxes overlaid on them. Note that the method of <ref type="bibr" target="#b9">[10]</ref> does not make use of this information. The improvements from our method are due to our detection potentials, as well as our superpixel based potentials. Note that all images are from the reduced validation set of VOC 2012 and have not been trained on at all. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experimental Results</head><p>Input image CRF-as-RNN <ref type="bibr" target="#b9">[10]</ref> Our method Ground truth <ref type="figure">Fig. 7</ref>: Examples of failure cases where our method has performed poorly. The first row shows an example of how the detection of the person has now resulted in the sofa being misclassified (although our system is able to reject the other false detection). Our superpixel potentials have a tendency to remove spurious noise by enforcing consistency within regions. However, as shown in the second row, sometimes the "noise" being removed is actually the correct label. In the other cases, we are limited by our pixelwise classification unaries which are poor. Our superpixel and detection potentials are not always able to compensate for this. Note that all images are from the reduced validation set of VOC 2012 and have not been trained on at all. The input images have the detection bounding boxes overlaid on them. Note that the method of <ref type="bibr" target="#b9">[10]</ref> does not make use of this information. Best viewed in color. A case where the superpixel worsens the result as, although the output is more consistent among superpixel regions, some pixels have had their correct labels removed. However, the correct detection improves the result, and the output of combining superpixel and detection potentials is actually better than either potential in isolation. (Row 6) Here, the detection (although correct) worsens the output due to its imprecise foreground mask. Superpixel potentials also exacerbate the result, since the legs of the chair and the chair's shadow are confused to be part of the same superpixel region. However, when the two potentials are combined, the result is slightly better than with only detection potentials.</p><p>Input image FCN-8s <ref type="bibr" target="#b2">[3]</ref> Deeplab <ref type="bibr" target="#b11">[12]</ref> CRF-as-RNN <ref type="bibr" target="#b9">[10]</ref> Our method Ground truth <ref type="figure">Fig. 9</ref>: Qualitative comparison with other current methods. Sample results of our method compared to other current techniques on VOC 2012. We reproduced the segmentation results of Deeplab from their original publication, whilst we reproduced the results of FCN-8s and CRF-as-RNN from their publicly-available source code. Best viewed in colour.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(i) d s do not take l d . In other words, it enforces the consistency among X(i) d s and Y d . An important property of the above definition of ψ Det d (.) is that it can be simplified as a sum of pairwise potentials between Y d and each X (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Effects of imperfect foreground segmentation (a,b) Detected objects, as well as the foreground masks obtained from GrabCut. (c,d) Output using detection potentials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Segmentation enhancement from superpixel based potentials (a) The output of our system without any superpixel potentials. (b) Superpixels obtained from the image using the method of<ref type="bibr" target="#b38">[39]</ref>. Only one "layer" of superpixels is shown. In practice, we used four. (c) The output using superpixel potentials. The result has improved as we encourage consistency over superpixel regions. This removes some of the spurious noise that was present previously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Error analysis on VOC 2012 reduced validation set The IoU is computed for boundary and interior regions for various trimap widths. An example of the Boundary and Interior regions for a sample image using a width of 9 pixels is shown in white in the top row. Black regions are ignored in the IoU calculation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of pairwise potentials, superpixel and pairwise potentials, detection and pairwise potentials, and a combination of all three (Row 1 and 2) These are examples where superpixel potentials help to remove spurious noise in the output but detection potentials do not affect the result. The final result still improves when all potentials are combined. (Row 3) Detection potentials greatly improve the result by recognising the train correctly (the pixelwise unaries are largest for "bus"). And superpixels, when combined with detections, slightly improve the output. (Row 4) An example where both superpixel and detection potentials improve the final output. (Row 5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1511.08119v4 [cs.CV] 29 Jul 2016</figDesc><table><row><cell></cell><cell></cell><cell>Baseline</cell><cell>Superpixels only Detections only</cell></row><row><cell></cell><cell>Object Detector</cell><cell></cell></row><row><cell></cell><cell>Pixelwise CNN</cell><cell>Higher Order CRF</cell></row><row><cell></cell><cell></cell><cell>trained end-to-end</cell></row><row><cell>Input</cell><cell>Superpixel Generator</cell><cell cols="2">Final Result</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of each higher order potential with baseline on VOC 2012 reduced validation set</figDesc><table><row><cell>Method</cell><cell>Reduced val set(%)</cell></row><row><cell>Baseline (unary + pairwise) [10]</cell><cell>72.9</cell></row><row><cell>Superpixels only</cell><cell>74.0</cell></row><row><cell>Detections only</cell><cell>74.9</cell></row><row><cell>Detections and Superpixels</cell><cell>75.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean IoU accuracy on VOC 2012 test set. All methods are trained with MS COCO [44] data</figDesc><table><row><cell>Method</cell><cell>Test set(%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Mean Intersection over Union (IoU) results on PASCAL Context validation set compared to other current methods.</figDesc><table><row><cell>Method</cell><cell cols="6">Ours BoxSup [35] ParseNet [49] CRF-RNN [10] FCN-8s [3] CFM [28]</cell></row><row><cell cols="2">Mean IoU (%) 41.3</cell><cell>40.5</cell><cell>40.4</cell><cell>39.3</cell><cell>37.8</cell><cell>34.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of mean IoU (%) obtained on VOC 2012 reduced validation set from end-to-end and piecewise training</figDesc><table><row><cell>Method</cell><cell>FCN-8s DCN</cell></row><row><cell>Unary only, fine-tuned on COCO</cell><cell>68.3 68.6</cell></row><row><cell>Pairwise CRF trained piecewise</cell><cell>69.5 70.7</cell></row><row><cell>Pairwise CRF trained end-to-end</cell><cell>72.9 72.5</cell></row><row><cell cols="2">Higher Order CRF trained piecewise 73.6 73.5</cell></row><row><cell cols="2">Higher Order CRF trained end-to-end 75.8 75.0</cell></row><row><cell>Test set performance of best model</cell><cell>77.9 76.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the mean Intersection over Union (IoU) accuracy of our approach and other state-of-the-art methods on the Pascal VOC 2012 test set. Scores for other methods were taken from the original authors' publications.</figDesc><table><row><cell cols="2">tv</cell><cell></cell></row><row><cell cols="2">plant sheep sofa train</cell><cell></cell></row><row><cell>per-</cell><cell>son</cell><cell></cell></row><row><cell cols="2">aero-bike bird boat bottle bus plane car cat chair cow table dog horse mbike</cell><cell>92.5 59.1 90.3 70.6 74.4 92.4 84.1 88.3</cell></row><row><cell>Mean</cell><cell>IoU(%)</cell><cell>77.9</cell></row><row><cell>Methods trained with</cell><cell>COCO</cell><cell>Our method</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Appendix A of this supplementary material presents the derivatives of the mean field updates which we use for inference in our Conditional Random Field (CRF). Appendix B shows detailed qualitative results for the experiments described in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivatives of Mean Field Updates</head><p>The pseudocode for the mean field inference algorithm with latent Y detection variables is shown below in Algorithm 1. We use the same notation used in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Mean Field Inference</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Associative hierarchical crfs for object class image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">What, where and how many? combining object detectors and crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="424" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Filter-based mean-field inference for random fields with higher-order terms and product label-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Textonboost for image understanding: Multiclass object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Á</forename><surname>Carreira-Perpiñán</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph cut based inference with co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="239" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">Objects in context. In: ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Harmony potentials for joint classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="702" to="709" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A dynamic conditional random field model for joint labeling of object and scene classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="733" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeply learning the messages in message passing inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="361" to="369" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Layered object models for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relating things and stuff via objectproperty interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1370" to="1383" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semantic segmentation with secondorder pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1799" to="1807" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep structured models for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2965" to="2973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning graphical model parameters with approximate marginal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning message-passing inference machines for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">P3 &amp; beyond: Solving energies with higher order cliques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baqu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<title level="m">Principled Parallel Mean-Field Inference for Discrete Random Fields. In: CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV, IEEE</publisher>
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards unified object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Free-form region description with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilated</forename><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno>12] 72.7 89.1 38.3 88.1 63.3 69.7</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
	<note>1 83.1 85.0 29.3 76.5 56.5 79.8 77.9 85.8 82.4 57.4 84.3 54.9 80.5 64.1 Methods trained without COCO Our method 73.9 89.3 40.0 81.6 65.1 71.7 90.1 81.3 85.7 32.4 82.1 62.2 82.6 83.7 84.5 81.1 60.8 85.2 49.6 80.0 69.9</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconvnet</surname></persName>
		</author>
		<idno>51] 72.5 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 CRF-as-RNN [10] 72.0</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
	<note>5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno>12] 71.6 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
