<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Multiple Feedback Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilei</forename><surname>Li</surname></persName>
							<email>qilei.li@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sichuan University Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sichuan University Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
							<email>ggjeon@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sichuan University Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
							<email>kailiu@scu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">College of Electrical Engineering</orgName>
								<orgName type="institution">Sichuan University Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sichuan University Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University Xi&apos;an</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Embedded Systems Engineering</orgName>
								<orgName type="institution">Incheon National University Incheon</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Multiple Feedback Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LI, LI, LU, JEON, LIU, YANG: GMFN FOR IMAGE SUPER-RESOLUTION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rapid development of deep learning (DL) has driven single image super-resolution (SR) into a new era. However, in most existing DL based image SR networks, the information flows are solely feedforward, and the high-level features cannot be fully explored. In this paper, we propose the gated multiple feedback network (GMFN) for accurate image SR, in which the representation of low-level features are efficiently enriched by rerouting multiple high-level features. We cascade multiple residual dense blocks (RDBs) and recurrently unfolds them across time. The multiple feedback connections between two adjacent time steps in the proposed GMFN exploits multiple high-level features captured under large receptive fields to refine the low-level features lacking enough contextual information. The elaborately designed gated feedback module (GFM) efficiently selects and further enhances useful information from multiple rerouted high-level features, and then refine the low-level features with the enhanced high-level information. Extensive experiments demonstrate the superiority of our proposed GMFN against stateof-the-art SR methods in terms of both quantitative metrics and visual quality. Code is available at https://github.com/liqilei/GMFN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image super-resolution (SR) aims to reconstruct a high-resolution (HR) image from its corrupted low-resolution (LR) measurement. It's an ill-posed problem since an LR c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</p><p>*  image can be degraded from multiple HR images. In recent years, the development of deep learning (DL) based high-level vision (skip connections <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> and attention mechanism <ref type="bibr" target="#b8">[9]</ref>) helps networks for image SR become much deeper: from 3 layers in SRCNN <ref type="bibr" target="#b2">[3]</ref> to about 400 layers in RCAN <ref type="bibr" target="#b31">[32]</ref>, and also made the effects of image SR a truly breakthrough <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>. Nevertheless, as the network deepens, the required parameters are rapidly increasing. To alleviate this problem, the recurrent structures were exploited in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>However, nearly all the DL based image SR networks are wholly feedforward: the features solely flow from the shallower layers to deeper ones, subsequently, the high-level features extracted from the top layer are directly used to reconstruct an SR image. For these feedforward networks, since the receptive fields in shallower layers are smaller than deeper ones, shallower layers cannot take the valuable contextual information into account. Such a shortcoming hinders the reconstruction ability to some extent.</p><p>The feedback mechanism in deep networks aims to refine the low-level features by propagating high-level features to the shallow layers. With the help of high-level information, low-level features become more representative and informative. It has been widely exploited in many high-level vision tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> but has rarely been employed for image SR. Although SRFBN <ref type="bibr" target="#b16">[17]</ref> explored the feasibility of feedback mechanism for image SR, its feedback connections only propagate the highest-level feature to a shallow layer, other high-level information captured under different sizes of receptive fields is omitted. Hence, such a design neither fully exploits high-level features, nor adequately refines the low-level features.</p><p>Based on the above considerations, we propose the gated multiple feedback network (GMFN) for image SR. Since not only the highest-level feature is effective in refining lowlevel features, we employ multiple feedback connections to transmit multiple high-level features to shallow layers. However, excessive high-level features may be overly redundant, and directly using them may conflict with the original low-level features. Consequently, we design gated feedback modules (GFMs) to adaptively select as well as enhance useful highlevel information to refine low-level features. Thanks to the valuable contextual information from the high-level features, the low-level features become more representative, which will intrinsically improve the reconstruction ability of the network. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our proposed GMFN shows better visual quality in comparison with other state-of-the-art image SR methods.</p><p>The contributions of our work are summarized as follows:</p><p>• We propose the gated multiple feedback network (GMFN) for accurate image SR. Extensive experiments demonstrate the superiority of the proposed GMFN among other state-of-the-art SR methods. Particularly, our final model unfolded with two time steps and each contains 7 residual dense blocks (RDBs) outperforms RDN <ref type="bibr" target="#b32">[33]</ref> which employs 16 RDBs.</p><p>• We design the multiple feedback connections to propagate multiple hierarchical highlevel features for refining the low-level features. Since high-level features are captured under large receptive fields, they possess more contextual information which is lacking in low-level features. With the help of valuable contextual information introduced by multiple feedback connections, low-level features become more representative, and then the reconstruction performance is intrinsically improved.</p><p>• We design the simple yet efficient gated feedback module (GFM) to adaptively select and further enhance useful information from multiple rerouted high-level features for refining low-level features. Since only the useful information is permitted to pass, the redundant information among high-level features is efficiently eliminated. The selected and enhanced high-level information enables low-level features to be more informative.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feedback mechanism</head><p>The feedback mechanism in deep network empowers the low-level features to become more representative and informative by propagating the high-level information extracted from deep layers to shallow layers. It has been widely studied for various computer vision tasks (e.g. classification <ref type="bibr" target="#b28">[29]</ref>, pose estimation <ref type="bibr" target="#b1">[2]</ref>, and so on <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>). The majority of feedback connections in these networks are single-to-single, which means only the highest-level features are transmitted to the shallowest layer. Following a different direction, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> applied the single-to-multiple feedback connections to scene parsing and salient object detection, in which the highest-level features are transmitted to multiple shallow layers. They claimed that delivering other high-level features back would introduce redundant information and it might hurt performance on high-level vision tasks. Nevertheless, we argue that such single-to-single and single-to-multiple feedback connection designs are not suitable for image SR task since different levels of features are captured under different receptive fields, every piece of them is significant in reconstructing an SR image.</p><p>Taking flaws of previous works into consideration, we introduce new types of feedback connections for accurate image SR, in which multiple hierarchical high-level features are transmitted to shallower layer(s). In other words, the proposed feedback connections are naturally multiple-to-single and multiple-to-multiple. Moreover, we design the gated feedback module to adaptively eliminate redundant information among propagated high-level features, and refine low-level features by using selected high-level information. The valuable contextual knowledge from the high-level information enables the low-level features to be more informative and representative, hence the reconstruction performance is intrinsically improved. Experimental results demonstrate that our gated multiple feedback connections obviously outperform both single-to-single and single-to-multiple ones (see Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep learning based image SR</head><p>Recently, deep learning based image SR technology has been rapidly developed over the pioneering work <ref type="bibr" target="#b2">[3]</ref>. The input of the network has changed from the interpolated LR image <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> to the original LR image <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>. By doing so, the required computational cost was quadratically saved, and the motion effect caused by interpolation operation was efficiently alleviated. Furthermore, the applications of various skip connections helped the networks went deeper and obtained better reconstruction performance. EDSR <ref type="bibr" target="#b18">[19]</ref> and RCAN <ref type="bibr" target="#b31">[32]</ref> employed residual skip connections <ref type="bibr" target="#b7">[8]</ref>, SRDenseNet <ref type="bibr" target="#b25">[26]</ref> applied dense skip connections <ref type="bibr" target="#b9">[10]</ref>, and RDN <ref type="bibr" target="#b32">[33]</ref> further integrated residual and dense skip connections together. However, these networks require a huge amount of parameters.</p><p>Recurrent structure can effectively reduce the parameters of the network. It has been widely applied to image SR. Specifically, DRCN <ref type="bibr" target="#b13">[14]</ref> and DRRN <ref type="bibr" target="#b24">[25]</ref> can be explained as recurrent neural networks (RNNs) if we regard the input LR image as the initial hidden state, and zero input as the input state <ref type="bibr" target="#b4">[5]</ref>. Based on this view, DSRN <ref type="bibr" target="#b4">[5]</ref> and NLRN <ref type="bibr" target="#b19">[20]</ref> designed dual-state and introduced non-local operations for image SR. In these RNN-based methods, however, the information flows from LR image to HR image are solely feedforward. Though SRFBN <ref type="bibr" target="#b16">[17]</ref> explored the feasibility of feedback mechanism for image SR by designing an RNN with single-to-single feedback connections which deliver the highest-level features to a shallow layer. We argue that SRFBN fails to fully use other high-level features captured under large receptive fields, thus it cannot efficiently refine the low-level features.</p><p>In contrast, we propose GMFN to make the use of multiple high-level features to enrich the representation of low-level features by recurrently using feedback connections. Except multiple feedback information flows, the proposed GMFN has three other main differences compared with the aforementioned RNN-based methods. First, there are multiple recurrent connections, rather than one or two, between two adjacent time steps in our proposed GMFN. Second, in contrast to block-wise recurrent connections, our recurrent connections in the proposed GMFN can bypass multiple blocks, and thus are more flexible. Third, the features carried by recurrent connections in our proposed GMFN are first sent to the gated feedback module (GFM) for selecting meaningful information rather than directly sent to the recurrent block at the next time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gated Multiple Feedback Network for Image SR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network framework</head><p>As mentioned in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>, the core merit of a feedback system is propagating output to input in an iterative manner. Following this formulation, our proposed gated multiple feedback network (GMFN) is naturally designed as a convolutional recurrent neural network unrolling T time steps, and the sub-network at each time step can be regarded as an independent convolutional neural network which aims at reconstructing an SR image using an original LR image. As shown in <ref type="figure">Fig. 2</ref>, each sub-network mainly consists of four parts: an initial low-level feature extraction block, multiple residual dense blocks (RDBs), multiple gated feedback modules (GFMs), and a reconstruction block (RB). The parameters of these four parts are shared across time. The communication between the sub-networks at two adjacent time steps is achieved by multiple groups of feedback connections. The GFM before one bottom RDB receive one group of feedback connections and further refines the low-level features using selected high-level information.</p><p>Given I LR as the input image of GMFN at the t-th time step, we apply two convolutional layers to extract initial low-level feature F t L,0 . The first layer and the second layer hold 3 × 3 and 1 × 1 sized convolutional kernels, respectively. F t L,0 can be obtained by </p><formula xml:id="formula_0">F t L,0 = H LFEB (I LR ) ,<label>(1)</label></formula><formula xml:id="formula_1">RDB 1 RDB-1 RDB-B RDB- - - b RB RDB b RDB-B More</formula><formula xml:id="formula_2">,0 t L F GFM-b GFM-b , 1 t L b F - GFM GFM - - 1 , t L b F , 1 t L B F - ,1 t L F , 1 t L b F - , t L b F , t H b F 1 1 , , ,..., t t L b L B F F Gate unit Refinement unit - - Concat 1st 2nd 8th ×0.2 Conv Conv Conv 1×1 Conv 1 , t L B F - Figure 2:</formula><p>The framework of our proposed gated multiple feedback network (GMFN).</p><p>where H LFEB (·) represents the function of the initial low-level feature extraction block. Then the extracted initial low-level feature F t L,0 is fed to multiple RDBs to learn hierarchical features.</p><p>Stacking more RDBs will provide more various sizes of receptive field in a sub-network, thus form a better hierarchy of extracted features. Such abundant hierarchical features better assist us in refining low-level features. Each refinement process is accomplished by the GFM placed before one RDB with one group of feedback connections. The details about the GFM will be discussed in Sec. 3.2. Supposing we cascade B RDBs at each time step, the final high-level feature F t L,B in the LR space can be obtained by</p><formula xml:id="formula_3">F t L,B = H GFM−RDB F t L,0 ,<label>(2)</label></formula><p>where H GFM−RDB (·) represents the function combining the operations of B RDBs and M GFMs. Specifically, owing to the lack of high-level information provided by the previous time step, there is no GMF placed before any RDB at the first time step (see <ref type="figure">Fig. 2</ref>). Following RDN <ref type="bibr" target="#b32">[33]</ref>, the number of the convolutional layers for per RDB is set to 8.</p><p>In reconstruction block, the extracted high-level feature F t L,B is first upscaled by a deconvolutional layer. Then, a 3 × 3 sized convolutional layer recovers a residual image using the upscaled feature. Finally, the recovered residual image is combined with the interpolated LR image to reconstruct the SR image I t SR at the t-th time step. The mathematical formulation of the reconstruction block can be expressed as:</p><formula xml:id="formula_4">I t SR = H RB F t L,B , I LR = H UF F t L,B + H ↑ (I LR ) ,<label>(3)</label></formula><p>where H RB (·), H UF (·) and H ↑ (·) represent the functions of the reconstruction block, the deconvolutional layer and the convolutional layer, and interpolated kernel, respectively. With T time steps unfolded in the proposed GMFN, we can obtain T SR images totally. Similarly, there are T HR images as the reconstruction target of each sub-network. We adopt L 1 loss function to optimize our GMFN. The loss function can be formulated as:</p><formula xml:id="formula_5">L(Θ) = 1 T T ∑ t=1 I t HR − I t SR 1<label>(4)</label></formula><p>where Θ represents the parameter set of GMFN, and I t HR denotes the target HR image at the t-th time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gated feedback module and multiple feedback connections</head><p>The gated feedback module (GFM) is employed to utilize multiple high-level features rerouted from the previous time step to refine the low-level feature extracted from shallow layers. As shown in <ref type="figure">Fig. 2</ref>, one GFM is composed of a gate unit and a refinement unit. The gate unit adaptively selects and enhances useful high-level information from multiple high-level features. The refinement unit first refines low-level features by using the selected meaningful high-level information, and further sends the refined low-level feature to the following RDB. The placement of GFM is determined by the level of features to be refined. According to the relative hierarchical relationship among multiple cascaded RDBs, we choose the input of multiple shallow RDBs as the low-level features need to be refined, and the output of multiple deep RDBs as the high-level features to be rerouted. Since the deepest RDBs can extract the most representative information in the LR space which especially facilitates the refinement processes of initial low-level features, we employ multiple groups of feedback connections to deliver multiple high-level features from the deepest RDBs to the shallowest ones. Each group of feedback connections is handled by one GFM. Let's denote S M = {1, 2, ..., M − 1, M} as the set of selected indexes of the shallowest M RDBs whose input is regarded as low-level features, and D N = {N, N + 1, ..., B − 1, B} as the set of selected indexes of the deepest B − N + 1 RDBs whose output is used to refine these low-level features. At the t-th time step, the output of the b-th RDB F t L,b can be obtained via</p><formula xml:id="formula_6">F t L,b =    H RDB,b H RU,b F t H,b , F t L,b−1 , if b ∈ S M and t &gt; 1, H RDB,b F t L,b−1 , otherwise,<label>(5)</label></formula><p>where H RDB,b (·) and H RU,b (·) represent the functions of the b-th RDB and the refinement unit in the b-th GFM, respectively, F t H,b , F t L,b−1 refers to the concatenation of F t H,b and F t L,b−1 , and F t H,b refers to the selected and enhanced high-level information from multiple high-level features which flow into the b-th GFM. These high-level features are extracted from the deepest RDBs, and are then carried by one group of feedback connections. Therefore, the selected and enhanced high-level information F t H,b can be given by</p><formula xml:id="formula_7">F t H,b =    H GU,b F t−1 L,N , ..., F t−1 L,B , if b &lt; N, H GU,b F t−1 L,b , ..., F t−1 L,B , otherwise,<label>(6)</label></formula><p>where H GU,b (·) represents the function of the gate unit in the b-th GFM. Based on the relative hierarchical relationship among multiple cascaded RDBs, Eq. 6 indicates that the b-th GFM only receive the output of RDBs whose indexes are equal or larger than b from the previous time step. For parameter and computation efficiency, we employ two 1 × 1 sized convolutional layers as the gate unit and the refinement unit in the b-th GFM, respectively. According to Eq. 5 and Eq. 6, the number of GFMs at each time step (except the first time step) and the number of groups of feedback connections between two adjacent time steps are equal to M, and the number of feedback connections in each group is determined by the value of N. Thus, we can adjust the values of M and N in selected index sets S M and D N to control how many low-level features need to be refined and high-level features will be rerouted, respectively. The mentioned feedback connections in Sec. 2.1 are special cases of our feedback formulation. In detail, we can easily set N = B to achieve single-to-single (M = 1) or single-to-multiple (M = 1) feedback connection(s) which only routes the highest-level feature back to the shallowest RDB(s). However, since we argue that every piece of highlevel information captured under different receptive fields is important for reconstructing an SR image, we set N = B to achieve multiple-to-single (M = 1) and multiple-to-multiple (M = 1) feedback manners which fully exploits high-level features to refine the low-level feature(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation details</head><p>We set unfolded time steps as T = 2 1 , and cascade B = 7 RDBs in the sub-network at each time step. Following the previous work <ref type="bibr" target="#b26">[27]</ref>, the residual scale factor for each RDB is set to 0.2. The number of convolutional kernels in the first layer and the last layer of the subnetwork is set to C 0 and C out , respectively. Because we mainly focus on the reconstruction of RGB images, C out naturally equals to 3. The number of convolutional kernels in other layers is set to C. In the proposed GMFN, all convolutional and deconvolutional layers are followed by a PReLU <ref type="bibr" target="#b6">[7]</ref> activation function except the last convolutional layer of each RDB and the reconstruction block. In the reconstruction block, a bilinear kernel is used to interpolate the LR image. For different upscale factors, the settings for the deconvolutional layer are same as <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>Datasets and evaluation metrics. We use 800 images from DIV2K for training, and augment training images with scaling, rotations, and flips. For testing, we employ five standard benchmark datasets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b29">[30]</ref>, B100 <ref type="bibr" target="#b20">[21]</ref>, Urban100 <ref type="bibr" target="#b10">[11]</ref>, and Manga109 <ref type="bibr" target="#b21">[22]</ref>. We generate LR images from HR images by using the Matlab function imresize with the option bicubic. The SR results are evaluated with PSNR and SSIM [28] metrics on Y channel (i.e., luminance) of transformed YCbCr space.</p><p>Training settings. For each iteration, 16 RGB LR patches with a size of 48×48 are fed to the network. The parameters are initialized using the He's method <ref type="bibr" target="#b6">[7]</ref>. Adam <ref type="bibr" target="#b14">[15]</ref> is employed to optimize the parameters with an initial learning rate of 2 × 10 −4 . The learning rate is halved for every 2 × 10 5 iterations. The model is implemented under Pytorch framework and trained on an NVIDIA 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Study of multiple feedback connections and GFM</head><p>In the following experiments, the number of convolutional kernels C 0 and C for the first layer and other layers are set to 128 and 32, respectively. Each model is trained under 2 × 10 5 iterations and evaluated on Urban100 dataset with scale factor ×4.  tions aim to transmit multiple high-level features to the first RDB. We compare seven cases of multiple-to-single feedback manners by setting M = 1 and N = 7, 6, · · · , 1 in selected index sets S M and D N , respectively. Specifically, for N = 7 among these cases, the feedback connection is single-to-single as SRFBN <ref type="bibr" target="#b16">[17]</ref>. For a better comparison, we employ singleto-multiple feedback manner <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> as the baseline. <ref type="figure" target="#fig_2">Fig. 3(a)</ref> illustrates that all multipleto-single feedback manners perform better than the single-to-multiple and single-to-single ones. As the propagated high-level information increases, the performance of the network gradually improves. This demonstrates that multiple high-level features are beneficial for refining low-level features. However, excessively introducing high-level features may conflict with the original low-level features, thus propagating more high-level features after the peak (i.e. N = 4) will hurt the reconstruction performance of the network.</p><p>Study of multiple-to-multiple feedback connections. We fix N = 4 with M = 1, 2, · · · , 7 to meet the requirements of the multiple-to-multiple manner. <ref type="figure" target="#fig_2">Fig. 3(b)</ref> shows that as more shallow RDBs receive high-level information, the performance gradually degrades. This is owing to the first RDB has already made full use of the information from the rerouted highlevel features. If the high-level features are propagated to other RDBs, they may conflict with newly refined low-level features and hinder the reconstruction ability of the network. Even so, multiple-to-single feedback connections still perform better than the single-to-multiple one. This further illustrates that not only the highest-level feature but multiple ones help to refine low-level features.</p><p>Study of anti-feedback connections. We design anti-feedback connections to further illustrate the effectiveness of the proposed multiple feedback connections. In detail, we reverse the feedback connections to transmit low-level information extracted from the shallowest RDB(s) to the deepest RDB. Similar to the definition of M and N, we take M and N to control how many low-level features to be transmitted and how many high-level features to be refined. As opposed to multiple-to-single feedback connections, we set N = 7 and combine various M to achieve multiple-to-single anti-feedback connections. As can be seen in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>, the anti-feedback connections shows worse reconstruction effect compared with the proposed multiple feedback connections. This demonstrates that exploiting low-level information to enhance high-level features is less efficient than using abundant high-level information to refine low-level features.</p><p>Study of the gated feedback module. The refinement unit in a GFM receives the feedback connection to achieve communication between two adjacent time steps. If we directly re- move all GFMs or all refinement units in the GFMs, the communication between the two time step would be disconnected. Thus, we only investigate the necessity of the gate unit in the GFM. For N = 4 and M = 1, equipped with the gate unit, our model achieves a PSNR value of 26.13. After removing the gate unit, multiple high-level features are directly concatenated with low-level features at the refinement unit, and the PSNR value under this circumstance drops to 26.06. The reason is that without the gate unit, directly concatenating redundant high-level features with low-level features will confuse the refinement unit, further hinder the reconstruction ability of the network. To better understand the gated feedback module, we visualize the averaged feature maps in <ref type="figure" target="#fig_3">Fig. 4</ref>. As can be seen, the gate unit adaptively selects the high frequency components, such as edges and outlines, of the hierarchical feedback high-level features and generates a more informative high-level features F 1 H,1 . With the help of the selected and enhanced high-level feature, the input low-level feature effectively accesses to high-level information, thus the refined low-level feature becomes more representative than the input low-level feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the state-of-the-arts</head><p>In this sub-section, the proposed GMFN is equipped with the multiple feedforward connections by setting M = 1 and N = 4. C 0 and C are enlarged to 256 and 64, respectively. We demonstrate the effectiveness of GMFN by comparing it with eight state-of-the-art SR methods: SRCNN <ref type="bibr" target="#b2">[3]</ref>, VDSR <ref type="bibr" target="#b12">[13]</ref>, DRRN <ref type="bibr" target="#b24">[25]</ref>, NLRN <ref type="bibr" target="#b19">[20]</ref>, EDSR <ref type="bibr" target="#b18">[19]</ref>, D-DBPN <ref type="bibr" target="#b5">[6]</ref>, RDN <ref type="bibr" target="#b32">[33]</ref>, and SRFBN <ref type="bibr" target="#b16">[17]</ref>. We re-evaluate these comparison methods in accordance with corresponding public implementations and report the quantitative and qualitative comparison results 2 in Tab. 1 and <ref type="figure" target="#fig_4">Fig. 5</ref>, respectively. As can be seen, the proposed GMFN performs best on both PSNR and SSIM metrics in most public datasets. Especially, with only 14 RDBs (T = 2, B = 7), our GMFN exhibits better reconstruction performance than RDN which has 16 RDBs. The qualitative results shown in <ref type="figure" target="#fig_4">Fig. 5</ref> indicate our GMFN can reconstruct a faithful SR image with sharper and clearer edges. It can recover more image details compared with other methods. The consistency between quantitative and qualitative results convincingly proves the superiority of the proposed GMFN.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose the gated multiple feedback network (GMFN) for accurate image SR. It successfully enriches the representation of low-level features by propagating multiple hierarchical high-level features to shallow layers. The elaborately designed gated feedback module (GFM) efficiently selects and enhances meaningful high-level information from multiple groups of feedback connections and uses the selected and enhanced high-level information to refine the low-level features. Extensive experiments on investigating and analyzing various feedback manners demonstrate the superiority of our proposed multiple feedback connections. With two time steps and each contains 7 RDBs, the proposed GMFN achieves better reconstruction performance compared to state-of-the-art image SR methods including RDN <ref type="bibr" target="#b32">[33]</ref> which contains 16 RDBs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The following items are contained in the supplementary material:  We testify the superiority of our feedback networks over the corresponding feedforward networks. The number of convolutional kernels C 0 and C for the first layer and other layers are set to 128 and 32, respectively. All models are trained under 2 × 10 5 iterations and evaluated on Urban100 dataset. We set M = 1 and N = 1 to represent multiple-to-single (MS) feedback network, M = 7 and N = 1 to represent multiple-to-multiple (MM) one, and mark them with 'FB'. Their feedforward counterparts (marked with 'FF') are implemented by disconnecting the loss to all time steps except the last one <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>. The experimental results shown <ref type="figure">Figure 6</ref> indicate that both MM_FB and MS_FB feedback networks outperform the corresponding feedforward networks. This confirms that our gated multiple feedback network has obvious advantages over the traditional feedforward networks.</p><formula xml:id="formula_8">A.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Study of time step</head><p>In this section, we investigate the influence of the time step T on the proposed GMFN. Under identical setting of multiple feedback connection, we set T = 1, 2, 3, and 4, respectively. The performance evaluated on Urban100 dataset is shown in <ref type="figure">Figure 7</ref>. It can be observed that with the help of multiple feedback connections, the reconstruction ability is significantly improved compared with the one without feedback connections (T =1). However, we also observed as T continues to increase, the reconstruction quality improves slightly. Hence, we set T = 2 in our main paper for better balance the reconstruction performance and the computational cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Model analysis</head><p>We compare the running time and the number of parameters of our final model with some representative state-of-the-art methods on Urban100 with scale factor ×4. <ref type="figure" target="#fig_6">Figure 8</ref> shows that the proposed GMFN can well balance the reconstruction accuracy, running time, as well as number of parameters. In terms of running time, GMFN runs orders of magnitude faster than RDN <ref type="bibr" target="#b32">[33]</ref>, EDSR <ref type="bibr" target="#b18">[19]</ref>, and so on. Compared with SRFBN <ref type="bibr" target="#b16">[17]</ref> and D-DBPN <ref type="bibr" target="#b5">[6]</ref>, which require similar running time, GMFN achieves a better reconstruction performance. Additionally, GMFN requires 6% fewer parameters than D-DBPN, 56% fewer parameters than RDN, and 77% fewer parameters than EDSR while obtaining a higher PSNR value. RCAN <ref type="bibr" target="#b31">[32]</ref> can attain a better reconstruction performance than all other comparison methods, but it holds relatively more parameters and a much deeper network design (about 400 layers). We will further extend our work following such design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More qualitative results</head><p>In <ref type="figure" target="#fig_0">Fig. 9-16</ref>, we provide more qualitative results to prove the superiority of the proposed GMFN.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative results for ×4 image SR on 'img_092' from Urban100 dataset. The proposed GMFN accurately recovers more image details compared with other state-of-theart image SR methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Study of multiple feedback connections. Single-to-multiple (SM) feedback manner is provided for a better comparison. (a) Performance of various multiple-to-single feedback connections. (b) Performance of various multiple-to-multiple feedback connections. (c) Performance of various single-to-multiple anti-feedback connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FeedbackFigure 4 :</head><label>4</label><figDesc>high-level features Selected and enhanced high-level feature Input low-level feature Refined low-level feature Visualization of averaged feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparison of our GMFN with other methods on ×4 image SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 T=4Figure 7 :</head><label>17</label><figDesc>Study of time step (T)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>(a) Accuracy and numbers of parameters trade-off. (b) Accuracy and average running time trade-off. Models are evaluated on Urban100 under scale factor ×4 on an NVIDIA 1080Ti GPU with an i7-7700K CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results on 'img_044' with scale factor ×4. The proposed GMFN better recovers grids on the ceiling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Qualitative results on 'ToutaMairimasu' with scale factor ×4. Only GMFN recoveres two horizontal lines as the HR image. Other comparison methods only restore one horizontal line erroneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Both authors have equally contribution. †Corresponding author arXiv:1907.04253v2 [cs.CV] 10 Jul 2019 HR/PSNR EDSR/19.14 D-DBPN/18.92 RDN/19.18 SRFBN/19.55 NLRN/18.65 GMFN(Ours)/19.70 Bicubic/16.58</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Study of multiple-to-single feedback connections. Multiple-to-single feedback connec-</figDesc><table><row><cell></cell><cell>26.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR</cell><cell>25.98 26.05 26.13 26.02</cell><cell>26.08</cell><cell>26.04</cell><cell></cell><cell>26.07</cell><cell cols="2">26.05 26.06</cell><cell>PSNR</cell><cell>26.13 25.98 26.05</cell><cell>26.03</cell><cell>26.09</cell><cell>26.03</cell><cell>26.06</cell><cell cols="2">26.03 25.98</cell><cell>PSNR</cell><cell>26.10 26.00 26.00 26.05</cell><cell>26.00</cell><cell>26.00</cell><cell>25.98 Feedback</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SM 26.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SM 26.03</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25.95</cell></row><row><cell></cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>N</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.8104 30.48/0.8628 31.35/0.8830 31.68/0.8888 31.94/0.8920 32.46/0.8968 32.47/0.8980 32.47/0.8990 32.47/0.8983 32.55/0.8991 Set14 ×2 30.24/0.8688 32.45/0.9067 33.05/0.9130 33.23/0.9136 33.57/0.9167 33.92/0.9195 33.85/0.9190 34.01/0.9212 33.82/0.9196 34.05/0.9211 ×3 27.55/0.7742 29.30/0.8215 29.78/0.8320 29.96/0.8349 30.25/0.8386 30.52/0.8462 -/-30.57/0.8468 30.51/0.8461 30.58/0.8473 ×4 26.00/0.7027 27.50/0.7513 28.02/0.7680 28.21/0.7721 28.44/0.7759 28.80/0.7876 28.82/0.7860 28.81/0.7871 28.81/0.7868 28.84/0.7888 B100 ×2 29.56/0.8431 31.36/0.8879 31.90/0.8960 32.05/0.8973 32.18/0.8991 32.32/0.9013 32.27/0.9000 32.34/0.9017 32.29/0.9010 32.34/0.9017 ×3 27.21/0.7385 28.41/0.7863 28.83/0.7990 28.95/0.8004 29.05/0.8024 29.25/0.8093 -/-29.26/0.8093 29.24/0.8084 29.27/0.8093 ×4 25.96/0.6675 26.90/0.7101 27.29/0.7260 27.38/0.7284 27.48/0.7304 27.71/0.7420 27.72/0.7400 27.72/0.7419 27.72/0.7409 27.74/0.7421 Urban100 ×2 26.88/0.8403 29.50/0.8946 30.77/0.9140 31.23/0.9188 31.77/0.9243 32.93/0.9351 32.55/0.9324 32.89/0.9353 32.62/0.9328 32.96/0.9361 .7866 27.58/0.8555 28.83/0.8870 29.18/0.8914 29.82/0.8982 31.02/0.9148 30.91/0.9137 31.00/0.9151 31.15/0.9160 31.24/0.9174Table 1: Quantitative evaluation under scale factors ×2, ×3 and ×4. The best performance is shown in bold and the second best performance is underlined.</figDesc><table><row><cell>Dataset</cell><cell>Scale</cell><cell>Bicubic</cell><cell>SRCNN [3]</cell><cell>VDSR [13]</cell><cell>DRRN [25]</cell><cell>NLRN [20]</cell><cell>EDSR [19]</cell><cell>D-DBPN [6]</cell><cell>RDN [33]</cell><cell>SRFBN [17]</cell><cell>GMFN (Ours)</cell></row><row><cell></cell><cell>×2</cell><cell cols="10">33.66/0.9299 36.66/0.9542 37.53/0.9590 37.74/0.9591 38.08/0.9610 38.11/0.9602 38.09/0.9600 38.24/0.9614 38.11/0.9609 38.21/0.9612</cell></row><row><cell>Set5 -</cell><cell>×3</cell><cell cols="6">30.39/0.8682 32.75/0.9090 33.67/0.9210 34.03/0.9244 34.30/0.9271 34.65/0.9280</cell><cell>-/-</cell><cell cols="3">34.71/0.9296 34.70/0.9292 34.73/0.9295</cell></row><row><cell></cell><cell cols="7">×4 28.42/0×3 24.46/0.7349 26.24/0.7989 27.14/0.8290 27.53/0.8378 27.90/0.8443 28.80/0.8653</cell><cell>-/-</cell><cell cols="3">28.80/0.8653 28.73/0.8641 28.87/0.8667</cell></row><row><cell></cell><cell>×4</cell><cell cols="10">23.14/0.6577 24.52/0.7221 25.18/0.7540 25.44/0.7638 25.78/0.7713 26.64/0.8033 26.38/0.7946 26.61/0.8028 26.60/0.8015 26.69/0.8048</cell></row><row><cell></cell><cell>×2</cell><cell cols="10">30.30/0.9339 35.60/0.9663 37.22/0.9750 37.60/0.9736 38.55/0.9768 39.10/0.9773 38.89/0.9775 39.18/0.9780 39.08/0.9779 39.13/0.9778</cell></row><row><cell>Manga109</cell><cell>×3</cell><cell cols="6">26.95/0.8556 30.48/0.9117 32.01/0.9340 32.42/0.9359 33.24/0.9414 34.17/0.9476</cell><cell>-/-</cell><cell cols="3">34.13/0.9484 34.18/0.9481 34.24/0.9487</cell></row><row><cell></cell><cell>×4</cell><cell>24.89/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Feedback networks vs. feedforward networks. B. Study of time step. C. Model analysis. D. More qualitative results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">表格 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell>T=1</cell><cell>T=2</cell><cell></cell><cell>T=3</cell><cell>T=4</cell></row><row><cell cols="10">A Feedback networks vs. feedforward networks Urban100 26.00 26.13 26.07 26.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell>表格 1-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>S-&gt;M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>26.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>26.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.20</cell><cell></cell><cell></cell></row><row><cell>PSNR</cell><cell>26.03 26.06</cell><cell>26.06</cell><cell>26.02</cell><cell></cell><cell>PSNR</cell><cell>26.05 26.13</cell><cell></cell><cell>26.13</cell><cell>26.07</cell><cell>26.14</cell></row><row><cell></cell><cell>25.99</cell><cell></cell><cell>25.99</cell><cell></cell><cell></cell><cell>25.98</cell><cell>26.00</cell><cell></cell></row><row><cell></cell><cell>25.95</cell><cell>MS_FB MS_FF 25.96</cell><cell>MM_FB MM_FF</cell><cell></cell><cell></cell><cell cols="2">25.90 T=1</cell><cell>T=2</cell><cell>T=3</cell></row><row><cell cols="5">Figure 6: Feedback networks (FB) vs. feed-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">forward networks (FF)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For more analysis about time steps please refer to supplement material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Comparison on running time and number of parameters is available in supplement material</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The research in our paper is sponsored by National Natural Science Foundation of China (No.61701327 and No.61711540303), Science Foundation of Sichuan Science and Technology Department (No.2018GZ0178).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Line Alberi-Morel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image super-resolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-path feedback recurrent neural networks for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Convolutional neural networks with intralayer recurrent connections for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Ronan Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ho Pinherio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babu</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiejie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Te-Lin</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno>img_062&apos; from Urban100</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Qualitative results on &apos;img_062&apos; with scale factor ×4. The proposed GMFN produces a faithful SR image and avoids artifacts as other methods</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<idno>HR/PSNR Bicubic/21.28</idno>
		<title level="m">Figure 11: Qualitative results on &apos;ParaisoRoad&apos; with scale factor ×4. Only the GMFN accurately restored the letter &quot;M</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Qualitative results on &apos;UchiNoNyansDiary&apos; with scale factor ×4. Only GMFN faithful recovers detail of headwear, while other comparison methods cause heavy blurring artifacts</title>
		<idno>253027&apos; from B100</idno>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Qualitative results on &apos;253027&apos; with scale factor ×4. Only GMFN correctly reconstructs the direction of the zebra&apos;s stripes</title>
		<idno>210088&apos; from B100</idno>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Qualitative results on &apos;210088&apos; with scale factor ×4. GMFN reconstructes a more vivid fisheye compared with other methods</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<title level="m">Figure 16: Qualitative results on &apos;butterfly&apos; with scale factor ×4</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
