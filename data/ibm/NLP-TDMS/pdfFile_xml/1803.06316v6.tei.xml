<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Gaussian Mixture Layer for Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
						</author>
						<title level="a" type="main">Temporal Gaussian Mixture Layer for Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal convolutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that are fully differentiable. We present our fully convolutional video models with multiple TGM layers for activity detection. The extensive experiments on multiple datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outperforming the state-of-the-arts 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Activity videos are spatio-temporal data: they are image frames with a specific width/height (XY) concatenated along time axis <ref type="bibr">(T)</ref>. Recognition from such videos requires capturing both spatial and temporal information, desirably using learned convolutional kernels. Temporal convolution is particularly beneficial in activity 'detection' tasks, which require making activity decisions at every frame given a continuous video <ref type="bibr" target="#b24">(Sigurdsson et al., 2016b;</ref><ref type="bibr" target="#b35">Yeung et al., 2015)</ref>. Previous methods investigated using 3-D XYT convolutional filters <ref type="bibr" target="#b27">(Tran et al., 2014;</ref><ref type="bibr" target="#b1">Carreira &amp; Zisserman, 2017)</ref> as well as the models with 2-D XY conv. layers followed by 1-D temporal conv. <ref type="bibr" target="#b29">(Tran et al., 2018)</ref>, pooling or attention layers <ref type="bibr" target="#b19">(Piergiovanni et al., 2017)</ref>.</p><p>Understanding complex multi-activity videos requires capturing information in long-term time intervals. Different frames contain different information, and the model needs to learn to take advantage of as many frames as possible, while abstracting them efficiently. Previous attempts of simply pooling representations over time or learning temporal Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</p><p>1 Code/models: https://github.com/piergiaj/tgm-icml19 conv. filters with a small number of frames (e.g., 16 or 64) was thus often insufficient to fully consider rich long-term temporal context. Simultaneously, bruteforcely increasing the temporal filter length (to look at more frames) results more learnable parameters, requiring more training data, which can be expensive when activities are rare.</p><p>In this paper, we introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer, and present how it can be used to efficiently capture longer-term temporal information in activity videos. Our temporal Gaussian mixture layer is a temporal convolutional layer, whose filters/kernels are controlled by a set of (temporal) Gaussian distribution parameters. Each of our temporal Gaussian distributions specify (temporally) 'where' the model should look, and our Gaussian mixture layer combines them as multiple convolutional filters to be applied on top of temporallycontinuous representations. This layer allows the video representation at each time step to be constructed while focusing on different neighboring temporal regions, instead of only focusing on its local segment. It is a convolutional layer governed by a much smaller set of parameters (i.e., locations/variances of the Gaussians as well as their mixture weights) that are fully differentiable. Importantly, the number of parameters of the TGM layer is independent of the length of the filter, allowing the model to capture longerterm temporal information without additional parameters.</p><p>The motivation behind our temporal Gaussian mixture layer is to learn the temporal structure of an activity as a composition of temporal Gaussian regions/attentions. Such structure allows the model to obtain a compact spatio-temporal representation abstracting each (long-term) time interval, using multiple temporal conv. layers with far fewer parameters. It is also related to the previous temporal attention works arXiv:1803.06316v6 [cs.CV] 1 Aug 2019 <ref type="bibr" target="#b19">(Piergiovanni et al., 2017)</ref>, but our model is designed to be fully convolutional to handle continuous data and it learns more compositional structures with multiple layers.</p><p>We present video-CNN models using our TGM layers for activity detection in continuous videos. Our model stacks TGM layers on top of several state-of-the-art CNNs such as I3D <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017)</ref>. This enables our model to capture longer-term temporal information than provided by the base CNNs, compositionally modeling the temporal structure with multiple TGM layers. Our model was evaluated on multiple public datasets including MultiTHUMOS and Charades, and was able to outperform the best previous activity detection CNNs by a meaningful margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Learning video representations for human activity recognition has been successful. CNN methods allow end-to-end learning of video features and representations optimized for the training data, performing superior to traditional works <ref type="bibr" target="#b0">(Aggarwal &amp; Ryoo, 2011)</ref> for video understanding.</p><p>Two-stream CNN models take a single RGB frame and a small number of optical flow frames as inputs to capture both motion and appearance information in videos <ref type="bibr" target="#b25">(Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b7">Feichtenhofer et al., 2016)</ref>. Models learning 3-D spatio-temporal (XYT) convolutional filters were designed and applied to many activity recognition tasks as well <ref type="bibr" target="#b27">(Tran et al., 2014;</ref><ref type="bibr" target="#b1">Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b28">Tran et al., 2017;</ref><ref type="bibr" target="#b9">Hara et al., 2017)</ref>. Large scale datasets for activity detection, such as THUMOS <ref type="bibr" target="#b11">(Jiang et al., 2014)</ref>, Ac-tivityNet <ref type="bibr" target="#b10">(Heilbron et al., 2015)</ref>, Kinetics <ref type="bibr" target="#b13">(Kay et al., 2017)</ref>, and Charades <ref type="bibr" target="#b24">(Sigurdsson et al., 2016b)</ref> provided these approach the necessary training data to learn the models. Such 3-D XYT CNNs were also used to capture spatio-temporal information for activity detection <ref type="bibr" target="#b33">(Xu et al., 2017;</ref><ref type="bibr" target="#b21">Shou et al., 2016;</ref><ref type="bibr" target="#b38">Zhao et al., 2017)</ref>. However, all these CNNs were limited to the consideration of a fixed local video segment (e.g., 16 frames in <ref type="bibr" target="#b27">(Tran et al., 2014)</ref> and 64-99 frames in <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017)</ref>) when making activity decisions.</p><p>Some works studied combining representations over longerterm temporal intervals <ref type="bibr" target="#b12">(Karpathy et al., 2014;</ref><ref type="bibr" target="#b16">Ng et al., 2015;</ref><ref type="bibr" target="#b32">Varol et al., 2017)</ref>, but it was generally done with a temporal pooling of local representations or spatio-temporal convolutions with slightly larger fixed intervals. Recurrent neural networks (RNNs) have also been used to model activity transitions between frames <ref type="bibr" target="#b35">(Yeung et al., 2015;</ref><ref type="bibr" target="#b6">Escorcia et al., 2016)</ref>, but they were strictly sequential and had limitations in maintaining temporal information over a longer temporal duration, particularly for videos with multiple complex activities. Recently, CNN models using temporal attention for activity videos <ref type="bibr" target="#b19">(Piergiovanni et al., 2017;</ref><ref type="bibr" target="#b18">Piergiovanni &amp; Ryoo, 2018b)</ref> were studied as well.</p><p>However, a fully convolutional model to analyze continuous videos while efficiently representing information in long term intervals has been lacking.</p><p>Several works have explored using parameterized convolutional kernels for point clouds <ref type="bibr" target="#b34">(Xu et al., 2018)</ref> or images <ref type="bibr" target="#b3">(Cohen et al., 2018)</ref>, but were quite different from our design to handle long-term temporal information.</p><p>Our layer is different from the previous standard spatiotemporal convolutional layers in that it relies on significantly fewer parameters by forcing filter shapes to be Gaussian compositions. Our temporal layer is also different from previous Gaussian Mixture Model layers <ref type="bibr">(Variani et al., 2015)</ref> in that our layer is convolutional while they are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer, and present how it can be used for activity recognition. Our Temporal Gaussian Mixture layer is a temporal convolutional layer to be applied on top of a sequence of representations (usually from frame-level or segment-level CNNs), whose filters/kernels are controlled by a set of (temporal) Gaussian distribution parameters. The motivation is to make each temporal Gaussian distribution specify (temporally) 'where to look' with respect to the activity center, and represent the activity as a collection/mixture of such temporal Gaussians convolved with video features. Our layer is fully differentiable and trainable using standard backpropagation.</p><p>Our TGM layer can be interpreted as a a form of 1-D convolution where the filters are determined by a mixture of Gaussians. However, we design our TGM layer differs from the standard temporal convolutional layers of learning 1-D (time) or 2-D (channel-by-time) filters in the following aspects (illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>):</p><p>1. Instead of learning temporal convolution filters of any arbitrary values, our filter is forced to have the form of a temporal Gaussian mixture shared across all framelevel channels. This allows the layer to rely on significantly fewer number of (fully differentiable) parameters, while capturing the concept of temporal structure/attention.</p><p>2. Our temporal Gaussian mixture layer handles multiple 3-D tensors internally to preserve channels from the frame-level CNN by adding a new temporal channel axis. Its input is 3-D (channel-by-channel-by-time), where one channel dimension is inherited from the frame-level CNN and this dimension size remains unchanged. This is a form of grouped convolution, and we call this more specifically temporal channel grouping (TC-grouping). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporal Gaussian Mixture layer</head><p>Our temporal Gaussian mixture layer takes a 3-D input with the dimensionality of C in ×D×T , where C in is the number of input channels, D is the dimensionality of the representations from frame-level (or segment-level) CNNs, and T is the time. Given such input, the TGM layer convolves it with C out number of 1 × L filters/kernels, generating a C out × D × T -dim representation as an output. L is the temporal length of the temporal Gaussian mixture filter. D is usually 1K or 4K and T is the number of time steps (frames) in each video (i.e., it varies per video). C out is the number of different mixtures, corresponding to the number of output channels in standard convolution. In <ref type="figure" target="#fig_1">Fig. 2</ref>, we compare our TGM layer with the different forms of temporal convolutions. We experimentally compare all these versions to confirm the benefits of our TGM layer.</p><p>Our layer is composed of a set of M Gaussians. Each Gaussian has 2 parameters: a centerμ and a widthσ. Each layer has additional hyper-parameters: L, the temporal duration and M , the number of Gaussians to learn. We constrain the learned center to be between 0 and L and σ to be positive:</p><formula xml:id="formula_0">µ = (L − 1) · tanh (μ) + 1 2 , σ 2 = exp (σ).<label>(1)</label></formula><p>We use the above µ and σ to construct the temporal Gaussian kernels. This acts as a strong sparsity constraint on the convolutional kernel as well as a drastic reduction of the number of learnable parameters. We construct a temporal Gaussian mixture convolutional kernel as:</p><formula xml:id="formula_1">K m,l = 1 Z exp − (l − µ m ) 2 2σ 2 m (2)</formula><p>where Z is a normalization constant such that L lK m,l = 1, resulting inK being an M × L matrix.</p><p>Instead of making the model learn a separate set of Gaussian distributions per activity class, we take the approach of maintaining multiple Gaussian distributions shared across classes and obtain a Gaussian 'mixture' filter by learning soft-attention weights. We learn a set of soft-attention weights per output channel i, ω ∈ R Cout×M . We create the soft-attention weights by applying the softmax function over the M Gaussians, enforcing each input channel weights sum to 1.</p><formula xml:id="formula_2">a i,m = exp ω i,m j exp ω i,j<label>(3)</label></formula><p>Based on temporal Gaussian distributionsK i and attention weights a i,m , the temporal convolution filters our TGM layer is computed as:</p><formula xml:id="formula_3">K i = m a i,mKi .<label>(4)</label></formula><p>This provides us convolutional filters having the form of a mixture of temporal Gaussians, controlled based on 2 · M + C in · C out · M parameters (instead of learning D 2 · L parameters without any constraint, as in standard temporal convolution where C &lt;&lt; D). An overview of this process is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><formula xml:id="formula_4">3.1.1. SINGLE TGM LAYER -DIRECT PER-CLASS ACTIVITY MODELING</formula><p>The representation we obtain by applying our base CNNs to each frame (or local segment) has the dimensionality of D, and stacking them along time axis provides us the representation with 1 × D × T -dim. That is, in the case of using only one TGM layer to capture activity representations, our C in is fixed to 1 and C out is fixed to be the number of activity classes. This is the simplest case of our model, attaching one TGM layer on top of the 1 × D × T representation.</p><p>Our convolutional kernel, K, has a learned Gaussian mixture for each activity class. Let the video features v be a</p><formula xml:id="formula_5">D × T matrix. Each K i is a 2-D convolutional filter with T D C in C in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>TGMs Representation for each C in ... <ref type="figure">Figure 4</ref>. Illustration of a TGM layer with channel combination. The kernels are applied to each input channel, Cin, and a 1x1 convolution is applied to combine the Cin input channels for each output channel, Cout.</p><formula xml:id="formula_6">1 1 * = C in T D C in 1 1 * = C in T D T D C out 1x1 Convolution to combine C in Concatenation of C out representations ... ... 1 L K i1 ... T D * * ... ... = = ... ... T K ij T D 1 L K 11 ... T D * * ... ... = = ... ... T K 1j T D</formula><p>a size of 1 × L, and convolving this with v provides us a representation S with C out number of D × T responses since C in is 1 in this case. This per-class representation can then be used as input to a fully-connected layer for activity classification. For i ∈ {1, 2, . . . , C out }:</p><formula xml:id="formula_7">s i = v * K i , S = [s 1 , s 2 , . . . , s Cout ]<label>(5)</label></formula><p>Fig. 2 visually illustrates how each TGM filter is convolved with the input <ref type="figure" target="#fig_1">(Fig. 2d</ref>), compared to the standard 1-D convolution ( <ref type="figure" target="#fig_1">Fig. 2a</ref>) or other forms of the temporal layers ( <ref type="figure" target="#fig_1">Fig. 2b-c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">MULTIPLE TGM LAYERS -GROUPED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONVOLUTION</head><p>We generalize the above formulation to allow the TGM layers to be sequentially applied. The idea is to enable our model to capture more complex, nonlinear temporal structure by having multiple levels of temporal layers. In this case, the input for each layer is C in ×D×T dimensional (instead of 1 × D × T ), where the input channels are the number of output channels from the previous layer. Our kernels at each layer, K i , are parameterized and learned as before.</p><p>By using grouped convolution with the number of groups set to C in , we can efficiently separate the input into per-channel values and convolve each of them with the designated K i kernel, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. That is, we learn a filter K i per channel by setting</p><formula xml:id="formula_8">C in = C out . For i ∈ [1, C out ], s i = f i * K i , S = [s 1 , s 2 , . . . s Cout ]<label>(6)</label></formula><p>Here, f is a C in × D × T tensor, where D is the dimensionality of the feature and T is the number of frames. The result of the per-channel convolution, s i , is a D × T representation. We concatenate these representations along the channel axis, resulting in S, a C out × D × T representation. As this convolution results in the same output shape, we can stack these layers. Each layer is able to capture increasing temporal resolution, allowing the model to capture levels of abstractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">MULTIPLE TGM LAYERS -CHANNEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMBINATION</head><p>In the above subsection, we introduced an approach of stacking multiple TGM layers to model a hierarchical composition of temporal representations. However, in the grouped convolution case, each output channel of the layer is solely dependent on its corresponding input channel. That is, each kernel only considers information from a single output channel of the previous layer.</p><p>Therefore, we further generalize our TGM layer so that the layer combines representations from multiple input channels for each output channel while using the learned temporal kernels. We learn a set of convolutional kernels K ∈ R Cout×Cin×L (i.e., we learn C out · C in Gaussian mixtures). Given f which is the C in ×D ×T representation, for each output channel i ∈ [1, C out ] and each input channel j ∈ [1, C in ] pair, we convolve the associated filters with the input.</p><formula xml:id="formula_9">G i,j = (f j * K i,j )<label>(7)</label></formula><p>where each G i,j is a D × T -dim representation.</p><p>We then learn a 1x1 convolution followed by a ReLU activation function for each i ∈ [1, C out ], which we call w i , that maps from C in channels to 1 channel. The 1x1 convolution learns to combine the channels from the previous layer. By design, the TGM kernel is positive and sums to 1. Adding the unconstrained 1x1 convolution adds non-linearity (using the ReLU activation function) to our layer while only adding C out · C in parameters. The layer is computed as:</p><formula xml:id="formula_10">s i = G i * w i = (f j * K i,j ) * w i , S = [s 1 , s 2 . . . , s Cout ]<label>(8)</label></formula><p>We then stack the s i representations along the channel axis to produce S, the C out × D × T -dim representation. This Per-Frame Class Classification process is illustrated in <ref type="figure">Fig. 4</ref>. This method generalizes our approach to allow the layer to take input of C in × D × T and produce output of C out × D × T . These layers can easily be stacked to learn a hierarchical representation.</p><formula xml:id="formula_11">C 2 xC 1 x1xL ... ... ... ... C 1 C 2 C 1 1-d conv C f xDx1 C f xT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video CNN models with TGM layers</head><p>Our goal is to do activity detection which we define as making a per-frame (or per-segment) classification. Given a video, at each time step t, we want to make the model decide which activity the frame corresponds to (including no-activity). As a baseline, we train a fully-connected layer that classifies each per-frame D-dimensional vector, v t . As multiple activities can occur at the same time, or no activities at all, we treat this as a mutli-label classification task. We minimize binary cross entropy:</p><formula xml:id="formula_12">L(v) = t,c z t,c log(p(c|v t )) + (1 − z t,c ) log(1 − p(c|v t )) (9)</formula><p>where z t,c is the ground truth label, 1 if activity c is occurring at time t and p(c|v t ) is the output of our model for class c at time t. <ref type="figure" target="#fig_4">Fig. 5</ref> shows an example CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and baselines</head><p>Implementation We used I3D <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017)</ref> and the two-stream version of InceptionV3 <ref type="bibr" target="#b26">(Szegedy et al., 2016)</ref> pretrained on Imagenet and Kinetics as our base per-frame CNNs. Our default L setting used for the TGM layers as well as the other baselines was as follows: when using I3D segment features (3 features per second from 24fps videos), the 1 layer models used L = 15 and the 3 layer models used L = 5. When using InceptionV3 frame feature (at 8 fps), the 1 layer models used L = 30 and the 3 layer models used L = 10. These layers were attached on top of the base CNN, as described in Subsection 3.2. Please check Appendix for implementation and training details and results on other datasets.</p><p>Baselines In order to confirm the advantages of our TGM layers, particularly against previous temporal models, we implemented many baselines. The first is (i) a standard perframe classifier in which the prediction at each time-step only depends on a single feature vector with no contextual temporal information. We also used (ii) LSTMs on top of per-frame representations, which were popularly used to capture temporal information <ref type="bibr" target="#b5">(Donahue et al., 2015)</ref>. We train a bi-directional LSTM with 512 hidden units to make per-frame predictions. We also tried (iii) the fixed pyramid temporal max-pooling of level 3 <ref type="bibr" target="#b20">(Ryoo et al., 2015)</ref>.</p><p>We also compare our model against (iv) the model with standard temporal convolutional layers (i.e., 1-D convolution with a D × L kernel) on top of per-frame representations. This is similar to the temporal conv. used in <ref type="bibr" target="#b29">(Tran et al., 2018)</ref>. Temporal lengths (i.e., L) of the 1-D conv. filters and the pooling windows were set to be identical to the TGM filters. That is, they capture the same temporal duration as TGMs. In all our experiments, we follow the standard evaluation setting of computing per-frame mean average precision (mAP) and report those values. <ref type="table">Table 1</ref>. Comparison of various architectures on MultiTHUMOS using both I3D per-segment and InceptionV3 per-frame features. We found that TGM layers with 1x1 convolution channel combination performed the best. Results are in mAP. Note that we use the same filter length for "Temporal Conv" and "TGM" models, as described in Section 4. We also compare to many different forms of the TGM layer:</p><p>(v) 1-D convolution with a single Gaussian mixture per input <ref type="figure" target="#fig_1">(Fig. 2b)</ref>; (vi) 1-D convolution with the kernel consisting of many Gaussian mixtures <ref type="figure" target="#fig_1">(Fig. 2c)</ref>; (vii) our TC-grouping with unconstrained kernels (see Appendix for figure); (viii) with a learned mixture of random temporal filters and (ix) with a learned mixture of fixed Gaussians.</p><p>In addition, we also tried the approach of combining our TGM layers with the super-event representations, which capture global context <ref type="bibr" target="#b18">(Piergiovanni &amp; Ryoo, 2018b)</ref>. We concatenated the learned super-event representation with our representations from TGM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">THUMOS / MultiTHUMOS</head><p>Dataset We conducted our experiments on both THU-MOS <ref type="bibr" target="#b11">(Jiang et al., 2014)</ref> and MultiTHUMOS <ref type="bibr" target="#b35">(Yeung et al., 2015)</ref> datasets, while using the more challenging MultiTHU-MOS as the main dataset. MultiTHUMOS is an extended version of the THUMOS dataset that densely annotates the continuous videos. The dataset consists of 65 different classes, compared to 20 in THUMOS, and contains on average 10.5 activities per video and 1.5 labels per frame and up to 25 activity instances in each video. This is in contrast to many other activity detection datasets such as ActivityNet <ref type="bibr" target="#b10">(Heilbron et al., 2015)</ref>, which only has on average ∼1 activity per video. THUMOS and MultiTHUMOS consists of YouTube videos of various sport activities like basketball/volleyball games, weight lifting, and track/field.</p><p>We followed the standard evaluation setting of each dataset (e.g., measuring per-frame mAP in MultiTHUMOS and using IoU=0.5 in THUMOS-14). There are 1010 validation videos and 1574 test videos. We used these continuous validation videos for the model training. We did not take ad- vantage of the separate training set with segmented videos; even without them, we outperformed the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compared baselines as well as multiple different versions of our architectures, shown in <ref type="table">Table 1</ref>. The model with our TGM layers consistently outperformed baseline I3D (or InceptionV3) while using the same per-segment representations. Learning 3 TGM layers further improved the performance. We also note that the use of a 1x1 convoltuion + ReLU non-linearity improves performance more than soft-attention (weighted averaging), confirming that this additional non-linearity is beneficial. On the other hand, we found that stacking multiple standard temporal convolutional layers does not improve performance, often performing worse than the baseline. While a single standard temporal conv. layer improves over the baseline, stacking multiple layers significantly increases the number of parameters to learn <ref type="table" target="#tab_1">(Table 2</ref>) and we suspect this causes overfitting with the limited amount of samples in the dataset.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, we explicitly compare the results of using an LSTM or temporal conv. with a similar number of parameters to our TGM. This was done by making their temporal conv. filters to share values across multiple channels. These Standard 1-D Convolution <ref type="figure" target="#fig_1">(Fig. 2a</ref>) 32.5 1-D Conv with shared Gaussian mixture <ref type="figure" target="#fig_1">(Fig. 2b)</ref> 28.6 1-D Conv with Gaussian mixtures <ref type="figure" target="#fig_1">(Fig. 2c</ref>) 33.2 TC-grouping with unconstrained kernel 32.8 Our TGM Layer 36.1 models result in nearly random performance, as they were not designed to cope with a small number of parameters. We also show results with a mixture of random (fixed) temporal filters and with a mixture of fixed Gaussians. These results confirm that (i) modeling the temporal structure as a learned Gaussian mixture is beneficial and that (ii) further learning the Gaussian distribution parameters is important.</p><p>Also, in <ref type="table">Table 4</ref>, we compare to the different forms of temporal convolution (as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>). We find that each filter using one Gaussian mixture across all channels is not beneficial, while using a Gaussian mixture per-input channel (i.e., standard 1-D conv. with Gaussian mixture kernels) outperforms standard 1-D conv. Further, we find that using TC-grouping outperforms standard 1-D conv even with unconstrained kernels, although this itself is not as effective as 1-D conv. with Gaussian mixtures. Finally, we find that our designed TGM layer performs the best, confirming that both modeling temporal information as Gaussian mixtures and our designed TC-grouping are useful for activity detection.</p><p>Learning multiple TGM layers with channel combination outperforms the grouped convolution version of TGM and all the baselines. We also experimented with a version using soft-attention weights to combine the TGM layer channels, in addition to our method <ref type="figure">(Fig. 4</ref>) of using 1x1 convolution followed by a ReLU (to gain non-linearity). We found that the 1x1 convolution performed better. We tested various number of Gaussian mixtures (i.e., output channels) and found that using 80 for the first and second layer and using 65 (i.e., number of classes) for the final layer performs best.</p><p>Tables 5 and 6 compare our model using TGM layers with multiple previous state-of-the-art approaches and baselines.</p><p>Our approach meaningfully outperforms all previous ap-  <ref type="figure">Figure 6</ref>. Illustration of the temporal regions classified as various basketball activities from a basketball game video in MultiTHU-MOS. The TGM layers greatly improve performance. mAP Two-stream <ref type="bibr" target="#b35">(Yeung et al., 2015)</ref> 27.6 Two-stream + LSTM <ref type="bibr" target="#b35">(Yeung et al., 2015)</ref> 28.1 Multi-LSTM <ref type="bibr" target="#b35">(Yeung et al., 2015)</ref> 29.6 Predictive-corrective <ref type="bibr" target="#b4">(Dave et al., 2017)</ref> 29.7 SSN <ref type="bibr" target="#b38">(Zhao et al., 2017)</ref> 30.3 I3D baseline 29.7 I3D + LSTM 29.9 I3D + temporal pyramid 31.2 I3D + super-events <ref type="bibr" target="#b18">(Piergiovanni &amp; Ryoo, 2018b)</ref>  proaches. Importantly, we are comparing our approach with different methods of capturing temporal information such as LSTMs and fixed temporal pyramid pooling while making them use the exactly same per-frame representations. We found that while all these methods capture some temporal information, the TGM layers provide the best performance. Further, combining the super-event representation (Piergiovanni &amp; Ryoo, 2018b) with our TGM feature also benefited detection, confirming that the TGMs and super-events capture different aspects of the activity videos. In <ref type="figure">Fig. 6</ref>, we show an example of the various models predictions on a basketball video. We outperform the previous state-of-the-art performance (mAP) by 10% (36.4 vs. 46.4). <ref type="table">Table 6</ref>. Performances of our approach compared to the state-ofthe-arts on the continuous version of THUMOS-14, with IoU=0.5. mAP R-C3D <ref type="bibr" target="#b33">(Xu et al., 2017)</ref> 28.9 SSN <ref type="bibr" target="#b38">(Zhao et al., 2017)</ref> 29.1 TAL-Net <ref type="bibr" target="#b2">(Chao et al., 2018)</ref> 42.8 I3D Baseline 43.5 I3D + super-events <ref type="bibr" target="#b18">(Piergiovanni &amp; Ryoo, 2018b)</ref>   mAP Predictive-corrective <ref type="bibr" target="#b4">(Dave et al., 2017)</ref> 8.9 Two-stream <ref type="bibr" target="#b23">(Sigurdsson et al., 2016a)</ref> 8.94 Two-stream+LSTM <ref type="bibr" target="#b23">(Sigurdsson et al., 2016a)</ref> 9.6 R-C3D <ref type="bibr" target="#b33">(Xu et al., 2017)</ref> 12.7 Sigurdsson et al. <ref type="bibr" target="#b23">(Sigurdsson et al., 2016a)</ref> 12.8 SSN <ref type="bibr" target="#b38">(Zhao et al., 2017)</ref> 16.4 I3D baseline 17.2 I3D + 3 temporal conv. layers (L = 5) 17.5 I3D + 3 temporal conv. layers (L = 30) 12.5 I3D + LSTM 18.1 I3D + fixed temporal pyramid 18.2 I3D + super-events <ref type="bibr">(Piergiovanni &amp; Ryoo, 2018b) 19.4</ref> I3D + 3 TGMs (L = 5) 20.6 I3D + 3 TGMs (L = 30) 21.5 I3D + 3 TGMs (L = 5) + super-events 21.8 I3D + 3 TGMs (L = 30) + super-events 22.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Charades</head><p>Dataset Charades <ref type="bibr" target="#b24">(Sigurdsson et al., 2016b</ref>) is a large scale dataset with 9848 videos across 157 activity classes. These videos were recorded in home environments of the participants based on provided scripts. Each video contains on an average of 6.8 activity instances, and there are often complex activities co-occurring. The activities were mainly performed at home. For example, some activity classes are 'preparing a meal', 'eating', 'sitting', 'cleaning', etc.</p><p>In our experiments, we follow the original Charades detection setting (i.e., Charades_v1_localize evaluation), which is the original setting used in many previous approaches <ref type="bibr" target="#b23">(Sigurdsson et al., 2016a;</ref><ref type="bibr" target="#b18">Piergiovanni &amp; Ryoo, 2018b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare our results with the state-of-the-arts in <ref type="table" target="#tab_7">Table 7</ref>. To our knowledge, our method is obtaining the best known performance in the original localization setting of the Charades dataset. Notably, it is performing better than I3D that obtained the best competition performance, while using the same feature. Our method also outperforms standard temporal convolution, LSTMs, and fixed pyramid pooling, as well as the use of latent super-events. When setting L = 30 and using 3 TGM layers, our model is able to capture around 800 frames (about ±15 seconds from each frame) of temporal information, significantly more than previous works (e.g., I3D only captures ±2 seconds). This confirms that a key advantage of the TGM layer is that the number of parameters is independent of the filter length.</p><p>In <ref type="table" target="#tab_2">Table 8 (and Table 3</ref> in Appendix), we compare different values of L. Due to the short duration of activities in Multi-THUMOS (3.3 seconds), we find that L = 5 with 3 layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiTHUMOS L=15 MultiTHUMOS L=30</head><p>Charades L=30 Charades L=15 <ref type="figure">Figure 7</ref>. Illustration of several learned TGM kernels. On Multi-THUMOS, it learns to focus on shorter intervals to capture shorter events. On Charades, the Gaussians have a larger σ value, resulting in filters that attend to longer temporal durations. <ref type="table">Table 8</ref>. Effect of L on MultiTHUMOS using only RGB I3D features. Note that the 3 TGM layer models capture information in larger temporal intervals than the 1 TGM layer models for the same values of L. We also compare to using standard one-layer 1-D conv layer with different values of L. performs the best. Larger values of L capture too much unneeded temporal information, but due to the Gaussian structure, it does not drastically harm performance. <ref type="figure">Figure 7</ref> shows that even with longer kernels, the Gaussians learn to focus mostly on the center of the interval and capture short intervals. Thus, having too long intervals does not drastically harm performance, which is in contrast to the standard 1-D convolution. Note that for Charades, the temporal kernels learned to capture much longer temporal durations, as the average activity in charades is 12.8 seconds and larger values of L are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We newly introduced the Temporal Gaussian Mixture (TGM) layer and demonstrated its effectiveness for multiactivity detection in continuous videos. Our layer is fully differentiable and trainable using standard backpropagation, designed to learn temporal structure. We were able to confirm that our layer performs superior to state-of-the-art methods on activity detection datasets including MultiTHU-MOS and Charades, obtaining the best known performance. We also tested our approach with two more public video datasets, MLB-YouTube <ref type="bibr" target="#b17">(Piergiovanni &amp; Ryoo, 2018a)</ref> and AVA <ref type="bibr" target="#b8">(Gu et al., 2017)</ref>, and confirmed its advantage over the previous works in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>As our base per-segment CNN, we use the I3D <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017)</ref> network pretrained on the ImageNet and Kinetics <ref type="bibr" target="#b13">(Kay et al., 2017)</ref> datasets. I3D obtained stateof-the-art results on segmented video tasks, and this allows us to obtain reliable v t . We also use two-stream version of InceptionV3 <ref type="bibr" target="#b26">(Szegedy et al., 2016)</ref> pretrained on Imagenet and Kinetics as our base per-frame CNN, and compared them. We chose InceptionV3 as it is deeper than previous two-stream CNNs such as <ref type="bibr" target="#b25">(Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b7">Feichtenhofer et al., 2016)</ref>. We extracted frames from the videos at 25 fps, computed TVL1 <ref type="bibr" target="#b37">(Zach et al., 2007)</ref> optical flow, clipped to <ref type="bibr">[−20, 20]</ref>. For InceptionV3, we computed features for every 3 frames (8 fps). For I3D, every frame was used as the input. I3D has a temporal stride of 8, resulting in 3 features per second (3 fps). By design, I3D has a temporal resolution of 99 frames, so each feature is able to capture up to 99 frames of temporal information.</p><p>We implemented our TGM layers as well as other baseline layers in PyTorch. Our default setting was as follows: for 3-layer models, we set L = 10 for frame-based features (i.e., InceptionV3) and L = 5 for segment-based features (i.e., I3D), as each segment already contains some temporal information. For 1-layer models, we set L = 30 for framebased features and L = 15 for segment-based features. We set M = 16 and C out = 80 and C out = 65 for the last TGM layer. We found these values to work well on a held out portion of the training set of MultiTHUMOS. In all models, we used one fully-connected layer at the end to make the per-frame or per-segment classification.</p><p>We trained our models using the Adam <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2014)</ref> optimizer with the learning rate set to 0.01. We decayed the learning rate by a factor of 10 after every 10 training epochs. We trained our models for 50 epochs. We plan to make all our source code and trained models publicly available once the paper is published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyperparameter Experiments</head><p>We conducted a set of experiments to compare the effects of the temporal duration, L, number of Gaussians, M , and the number of output channels, C out . For these experiments, we only used the one-stream version of I3D with RGB inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of L:</head><p>In <ref type="table">Table 11</ref>, we compare different values of L. For these experiments, we use M = 16 and C out = 16. We find that the 3-layer model with L = 5 performs the best. With I3D features, this allows the model to capture up to 8 seconds of information. The average activity in MultiTHUMOS is 3.3 seconds long and the maximum is 14.7 seconds long, and with this setting, the model is able to capture enough temporal context to perform well. Larger values of L capture too much temporal information, but due to the Gaussian structure, it does not drastically harm performance. <ref type="figure" target="#fig_0">Figure 10</ref> shows that even with longer kernels, the Gaussians learn to focus mostly on the center of the interval and capture the rough duration of the activities. Thus, having too long intervals does not drastically harm performance, which is in contrast to the standard 1-D convolution. Note that for Charades, the temporal kernels are learned to capture much longer temporal duration, as the average activity in charades is 12.8 seconds and larger values of L perform better. <ref type="figure" target="#fig_0">Figure 10</ref> illustrates examples of the learned TGM kernels of various lengths. The figure shows that the kernels focus on short temporal intervals on MultiTHUMOS even if we make the filters longer, as the activities are an average of 3.3 seconds long. On Charades, the TGM kernels learn to capture much longer intervals, as the activities are an average of 12.8 seconds long. We believe that this suggests TGMs are learning to capture information from the important necessary intervals.</p><p>In <ref type="table">Table 11</ref>, we also report the results of using a standard 1-D conv. layer with different L values. The number of parameters in our TGM layer is independent of L, however, with the standard 1-D conv. layer, the number of parameters increases as L increases. We find that increasing L with 1-D convolution helps for small values of L, but for L &gt; 15, the performance drastically drops, while TGM layers only show a small decrease. <ref type="table">Table 11</ref>. Effect of L on MultiTHUMOS and Charades using only RGB I3D features. Note that the 3 TGM layer models have larger temporal resolution than the 1 TGM layer models for the same values of L. We also compare to using standard one-layer 1-D conv layer with different values of L. Standard 1-D Convolution <ref type="figure" target="#fig_5">(Fig. 8a</ref>) 32.5 1-D Conv with 1 Gaussian <ref type="figure" target="#fig_5">(Fig. 8b)</ref> 28.6 1-D Conv with many Gaussians <ref type="figure" target="#fig_5">(Fig. 8c)</ref> 33.2 TC-Conv with unconstrained kernel <ref type="figure">(Fig. 9</ref>) 32.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiTHUMOS</head><p>Our TGM Layer 36.1</p><p>Effect of M : In <ref type="table" target="#tab_9">Table 9</ref>, we compare different values of M . For these experiments, we set L = 15 and C out = 16. We find that M = 16 performs best, suggesting that smaller values of M restrict the possible temporal kernels too much. We also observe that larger values of M performs slightly worse than M = 16 (but not much), likely because they introduce more parameters than needed. When M and L have similar values, it allows the model to learn a sufficient number of Gaussians and create a diverse range of temporal kernels. When M is larger than L, it results in learning a kernel similar to standard 1-D convolution.</p><p>Effect of C out : In <ref type="table">Table 10</ref>, we compare different values of C out . For these experiments, L = 15, we used 1-layer and M = 16. We find that C out performs best when set to 16 or larger on these datasets. Larger values of C out seem to capture redundant information, as it does not lower performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of Different Layer Forms</head><p>To confirm the various aspects of our design, we conducted experiments comparing different types of temporal convolution. In <ref type="figure" target="#fig_5">Fig. 8a</ref> we illustrate the standard 1-D convolution, taking D × T input and producing a C × T output, where D is the number of input channels and C is the number of output channels. In <ref type="figure" target="#fig_5">Fig. 8b</ref>, we illustrate the method of applying a Gaussian mixture kernel as 1-D convolution.</p><p>Here, the Gaussian mixture kernel is shared by all D input channels and we learn a C number of such kernels. In <ref type="figure" target="#fig_5">Fig.  8c</ref>, we illustrate the approach of applying a Gaussian mixture kernel as 1-D convolution while learning D different Gaussian mixtures. This is very similar to the standard 1-D convolution, except that the filter values are constrained to have the shape of Gaussian mixtures. <ref type="figure">Fig. 9</ref> illustrates one more baseline. This is similar to our full TGM layer with the channel-combination ( <ref type="figure">Fig. 4</ref> in main paper). However, in this baseline, instead of learning Gaussian mixtures, we learn C in · C out number of 1 × L kernels. The kernel values are left unconstrained. While the TGM layer has 2 · M + C in · C out · M + C in · C out parameters, this layer has L · C in · C out · M + C in · C out , which is more than the TGM layer.</p><p>In <ref type="table" target="#tab_1">Table 12</ref>, we compare the results of the various abovementioned layers on MultiTHUMOS using RGB I3D features. We find that the <ref type="figure" target="#fig_5">Fig. 8b</ref> method performs poorly, while the <ref type="figure" target="#fig_5">Fig. 8c</ref> method slightly outperforms the standard 1-D convolution. The <ref type="figure">Fig. 9</ref> method is slightly better than the standard 1-D convolution, but performs worse than <ref type="figure" target="#fig_5">Fig.  8c</ref>. However, none of these layers perform as well as our TGM layer, confirming that both the design of learning Gaussian mixtures and maintaining temporal channel axis are important for activity detection.   <ref type="figure">Figure 9</ref>. A temporal convolutional layer with channel combination similar to <ref type="figure">Fig. 4 (in main paper)</ref>. The difference is that this layer does not learn Gaussian mixtures, but unconstrained 1-D temporal kernels. some of these classes are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. Each continuous clip contains on average of 7.2 activities, giving a total of over 15,000 activity instances in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on Additional Datasets</head><p>What makes this dataset challenging is that the variation between classes is very small. In ActivityNet <ref type="bibr" target="#b10">(Heilbron et al., 2015)</ref>, for example, the difference between swimming and brushing hair is drastic. The background, motion, and even size of the person in the video is different. However, in broadcast baseball videos, the difference between a ball and a strike, or a swing and a bunt, are small. All actions are recorded from the same camera angle as we can confirm from <ref type="figure" target="#fig_0">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2. RESULTS</head><p>In <ref type="table" target="#tab_2">Table 13</ref>, we compare various approaches on this dataset. Our TGM layers improve over the baseline by ∼6% (40.1 vs. 34.2). Additionally, we compare to methods using the superevent representation <ref type="bibr" target="#b18">(Piergiovanni &amp; Ryoo, 2018b)</ref>, which previously achieved state-of-the-art performance on several activity detection datasets. On this dataset, our approach outperforms the super-event representation, and further the concatenation of our TGM representation with such superevent representation performs best by a significant margin (∼13% compared to the baseline). This suggests that TGMs and super-event capture different temporal information and are both useful to the detection task.</p><p>We further find that using multiple, standard temporal convolution layers leads to worse performance, likely due to overfitting from the large number of parameters. While using multiple TGM layers improves performance, confirming that the Gaussian structure and sparsity constraint benefits model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. AVA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1. DATASET</head><p>AVA <ref type="bibr" target="#b8">(Gu et al., 2017)</ref> is a large-scale video dataset containing of 80 atomic action classes in 57k video clips. These clips are drawn from movies. Existing datasets, such as Charades, have very specific actions that depend on objects, such as holding a cup vs. holding a picture. In AVA, the actions are intentionally generic, such as sit, stand, hold, carry, etc. Further, the AVA dataset is annotated with both spatial and temporal locations of activities. Since we are interested in temporal activity detection, we follow the setting of Piergiovanni &amp; Ryoo (2018b) and label each frame with the occurring activities while ignoring the spatial location. We evaluate performance following the same method as Mul-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiTHUMOS L=15 MultiTHUMOS L=30</head><p>Charades L=30 Charades L=15 <ref type="figure" target="#fig_0">Figure 10</ref>. Illustration of several learned TGM kernels. On MultiTHUMOS, it learns to focus on shorter intervals to capture shorter events. On Charades, the Gaussians have a larger σ value, resulting in filters that attend to longer temporal durations.   mAP Random 2.65 I3D baseline 7.5 I3D + 3 temporal conv. layers 7.9 I3D + LSTM 7.8 I3D + super-events <ref type="bibr" target="#b18">(Piergiovanni &amp; Ryoo, 2018b)</ref> 9.8 I3D + 1 TGMs 11.2 I3D + 3 TGMs 14.5 I3D + 3 TGMs + super-events 14.9</p><p>tiTHUMOS, Charades and MLB-YouTube by measuring per-frame mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2. RESULTS</head><p>In <ref type="table" target="#tab_13">Table 14</ref>, we present the results of our model. We again find that temporal convolution and LSTMs provide some benefit over the baseline, but TGM layers further improve performance. Again, combining the TGM, which captures local temporal structure, with super-events which capture global temporal structure, provides the best performance by ∼ 7.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Context Gaiting</head><p>Context gating <ref type="bibr" target="#b15">(Miech et al., 2017)</ref> is an layer designed to capture relationships between network activations. However, it is designed for segmented video clip classification, as it originally takes a fixed-size input. Applying it to variable length continuous videos in a sliding-window fashion is possible, and we conducted this experiment with a window size of 30 (same temporal resolution as ours). When context gating is applied on top of I3D features, it gives 35.8 on MultiTHUMOS, lower than ours (44.3).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example illustrating how our Temporal Gaussian Mixture layer is computed. Multiple (M ) temporal Gaussian distributions are learned, and they are combined with the learned soft attention (mixing) weights to form the C temporal convolution filters. L is the temporal length of the filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a-c) Different forms of 1-D temporal convolutions which take a D × T input and produces a C × T output based on C number of D × L kernels: (a) the standard 1-D convolution, (b) using Gaussian mixtures for 1-D convolution while sharing Gaussian mixtures across input channels, and (c) using D different Gaussian mixtures for 1-D convolution. (d) Our TGM layer (with TC-grouping) in its simplest form (i.e., 1-layer case) applying the 1 × L temporal kernel in a 2-D convolutional fashion, maintaining both time and feature axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of a TGM layer with grouped convolution. This layer learns a set of C Gaussian mixtures that are convolved with the input channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>An overview of an example video CNN model with two TGM layers. Because of its fully convolutional design, it is able to handle videos with any length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>(a-c) Different forms of 1-D temporal convolutions which take a D × T input and produces a C × T output based on C number of D × L kernels: (a) the standard 1-D convolution, (b) using Gaussian mixtures for 1-D convolution while sharing Gaussian mixtures across input channels, and (c) using D different Gaussian mixtures for 1-D convolution. (d) Our TGM layer in its simplest form (i.e., 1-layer case) applying the 1 × L temporal kernel in a 2-D convolutional fashion, maintaining both time and feature axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Examples of several of the activities in the MLB-YouTube dataset: (a) Pitch, (b) Hit, (c) Bunt, (d) Hit by pitch, (e) No activity. This shows the difficulty of this dataset, as the difference between hit and bunt, swing and no swing are very small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Additional number of parameters for models when added to the base architecture (e.g., I3D or Inception V3).</figDesc><table><row><cell>Model</cell><cell># of parameters</cell></row><row><cell>LSTM</cell><cell>10.5M</cell></row><row><cell>1 Temporal Conv</cell><cell>10.5M</cell></row><row><cell>3 Temporal Conv</cell><cell>31.5M</cell></row><row><cell>1 TGM Layer</cell><cell>10K</cell></row><row><cell>3 TGM Layers</cell><cell>100K</cell></row><row><cell>5 TGM Layers</cell><cell>200K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of previous methods with comparable number of parameters and random forms of our TGM layer (using twostream I3D on MultiTHUMOS).</figDesc><table><row><cell>Model</cell><cell>mAP</cell></row><row><cell>LSTM with 100k parameters</cell><cell>6.5</cell></row><row><cell>Temporal Conv. with 100k parameters</cell><cell>7.3</cell></row><row><cell>TGM with random temporal filters</cell><cell>34.5</cell></row><row><cell>TGM with fixed Gaussians</cell><cell>38.5</cell></row><row><cell>Full TGM</cell><cell>44.3</cell></row><row><cell cols="2">Table 4. Comparison of the different forms of temporal convolution</cell></row><row><cell cols="2">on MultiTHUMOS using RGB I3D features. We set L = 15 and</cell></row><row><cell>used 1 layer models for these experiments.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Performances of the state-of-the-art methods and our approach on MultiTHUMOS. Our approach meaningfully outperforms all previous results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Per-frame mAP on Charades, evaluated with the 'Cha-rades_v1_localize' setting. I3D models are two-stream, using both RGB and optical flow inputs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Comparison of various values of M on MultiTHUMOS and Charades using RGB I3D features. For these experiments, 1 layer was used with L = 15 and Cout = 16.</figDesc><table><row><cell></cell><cell cols="2">MultiTHUMOS Charades</cell></row><row><cell>M = 2</cell><cell>27.8</cell><cell>15.5</cell></row><row><cell>M = 4</cell><cell>33.1</cell><cell>16.2</cell></row><row><cell>M = 8</cell><cell>34.8</cell><cell>17.5</cell></row><row><cell>M = 16</cell><cell>36.1</cell><cell>17.5</cell></row><row><cell>M = 32</cell><cell>35.7</cell><cell>17.1</cell></row><row><cell>M = 64</cell><cell>35.8</cell><cell>17.3</cell></row><row><cell cols="3">Table 10. Comparison of values of Cout on MultiTHUMOS and</cell></row><row><cell cols="3">Charades using RGB I3D features. For these experiments, 1 layer</cell></row><row><cell cols="2">was used with L = 15 and M = 16.</cell><cell></cell></row><row><cell></cell><cell cols="2">MultiTHUMOS Charades</cell></row><row><cell>C out = 1</cell><cell>33.5</cell><cell>16.2</cell></row><row><cell>C out = 4</cell><cell>34.2</cell><cell>17.4</cell></row><row><cell>C out = 8</cell><cell>35.5</cell><cell>17.5</cell></row><row><cell>C out = 16</cell><cell>36.1</cell><cell>17.5</cell></row><row><cell>C out = 32</cell><cell>36.0</cell><cell>17.2</cell></row><row><cell>C out = 64</cell><cell>36.1</cell><cell>17.4</cell></row><row><cell>C out = 80</cell><cell>36.1</cell><cell>17.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Result mAP on the MLB-YouTube dataset using InceptionV3 and I3D to obtain features. Our TGM layers significantly outperform the baseline models.</figDesc><table><row><cell>Model</cell><cell cols="3">Spatial Temporal Two-stream</cell></row><row><cell>Random</cell><cell>13.4</cell><cell>13.4</cell><cell>13.4</cell></row><row><cell>InceptionV3</cell><cell>31.2</cell><cell>31.8</cell><cell>31.9</cell></row><row><cell>InceptionV3 + LSTM</cell><cell>32.1</cell><cell>33.5</cell><cell>34.1</cell></row><row><cell>InceptionV3 + 1 temporal conv</cell><cell>32.8</cell><cell>34.4</cell><cell>35.2</cell></row><row><cell>InceptionV3 + 3 temporal conv</cell><cell>28.4</cell><cell>29.8</cell><cell>30.1</cell></row><row><cell>InceptionV3 + super-events</cell><cell>31.5</cell><cell>36.2</cell><cell>39.6</cell></row><row><cell>InceptionV3 + 1 TGM</cell><cell>32.4</cell><cell>36.3</cell><cell>37.4</cell></row><row><cell>InceptionV3 + 3 TGM</cell><cell>33.2</cell><cell>38.2</cell><cell>38.2</cell></row><row><cell>InceptionV3 + 3 TGM+super-events</cell><cell>34.6</cell><cell>42.4</cell><cell>42.9</cell></row><row><cell>I3D</cell><cell>33.8</cell><cell>35.1</cell><cell>34.2</cell></row><row><cell>I3D + LSTM</cell><cell>36.2</cell><cell>37.3</cell><cell>39.4</cell></row><row><cell>I3D + 1 temporal conv</cell><cell>37.3</cell><cell>38.6</cell><cell>39.9</cell></row><row><cell>I3D + 3 temporal conv</cell><cell>32.4</cell><cell>34.6</cell><cell>35.6</cell></row><row><cell>I3D + super-events</cell><cell>38.7</cell><cell>38.6</cell><cell>39.1</cell></row><row><cell>I3D + 1 TGM</cell><cell>35.5</cell><cell>37.5</cell><cell>38.5</cell></row><row><cell>I3D + 3 TGM</cell><cell>36.5</cell><cell>38.4</cell><cell>40.1</cell></row><row><cell>I3D + 3 TGM+super-events</cell><cell>39.4</cell><cell>46.0</cell><cell>47.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Results on AVA dataset with the temporal annotationonly setting (i.e., frame classification without using bounding box training labels).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Department of Computer Science, Indiana University. Correspondence to: AJ Piergiovanni &lt;ajpiergi@indiana.edu&gt;, Michael Ryoo &lt;mryoo@indiana.edu&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Science Foundation (IIS-1812943 and CNS-1814985), and by the ICT R&amp;D program of MSIP/IITP, Republic of Korea (17ZF1200, Development of XDMedia Solution for Invigoration of Realistic Media Industry).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10130</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Spherical cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Predictivecorrective networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03615</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08421</idno>
		<title level="m">A video dataset of spatiotemporally localized atomic visual actions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition</title>
		<meeting>the ICCV Workshop on Action, Gesture, and Emotion Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Challenge</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition in baseball videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Computer Vision in Sports</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning latent superevents to detect multiple activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Association for Artificial Intelligence (AAAI)</title>
		<meeting>the American Association for Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="896" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Cdc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01515</idno>
		<title level="m">Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06371</idno>
		<title level="m">Asynchronous temporal fields for action recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A gaussian mixture model layer jointly optimized with discriminative features within a deep neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4270" to="4274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long-term Temporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07814</idno>
		<title level="m">Region convolutional 3d network for temporal activity detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spidercnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos. International Journal of Computer Vision (IJCV)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06228</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
