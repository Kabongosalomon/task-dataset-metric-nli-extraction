<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FIXING THE TRAIN-TEST RESOLUTION DISCREPANCY: FIXEFFICIENTNET</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FIXING THE TRAIN-TEST RESOLUTION DISCREPANCY: FIXEFFICIENTNET</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an extensive analysis of the performance of the EfficientNet image classifiers with several recent training procedures, in particular one that corrects the discrepancy between train and test images <ref type="bibr" target="#b0">[1]</ref>. The resulting network, called FixEfficientNet, significantly outperforms the initial architecture with the same number of parameters.</p><p>For instance, our FixEfficientNet-B0 trained without additional training data achieves 79.3% top-1 accuracy on Im-ageNet with 5.3M parameters. This is a +0.5% absolute improvement over the Noisy student EfficientNet-B0 trained with 300M unlabeled images. An EfficientNet-L2 pre-trained with weak supervision on 300M unlabeled images and further optimized with FixRes achieves 88.5% top-1 accuracy (top-5: 98.7%), which establishes the new state of the art for ImageNet with a single crop.</p><p>These improvements are thoroughly evaluated with cleaner protocols than the one usually employed for Imagenet, and particular we show that our improvement remains in the experimental setting of ImageNet-v2, that is less prone to overfitting, and with ImageNet Real Labels. In both cases we also establish the new state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In order to obtain the best possible performance from Convolutional neural nets (CNNs), the training and testing data distributions should match. However, in image recognition, data pre-processing procedures are often different for training and testing: the most popular practice is to extract a rectangle with random coordinates from the image to artificially increase the amount of training data. This Region of Classification (RoC) is then resized to obtain an image, or crop, of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a center crop. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the data distribution seen by the CNN.</p><p>Over the years, training and testing pre-processing procedures have evolved, but so far they have been optimized separately <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr">Touvron et al.</ref> show <ref type="bibr" target="#b0">[1]</ref> that this separate optimization has a detrimental effect on the test-time performance of models. They address this problem with the FixRes method, We apply this method to the recent EfficientNet <ref type="bibr" target="#b3">[4]</ref> architecture, which offers an excellent compromise between number of parameters and accuracy. This evaluation paper shows that properly combining FixRes and EfficientNet further improves the state of the art <ref type="bibr" target="#b3">[4]</ref>. Noticeably,</p><p>• We report the best performance without external data on ImageNet (top1: 85.7%);</p><p>• We validate the significance of our results on the ImageNet-v2 test set, an improved evaluation setup that clearly separates the validation and test sets. Fix-EfficientNet achieves the best performance.</p><p>This paper is organized as follows. In Section 2 we introduce the corrected training procedure for EfficientNet, that produces FixEfficientNet. Section 3 analyzes our extensive evaluation and compare FixEfficientNet with the state of the art. Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TRAINING WITH FIXRES: UPDATES</head><p>Recent research in image classification tends towards larger networks and higher resolution images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. For instance, the state-of-the-art in the ImageNet ILSVRC 2012 benchmark is currently held by the EfficientNet-L2 <ref type="bibr" target="#b7">[8]</ref> architecture with 480M parameters using 800×800 images for training. Similarly, the state-of-the-art model learned from scratch is currently EfficientNet-B8 <ref type="bibr" target="#b8">[9]</ref> with 88M parameters using 672×672 images for training. In this note, we focus on the EfficientNet architecture <ref type="bibr" target="#b3">[4]</ref> due to its good accuracy/cost trade-off and its popularity.</p><p>Data augmentation is routinely employed at training time to improve model generalization and reduce overfitting. In this note, we use the same augmentation setup as in the original FixRes paper <ref type="bibr" target="#b0">[1]</ref>. In addition, we have integrated label smoothing, which is orthogonal to the approach. FixRes is a very simple fine-tuning that re-trains the classifier or a few top layers at the target resolution. Therefore, it has several advantages:</p><p>1. it is computationally cheap, the back-propagation is not performed on the whole network;</p><p>2. it works with any CNN classification architecture and is complementary with the other tricks mentioned above;</p><p>3. it can be applied on a CNN that comes from a possibly non reproducible source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We experiment on the ImageNet-2012 benchmark <ref type="bibr" target="#b9">[10]</ref>, and report standard performance metrics (top-1 and top-5 accuracies) on a single image crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setting</head><p>We focus on the EfficientNet <ref type="bibr" target="#b3">[4]</ref> architectures. In the literature, wo versions provide the best performance: Efficient-Net trained with adversarial examples <ref type="bibr" target="#b8">[9]</ref>, and Efficient-Net trained with Noisy student <ref type="bibr" target="#b7">[8]</ref> pre-trained in a weaklysupervised fashion on 300 million unlabeled images. We start from the EfficientNet models in rwightman's GitHub repository <ref type="bibr" target="#b10">[11]</ref>. These models have been converted from the original Tensorflow to PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training.</head><p>We mostly follow the FixRes <ref type="bibr" target="#b0">[1]</ref> training protocol. The only difference is that we combine the FixRes dataaugmentation with label smoothing during the fine-tuning. <ref type="table">Table 1 and Table 2</ref> compare our results with those of the Ef-ficientNet reported in the literature. All our FixEfficientNets outperform the corresponding EfficientNet (see <ref type="figure">Figure 1</ref>). As a result and to the best of our knowledge, our FixEfficientNet-L2 surpasses all other results reported in the literature. It achieves 88.5% Top-1 accuracy and 98.7% Top-5 accuracy on the ImageNet-2012 validation benchmark <ref type="bibr" target="#b9">[10]</ref>. Clean labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with the state of the art</head><p>In order to complement this evaluation, Table 3 present the results with the ImageNet clean labels proposed by Beyer et all. <ref type="bibr" target="#b4">[5]</ref>. With 90.9% Top-1 accuracy and 98.8% Top-5 accuracy FixEfficientNet-L2 surpasses all other results reported in the literature with this labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Significance of the results</head><p>Several runs of the same training incur variations of about 0.1 accuracy points on Imagenet due to random initialization and mini-batch sampling. In general, since the Imagenet 2012 test set is not available, most works tune the hyper-parameters on the validation set, ie. there is no distinction between validation and test set. This setting, while widely adopted, is not legitimate and can cause overfitting to go unnoticed.</p><p>EfficientNets employ Neural Architecture Search, which significantly enlarges the hyper-parameter space. Additionally, the ImageNet validation images were used to filter the images from the unlabelled set <ref type="bibr" target="#b7">[8]</ref>. Therefore the pre-trained models may benefit from more overfitting on the validation set. We quantify this in the experiments presented below.</p><p>Since we use pre-trained EfficientNet for our initialization, our results are comparable to those from the Noisy Student <ref type="bibr" target="#b7">[8]</ref>, which uses the same degree of overfitting, but not directly with other semi-supervised approaches like that of Yalniz et al. <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation on ImageNet-V2</head><p>The ImageNet-V2 <ref type="bibr" target="#b16">[17]</ref> dataset was introduced to overcome the lack of a test split in the Imagenet dataset. ImageNet-V2 consists of 3 novel test sets that replace the ImageNet test set, which is no longer available. They were carefully designed to match the characteristics of the original test set. One of these test sets, Matched Frequency is the closest to the Im-ageNet validation set. To ensure that observed improvements are not due to overfitting, we evaluate all our models on the Matched Frequency version of the ImageNet-v2 <ref type="bibr" target="#b16">[17]</ref> dataset. We evaluate the other methods in the same way. We present the results in <ref type="table" target="#tab_4">Tables 4 and 5</ref>.  <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref>. For a given performance on Imagenet-val, overfitted models tend to have a lower performance on ImageNet-v2 and therefore are below the approaches that generalize better.</p><p>The original study of <ref type="bibr" target="#b16">[17]</ref> shows that there is significant overfitting of various models to the Imagenet 2012 valuation set, but that it does not impact the relative order of the models.</p><p>Quantifying the overfitting on Imagenet. As mentioned earlier, several choices in the Noisy Student <ref type="bibr" target="#b7">[8]</ref> method are prone to overfitting. We verify this hypothesis and quantify its extent by comparing the relative accuracy of this approach with another semi-supervised approach [2] both on ImageNet and ImageNet-V2 <ref type="bibr" target="#b16">[17]</ref>.</p><p>Without overfitting, models performing similarly on Imagenet should also have similar performances on ImageNet-V2 <ref type="bibr" target="#b16">[17]</ref>. However, for a comparable performance on Ima-geNet, when evaluating on ImageNet-V2, the Billion scale models of Yalniz et al. <ref type="bibr" target="#b1">[2]</ref> outperform the EfficientNets from Noisy Student. For example, FixResNeXt-101 32x4d <ref type="bibr" target="#b0">[1]</ref> has the same performance as EfficientNet-B3 <ref type="bibr" target="#b7">[8]</ref> on ImageNet but on ImageNet-V2 FixResNeXt-101 32x4d <ref type="bibr" target="#b0">[1]</ref> is better (+0.7% Top-1 accuracy).</p><p>This shows that the EfficientNet Noisy student <ref type="bibr" target="#b7">[8]</ref> tends to overfit and does not generalize as well as the (prior) semisupervised work <ref type="bibr" target="#b1">[2]</ref> or other works of the literature. <ref type="figure">Figure 2</ref> illustrates this effect. The FixRes fine-tuning procedure is neutral with respect to overfitting: overfitted models remain overfitted and conversely.  Comparison with the state of the art. Despite overfitting, EfficientNet remains very competitive on ImageNet-V2, as reported in <ref type="table" target="#tab_7">Table 6</ref>. Interestingly, the FixEfficientNet-L2 that we fine-tuned from EfficientNet establishes the new state of the art with additional data on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>The "Fixing Resolution" is a method that improves the performance of any model. It is a method that is applied as a fine-tuning step after the conventional training, during a few epochs only, which makes it very flexible. It is easily integrated into any existing training pipeline. In our paper we proposed a thorough evaluation of the combination of the current state-of-the-art models, namely EfficientNet, with this improved training method.</p><p>We provide an open-source implementation of our method 1 .  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Results on ImageNet with extra training data. We start from pre-trained models<ref type="bibr" target="#b7">[8]</ref> learned using 300M additional unlabeled images (single crop evaluation). See Section 3.3 about the significance of these results. Results on ImageNet without external data (single Crop evaluation). FixEfficientNet outperforms the previous EfficientNet AdvProp<ref type="bibr" target="#b8">[9]</ref> state of the art in this setup, see Section 3.3 for the significance of these results.</figDesc><table><row><cell>Model</cell><cell>#params</cell><cell>train res</cell><cell cols="2">EfficientNet [8] test Top-1 Top-5 test Top-1 Top-5 FixEfficientNet res (%) (%) res (%) (%)</cell></row><row><cell cols="4">B0 5.3M 224 224 78.8</cell><cell>94.5 320 80.2</cell><cell>95.4</cell></row><row><cell cols="4">B1 7.8M 240 240 81.5</cell><cell>95.8 384 82.6</cell><cell>96.5</cell></row><row><cell cols="4">B2 9.2M 260 260 82.4</cell><cell>96.3 420 83.6</cell><cell>96.9</cell></row><row><cell cols="4">B3 12M 300 300 84.1</cell><cell>96.9 472 85.0</cell><cell>97.4</cell></row><row><cell cols="4">B4 19M 380 380 85.3</cell><cell>97.5 472 85.9</cell><cell>97.7</cell></row><row><cell cols="4">B5 30M 456 456 86.1</cell><cell>97.8 576 86.4</cell><cell>97.9</cell></row><row><cell cols="4">B6 43M 528 528 86.4</cell><cell>97.9 680 86.7</cell><cell>98.0</cell></row><row><cell cols="4">B7 66M 600 600 86.9</cell><cell>98.1 632 87.1</cell><cell>98.2</cell></row><row><cell cols="4">L2 480M 475 800 88.4</cell><cell>98.7 600 88.5</cell><cell>98.7</cell></row><row><cell>Model</cell><cell>#params</cell><cell>train res</cell><cell cols="2">EfficientNet [9] test Top-1 Top-5 test Top-1 Top-5 FixEfficientNet res (%) (%) res (%) (%)</cell></row><row><cell cols="4">B0 5.3M 224 224 77.6</cell><cell>93.3 320 79.3</cell><cell>94.6</cell></row><row><cell cols="4">B1 7.8M 240 240 79.6</cell><cell>94.3 384 81.3</cell><cell>95.7</cell></row><row><cell cols="4">B2 9.2M 260 260 80.5</cell><cell>95.0 420 82.0</cell><cell>96.0</cell></row><row><cell>B3</cell><cell cols="3">12M 300 300 81.9</cell><cell>95.6 472 83.0</cell><cell>96.4</cell></row><row><cell>B4</cell><cell cols="3">19M 380 380 83.3</cell><cell>96.4 512 84.0</cell><cell>97.0</cell></row><row><cell>B5</cell><cell cols="3">30M 456 456 84.3</cell><cell>97.0 576 84.7</cell><cell>97.2</cell></row><row><cell>B6</cell><cell cols="3">43M 528 528 84.8</cell><cell>97.1 576 84.9</cell><cell>97.3</cell></row><row><cell>B7</cell><cell cols="3">66M 600 600 85.2</cell><cell>97.2 632 85.3</cell><cell>97.4</cell></row><row><cell cols="4">B8 87.4M 672 672 85.5</cell><cell>97.3 800 85.7</cell><cell>97.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on ImageNet Real labels<ref type="bibr" target="#b4">[5]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="8">No Extra-Training Data EfficientNet [8] FixEfficientNet EfficientNet [8] FixEfficientNet Extra-Training Data Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5</cell></row><row><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>B0</cell><cell>83.7</cell><cell>95.8</cell><cell>85.8</cell><cell>96.8</cell><cell>84.5</cell><cell>96.4</cell><cell>86.5</cell><cell>97.3</cell></row><row><cell>B1</cell><cell>85.1</cell><cell>96.4</cell><cell>87.0</cell><cell>97.4</cell><cell>86.7</cell><cell>97.2</cell><cell>88.1</cell><cell>98.0</cell></row><row><cell>B2</cell><cell>86.0</cell><cell>96.8</cell><cell>87.7</cell><cell>97.6</cell><cell>87.3</cell><cell>97.6</cell><cell>88.8</cell><cell>98.2</cell></row><row><cell>B3</cell><cell>87.2</cell><cell>97.4</cell><cell>88.3</cell><cell>98.0</cell><cell>88.4</cell><cell>98.0</cell><cell>89.2</cell><cell>98.4</cell></row><row><cell>B4</cell><cell>88.3</cell><cell>97.9</cell><cell>89.2</cell><cell>98.3</cell><cell>89.4</cell><cell>98.4</cell><cell>89.8</cell><cell>98.5</cell></row><row><cell>B5</cell><cell>88.9</cell><cell>98.2</cell><cell>89.4</cell><cell>98.4</cell><cell>89.7</cell><cell>98.5</cell><cell>90.0</cell><cell>98.6</cell></row><row><cell>B6</cell><cell>89.3</cell><cell>98.3</cell><cell>89.6</cell><cell>98.4</cell><cell>89.8</cell><cell>98.5</cell><cell>90.1</cell><cell>98.6</cell></row><row><cell>B7</cell><cell>89.4</cell><cell>98.3</cell><cell>89.7</cell><cell>98.5</cell><cell>90.1</cell><cell>98.6</cell><cell>90.3</cell><cell>98.7</cell></row><row><cell>B8</cell><cell>89.6</cell><cell>98.3</cell><cell>90.0</cell><cell>98.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>L2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90.6</cell><cell>98.8</cell><cell>90.9</cell><cell>98.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results on ImageNet-V2<ref type="bibr" target="#b16">[17]</ref> Matched Frequency with extra-training data. We start from pre-trained models<ref type="bibr" target="#b7">[8]</ref> that have been learned using 300M additional unlabeled images (single crop evaluation).</figDesc><table><row><cell>Model</cell><cell>#params</cell><cell>train res</cell><cell cols="2">EfficientNet [8] test Top-1 Top-5 test Top-1 Top-5 FixEfficientNet res (%) (%) res (%) (%)</cell></row><row><cell cols="4">B0 5.3M 224 224 67.7</cell><cell>88.1 320 69.4</cell><cell>89.6</cell></row><row><cell cols="4">B1 7.8M 240 240 70.9</cell><cell>90.1 384 72.7</cell><cell>91.4</cell></row><row><cell cols="4">B2 9.2M 260 260 72.3</cell><cell>91.1 420 73.6</cell><cell>92.0</cell></row><row><cell cols="4">B3 12M 300 300 73.9</cell><cell>91.9 472 75.0</cell><cell>93.0</cell></row><row><cell cols="4">B4 19M 380 380 75.7</cell><cell>93.1 472 76.2</cell><cell>93.6</cell></row><row><cell cols="4">B5 30M 456 456 76.8</cell><cell>93.6 576 77.0</cell><cell>94.0</cell></row><row><cell cols="4">B6 43M 528 528 77.3</cell><cell>93.9 680 77.5</cell><cell>94.3</cell></row><row><cell cols="4">B7 66M 600 600 78.5</cell><cell>94.4 632 78.6</cell><cell>94.7</cell></row><row><cell cols="4">L2 480M 475 800 80.3</cell><cell>95.8 600 80.8</cell><cell>96.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results on ImageNet-V2 [17] Matched Frequency without external data (single Crop evaluation).</figDesc><table><row><cell>Model</cell><cell>#params</cell><cell>train res</cell><cell cols="2">EfficientNet [9] test Top-1 Top-5 test Top-1 Top-5 FixEfficientNet res (%) (%) res (%) (%)</cell></row><row><cell cols="4">B0 5.3M 224 224 65.5</cell><cell>85.6 320 67.8</cell><cell>87.9</cell></row><row><cell cols="4">B1 7.8M 240 240 67.5</cell><cell>87.8 384 70.1</cell><cell>89.6</cell></row><row><cell cols="4">B2 9.2M 260 260 68.9</cell><cell>88.4 420 70.8</cell><cell>90.2</cell></row><row><cell>B3</cell><cell cols="3">12M 300 300 70.9</cell><cell>89.4 472 72.7</cell><cell>90.9</cell></row><row><cell>B4</cell><cell cols="3">19M 380 380 72.9</cell><cell>91.0 512 73.9</cell><cell>91.8</cell></row><row><cell>B5</cell><cell cols="3">30M 456 456 74.6</cell><cell>92.0 576 75.1</cell><cell>92.4</cell></row><row><cell>B6</cell><cell cols="3">43M 528 528 75.4</cell><cell>92.4 576 75.4</cell><cell>92.6</cell></row><row><cell>B7</cell><cell cols="3">66M 600 600 76.1</cell><cell>93.0 632 75.8</cell><cell>93.2</cell></row><row><cell cols="4">B8 87.4M 672 672 76.1</cell><cell>92.7 800 75.9</cell><cell>93.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Performance comparison and state of the art on ImageNet-v2, single crop with external data, sorted by top-1 accuracy. NS: Noisy Student<ref type="bibr" target="#b7">[8]</ref>. BS: Billion-scale<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell>Model</cell><cell>size</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>EfficientNet-B0 NS [8]</cell><cell>5.3M</cell><cell>67.7</cell><cell>88.1</cell></row><row><cell>FixEfficientNet-B0</cell><cell>5.3M</cell><cell>69.4</cell><cell>89.6</cell></row><row><cell>EfficientNet-B1 NS [8]</cell><cell>7.8M</cell><cell>70.9</cell><cell>90.1</cell></row><row><cell>ResNet50 BS [2]</cell><cell>25.6M</cell><cell>71.7</cell><cell>90.5</cell></row><row><cell>EfficientNet-B2 NS [8]</cell><cell>9.1M</cell><cell>72.3</cell><cell>91.1</cell></row><row><cell cols="2">ResNeXt-50 32x4d BS [2] 25.1M</cell><cell>72.6</cell><cell>90.9</cell></row><row><cell>FixEfficientNet-B1</cell><cell>7.8M</cell><cell>72.7</cell><cell>91.4</cell></row><row><cell>FixEfficientNet-B2</cell><cell>9.1M</cell><cell>73.6</cell><cell>92.0</cell></row><row><cell>EfficientNet-B3 NS [8]</cell><cell>12.2M</cell><cell>73.9</cell><cell>91.9</cell></row><row><cell cols="2">ResNeXt-101 32x4d BS [2] 42.0M</cell><cell>74.2</cell><cell>92.0</cell></row><row><cell cols="2">FixResNeXt-101 32x4d [1] 42.0M</cell><cell>74.6</cell><cell>92.7</cell></row><row><cell>FixEfficientNet-B3</cell><cell>12.2M</cell><cell>75.0</cell><cell>93.0</cell></row><row><cell cols="2">ResNeXt-101 32x8d BS [2] 88.0M</cell><cell>75.5</cell><cell>92.8</cell></row><row><cell cols="2">ResNeXt-101 32x16d BS [2]193.0M</cell><cell>75.6</cell><cell>93.3</cell></row><row><cell>EfficientNet-B4 NS [8]</cell><cell>19.3M</cell><cell>75.7</cell><cell>93.1</cell></row><row><cell>FixEfficientNet-B4</cell><cell>19.3M</cell><cell>76.2</cell><cell>93.6</cell></row><row><cell cols="2">FixResNeXt-101 32x8d [1] 88.0M</cell><cell>76.3</cell><cell>93.4</cell></row><row><cell cols="2">FixResNeXt-101 32x16d [1] 193.0M</cell><cell>76.7</cell><cell>93.4</cell></row><row><cell>EfficientNet-B5 NS [8]</cell><cell>30.4M</cell><cell>76.8</cell><cell>93.6</cell></row><row><cell>FixEfficientNet-B5</cell><cell>30.4M</cell><cell>77.0</cell><cell>94.0</cell></row><row><cell>EfficientNet-B6 NS [8]</cell><cell>43.0M</cell><cell>77.3</cell><cell>93.9</cell></row><row><cell>FixEfficientNet-B6</cell><cell>43.0M</cell><cell>77.5</cell><cell>94.3</cell></row><row><cell cols="2">FixResNeXt-101 32x48d [1] 829.0M</cell><cell>77.8</cell><cell>93.9</cell></row><row><cell>EfficientNet-B7 NS [8]</cell><cell>66.4M</cell><cell>78.5</cell><cell>94.4</cell></row><row><cell>FixEfficientNet-B7</cell><cell>66.4M</cell><cell>78.6</cell><cell>94.7</cell></row><row><cell>EfficientNet-L2 NS [8]</cell><cell>480.3M</cell><cell>80.5</cell><cell>95.7</cell></row><row><cell>FixEfficientNet-L2</cell><cell>480.3M</cell><cell>80.8</cell><cell>96.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• We report the best accuracy (top1: 88.5%) with external data on ImageNet, and with ImageNet with Reallabels [5] ;</p><p>• We achieve state-of-the-art compromises between accuracy and number of parameters, see <ref type="figure">Figure 1</ref>;</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hérve</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Ismet Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Are we done with Im-ageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy-Oukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09665</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pre-trained efficientnet models</title>
		<ptr target="https://github.com/rwightman/pytorch-image-models/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Girshick, Kaiming He, and Piotr Dollár</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13678</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Designing network design spaces</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
