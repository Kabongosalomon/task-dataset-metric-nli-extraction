<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
							<email>zhangyic17@tsinghua.org.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Processing and Machine Intelligence Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Processing and Machine Intelligence Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">China Mobile Research Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlan</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">China Mobile Research Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copyaugmented Seq2Seq model instantiation of LABES 1 . In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervisedonly and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Belief tracking (also known as dialog state tracking) is an important component in task-oriented dialog systems. The system tracks user goals through multiple dialog turns, i.e. infers structured belief states expressed in terms of slots and values (e.g. in <ref type="figure">Figure 1</ref>), to query an external database <ref type="bibr" target="#b11">(Henderson et al., 2014)</ref>. Different belief tracking models have been proposed in recent years, either trained independently <ref type="bibr" target="#b26">(Mrkšić et al., 2017;</ref><ref type="bibr"></ref> I need to find a Thai restaurant that's in the south section of the city.</p><p>There are three restaurants in the south part of town that serve Thai food. Do you have a cuisine preference? belief state DB # match: 3 restaurant-food: Thai ; restaurant-area: south <ref type="figure">Figure 1</ref>: The cues for inferring belief states from user inputs and system responses. The system response reveals the belief state either directly in the form of word repetition (red), or indirectly in the form of the database query result (green) determined by the belief state.  or within end-to-end (E2E) trainable dialog systems <ref type="bibr">(Wen et al., 2017a,b;</ref><ref type="bibr" target="#b24">Liu and Lane, 2017;</ref><ref type="bibr" target="#b35">Shu et al., 2019;</ref><ref type="bibr" target="#b23">Liang et al., 2020;</ref>.</p><p>Existing belief trackers mainly depend on supervised learning with human annotations of belief states for every user utterance. However, collecting these turn-level annotations is labor-intensive and time-consuming, and often requires domain knowledge to identify slots correctly. Building E2E trainable dialog systems, called E2E dialog systems for short, even further magnifies the demand for increased amounts of labeled data <ref type="bibr" target="#b7">(Gao et al., 2020;</ref>.</p><p>Notably, there are often easily-available unlabeled dialog data such as between customers and trained human agents accumulated in real-world customer services. In this paper, we are interested in reducing the reliance on belief state annotations in building E2E task-oriented dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. Intuitively, the dialog data, even unlabeled, can be used to enhance the performance of belief tracking and thus benefit the whole dialog system, because there are cues from user inputs and system responses which reveal the belief states, as shown in <ref type="figure">Figure 1</ref>.</p><p>Technically, we propose a latent variable model for task-oriented dialogs, called the LAtent BElief State (LABES) dialog model. The model generally consists of multiple (e.g. T ) turns of user inputs u 1:T and system responses r 1:T which are observations, and belief states b 1:T which are latent variables. Basically, LABES is a conditional generative model of belief states and system responses given user inputs, i.e. p θ (b 1:T , r 1:T |u 1:T ). Once built, the model can be used to infer belief states and generate responses. More importantly, such latent variable modeling enables us to develop semi-supervised learning on a mix of labeled and unlabeled data under the principled variational learning framework <ref type="bibr" target="#b17">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b36">Sohn et al., 2015)</ref>. In this manner, we hope that the LABES model can exploit the cues for belief tracking from user inputs and system responses. Furthermore, we develop LABES-S2S, which is a specific model instantiation of LABES, employing copy-augmented Seq2Seq <ref type="bibr" target="#b9">(Gu et al., 2016)</ref> based conditional distributions in implementing p θ (b 1:T , r 1:T |u 1:T ).</p><p>We show the advantage of our model compared to other E2E task-oriented dialog models, and demonstrate the effectiveness of our semisupervised learning scheme on three benchmark task-oriented datasets: CamRest676 (Wen et al., 2017b), In-Car  and MultiWOZ <ref type="bibr" target="#b1">(Budzianowski et al., 2018)</ref> across various scales and domains. In supervised experiments, LABES-S2S obtains state-of-the-art results on CamRest676 and In-Car, and outperforms all the existing models which do not leverage large pretrained language models on MultiWOZ. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and prior semisupervised baselines. Remarkably, we can reduce the annotation requirements to 50% without performance loss on MultiWOZ, which is equivalent to saving around 30,000 annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>On use of unlabeled data for belief tracking. Classic methods such as self-training <ref type="bibr" target="#b32">(Rosenberg et al., 2005)</ref>, also known as pseudo-labeling <ref type="bibr" target="#b20">(Lee, 2013)</ref>, has been applied to belief tracking <ref type="bibr" target="#b37">(Tseng et al., 2019)</ref>. Recently, the pretraining-and-finetuning approach has received increasing interests <ref type="bibr" target="#b10">(Heck et al., 2020;</ref><ref type="bibr">Peng et al., 2020;</ref><ref type="bibr">Hosseini-Asl et al., 2020)</ref>. The generative model based semisupervised learning approach, which blends unsupervised and supervised learning, has also been studied <ref type="bibr" target="#b39">(Wen et al., 2017a;</ref>. Notably, the two approaches are orthogonal and could be jointly used. Our work belongs to the second approach, aiming to leverage unlabeled dialog data beyond of using general text corpus. A related work close to ours is SEDST , which also perform semi-supervised learning for belief tracking. Remarkably, our model is optimized under the principled variational learning framework, while SEDST is trained with an ad-hoc combination of posterior regularization and auto-encoding. Experimental in §6.2 show the superiority of our model over SEDST. See Appendix A for differences in model structures between SEDST and LABES-S2S.</p><p>End-to-end task-oriented dialog systems. Our model belongs to the family of E2E task-oriented dialog models <ref type="bibr">(Wen et al., 2017a,b;</ref><ref type="bibr" target="#b22">Li et al., 2017;</ref><ref type="bibr" target="#b25">Mehri et al., 2019;</ref><ref type="bibr">Peng et al., 2020;</ref><ref type="bibr">Hosseini-Asl et al., 2020)</ref>. We borrow some elements from the Sequicity  model, such as representing the belief state as a natural language sequence (a text span), and using copy-augmented Seq2Seq learning <ref type="bibr" target="#b9">(Gu et al., 2016)</ref>. But compared to Sequicity and all its follow-up works <ref type="bibr" target="#b35">Shu et al., 2019;</ref><ref type="bibr" target="#b23">Liang et al., 2020)</ref>, a feature in our LABES-S2S model is that the transition between belief states across turns and the dependency between system responses and belief states are well statistically modeled. This new design results in a completely different graphical model structure, which enables rigorous probabilistic variational learning. See Appendix A for details.</p><p>Latent variable models for dialog. Latent variables have been used in dialog models. For non task-oriented dialogs, latent variables are introduced to improve diversity <ref type="bibr" target="#b33">(Serban et al., 2017;</ref><ref type="bibr" target="#b48">Zhao et al., 2017;</ref>, control language styles  or incorporate knowledge <ref type="bibr" target="#b16">(Kim et al., 2020)</ref> in dialog generation. For task-oriented dialogs, there are prior studies which use latent internal states via hidden Markov models <ref type="bibr" target="#b44">(Zhai and Williams, 2014)</ref> or variational autoencoders  to discover the underlying dialog structures. In <ref type="bibr" target="#b39">Wen et al. (2017a)</ref> and , dialog acts are treated as latent variables, together with variational learning and reinforcement learning, aiming to improve response generation. To the best of our knowledge, we are the first to model belief state as discrete latent variables, and propose to learn these structured representations via the variational principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Latent Belief State Dialog Models</head><p>We first introduce LABES as a general dialog modeling framework in this section. For dialog turn t, let u t be the user utterance, b t be the current belief state after observed u t and r t be the corresponding system response. In addition, denote c t as the dialog context or model input at turn t, such as c t {r t−1 , u t } as in this work. Note that c t can include longer dialog history depending on specific implementations. Let d t be the database query result which can be obtained through a databaselookup operation given the belief state b t .</p><p>Our goal is to model the joint distribution of belief states and system responses given the user inputs, p θ (b 1:T , r 1:T |u 1:T ), where T is the total number of turns and θ denotes the model parameters.</p><p>In LABES, we assume the joint distribution follows the directed probabilistic graphical model illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, which can be formulated as: </p><formula xml:id="formula_0">= T t=1 p θ (b t |b t−1 ,c t )p θ (r t |c t ,b t ,d t )</formula><p>where b 0 is an empty state. Intuitively, we refer the conditional distribution p θ (b t |b t−1 ,c t ) as the belief state decoder, and p θ (r t |c t ,b t ,d t ) the response decoder in the above decomposition. Note that the probability p(d t |b t ) is omitted as database result d t is deterministically obtained given b t . Thus the system response can be generated as a three-step process: first predict the belief state b t , then use b t to query the database and obtain d t , finally generate the system response r t based on all the conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Learning</head><p>We introduce an inference model q φ (b t |b t−1 , c t , r t ) (described by dash arrows in <ref type="figure" target="#fig_2">Figure 2</ref>) to approximate the true posterior p θ (b t |b t−1 , c t , r t ). Then we can derive the variational evidence lower bound  (ELBO) for unsupervised learning as follows:</p><formula xml:id="formula_1">J un = E q φ (b 1:T ) log p θ (b 1:T , r 1:T |u 1:T ) q φ (b 1:T |u 1:T , r 1:T ) = T t=1 E q φ (b 1:T ) log p θ (r t |c t , b t , d t ) −αKL q φ (b t |b t−1 , c t , r t ) p θ (b t |b t−1 , c t ) where q φ (b 1:T ) T t=1 q φ (b t |b t−1 , c t , r t )</formula><p>and α is a hyperparameter to control the weight of the KL term introduced by <ref type="bibr" target="#b12">Higgins et al. (2017)</ref>.</p><p>Optimizing J un requires drawing posterior belief state samples b 1:T ∼ q φ (b 1:T |u 1:T , r 1:T ) to estimate the expectations. Here we use a sequential sampling strategy similar to <ref type="bibr" target="#b16">Kim et al. (2020)</ref>, where each b t sampled from q φ (b t |b t−1 , c t , r t ) at turn t is used as the condition to generate the next turn's belief state b t+1 . For calculating gradients with discrete latent variables, which is non-trivial, some methods have been proposed such as using a score function estimator <ref type="bibr" target="#b41">(Williams, 1992)</ref> or categorical reparameterization trick <ref type="bibr" target="#b14">(Jang et al., 2017)</ref>. In this paper, we employ the simple Straight-Through estimator <ref type="bibr" target="#b0">(Bengio et al., 2013)</ref>, where the sampled discrete token indexes are used for forward computation, and the continuous softmax probability of each token is used for backward gradient calculation. Although the Straight-Through estimator is biased, we find it works pretty well in our experiments, therefore leave the exploration of other optimization methods as future work.  , rectangles in different colors denote different word embeddings, and the embedding of domain names and slot names are concatenated as the initial input. Note that the same (i.e. weight-tied) decoder is shared across all slots. Decoding stops when a slot-specific end-ofsentence symbol is generated, which is possible to be the first output if the slot does not appear in the dialog.</p><formula xml:id="formula_2">ℎ −1 ℎ ℎ ℎ +1 ℎ ℎ −1 −1 −1 −1 ℎ +1 +1 ℎ −1 −2 ℎ −1 ℎ turn turn − 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised Learning</head><p>When b t labels are available, we can easily train the generative model p θ and inference model q φ via supervised maximum likelihoods:</p><formula xml:id="formula_3">J sup = T t=1 log p θ (b t |b t−1 , c t )+log p θ (r t |c t , b t , d t ) + log q φ (b t |b t−1 , c t , r t )</formula><p>When a mix of labeled and unlabeled data is available, we perform semi-supervised learning using a combination of the supervised objective J sup and the unsupervised objective J un . Specifically, we first pretrain p θ and q φ on small-sized labeled data until convergence. Then we draw supervised and unsupervised minibatches from labeled and unlabeled data and perform stochastic gradient ascent over J sup and J un , respectively. We use supervised pretraining first because training q φ (b t |b t−1 , c t , r t ) to correctly generate slot values and special outputs such as "dontcare" and end-ofsentence tokens as much as possible is important to improve sample efficiency in subsequent semisupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LABES-S2S: A Copy-Augmented Seq2Seq Instantiation</head><p>In the above probabilistic dialog model LABES, the belief state decoder p θ (b t |b t−1 ,c t ) and the response decoder p θ (r t |c t ,b t ,d t ) can be flexibly im-plemented. In this section we introduce LABES-S2S as an instantiation of the general LABES model based on copy-augmented Seq2Seq conditional distributions <ref type="bibr" target="#b9">(Gu et al., 2016)</ref>, which is shown in <ref type="figure" target="#fig_4">Figure 3</ref>(a) and described in the following. The responses are generated through two Seq2Seq processes: 1) decode the belief state given dialog context and last turn's belief state and 2) decode the system response given dialog context, the decoded belief state and database query result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Belief State Decoder</head><p>The belief state decoder is implemented via a Seq2Seq process, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>(b). Inspired by <ref type="bibr" target="#b35">Shu et al. (2019)</ref>, we use a single GRU decoder to decode the value for each informable slot separately, feeding the embedding of each slot name as the initial input. In multi-domain setting, the domain name embedding is concatenated with the slot name embedding to distinguish slots with identical names in different domains . We use two bi-directional GRUs <ref type="bibr" target="#b2">(Cho et al., 2014)</ref> to encode the dialog context c t and previous belief state b t−1 into a sequence of hidden vectors h enc ct and h enc b t−1 respectively, which are the inputs to the belief state decoder. As there are multiple slots, and their values can also consist of multiple tokens, we denote the i-th token of slot s by b s,i t . To decode each token b s,i t , we first compute an attention vector over the encoder vectors. Then the attention vector and the embedding of the last decoded token e(b s,i−1 t ) are concatenated and fed into the decoder GRU to get the decoder hidden state h dec b s,i t , denoted as h dec s,i for simplicity:</p><formula xml:id="formula_4">a s,i t = Attn(h enc ct • h enc b t−1 , h dec s,i ) h dec s,i = GRU(a s,i t • e(b s,i−1 t ), h dec s,i−1 ) h dec s,i = dropout h dec s,i • e(b s,i−1 t )</formula><p>where • denotes vector concatenation. We use the last hidden state of the dialog context encoder as h dec s,0 , and the slot name embedding as e(b s,0 t ). We reuse e(b s,i−1 t ) to formĥ dec s,i to give more emphasis on the slot name embedding and add a dropout layer to reduce overfitting.ĥ dec s,i is then used to compute a generative score ψ gen for each token w in the vocabulary V, and a copy score ψ cp for words appeared in c t and b t−1 . Finally, these two scores are combined and normalized to form the final decoding probability following:</p><formula xml:id="formula_5">ψ gen (b s,i t = w) = v T w W genĥ dec s,i , w ∈ V ψ cp (b s,i t = x j ) = h encT x j W cpĥ dec s,i , x j ∈ c t ∪ b t−1 p(b s,i t = w) = 1 Z e ψgen(w) + j:x j =w e ψcp(x j )</formula><p>where W gen and W cp are trainable parameters, v w is the one-hot representation of w, x j is the j-th token in c t ∪ b t−1 and Z is the normalization term. With copy mechanism, it is easier for the model to extract words mentioned by the user and keep the unchanged values from previous belief state. Meanwhile, the decoder can also generate tokens not appeared in input sequences, e.g. the special token "dontcare" or end-of-sentence symbols. Since the decoding for each slot is independent with each other, all the slots can be decoded in parallel to speed up.</p><p>The posterior network q φ (b t |b t−1 , c t , r t ) is constructed through a similar process, where the only difference is that the system response r t is also encoded and used as an additional input to the decoder. Note that the posterior network is separately parameterized with φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response Decoder</head><p>The response decoder is implemented via another Seq2Seq process. After obtaining the belief state b t , we use it to query a database to find entities that meet user's need, e.g. Thai restaurants in the south area. The query result d t is represented as a 5dimension one-hot vector to indicate 0, 1, 2, 3 and &gt;3 matched results respectively. We only need the number of matched entities instead of their specific information as the input to the response decoder, because we generate delexicalized responses with placeholders for specific slot values (as shown in <ref type="table" target="#tab_5">Table 4</ref>) to improve data efficiency <ref type="bibr" target="#b38">(Wen et al., 2015)</ref>. The values can be filled through simple rule-based post-processing afterwards.</p><p>Instead of directly decoding the response from the belief state decoder's hidden states , we again use the bi-directional GRU (the one used to encode b t−1 ) to encode the current belief state b t into hidden vectors h enc bt . Then for each token r i t in the response, the decoder state h dec r t,i can be computed as follows:</p><formula xml:id="formula_6">a i t = Attn(h enc ct • h enc bt , h dec r t,i ) h dec r t,i = GRU(a i t • e(r i−1 t ) • d t , h dec r t,i−1 ) h dec r t,i = h dec r t,i • a i t • d t</formula><p>Note that dropout is not used forĥ dec r t,i , since response generation is not likely to overfit, compared to belief tracking in practice. We omit the probability formulas because they are almost the same as in the belief state decoder, except for changing the copy source from c t ∪ b t−1 to c t ∪ b t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate the proposed model on three benchmark task-oriented dialog datasets: the Cambridge Restaurant (CamRest676) (Wen et al., 2017b), Stanford In-Car Assistant (In-Car)  and MultiWOZ <ref type="bibr" target="#b1">(Budzianowski et al., 2018)</ref>, with 676/3031/10438 dialogs respectively. In particular, MultiWOZ is one of the most challenging dataset up-to-date given its multi-domain setting, complex ontology and diverse language styles. As there are some belief state annotation errors in MultiWOZ, we use the corrected version MultiWOZ 2.1 <ref type="bibr" target="#b4">(Eric et al., 2019)</ref> in our experiments. See Appendix B for more detailed introductions and statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We evaluate the model performance under the end-to-end setting, i.e. the model needs to first predict belief states and then generate response based on its own belief predictions. For evaluating belief tracking performance, we use the commonly used joint goal accuracy, which is the proportion of dialog turns where all slot values are correctly predicted. For evaluating response generation, we use BLEU <ref type="bibr" target="#b27">(Papineni et al., 2002)</ref> to measure the general language quality. The response quality towards task completion is measured by dataset-specific metrics to facilitate comparison with prior works. For CamRest676 and In-Car, we use Match and SuccF1 following . For MultiWOZ, we use Inform and Success as in <ref type="bibr" target="#b1">Budzianowski et al. (2018)</ref>, and also a combined score computed through (Inform+Success)×0.5+BLEU as the overall response quality suggested by <ref type="bibr" target="#b25">Mehri et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>In our experiments, we compare our model to various Dialog State Tracking (DST) and End-to-End (E2E) baseline models. Recently, large-scale pretrained language models (LM) such as BERT (Devlin et al., 2019) and GPT-2 <ref type="bibr" target="#b30">(Radford et al., 2019)</ref> are used to improve the performance of dialog models, however in the cost of tens-fold larger model sizes and computations. We distinguish them from light-weighted models trained from scratch in our comparison.</p><p>Independent DST Models: For CamRest676, we compare to StateNet  and TripPy <ref type="bibr" target="#b10">(Heck et al., 2020)</ref>, which are the SOTA model without/with BERT respectively. For Multi-WOZ, we compare to BERT-free models TRADE , NADST <ref type="bibr" target="#b19">(Le et al., 2020b)</ref> and CSFN-DST <ref type="bibr" target="#b49">(Zhu et al., 2020)</ref>, and BERT-based models including TripPy, the BERT version of CSFN and DST-Picklist .</p><p>E2E Models: E2E models can be divided into three sub-categories. The TSCP , SEDST , FSDM <ref type="bibr" target="#b35">(Shu et al., 2019)</ref>, MOSS <ref type="bibr" target="#b23">(Liang et al., 2020)</ref> and DAMD  are based on the copy-augmented Seq2Seq learning framework proposed by . LIDM (Wen et al., 2017a), SFN <ref type="bibr" target="#b25">(Mehri et al., 2019)</ref> and UniConv <ref type="bibr" target="#b18">(Le et al., 2020a)</ref> are modular designed, connected through neural states and trained end-to-end. SimpleTOD (Hosseini-Asl et al., 2020) and SOLOLIST <ref type="bibr">(Peng et al., 2020)</ref> are two recent models, which both use a single autoregressive language model, initialized from GPT-2, to build the entire system.</p><p>Semi-Supervised Methods: First, we compare with SEDST  for semi-supervised belief tracking performance. SEDST is also a E2E dialog model based on copy-augmented Seq2Seq learning (see Appendix A for more details). Over unlabled dialog data, SEDST is trained through posterior regularization (PR), where a posterior network is used to model the posterior belief distribution given system responses, and then guide the learning of prior belief tracker through minimizing the KL divergence between them. Second, based on the LABES-S2S model, we compare our variational learning (VL) method to a classic semi-supervised learning baseline, self-training (ST), which performs as its name suggests. Specifically, after supervised pretraining over small-sized labeled dialogs, we run the system to generate pseudo belief states b t over unlabeled dialogs, and then train the response decoder p θ (r t |b t , c t , d t ) in a supervised manner. The gradients will propagate through the discrete belief states by the Straight Through gradient estimator <ref type="bibr" target="#b0">(Bengio et al., 2013)</ref> over the computational graph, thus also adjusting the belief state decoder p θ (b t |b t−1 , c t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>In our experiments, we report both the best result and the statistical result obtained from multiple independent runs with different random seeds. Details are described in the caption of each table. The implementation details of our model is available in Appendix C. Results are organized to show the advantage of our proposed LABES-S2S model over existing models ( §6.1) and the effectiveness of our semi-supervised learning method ( §6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmark Performance</head><p>We first train our LABES-S2S model under full supervision and compare with other baseline models on the benchmarks. The results are given in <ref type="table" target="#tab_1">Table  1 and Table 2</ref>.</p><p>As shown in <ref type="table">Table 1</ref>, LABES-S2S obtains new SOTA joint goal accuracy on CamRest676 and the highest match scores on both CamRest676 and In-Car datasets. Its BLEU scores are also beyond or close to the previous SOTA models. The relatively low SuccF1 is due to that in LABES-S2S, we do not apply additional dialog act modeling and reinforcement fine-tuning to encourage slot token generation as in other E2E models.  all the models without using large pretrained LMs, LABES-S2S performs the best in belief tracking joint goal accuracy and 3 out of the 4 response generation metrics. Although the response generation performance is not as good as recent GPT-2 based SimpleTOD and SOLOLIST, our model is much smaller and thus computational cheaper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Semi-Supervised Experiments</head><p>In our semi-supervised experiments, we first split the data according to a fixed proportion, then train the models using only labeled data (SupOnly), or using both labeled and unlabeled data (Semi) with the proposed variational learning method (Semi-VL), self-training (Semi-ST) and posterior regularization (Semi-PR) introduced in §5.3 respectively.</p><p>We conduct experiments with 50% and 25% labeled data on CamRest676 and In-Car following , and change the labeled data proportion from 10% to 100% on MultiWOZ. The results are shown in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure" target="#fig_5">Figure 4</ref>.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we can see that semi-supervised learning methods outperform the supervised learning baseline consistently in all experiments for the two datasets. In particular, the improvement of Semi-VL over SupOnly on our model is significantly larger than Semi-PR over SupOnly on SEDST in most metrics, and Semi-VL obtains a joint goal accuracy of 1.3%∼3.9% higher over Semi-ST. These results indicate the superiority of our LABES modeling framework in utilizing unlabeled data over other semi-supervised baselines. Since LABES mainly improves modeling of belief states, it is more relevant to examine the belief tracking metrics such as joint goal accuracy and match rate (partly determined by the belief tracking accuracy).   Note that Semi-VL and Semi-ST are fed with the same set of system responses, thus they obtain similar SuccF1 and BLEU scores in <ref type="table" target="#tab_3">Table 3</ref>, which mainly measure the response quality.</p><p>The results on MultiWOZ shown in Figure 4 also support the above conclusions. From the plot of metric scores w.r.t labeling proportions, we can see how many labels can be reduced clearly. Our LABES-S2S model trained with Semi-VL obtains a joint goal accuracy of 49.47% and a combined score of 89.21 on only 50% of labeled data, which is very close to 50.05% and 88.01 obtained under 100% supervision. This indicates that we can reduce 50% of labels without losing performance, which results in reducing around 30,000 belief state annotations given the size of MultiWOZ. Moreover, it can be seen from <ref type="figure" target="#fig_5">Figure 4</ref> that our Semi-VI can improve the belief tracking and response generation performance when labeling only 10% of dialogues, and the smaller amount of labels, the larger gain obtained by Semi-VI.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Case Study</head><p>We give two examples where the model trained with Semi-VL improves over the supervisedtraining-only baseline. In both examples, the user indicates his/her goal implicitly with a short reply. These rarely occurred corner cases are missed by the baseline model, but successfully captured after semi-supervised learning. Moreover, we can see that Semi-VL helps our model learn the cue word "British" which contributes to a more informative response in the first dialog, and in the second dialog, avoid the incoherent error caused by error propagation, thus improve the response generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper we are interested in reducing belief state annotation cost for building E2E task-oriented dialog systems. We propose a conditional generative model of dialogs -LABES, where belief states are modeled as latent variables, and unlabeled dialog data can be effectively leveraged to improve belief tracking through semi-supervised variational learning. Furthermore, we develop LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. We show the strong benchmark performance of LABES-S2S and the effectiveness of our semi-supervised learning method on three benchmark datasets. In our experiments on Multi-WOZ, we can save around 50%, i.e. around 30,000 belief state annotations without performance loss.</p><p>There are some interesting directions for future work. First, the LABES model is general and can be enhanced by, e.g. incorporating largescale pre-trained language models, allowing other options for the belief state decoder and the response decoder such as Transformer based. Second, we can analogously introduce dialog acts a 1:T as latent variables to define the joint distribution p θ (b 1:T , a 1:T , r 1:T |u 1:T ), which can be trained with semi-supervised learning and reinforcement learning as well. model introduces an additional b t encoder and uses the encoder hidden states h enc bt to generate system response and next turn's belief state, thus the conditional probability p θ (r t |b t , c t ) and state transition probability p θ (b t |b t−1 , c t ) are well defined by two complete Seq2Seq processes. Second, the difference in models can also be clearly seen from the probabilistic graphical model structures as shown in <ref type="figure" target="#fig_7">Figure 6</ref>. LABES-S2S is a conditional generative model where the belief states are latent variables. In contrast, Sequicity/SEDST do not treat the belief states as latent variables.</p><p>Third, the above differences in models lead to differences in learning methods for Sequicity/SEDST and LABES-S2S. Sequicity can only be trained on labeled data via multi-task supervised learning. SEDST resorts to an ad-hoc combination of posterior regularization and auto-encoding for semi-supervised learning. Remarkably, LABES-S2S is optimized under the principled variational learning framework. and belief states are modeled very weakly in Sequicity/SEDST, only owing to the copy mechanism. For simpliciy, we omit such relations in both Figure 5 and 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>In our experiments, we evaluate different models on three benchmark task-oriented datasets with different scales and ontology complexities ( <ref type="table" target="#tab_8">Table 6</ref>). The Cambridge Restaurant (CamRest676) dataset (Wen et al., 2017b) contains single-domain dialogs where the system assists users to find a restaurant. The Stanford In-Car Assistant (In-Car) dataset  consists of dialogs between a user and a in-car assistant system covering three tasks: calendar scheduling, weather information retrieval and point-of-interest navigation. The MultiWOZ <ref type="bibr" target="#b1">(Budzianowski et al., 2018)</ref> dataset is a large-scale human-human multi-domain dataset containing dialogs in seven domains including attraction, hotel, hospital, police, restaurant, train, and taxi. It is more challenging due to its multi-domain setting, complex ontology and diverse language styles. As there are some belief state annotation errors in Mul-tiWOZ, we use the corrected version MultiWOZ 2.1 <ref type="bibr" target="#b4">(Eric et al., 2019)</ref> in our experiments. We follow the data preprocessing setting in , whose data cleaning is developed based on .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>In our implementation of LABES-S2S, we use 1-layer bi-directinonal GRU as encoders and standard GRU as decoders. The hidden sizes are 100/100/200, vocabulary sizes are 800/1400/3000, and learning rates of Adam optimizer are 3e −3 /3e −3 /5e −5 for CamRest676/In-Car/MultiWOZ respectively. In all experiments, the embedding size is 50 and we use GloVe <ref type="bibr" target="#b29">(Pennington et al., 2014)</ref> to initialize the embedding matrix. Dropout rate is 0.35 and λ for variational inferece is 0.5, which are selected via grid search from {0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5} and {0.1, 0.3, 0.5, 0.7, 1.0, 1.5} respectively. The   learning rate decays by half every 2 epochs if no improvement is observed on development set. Training early stops when no improvement is observed on development set for 4 epochs. We use 10-width beam search for CamRest676 and greedy decoding for other datasets. All the models are trained on a NVIDIA Tesla-P100 GPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>p θ (b 1:T , r 1:T |u 1:T )= p θ (b 1:T |u 1:T )p θ (r 1:T |b 1:T , u 1:T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The probabilistic graphical model of LABES. Solid arrows describe the conditional generative model p θ , and dash arrows describe the approximate posterior model q φ . Note that we set c t {r t−1 , u t } in our model, and omit u t from the graph for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Structure of the belief state decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) shows the computational graph of LABES-S2S. In (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Performance of different methods w.r.t labeling proportion on MultiWOZ 2.1. The dash line corresponds to the baseline trained with 100% labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of computational graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of probabilistic graphical model structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 Table 1 :</head><label>21</label><figDesc>Results on CamRest676 and In-Car. The model with the highest joint goal accuracy on the development set of CamRest676 is shown as the best result, as similarly reported in prior work. Statistical results are reported as the mean and standard deviation of 5 runs. * denotes results obtained by our run of the open-source code.</figDesc><table><row><cell>shows the MultiWOZ results. Among</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on MultiWOZ 2.1. The model with the highest validation joint goal accuracy is shown as the best result, as similarly reported in prior work. The standard deviations for the statistical results are inTable 5in the appendix.</figDesc><table /><note>* denotes results obtained by our run of the open-source code.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>SupOnly denotes training with only labeled data, and Semi denotes training with both labeled and unlabeled data in each dataset. ST, VL and PR denote self-training, variational learning and posterior regularization respectively. Results of SEDST are obtained by our run of the open-source code. All the scores in this table are the mean from 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison of two example turns generated</cell></row><row><cell>by our model with supervised learning only (SupOnly)</cell></row><row><cell>and semi-supervised variational learning (Semi-VL).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>89±1.51 63.30±2.35 17.92±0.35 88.01±2.10</figDesc><table><row><cell>Model</cell><cell>Belief Tracking</cell><cell></cell><cell cols="2">Response Generation</cell></row><row><cell></cell><cell>Joint Goal</cell><cell>Inform</cell><cell>Success</cell><cell>BLEU</cell><cell>Combined</cell></row><row><cell>LABES-S2S (statistical)</cell><cell>50.05±0.92</cell><cell>76.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Statistical results of our LABES-S2S model with standard deviations on MultiWOZ 2.1.</figDesc><table><row><cell></cell><cell cols="3">CamRest676 In-Car MultiWOZ</cell></row><row><cell>#Dialog</cell><cell>676</cell><cell>3031</cell><cell>10438</cell></row><row><cell>Avg. #Turn</cell><cell>4.1</cell><cell>5.2</cell><cell>6.9</cell></row><row><cell>#Domain</cell><cell>1</cell><cell>3</cell><cell>7</cell></row><row><cell>#Info. Slot</cell><cell>3</cell><cell>11</cell><cell>31</cell></row><row><cell>#Req. Slot</cell><cell>7</cell><cell>11</cell><cell>38</cell></row><row><cell>#Values</cell><cell>99</cell><cell>284</cell><cell>4510</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Statistics of dialog datasets. Info and Req are shorthands for informable and requestable respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Strictly speaking, the transition between belief states across turns and the dependency between system responses</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by NSFC 61976122, Ministry of Education and China Mobile joint funding MCM20170301.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Comparisons with Prior Work</head><p>In this section, we comment on the differences between our LABES-S2S model and Sequicity  in both models and learning methods. Note that SEDST ) employs the same model structure as Sequicity. First, <ref type="figure">Figure  5</ref> shows the difference in computational graphs between Sequicity/SEDST and LABES-S2S. For Sequicity/SEDST, b t and r t are decoded directly from the belief state decoder's hidden states h dec bt , thus the conditional probability of r t given b t and the state transition probability between b t−1 and b t are not considered 2 . In contrast, LABES-S2S</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyag</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01669</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discrete cvae for response generation on short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1898" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07462</idno>
		<title level="m">Paraphrase augmented task-oriented dialog generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structuring latent spaces for stylized response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1814" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nurul</forename><surname>Carel Van Niekerk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Lubis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsien-Chin</forename><surname>Geishauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Moresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gašić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02877</idno>
		<title level="m">Trippy: A triple copy strategy for value independent neural dialog state tracking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL)</title>
		<meeting>the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00796</idno>
		<title level="m">Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explicit state tracking with semisupervisionfor neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequential latent knowledge selection for knowledge-grounded dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Uniconv: A unified conversational neural architecture for multidomain task-oriented dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14307</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-autoregressive dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2018: 56th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end taskcompletion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Moss: End-to-end dialog system framework with modular supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 : The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network model with belief tracking for task-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2506" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structured fusion networks for dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1777" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05298</idno>
		<title level="m">Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards universal dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaige</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2780" to="2786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised dialog structure learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flexibly-structured model for task-oriented dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Namazifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaixiu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gökhan</forename><surname>Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised bootstrapping of dialogue state trackers for task-oriented modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1273" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Latent intention dialogue models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3732" to="3741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina M Rojas</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019 : The 57th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="808" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Alternating recurrent dialog model with largescale pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03756</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discovering latent structure in task-oriented dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03544</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Taskoriented dialog systems that consider multiple appropriate responses under the same context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 : The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking action spaces for reinforcement learning in end-to-end dialog agents with latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaige</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019: Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1208" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Efficient context and schema fusion networks for multidomain dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03386</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
