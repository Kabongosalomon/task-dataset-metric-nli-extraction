<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Residual Output Layers for Neural Language Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
						</author>
						<title level="a" type="main">Deep Residual Output Layers for Neural Language Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be highrank to better model natural language if it is better at capturing the structure of the output space.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning the structure of the output space benefits a wide variety of tasks, such as object recognition and novelty detection in images <ref type="bibr" target="#b50">(Weston et al., 2011;</ref><ref type="bibr" target="#b44">Socher et al., 2013;</ref><ref type="bibr" target="#b8">Frome et al., 2013;</ref><ref type="bibr" target="#b55">Zhang et al., 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2018a)</ref>, zero-shot prediction in texts <ref type="bibr" target="#b6">(Dauphin et al., 2014;</ref><ref type="bibr" target="#b52">Yazdani &amp; Henderson, 2015;</ref><ref type="bibr" target="#b34">Nam et al., 2016;</ref><ref type="bibr" target="#b40">Rios &amp; Kavuluru, 2018)</ref>, and structured prediction in either images or text <ref type="bibr" target="#b45">(Srikumar &amp; Manning, 2014a;</ref><ref type="bibr" target="#b7">Dyer et al., 2015;</ref><ref type="bibr" target="#b1">Belanger &amp; McCallum, 2016;</ref><ref type="bibr" target="#b13">Graber et al., 2018)</ref>. When the space of output labels is large or their data is sparse, treating labels as independent classes makes learning difficult, because identifying one label is not helped by data for other labels. This problem can be addressed by learning output label em-beddings to capture the similarity structure of the output label space, so that data for similar labels can help classification, even to the extent of enabling few-shot or even zero-shot classification. This approach has been particularly successful in natural language generation tasks, where word embeddings give a useful similarity structure for next-wordprediction in tasks such as machine translation <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> and language modeling <ref type="bibr" target="#b30">(Merity et al., 2017)</ref>.</p><p>Existing neural language models typically use a log-linear classifier to predict words <ref type="bibr" target="#b49">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Chen et al., 2018b)</ref>. We can view the output label weights as a word embedding, and the input encoder as mapping the context to a vector in the same embedding space. Then the similarity between these two embeddings in this joint input-label space is measured with a dot product followed by the softmax function. We will refer to this part as the classifier, distinct from the input encoder which only depends on the input and the label encoder which only depends on the label. To improve performance and reduce model size, sometimes the output label weights are tied to the input word embedding vectors <ref type="bibr" target="#b18">(Inan et al., 2016;</ref><ref type="bibr" target="#b39">Press &amp; Wolf, 2017)</ref>, but there is no parameter sharing taking place across different words, which limits the effective transfer between them.</p><p>Recent work has shown improvements over specific vanilla recurrent architectures by sharing parameters across outputs through a bilinear mapping on neural language modeling <ref type="bibr" target="#b16">(Gulordava et al., 2018)</ref> or a dual nonlinear mapping on neural machine translation <ref type="bibr" target="#b37">(Pappas et al., 2018)</ref>, which can make the classifier more powerful. However, the shallow modeling constraints and the lack of regularization capabilities limit their applicability on arbitrary tasks and model architectures. Orthogonal to these studies, <ref type="bibr" target="#b51">Yang et al. (2018)</ref> achieved state-of-the-art improvements on language modeling by increasing the power of the classifier using a mixture of softmax functions, albeit at the expense of computational efficiency. A natural question arises of whether one can make the classifier more powerful by simply increasing the power of the label mapping while using a single softmax function without modifying its dimensionality or rank.</p><p>In this paper, we attempt to answer this question by investigating alternative neural architectures for learning the embedding of an output label in the joint input-label space which address the aforementioned limitations. In particular, arXiv:1905.05513v2 [cs.CL] 22 May 2019 we propose a deep residual nonlinear output mapping from word embeddings to the joint input-output space, which better captures the output structure while it avoids overfitting with two different dropout strategies between layers, and preserves useful information with residual connections to the word embeddings and, optionally, to the outputs of previous layers. 1 For the rest of the model, we keep the same input encoder architecture and still use the dot product and softmax function for output label prediction.</p><p>We demonstrate on language modeling and machine translation that we can match or improve state-of-the-art recurrent and self-attention architectures by simply increasing the power of the output mapping, while using a single softmax operation and without changing the dimensionality or rank of the classifier. The results suggest that the classifier does not necessarily need to be high rank to better model language if it better captures the output space structure. Further analysis reveals the significance of different model components and improvements on predicting low frequency words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background: Neural Language Generation</head><p>The output layer of neural models for language generation tasks such as language modeling <ref type="bibr" target="#b2">(Bengio et al., 2003;</ref><ref type="bibr" target="#b31">Mikolov &amp; Zweig, 2012;</ref><ref type="bibr" target="#b30">Merity et al., 2017)</ref>, machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr">Luong et al., 2015;</ref><ref type="bibr" target="#b20">Johnson et al., 2017)</ref> and summarization <ref type="bibr" target="#b41">(Rush et al., 2015;</ref><ref type="bibr" target="#b38">Paulus et al., 2018)</ref>, typically consists of a linear unit with a weight matrix W ∈ IR d h ×|V| and a bias vector b ∈ IR |V| followed by a softmax activation function, where V is the vocabulary. Thus, at a given time t, the output probability distribution for the current output y t conditioned on the inputs i.e. the previous outputs, y t−1 1 = (y 1 , y 2 , · · · , y t−1 ) with y i ∈ {0, 1} |V| :</p><formula xml:id="formula_0">|V| j y ij = 1 ∀ i ∈ N , is defined as: p(y t |y t−1 1 ) ∝ exp(W T h t + b),<label>(1)</label></formula><p>where h t is the input encoder's hidden representation at time t with d h dimensions. The parameterisation in Eq. 1 makes it difficult to learn the structure of the output space or to transfer this information from one label to another because the parameters for output label i, W T i , are independent from the parameters for any other output label j, W T j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weight Tying</head><p>Learning the structure of the output space can be helped by learning it jointly with the structure of input word embeddings, but this still does not support the transfer of learned information across output labels. In particular, since the output labels are words and thus the output parameters W T have one row per word, it is common to tie these parameters with those of the input word embeddings E ∈ IR |V|×d , by setting W = E T <ref type="bibr" target="#b18">(Inan et al., 2016;</ref><ref type="bibr" target="#b39">Press &amp; Wolf, 2017)</ref>. Making this substitution in Eq 1, we obtain:</p><formula xml:id="formula_1">p(y t |y t−1 1 ) ∝ exp(Eh t + b)<label>(2)</label></formula><p>Although there is no explicit transfer across outputs, this parameterisation can implicitly learn the output structure, as can be seen if we assume an implicit factorization of the input embeddings, E ≈ E l W l as in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bilinear Mapping</head><p>The above bilinear form, excluding the bias, is similar to the form of joint input-output space learning models <ref type="bibr" target="#b52">(Yazdani &amp; Henderson, 2015;</ref><ref type="bibr" target="#b34">Nam et al., 2016)</ref> which have been proposed in the context of zero-shot text classification. This motivates the learning of explicit relationships across outputs and inputs through parameter sharing via W l as above.</p><p>By substituting this factorization in Eq 2, we obtain:</p><formula xml:id="formula_2">p(y t |y t−1 1 ) ∝ exp(E l W l h t + b)<label>(3)</label></formula><p>where W l ∈ IR d×d h is the bilinear mapping and E, h t are the output embeddings and the encoded input respectively, as above. This parametrization has been previously also proposed by <ref type="bibr" target="#b16">Gulordava et al. (2018)</ref> for language modeling albeit with a different motivation, namely to decouple the hidden state from the word embedding prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dual Nonlinear Mapping</head><p>Another existing output layer parameterisation which explicitly learns the structure of the output is from <ref type="bibr" target="#b37">(Pappas et al., 2018)</ref>. Specifically, two nonlinear functions, g out (·) and g in (·), are introduced which aim to capture the output and context structure respectively:</p><formula xml:id="formula_3">p(y t |y t−1 1 ) ∝ exp g out (E)g in (h t ) + b ,<label>(4)</label></formula><formula xml:id="formula_4">∝ exp σ(EU + b u )σ(Vh t + b v ) + b (5)</formula><p>where σ(·) is a nonlinear activation function such as ReLU or Tanh, the matrix U ∈ IR d×dj and bias b u ∈ IR dj are the linear projection of the encoded outputs, and the matrix V ∈ IR dj×d h and bias b v ∈ IR dj are the linear projection of the context, and b ∈ IR V captures the biases of the target outputs in the vocabulary.</p><p>The parameterisation of Eq. 5 enables learning a more rich output structure than the bilinear mapping of Eq. 3 because it learns nonlinear relationships. Both, however, allow for controlling the capacity of the output layer independently of the dimensionality of the context h t and the word embedding E, by increasing the breadth of the joint projection, e.g. the dimensionality of the U and V matrices in Eq. 5 above. This increased capacity can be seen in the inequalities below for the number of parameters of the output layers discussed so far, assuming a fixed |V|, d, d h :</p><formula xml:id="formula_5">C tied &lt; C bilinear ≤ C dual ≤ C base ,<label>(6)</label></formula><p>where C tied , C base , C bilinear and C dual respectively correspond to the number of dedicated parameters of an output layer with (Eq. 2) and without (Eq. 1) weight tying, using the bilinear mapping (Eq. 3) and the dual nonlinear mapping (Eq. 5) which are assumed to be nonzero except C tied .</p><p>Given this analysis, we identify and aim to address the following limitations of the previously proposed output layer parameterisations for language generation:</p><p>(a) Shallow modeling of the label space. Output labels are mapped into the joint space with a single (possibly nonlinear) projection. Its power can only be increased by increasing the dimensionality of the joint space.</p><p>(b) Tendency to overfit. Increasing the dimensionality of the joint space and thus the power of the output classifier can lead to undesirable effects such as overfitting in certain language generation tasks, which limits its applicability to arbitrary domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Residual Output Layers</head><p>To address the aforementioned limitations we propose a deep residual output layer architecture for neural language generation which performs deep modeling of the structure of the output space while it preserves acquired information and avoids overfitting. Our formulation adopts the general form and the basic principles of previous output layer parametrizations which aim to capture the output structure explicitly in Section 2.3, namely (i) learning rich output structure, (ii) controlling the output layer capacity independently of the dimensionality of the vocabulary, the encoder and the word embedding, and, lastly, (iii) avoiding costly label-set-size dependent parameterisations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>A general overview of the proposed architecture for neural language generation is displayed in <ref type="figure">Fig. 1</ref>. We base our output layer formulation starting on the general form of the dual nonlinear mapping of Eq. 4:</p><formula xml:id="formula_6">p(y t |y t−1 1 ) ∝ exp g out (E)g in (h t ) + b .<label>(7)</label></formula><p>The input network g in (·) takes as input a sequence of words represented by their input word embeddings E which have been encoded in a context representation h t for the given time step t. The output or label network g out (·) takes as input the word(s) describing each possible output label and encodes them in a label embedding E (k) where k is the depth of the label encoder network. Next, we define these two proposed networks, and then we discuss how the model is trained and how it relates to previous output layers.</p><formula xml:id="formula_7">g out E g in w 1 , w 2 , …, w T w 1 , w 2 , …, w |V| E h t (k) b y .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input text</head><p>Output text <ref type="figure">Figure 1</ref>. General overview of the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Label Encoder Network</head><p>For language generation tasks, the output labels are each a word in the vocabulary V. We assume that these labels are represented with their associated word embedding, which is a row in E. In general, there may be additional information about each label, such as dictionary entries, cross-lingual resources, or contextual information, in which case we can add an initial encoder for these descriptions which outputs a label embedding matrix E ∈ IR |V|×d . In this paper we make the simplifying assumption that E = E and leave the investigation of additional label information to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">LEARNING OUTPUT STRUCTURE</head><p>To obtain a label representation which is able to encode rich output space structure, we define the g out (·) function to be a deep neural network with k layers which takes the label embedding E as input and outputs its deep label mapping at the last layer, g out (E) = E (k) , as follows:</p><formula xml:id="formula_8">E (k) = f (k) out (E (k−1) ),<label>(8)</label></formula><p>where k is the depth of the network and each function f (i) out (·) at the i th layer is a nonlinear projection of the following form:</p><formula xml:id="formula_9">f (i) out (E (i−1) ) = σ(E (i−1) U (i) + b (i) u ),<label>(9)</label></formula><p>where σ(·) is a nonlinear activation function such as ReLU or Tanh, and the matrix U (i) ∈ IR d×dj and the bias b (i) u ∈ IR dj are the linear projection of the encoded outputs at the i th layer. Note that when we restrict the above label network to have one layer depth the projection is equivalent to the label mapping from previous work in Eq. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">PRESERVING INFORMATION</head><p>The multiple layers of projections in Eq. 8 force the relationship between word embeddings E and label embeddings E (k) to be highly nonlinear. To preserve useful information from the original word embeddings and to facilitate the learning of the label network we add a skip connection directly to the input embedding. Optionally, for very deep label networks, we also add a residual connection to previous layers as in <ref type="bibr" target="#b17">(He et al., 2016)</ref>. With these additions the projection at the k th layer becomes:</p><formula xml:id="formula_10">E (k) = f (k) out (E (k−1) ) + E (k−1) + E<label>(10)</label></formula><p>Deep Residual Output Layers for Neural Language Generation</p><formula xml:id="formula_11">E (0) f out E (0) (k-2) E (k-1) f out (k) f out (k-1) … E (k) E Figure 2.</formula><p>The proposed deep residual label network architecture for neural language generation. Straight lines represent the input to a function and curved lines represent shortcut or residual connections implying addition operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">CONTROLLING OUTPUT NETWORK POWER</head><p>We can characterize the power of the proposed output network in terms of its number of parameters Θ drill , including the proposed label encoder and output classifier:</p><formula xml:id="formula_12">C drill ≈ |Θ drill | = k × (d × d) + |V|.<label>(11)</label></formula><p>By controlling the depth of the label encoder we can make the number of parameters equal to that of each other output network. For weight tying, C tied , this is k = 0, for full linear weights, C base , this is k = |V| d , for a bilinear mapping, C bilinear , this is k = 1, and for a dual nonlinear mapping, C joint , this is k = 2dj d . Hence, the power of the output network can be adjusted freely depending on the task at hand within the full spectrum of options defined by Ineq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">AVOIDING OVERFITTING</head><p>The ability to increase power may be useful for highresource data regimes, however it can lead to overfitting in the case we are in a low-resource data regime. To make sure that our network is robust to both data availability regimes, we choose to apply standard <ref type="bibr" target="#b47">(Srivastava et al., 2014)</ref> or variational <ref type="bibr" target="#b10">(Gal &amp; Ghahramani, 2016b)</ref> dropout in between each of the k layers of the projection. Assuming δ(·) to be the dropout mask sampling function, the above goal is achieved by modifying the function f (i) out (·) at the i th layer from Eq 9 as follows:</p><formula xml:id="formula_13">f (i) out (E (i−1) ) = δ f (i) out (E (i−1) ) f (i) out (E (i−1) ). (12)</formula><p>In standard dropout, a new binary dropout mask is sampled every time the dropout function is called. This means that new dropout masks are sampled independently for each dimension of each different label representation. In contrast, variational dropout samples a binary dropout mask only once upon the first call and then repeatedly uses that locked dropout mask for all label representations within the forward and backward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Context Network</head><p>The context representation h t in most language generation tasks is typically the output of a deep neural network, and thus it can capture, in principle, the nonlinear structure of the dual nonlinear mapping in Section 2.3. Eq. 2.3 has an additional nonlinearity g in (·) in order to allow the dimensionality of the joint space to be larger than that of the context encoder's output h t . However, in our proposed model we increase the power of the output network by increasing the depth of the label encoder, keeping the size of the joint space fixed. Thus, for our models, we make the simplifying assumption that there is no additional nonlinearity after the context encoder, setting g in (·) = I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Considerations</head><p>To perform maximum likelihood estimation of the model parameters, we use the negative log-likelihood of the data as our training objective. This involves computing the conditional likelihood of predicting the next word, as explained above. The normalized exponential function we use for converting the network scores to probability estimates is the typical softmax activation function.</p><p>In principle, our output layer parameterisation requires more computations than a typical softmax linear unit, with or without weight tying. Hence, it tends to get slower as the depth of the label encoder or the size of the vocabulary increases. In case either of them becomes extremely large, we can resort to recent sampling-based or hierarchical softmax approximation methods such as the ones proposed by <ref type="bibr" target="#b19">Jean et al. (2015)</ref> and <ref type="bibr" target="#b15">Grave et al. (2017)</ref>. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Relation to Previous Output Layer Forms</head><p>Our output layer parameterisation has the same general form as the one with the dual nonlinear mapping in Eq. 4. Hence, it preserves the property of being a generalization of output layers based on bilinear mapping and weight tying described in Section 2.3. The bilinear form in Eq. 3 can be simply derived from the general form of Eq 7 if we restrict the output mapping depth to be equal to one, set its bias equal to zero, and make the σ(.) activation function linear, so we have U (0) = W l . By further setting the matrix U to be the identity matrix, we can also derive the output layer form based on weight tying in Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model #Param Validation Test</head><p>Mikolov &amp; Zweig <ref type="formula" target="#formula_0">(2012)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate on three language generation tasks. The first two tasks are standard language modeling tasks, i.e. predicting the next word given the sequence of previous words. The third task is a conditional language modeling task, namely neural machine translation, i.e. predicting the next word in the target language given the source sentence and the previous words in the translation. To demonstrate the generality of the proposed output mapping we incorporate it in three different neural architectures which are considered state-of-the-art for their corresponding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Language Modeling</head><p>Datasets and Metrics. Following previous work in language modeling <ref type="bibr" target="#b51">(Yang et al., 2018;</ref><ref type="bibr" target="#b25">Krause et al., 2018;</ref><ref type="bibr" target="#b30">Merity et al., 2017;</ref><ref type="bibr" target="#b28">Melis et al., 2017)</ref>, we evaluate the proposed model in terms of perplexity on two widely used language modeling datasets, namely Penn Treebank <ref type="bibr" target="#b32">(Mikolov et al., 2010)</ref> and WikiText-2 <ref type="bibr" target="#b30">(Merity et al., 2017)</ref> which have vocabularies of 10,000 and 33,278 words, respectively. For fair comparison, we use the same regularization and optimization techniques with <ref type="bibr" target="#b30">Merity et al. (2017)</ref>.</p><p>Model Configuration. To compare with the state-of-theart we use the proposed output layer within the best architecture by <ref type="bibr" target="#b30">Merity et al. (2017)</ref>, which is a highly regularized 3-layer LSTM with 400-dimensional embeddings and 1150-dimensional hidden states, noted as AWD-LSTM. Our hyper-parameters were optimized based on validation perplexity, as follows: 4-layer label encoder depth, 400dimensional label embeddings, 0.6 dropout rate, residual connection to E, uniform weight initialization in the interval [−0.1, 0.1], for both datasets, and, furthermore, sigmoid activation and variational dropout for PennTreebank, as well as relu activation and standard dropout for Wikitext-2. The rest of the hyper-parameters were set to the optimal ones found for each dataset by <ref type="bibr" target="#b30">Merity et al. (2017)</ref>.</p><p>For the implementation of the AWD-LSTM we used the language modeling toolkit in Pytorch provided by <ref type="bibr" target="#b30">Merity et al. (2017)</ref>, 3 and for the dynamic evaluation the code in Pytorch provided by <ref type="bibr" target="#b25">Krause et al. (2018)</ref>. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">RESULTS</head><p>The results in terms of perplexity for our models, denoted by DRILL, and several competitive baselines, are displayed in <ref type="table" target="#tab_0">Table 1</ref> for PennTreebank and <ref type="table" target="#tab_1">Table 2</ref> for Wikitext-2. For the single-softmax models (above the double lines), for both datasets, our models improve over the state-of-the-art by +1.6 perplexity on PennTreebank and by +3.9 perplexity on Wikitext-2. Moreover, when our model is combined with the dynamic evaluation approach proposed by <ref type="bibr" target="#b25">Krause et al. (2018)</ref>, it improves even more over these models by +1.7 on PennTreebank and by +2.3 on Wikitext-2.</p><p>In contrast to other more complicated previous models, our model uses a standard LSTM architecture, following the work of <ref type="bibr" target="#b30">Merity et al. (2017)</ref>; <ref type="bibr" target="#b28">Melis et al. (2017)</ref>. For instance, <ref type="bibr" target="#b56">Zilly et al. (2017)</ref> uses of a recurrent highway network which is an extension of an LSTM to allow multiple  Interestingly, our model also significantly reduces the performance gap against multiple softmax models. In particular, when our finetuned model is compared to the corresponding mixture-of-softmaxes (MoS) model, which makes use of 15 softmaxes in the classifier, it reduces the difference against AWD-LSTM from 2.8 to 1.2 points on PennTreebank and from 4.3 to 0.4 points on WikiText-2. When our model is compared to MoS with dynamic evaluation, the difference is reduced from 3.4 points to 1.7 points on PennTreebank and from 3.6 to 1.3 on WikiText-2. Note that the rank of the log-probability matrix for MoS on PennTreebank is 9,981, while for AWD-LSTM and our model the rank is only 400. This observation questions the high-rank hypothesis of MoS, which states that the log-probability matrix has to be high rank to better capture language. Our results suggest that the log-probability matrix does not need to be high rank if the classifier is better at capturing the output space structure.</p><p>Furthermore, as shown in <ref type="table" target="#tab_2">Table 3</ref>, the MoS model is far slower than AWD-LSTM, even for these small datasets and reduced dimensionality settings, 5 whereas adding our label encoder to AWD-LSTM results in only a small speed difference. In particular, on PennTreebank the MoS model takes about 139 seconds per epoch while AWD-LSTM about 47 seconds per epoch, which makes it slower by a factor of 3.0×, whereas our model is only 1.1× slower than this 5 Note that even though the MoS models have a comparable number of parameters to the other models, they use smaller values for several crucial hyper-parameters, such as word embedding size, hidden state size and batch size, likely to make the training speed more manageable and avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PennTreebank Wikitext-2 AWD-LSTM 47 sec (1.0×) 89 sec (1.0×) AWD-LSTM-DRILL 53 sec (1.1×) 106 sec (1.2×) AWD-LSTM-MoS 139 sec (3.0×) 862 sec (9.7×) baseline. On Wikitext-2, the differences are even more pronounced due to the larger size of the vocabulary. The MoS model takes about 862 seconds per epoch while AWD-LSTM takes about 89 seconds per epoch, which makes it slower by a factor of 9.7×, whereas our model with 4-layers is only 1.2× slower than the baseline. We attempted to combine our label encoder with the MoS model, but its training speed exceeded our computation budget.</p><p>Overall, these results demonstrate that the proposed deep residual output mapping improves significantly the stateof-the-art single-softmax neural architecture for language modeling, namely AWD-LSTM, without hurting its efficiency. Hence, it could be a useful and practical addition to other existing architectures. In addition, our model remains competitive against models based on multiple softmaxes and could be combined with them in the future, since our work is orthogonal to using multiple softmaxes. To demonstrate that our model is also applicable to larger datasets as well, in Section 4.2 below we apply our method to neural machine translation. But before moving to that experiment, we first perform an ablation analysis of these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">ABLATION ANALYSIS</head><p>To give further insights into the source of the improvement from our output layer parameterisation, in <ref type="table">Table 4</ref> we compare its ablated variants with previous output layer parameterisations. Each alternative is combined with the state-ofthe-art encoder network AWD-LSTM <ref type="bibr" target="#b30">(Merity et al., 2017)</ref>. We observe that full softmax produces the highest perplexity scores, despite having almost 20M parameters more than the other models. This shows that the power of the output layer or classifier, as measured by number of parameters, is not indicative of generalization ability.</p><p>The output layer with weight tying <ref type="bibr" target="#b39">(Press &amp; Wolf, 2017)</ref>, noted [PW17], has lower perplexity than the full softmax by 9.5 points. The bilinear mapping <ref type="bibr" target="#b16">(Gulordava et al., 2018)</ref>, noted [G18], has lower perplexity than the full softmax by 8.3 points, but it is still higher than weight tying by 1.2 points. The dual nonlinear mapping <ref type="bibr" target="#b37">(Pappas et al., 2018)</ref>, noted [PH18], has even lower perplexity compared to the full softmax by 10.4 points, and has lower perplexity than weight tying by 0.9 points. 6 DRILL with only 1-layer depth is slightly better than [PH18], and with 2-layers depth outperforms all previous output mappings, improving over full softmax by 10.8 points, weight tying by 1.3 points, and dual non-linear mapping by 0.4 points. Increasing the depth even more provides further improvements of up to 0.3 points. This shows the benefits of learning deep output label mappings, as opposed to shallower ones. Lastly, DRILL with residual connections between layers has an increase of 1.8 perplexity points, likely because of an effective reduction in depth, and not using variational dropout has a significant increase in perplexity of namely 5 points, which highlights the importance of regularization between layers for this task.</p><p>To verify the hypothesis that our output layer facilitates information transfer across words, we also analyzed the loss for words in different frequency bands, created by computing statistics on the training set. <ref type="figure" target="#fig_2">Figure 3</ref> displays the mean relative cross-entropy difference (%) between our output layer and the previous output layers for the different word frequency bands on the test set of PennTreebank. Overall, the graph shows that most of the improvements in perplexity between 5% to 17.5% brought by DRILL against baselines comes from predicting more accurately the words in lower word frequency bands (1 to 100 occurrences). The results are consistent with <ref type="table">Table 4</ref>, since the second best output layer is the one with the bilinear mapping followed by the bilinear mapping and weight tying baselines. One exception occurs in the highest frequency band, where DRILL has 2.5% higher perplexity than the bilinear mapping, but this difference is less significant because it is computed based on 16 unique words as opposed to the lowest frequency band which corresponds to 4116 unique words. These results validate our hypothesis that learning a deeper label encoder leads to better transfer of learned information across labels. More specifically, because low frequency words lack data to individually learn the complex structure of the output space, transfer of learned information from other words is crucial to improving performance, whereas this is not the case for higher frequency words. This analysis suggests that our model could also be useful for zero-resource scenarios, where labels need to be predicted without any training data, similarly to other joint input-output space models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Neural Machine Translation</head><p>Dataset and Metrics. Following previous work in neural machine translation <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref>, we train on the WMT 2014 English-German dataset with ∼4.5M sentence pairs, using the Newstest2013 set for validation and the Newstest2014 set for testing. We pre-process the texts using the BPE algorithm <ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref> with 32K operations. Following the standard evaluation practices in the field <ref type="bibr" target="#b3">(Bojar et al., 2017)</ref>, the translation quality is measured using BLEU score <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref> on tokenized text.  <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref>, except that we did not perform model averaging over last 5 for Transformer (base) model. To ensure fair comparison, we trained the Transformer (base) from scratch for the same number of training steps as ours, namely 350K, and thereby reproduced about the same score as in <ref type="bibr" target="#b49">(Vaswani et al., 2017</ref>) with a slight difference of +0.1 point. For the implementation of the Transformer, we used OpenNMT <ref type="bibr" target="#b24">(Klein et al., 2017)</ref>. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>The results displayed in <ref type="table" target="#tab_6">Table 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Other Related Work</head><p>Several studies focus on learning the structure of the output space from texts for zero-shot classification <ref type="bibr" target="#b6">(Dauphin et al., 2014;</ref><ref type="bibr" target="#b34">Nam et al., 2016;</ref><ref type="bibr" target="#b40">Rios &amp; Kavuluru, 2018;</ref><ref type="bibr" target="#b36">Pappas &amp; Henderson, 2019)</ref> and structured prediction <ref type="bibr" target="#b46">(Srikumar &amp; Manning, 2014b;</ref><ref type="bibr" target="#b7">Dyer et al., 2015;</ref><ref type="bibr" target="#b53">Yeh et al., 2018)</ref>. Fewer such studies exist for neural language generation, for instance the ones described in Section 2. Their mappings can increase the power of the classifier by controlling its dimensionality or rank, but unlike ours, they have limited 8 http://github.com/OpenNMT/OpenNMT-py Model BLEU Bidirectional GRU <ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref> 22.8 ByteNet <ref type="bibr" target="#b21">(Kalchbrenner et al., 2016)</ref> 23.7 GNMT + RL <ref type="bibr" target="#b20">(Johnson et al., 2017)</ref> 24.6 ConvS2S <ref type="bibr" target="#b12">(Gehring et al., 2017)</ref> 25.1 MoE  26.0 GNMT + RL Ensemble <ref type="bibr" target="#b20">(Johnson et al., 2017)</ref> 26.3 ConvS2S Ensemble <ref type="bibr" target="#b12">(Gehring et al., 2017)</ref> 26.3 Transformer (base) <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> 27.3 Transformer-Dual (base) <ref type="bibr">[PH18]</ref> 27.5 Ours -Transformer-DRILL (base)</p><p>28.1</p><p>Transformer (big) <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> 28.4 RNMT+ <ref type="bibr" target="#b5">(Chen et al., 2018b)</ref> 28.5 RNMT+ cascaded <ref type="bibr" target="#b5">(Chen et al., 2018b)</ref> 28.6 RNMT+ multicol <ref type="bibr" target="#b5">(Chen et al., 2018b)</ref> 28.8 expressivity and a tendency to overfit. <ref type="bibr" target="#b51">Yang et al. (2018)</ref> showed that the softmax layer which is low-rank creates a 'bottleneck' problem, i.e. limits model expressivity, and increased the classifier rank by using a mixture of softmaxes. <ref type="bibr" target="#b48">Takase et al. (2018)</ref> improved MoS by computing the mixture based on the last and the middle recurrent layers. Two alternative ways to increase the classifier rank are obtained by multiplying the softmax with a non-parametric sigmoid function <ref type="bibr" target="#b22">(Kanai et al., 2018)</ref>, and by learning parametric monotonic functions on top of the logits <ref type="bibr" target="#b11">(Ganea et al., 2019)</ref>. Both of these methods have close to or higher perplexity than ours without using MoS, even though we keep the rank or power of the classifier the same. Instead, we specifically increase the power of the output label encoder, and the obtained results suggest that the classifier does not necessarily need to be high-rank to better capture language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Typical log-linear classifiers for neural language modeling tasks can be significantly improved by learning a deep residual output label encoding, regardless of the input encoding architecture. Deeper representations of the output structure lead to better transfer across the output labels, especially the low-resource ones. The results on three tasks show that the proposed output layer parameterisation can match or improve state-of-the-art context encoding architectures and outperform previous output layer parameterisations based on a joint input-output space, while preserving their basic principles and generality. Our findings should apply on other conditional neural language modeling tasks, such as image captioning and summarization. As future work, it would be interesting to learn from more elaborate descriptions or contextualized representations of the output labels and investigate their transferability in different tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>et al. (2018) -AWD-LSTM-MoS + dynamic evaluation † 22M 48.33 47.69</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Mean relative cross-entropy loss difference (%) between each baseline output layer (B) and our output layer (DRILL) computed over different word frequency intervals on PennTreebank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table /><note>Model perplexity with a single softmax (upper part) and multiple softmaxes (lower part) on validation and test sets on Penn Treebank. Baseline results are obtained from Merity et al. (2017) and Krause et al. (2018). † indicates the use of dynamic evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Model perplexity with a single softmax (upper part) and multiple softmaxes (lower part) on validation and test sets on WikiText-2. Baseline results are obtained from<ref type="bibr" target="#b30">Merity et al. (2017)</ref> and<ref type="bibr" target="#b25">Krause et al. (2018)</ref>. † indicates the use of dynamic evaluation.</figDesc><table><row><cell>hidden state updates per time step, Zoph &amp; Le (2016) uses</cell></row><row><cell>reinforcement learning to generate an RNN cell which is</cell></row><row><cell>even more complicated than an LSTM cell, and Merity et al.</cell></row><row><cell>(2016) makes use of a probabilistic mixture model which</cell></row><row><cell>combines a typical language model with a pointer network</cell></row><row><cell>which reproduces words from the recent context.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average time taken per epoch on the two datasets: Pen-nTreebank (|V| ≈ 20K) and Wikitext-2 (|V| ≈ 33K).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.0 dropout rate, sigmoid activation function, residual connection to E, and uniform weight initialization in [−0.1, 0.1]. The rest of the hyper-parameters were set to the optimal ones in</figDesc><table><row><cell>dings, 0</cell></row><row><cell>Model configuration. We compare against the state-of-</cell></row><row><cell>the-art Transformer (base) architecture from Vaswani et al.</cell></row><row><cell>(2017) with a 6-layer encoder and decoder depth, 512-</cell></row><row><cell>dimensional word embeddings, 2048-dimensional hidden</cell></row><row><cell>feed-forward states and 8 heads. 7 Our hyper-parameters</cell></row><row><cell>were optimized based on validation accuracy, as follows:</cell></row><row><cell>2-layer label encoder depth, 512-dimensional label embed-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Translation results in terms of BLEU on English to German with a 32K BPE vocabulary.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Idiap Research Institute, Martigny, Switzerland. Correspondence to: Nikolaos Pappas &lt;nikolaos.pappas@idiap.ch&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code and settings are available at http://github. com/idiap/drill.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that in practice, for our experiments with vocabularies up to 32K, we did not need to resort to a softmax approximation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://github.com/salesforce/awd-lstm-lm 4 http://github.com/benkrause/dynamicevaluation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For fair comparison, we also used dropout and residual connections to E and ht when they lead to better validation performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We chose the base model because it can be trained much faster than the big model (12 hours vs 3.5 days), for efficiency reasons.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the European Union through SUMMA project (n. 688139) and the Swiss National Science Foundation within INTERPID project (FNS-30106).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1409.0473.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045390.3045495" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="983" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janvin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<idno>1532- 4435</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=944919.944966" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (wmt17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-4717" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Shared Task Papers</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition using semanticspreserving adversarial embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1043" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P18-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zero-shot learning and clustering for semantic utterance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1401.0509" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1033</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>978-1-5108-3881-9</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3157096.3157211" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>978-1-5108-3881-9</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3157096.3157211" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Breaking the softmax bottleneck via learnable monotonic pointwise non-linearities. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.08077" />
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/gehring17a.html" />
	</analytic>
	<monogr>
		<title level="m">International Deep Residual Output Layers for Neural Language Generation Convention Centre</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of the 34th International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep structured prediction with nonlinear output transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Meshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7869-deep-structured-prediction-with-nonlinear-output-transformations.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6320" to="6331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno>abs/1612.04426</idno>
		<ptr target="http://arxiv.org/abs/1612.04426" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How to represent a word and predict it, too: Improving tied architectures for language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boleda</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D18-1323" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="2936" to="2941" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>doi: 10</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01462</idno>
		<ptr target="http://arxiv.org/abs/1611.01462" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15" to="1001" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Vigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>2307-387X</idno>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1081" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
		<ptr target="http://arxiv.org/abs/1610.10099" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reanalysis of the softmax bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigsoftmax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7312-sigsoftmax-reanalysis-of-the-softmax-bottleneck.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="286" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3016100.3016285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-4012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/krause18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Portugal</forename><surname>Lisbon</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1707.05589</idno>
		<ptr target="http://arxiv.org/abs/1707.05589" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models. CoRR, abs/1609.07843</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1609.07843" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models. CoRR, abs/1708.02182</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.02182" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT.2012.6424228</idno>
		<ptr target="https://ieeexplore.ieee.org/document/6424228" />
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<editor>Kobayashi, T., Hirose, K., and Nakamura, S.</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">; C J C</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Burges</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">All-in text: Learning document, label, and word representations jointly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12058" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, AR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1948" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>doi: 10. 3115/1073083.1073135</idno>
		<ptr target="http://www.aclweb.org/anthology/P02-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A generalized inputlabel embedding for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gile</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00259</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00259" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="139" to="155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond weight tying: Learning joint input-output embeddings for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W18-6308" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="73" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkAClQgA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-2025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot and zero-shot multilabel learning for structured label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1352" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="3132" to="3142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<ptr target="https://aclanthology.coli.uni-saarland.de/papers/D15-1044/d15-1044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="https://aclanthology.coli.uni-saarland.de/papers/P16-1162/p16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1701.06538</idno>
		<ptr target="http://arxiv.org/abs/1701.06538" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning distributed representations for structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D. ; Z</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5323-learning-distributed-representations-for-structured-output-prediction.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3266" to="3274" />
		</imprint>
	</monogr>
	<note>Ghahramani,</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning distributed representations for structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2969033.2969191" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3266" to="3274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Direct output connection for a high-rank language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagata</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1489" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="4599" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gar</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>nett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">WSABIE: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno>978-1-57735-515- 1</idno>
		<ptr target="https://ai.google/research/pubs/pub37180" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>abs/1711.03953</idno>
		<ptr target="http://arxiv.org/abs/1711.03953" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A model of zero-shot learning of spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1027</idno>
		<ptr target="http://aclweb.org/anthology/D15-1027" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="244" to="249" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep latent spaces for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.00418" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.2329" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Recurrent neural network regularization. CoRR, abs/1409.2329</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast zero-shot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1605.09759" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/zilly17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1611.01578</idno>
		<ptr target="http://arxiv.org/abs/1611.01578" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
