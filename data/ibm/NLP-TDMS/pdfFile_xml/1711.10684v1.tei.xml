<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE GEOSCIENCE AND REMOTE SENSING LETTERS 1 Road Extraction by Deep Residual U-Net</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjie</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">IEEE GEOSCIENCE AND REMOTE SENSING LETTERS 1 Road Extraction by Deep Residual U-Net</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Road extraction</term>
					<term>Convolutional Neural Network</term>
					<term>Deep Residual U-Net</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network which combines the strengths of residual learning and U-Net is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model is two-fold: first, residual units ease training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters however better performance. We test our network on a public road dataset and compare it with U-Net and other two state of the art deep learning based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R OAD extraction is one of the fundamental tasks in the field of remote sensing. It has a wide range of applications such as automatic road navigation, unmanned vehicles, urban planning, and geographic information update, etc. Although it has been received considerable attentions in the last decade, road extraction from high resolution remote sensing images is still a challenging task because of the noise, occlusions and complexity of the background in raw remote sensing imagery.</p><p>A variety of methods have been proposed to extract roads from remote sensing images in recent years. Most of these methods can be divided into two categories: road area extraction and road centerline extraction. Road area extraction <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref> can generate pixel-level labeling of roads, while road centerline extraction <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref> aims at detecting skeletons of a road. There are also methods extract both road areas and centerline, simultaneously <ref type="bibr" target="#b9">[9]</ref>. Since road centerline can be easily obtained from road areas using algorithms such as morphological thinning <ref type="bibr" target="#b10">[10]</ref>, this letter focuses on road area extraction from high resolution remote sensing images.</p><p>Road area extraction can be considered as a segmentation or pixel-level classification problem. For instance, Song and Civco <ref type="bibr" target="#b11">[11]</ref> proposed a method utilizing shape index feature and support vector machine (SVM) to detect road areas. Das et al. <ref type="bibr" target="#b12">[12]</ref> exploited two salient features of roads and designed a multistage framework to extract roads from high resolution multi-spectral images using probabilistic SVM. Alshehhi and Marpu <ref type="bibr" target="#b5">[6]</ref> proposed an unsupervised road extraction method based on hierarchical graph-based image segmentation.</p><p>Recent years have witnessed great progress in deep learning. Methods based on deep neural networks have achieved stateof-the-art performance on a variety of computer vision tasks, such as scene recognition <ref type="bibr" target="#b13">[13]</ref> and object detection <ref type="bibr" target="#b14">[14]</ref>. Researchers in remote sensing community also seek to leverage the power of deep neural networks to solve the problems of interpretation and understanding of remote sensing data <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[15]</ref>- <ref type="bibr" target="#b18">[18]</ref>. These methods provide better results than traditional ones, showing great potential of applying deep learning techniques to analyze remote sensing tasks.</p><p>In the field of road extraction, one of the first attempts of applying deep learning techniques was made by Mnih and Hinton <ref type="bibr" target="#b1">[2]</ref>. They proposed a method employing restricted Boltzmann machines (RBMs) to detect road areas from high resolution aerial images. To achieve better results, a preprocessing step before the detection and a post-processing step after the detection were applied. The pre-processing was deployed to reduce the dimensionality of the input data. The post-processing was employed to remove disconnected blotches and fill in the holes in the roads. Different from Mnih and Hinton's method <ref type="bibr" target="#b1">[2]</ref> that use RBMs as basic blocks to built deep neural networks, Saito et al. <ref type="bibr" target="#b4">[5]</ref> employed Convolutional Neural Network (CNNs) to extract buildings and roads directly from raw remote sensing imagery. This method achieves better results than Mnih and Hinton's method <ref type="bibr" target="#b1">[2]</ref> on the Massachusetts roads dataset.</p><p>Recently, lots of works have suggested that a deeper network would have better performance <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>. However, it is very difficult to train a very deep architecture due to problems such as vanishing gradients. To overcome this problem, He et al. <ref type="bibr" target="#b21">[21]</ref> proposed the deep residual learning framework that utilize an identity mapping <ref type="bibr" target="#b22">[22]</ref> to facilitate training. Instead of using skip connection in Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b23">[23]</ref>, Ronneberger et al. <ref type="bibr" target="#b24">[24]</ref> proposed the U-Net that concatenate feature maps from different levels to improve segmentation accuracy. U-Net combines low level detail information and high level semantic information, thus achieves promising performance on biomedical image segmentation <ref type="bibr" target="#b24">[24]</ref>.</p><p>Inspired by the deep residual learning <ref type="bibr" target="#b21">[21]</ref> and U-Net <ref type="bibr" target="#b24">[24]</ref>, in this letter we propose the deep residual U-Net, an architecture that take advantage of strengths from both deep residual learning and U-Net architecture. The proposed deep residual U-Net (ResUnet) is built based on the architecture of U-Net. The differences between our deep ResUnet and U-Net are in two-fold. First, we use residual units instead of plain neural units as basic blocks to build the deep ResUnet. Second, the cropping operation is unnecessary thus removed from our network, leading to a much more elegant architecture and better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>A. Deep ResUnet 1) U-Net: In semantic segmentation, to get a finer result, it is very important to use low level details while retaining high level semantic information <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>. However, training such a deep neural network is very hard especially when only limited training samples are available. One way to solve this problem is employing a pre-trained network then finetuning it on the target dataset, as done in <ref type="bibr" target="#b23">[23]</ref>. Another way is employing extensive data augmentation, as done in U-Net <ref type="bibr" target="#b24">[24]</ref>. In addition to data augmentation, we believe the architecture of U-Net also contributes to relieving the training problem. The intuition behind this is that copying low level features to the corresponding high levels actually creates a path for information propagation allowing signals propagate between low and high levels in a much easier way, which not only facilitating backward propagation during training, but also compensating low level finer details to high level semantic features. This somehow shares similar idea to that of residual neural network <ref type="bibr" target="#b21">[21]</ref>. In this letter, we show that the performance of U-Net can be further improved by substituting the plain unit with a residual unit.</p><p>2) Residual unit: Going deeper would improve the performance of a multi-layer neural network, however could hamper the training, and a degradation problem maybe occur <ref type="bibr" target="#b21">[21]</ref>. To overcome these problems, He et al. <ref type="bibr" target="#b21">[21]</ref> proposed the residual neural network to facilitate training and address the degradation problem. The residual neural network consists of a series of stacked residual units. Each residual unit can be illustrated as a general form:</p><formula xml:id="formula_0">y l = h(x l ) + F(x l , W l ), x l+1 = f (y l ),<label>(1)</label></formula><p>where x l and x l+1 are the input and output of the l-th residual unit, F(·) is the residual function, f (y l ) is activation function and h(x l ) is a identity mapping function, a typical one is h(x l ) = x l . <ref type="figure" target="#fig_0">Fig. 1</ref> shows the difference between a plain and residual unit. There are multiple combinations of batch  <ref type="bibr" target="#b22">[22]</ref> and suggested a full pre-activation design as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. In this work, we also employ full pre-activation residual unit to build our deep residual U-Net.</p><p>3) Deep ResUnet: Here we propose the deep ResUnet, a semantic segmentation neural network which combines strengths of both U-Net and residual neural network. This combination bring us two benefits: 1) the residual unit will ease training of the network; 2) the skip connections within a residual unit and between low levels and high levels of the network will facilitate information propagation without degradation, making it possible to design a neural network with much fewer parameters however could achieve comparable ever better performance on semantic segmentation.</p><p>In this work, we utilize a 7-level architecture of deep ResUnet for road area extraction, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The network comprises of three parts: encoding, bridge and decoding. <ref type="bibr" target="#b0">1</ref> The first part encodes the input image into compact representations. The last part recovers the representations to a pixel-wise categorization, i.e. semantic segmentation. The middle part serves like a bridge connecting the encoding and decoding paths. All of the three parts are built with residual units which consist of two 3 × 3 convolution blocks and an identity mapping. Each convolution block includes a BN layer, a ReLU activation layer and a convolutional layer. The identity mapping connects input and output of the unit. Encoding path has three residual units. In each unit, instead of using pooling operation to downsample the feature map size, a stride of 2 is applied to the first convolution block to reduce the feature map by half. Correspondingly, decoding path composes of three residual units, too. Before each unit, there is an up-sampling of feature maps from lower level and a concatenation with the feature maps from the corresponding encoding path. After the last level of decoding path, a 1 × 1 convolution and a sigmod activation layer is used to project the multi-channel feature maps into the desired segmentation. In total we have 15 convolutional layers comparing with 23 layers of U-Net. It is worth noting that the indispensable cropping in U-Net is unnecessary in our network. The parameters and output size of each step are presented in <ref type="table" target="#tab_0">Table I</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss function</head><p>Given a set of training images and the corresponding ground truth segmentations {I i , s i }, our goal is to estimate parameters W of the network, such that it produce accurate and robust road areas. This is achieved through minimizing the loss between the segmentations generated by N et(I i ; W ) and the ground truth s i . In this work, we use Mean Squared Error (MSE) as the loss function:</p><formula xml:id="formula_1">L(W ) = 1 N N i=1 ||N et(I i ; W ) − s i || 2 ,<label>(2)</label></formula><p>where N is the number of the training samples. We use the stochastic gradient descent (SGD) to train our network. One should know that other loss functions that are derivable can also be used to train the network. For instance, U-Net adopted pixel-wise cross entropy as loss function to optimize the model. <ref type="bibr" target="#b0">1</ref> U-Net used "contracting" and "expansive" paths to denote the feature extraction and up-convolution stages of the network. In this letter, we prefer the terms encoding and decoding because we think it is more meaningful and easer to understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Result refinement</head><p>The input and output of our semantic segmentation network have the same size in width and height, both are 224×224. The pixels near boundaries of the output have lower accuracy than center ones due to zero padding in the convolutional layer. To get a better result, we use an overlap strategy to produce the segmentation results of a large image. The input sub-images are cropped from the original image with an overlap of o (o = 14 in our experiments). The final results are obtained by stitching all sub-segmentations together. The values in the overlap regions are averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>To demonstrate the accuracy and efficiency of the proposed deep ResUnet, we test it on Massachusetts roads dataset 2 and compare it with three state of the art methods, including Mnih's <ref type="bibr" target="#b1">[2]</ref> method, Saito's method <ref type="bibr" target="#b4">[5]</ref> and U-Net <ref type="bibr" target="#b24">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The Massachusetts roads dataset was built by Mihn et al. <ref type="bibr" target="#b1">[2]</ref>. The dataset consists of 1171 images in total, including 1108 images for training, 14 images for validation and 49 images for testing. The size of all the images in this dataset is 1500 × 1500 pixels with a resolution of 1.2 meter per pixel. This dataset roughly covers 500 km 2 space crossing from urban, sub-urban to rural areas and a wide range of ground objects including roads, rivers, sea, various buildings, vegetations, schools, bridges, ports, vehicles, etc. In this work, we train our network on the training set of this dataset and report results on its test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>The proposed model was implemented using Keras <ref type="bibr" target="#b25">[25]</ref> framework and optimized by minimizing Eqn. 2 through SGD algorithm. There are 1108 training images sized 1500 × 1500 available for training. Theoretically, our network can take arbitrary size image as input, however it will need amount of GPU memory to store the feature maps. In this letter, we utilize fixed-sized training images (224 × 224 as described in <ref type="table" target="#tab_0">Table I</ref>) to train the model. These training images are randomly sampled from the original images. At last, 30,000 samples are generated and fed into the network to learn the parameters. It should be noted that, no data augmentation is used during training. We start training the model with a mini-batch size of 8 on a NVIDIA Titan 1080 GPU. The learning rate was initially set to 0.001 and reduced by a factor of 0.1 in every 20 epochs. The network will converge in 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation metrics</head><p>The most common metrics for evaluating a binary classification method are precision and recall. In remote sensing, these metrics are also called correctness and completeness. The precision is the fraction of predicted road pixels which are labeled as roads and the recall is the fraction of all the labeled road pixels that are correctly predicted.</p><p>Because of the difficulty in correctly labeling all the road pixels, Mnih et al. <ref type="bibr" target="#b1">[2]</ref> introduced the relaxed precision and recall scores <ref type="bibr" target="#b26">[26]</ref> into road extraction. The relaxed precision is defined as the fraction of number of pixels predicted as road within a range of ρ pixels from pixels labeled as road. The relaxed recall is the fraction of number of pixels labeled as road that are within a range of ρ pixels from pixels predicted as road. In this experiment, the slack parameter ρ is set to 3, which is consistent with previous studies <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. We also report break-even points of different methods. The break-even point is defined as the point on the relaxed precision-recall curve where its precision value equals its recall value. In other words, break-even point is the intersection of precision-recall curve and line y = x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparisons</head><p>Comparisons with three state of the art deep learning based road extraction methods are conducted on the test set of Massachusetts roads dataset. The break-even points of the proposed and comparing methods are reported in <ref type="table" target="#tab_0">Table II</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref> presents the relaxed precision-recall curves of U-Net and our network and their break-even points, along with breakeven points of comparing methods. It can be seen that our method performs better than all other three approaches in terms of relaxed precision and recall. Although the parameters of our network is only 1/4 of U-Net (7.8M versus 30.6M), promising improvement are achieved on the road extraction task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Breakeven point Mnih-CNN <ref type="bibr" target="#b1">[2]</ref> 0.8873 Mnih-CNN+CRF <ref type="bibr" target="#b1">[2]</ref> 0.8904 Mnih-CNN+Post-Processing <ref type="bibr" target="#b1">[2]</ref> 0.9006 Saito-CNN <ref type="bibr" target="#b4">[5]</ref> 0.9047 U-Net <ref type="bibr" target="#b24">[24]</ref> 0.9053 ResUnet 0.9187 <ref type="figure">Fig. 4</ref> illustrates four example results of Saito et al. <ref type="bibr" target="#b4">[5]</ref>, U-Net <ref type="bibr" target="#b24">[24]</ref> and the proposed ResUnet. It can be seen, our method shows cleaner results with less noise than the other two methods. Especially when there are two-lane roads, our method can segmentation each lane with high confidence, generating clean and sharp two-lane roads, while other methods may confuse lanes with each other, as demonstrate in the third row of <ref type="figure">Fig. 4</ref>. Similarly, in the intersection regions, our method also produces better results.</p><p>Context information is very important when analyzing objects with complex structures. Our network considers context information of roads, thus can distinguish roads from similar objects such as building roofs, airfield runways. From the first row of <ref type="figure">Fig. 4</ref> we can see that, even the runway has very similar features to a highway, our method can successfully segmentation side road from the runway. In addition to this, the context information also make it robust to occlusions. For example, parts of the roads on the rectangle of the second row are covered by trees. Saito's method and U-Net cannot detect road under the trees, however our method labeled them successfully. A failure case is shown in the yellow rectangle of the last row. Our method missed the roads in the parking lot. This is mainly because most of roads in parking lots are not labeled. Therefore, although these roads share the same features to the normal ones, considering the context information our network regard them as backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this letter, we have proposed the ResUnet for road extraction from high resolution remote sensing images. The proposed network combines the strengths of residual learning and U-Net. The skip connections within the residual units and between the encoding and decoding paths of the network will facilitate information propagations both in forward and backward computations. This property not only ease training but also allows us to design simple yet powerful neural networks. The proposed network outperforms U-Net with only 1/4 of its parameters, as well as other two state of the art deep learning based road extraction methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ConvolutionFig. 1 .</head><label>1</label><figDesc>arXiv:1711.10684v1 [cs.CV] 29 Nov 2017 Building blocks of neural networks. (a) Plain neural unit used in U-Net and (b) residual unit with identity mapping used in the proposed ResUnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of the proposed deep ResUnet. normalization (BN), ReLU activation and convolutional layers in a residual unit. He et al. presented a detailed discussion on impacts of different combinations in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The relaxed precision-recall curves of U-Net and the proposed method on Massachusetts roads dataset. The marks ' ' and '×' are break-even points of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>NETWORK STRUCTURE OF RESUNET.</figDesc><table><row><cell></cell><cell>Unit level</cell><cell>Conv layer</cell><cell>Filter</cell><cell>Stride</cell><cell>Output size</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>224 × 224 × 3</cell></row><row><cell></cell><cell>Level 1</cell><cell>Conv 1 Conv 2</cell><cell>3 × 3/64 3 × 3/64</cell><cell>1 1</cell><cell>224 × 224 × 64 224 × 224 × 64</cell></row><row><cell>Encoding</cell><cell>Level 2</cell><cell>Conv 3 Conv 4</cell><cell>3 × 3/128 3 × 3/128</cell><cell>2 1</cell><cell>112 × 112 × 128 112 × 112 × 128</cell></row><row><cell></cell><cell>Level 3</cell><cell>Conv 5 Conv 6</cell><cell>3 × 3/256 3 × 3/256</cell><cell>2 1</cell><cell>56 × 56 × 256 56 × 56 × 256</cell></row><row><cell>Bridge</cell><cell>Level 4</cell><cell>Conv 7 Conv 8</cell><cell>3 × 3/512 3 × 3/512</cell><cell>2 1</cell><cell>28 × 28 × 512 28 × 28 × 512</cell></row><row><cell></cell><cell>Level 5</cell><cell>Conv 9 Conv 10</cell><cell>3 × 3/256 3 × 3/256</cell><cell>1 1</cell><cell>56 × 56 × 256 56 × 56 × 256</cell></row><row><cell>Decoding</cell><cell>Level 6</cell><cell>Conv 11 Conv 12</cell><cell>3 × 3/128 3 × 3/128</cell><cell>1 1</cell><cell>112 × 112 × 128 112 × 112 × 128</cell></row><row><cell></cell><cell>Level 7</cell><cell>Conv 13 Conv 14</cell><cell>3 × 3/64 3 × 3/64</cell><cell>1 1</cell><cell>224 × 224 × 64 224 × 224 × 64</cell></row><row><cell>Output</cell><cell></cell><cell>Conv 15</cell><cell>1 × 1</cell><cell>1</cell><cell>224 × 224 × 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISONS</head><label>II</label><figDesc>OF THE PROPOSED AND OTHER THREE DEEP LEARNING BASED ROAD EXTRACTION METHOD ON MASSACHUSETTS ROADS DATASET IN TERMS OF BREAKEVEN POINT. A HIGHER BREAKEVEN POINT INDICATES A BETTER PERFORMANCE IN PRECISION AND RECALL.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.cs.toronto.edu/˜vmnih/data/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Road centreline extraction from highresolution imagery based on multiscale structural features and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1977" to="1987" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to detect roads in high-resolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="210" to="223" />
		</imprint>
	</monogr>
	<note type="report_type">ECCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Road network detection using probabilistic and graph theoretical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Unsalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sirmacek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGRS</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4441" to="4453" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Urban road extraction via graph cuts based probability propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="5072" to="5076" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple object extraction from aerial imagery with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ELECTRON IMAGING</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical graph-based segmentation for extracting road networks from high-resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alshehhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Marpu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P&amp;RS</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="245" to="260" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Main road extraction from ZY-3 grayscale imagery based on directional mathematical morphology and VGI prior knowledge in urban areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">138071</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Example results on the test set of Massachusetts roads dataset. (a) Input image; (b) Ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saito</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>5]; (d) U-Net [24]; (e) The proposed ResUnet. Zoom in to see more details</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Connected component-based technique for automatic extraction of road centerline in high resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selvathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Image Video Process</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGRS</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3322" to="3337" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Road centerline extraction via semisupervised segmentation and multidirection nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GRSL</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="549" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Road extraction using SVM and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Civco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PE&amp;RS</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1365" to="1371" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Use of salient features for the design of a multistage framework to extract roads from high-resolution multispectral satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Mirnalinee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGRS</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3906" to="3931" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1137</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CNN based suburban building detection using monocular high resolution google earth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="661" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A CNN based functional zone classification method for aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5449" to="5452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relaxed precision and recall for ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Euzenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Integrating ontology</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
