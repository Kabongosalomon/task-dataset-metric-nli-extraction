<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Neural Network Architectures for Matching Natural Language Sentences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
							<email>baotianchina@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
							<email>lu.zhengdong@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab Huawei Technologies Co. Ltd. Sha Tin</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab Huawei Technologies Co. Ltd. Sha Tin</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Qingcai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology Shenzhen Graduate School</orgName>
								<address>
									<settlement>Xili</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Neural Network Architectures for Matching Natural Language Sentences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic matching is of central importance to many natural language tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layerby-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Matching two potentially heterogenous language objects is central to many natural language applications <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref>. It generalizes the conventional notion of similarity (e.g., in paraphrase identification <ref type="bibr" target="#b18">[19]</ref>) or relevance (e.g., in information retrieval <ref type="bibr" target="#b26">[27]</ref>), since it aims to model the correspondence between "linguistic objects" of different nature at different levels of abstractions. Examples include top-k re-ranking in machine translation (e.g., comparing the meanings of a French sentence and an English sentence <ref type="bibr" target="#b4">[5]</ref>) and dialogue (e.g., evaluating the appropriateness of a response to a given utterance <ref type="bibr" target="#b25">[26]</ref>). Natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. A successful sentence-matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions. Towards this end, we propose deep neural network models, which adapt the convolutional strategy (proven successful on image <ref type="bibr" target="#b10">[11]</ref> and speech <ref type="bibr" target="#b0">[1]</ref>) to natural language. To further explore the relation between representing sentences and matching them, we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple-to-comprehensive fusion of matching patterns with the same convolutional architecture. Our model is generic, requiring no prior knowledge of natural language (e.g., parse tree) and putting essentially no constraints on the matching tasks. This is part of our continuing effort 1 in understanding natural language objects and the matching between them <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Our main contributions can be summarized as follows. First, we devise novel deep convolutional network architectures that can naturally combine 1) the hierarchical sentence modeling through layer-by-layer composition and pooling, and 2) the capturing of the rich matching patterns at different levels of abstraction; Second, we perform extensive empirical study on tasks with different scales and characteristics, and demonstrate the superior power of the proposed architectures over competitor methods.</p><p>Roadmap We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling, and how it is related to existing sentence models. Based on that, in Section 3, we propose two architectures for sentence matching, with a detailed discussion of their relation. In Section 4, we briefly discuss the learning of the proposed architectures. Then in Section 5, we report our empirical study, followed by a brief discussion of related work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Convolutional Sentence Model</head><p>We start with proposing a new convolutional architecture for modeling sentences. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, it takes as input the embedding of words (often trained beforehand with unsupervised methods) in the sentence aligned sequentially, and summarize the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. As in most convolutional models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1]</ref>, we use convolution units with a local "receptive field" and shared weights, but we design a large feature map to adequately model the rich structures in the composition of words. Convolution As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the convolution in Layer-1 operates on sliding windows of words (width k 1 ), and the convolutions in deeper layers are defined in a similar way. Generally,with sentence input x, the convolution unit for feature map of type-f (among F of them) on Layer-is</p><formula xml:id="formula_0">z ( ,f ) i def = z ( ,f ) i (x) = σ(w ( ,f )ẑ ( −1) i + b ( ,f ) ), f = 1, 2, · · · , F<label>(1)</label></formula><p>and its matrix form is z</p><formula xml:id="formula_1">( ) i def = z ( ) i (x) = σ(W ( )ẑ ( −1) i + b ( ) ), where • z ( ,f ) i (x)</formula><p>gives the output of feature map of type-f for location i in Layer-;</p><p>• w ( ,f ) is the parameters for f on Layer-, with matrix form</p><formula xml:id="formula_2">W ( ) def = [w ( ,1) , · · · , w ( ,F ) ];</formula><p>• σ(·) is the activation function (e.g., Sigmoid or Relu <ref type="bibr" target="#b6">[7]</ref>)</p><p>•ẑ ( −1) i denotes the segment of Layer-−1 for the convolution at location i , whilê</p><formula xml:id="formula_3">z (0) i = x i:i+k1−1 def = [x i , x i+1 , · · · , x i+k1−1 ]</formula><p>concatenates the vectors for k 1 (width of sliding window) words from sentence input x. Max-Pooling We take a max-pooling in every two-unit window for every f , after each convolution</p><formula xml:id="formula_4">z ( ,f ) i = max(z ( −1,f ) 2i−1 , z ( −1,f ) 2i ), = 2, 4, · · · .</formula><p>The effects of pooling are two-fold: 1) it shrinks the size of the representation by half, thus quickly absorbs the differences in length for sentence representation, and 2) it filters out undesirable composition of words (see Section 2.1 for some analysis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Length Variability</head><p>The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy. More specifically, we put all-zero padding vectors after the last word of the sentence until the maximum length. To eliminate the boundary effect caused by the great variability of sentence lengths, we add to the convolutional unit a gate which sets the output vectors to all-zeros if the input is all zeros. For any given sentence input x, the output of type-f filter for location i in the th layer is given by</p><formula xml:id="formula_5">z ( ,f ) i def = z ( ,f ) i (x) = g(ẑ ( −1) i ) · σ(w ( ,f )ẑ ( −1) i + b ( ,f ) ),</formula><p>(2) where g(v) = 0 if all the elements in vector v equals 0, otherwise g(v) = 1. This gate, working with max-pooling and positive activation function (e.g., Sigmoid), keeps away the artifacts from padding in all layers. Actually it creates a natural hierarchy of all-zero padding (as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>), consisting of nodes in the neural net that would not contribute in the forward process (as in prediction) and backward propagation (as in learning). The convolutional unit, when combined with max-pooling, can act as the compositional operator with local selection mechanism as in the recursive autoencoder <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure" target="#fig_1">Figure  2</ref> gives an example on what could happen on the first two layers with input sentence "The cat sat on the mat". Just for illustration purpose, we present a dramatic choice of parameters (by turning off some elements in W <ref type="bibr" target="#b0">(1)</ref> ) to make the convolution units focus on different segments within a 3-word window. For example, some feature maps (group 2) give compositions for "the cat" and "cat sat", each being a vector. Different feature maps offer a variety of compositions, with confidence encoded in the values (color coded in output of convolution layer in <ref type="figure" target="#fig_1">Figure 2</ref>). The pooling then chooses, for each composition type, between two adjacent sliding windows, e.g., between "on the" and "the mat" for feature maps group 2 from the rightmost two sliding windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Some Analysis on the Convolutional Architecture</head><p>Relation to Recursive Models Our convolutional model differs from Recurrent Neural Network (RNN, <ref type="bibr" target="#b14">[15]</ref>) and Recursive Auto-Encoder (RAE, <ref type="bibr" target="#b20">[21]</ref>) in several important ways. First, unlike RAE, it does not take a single path of word/phrase composition determined either by a separate gating function <ref type="bibr" target="#b20">[21]</ref>, an external parser <ref type="bibr" target="#b18">[19]</ref>, or just natural sequential order <ref type="bibr" target="#b19">[20]</ref>. Instead, it takes multiple choices of composition via a large feature map (encoded in w ( ,f ) for different f ), and leaves the choices to the pooling afterwards to pick the more appropriate segments(in every adjacent two) for each composition. With any window width k ≥ 3, the type of composition would be much richer than that of RAE. Second, our convolutional model can take supervised training and tune the parameters for a specific task, a property vital to our supervised learning-to-match framework. However, unlike recursive models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, the convolutional architecture has a fixed depth, which bounds the level of composition it could do. For tasks like matching, this limitation can be largely compensated with a network afterwards that can take a "global" synthesis on the learned sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation to "Shallow" Convolutional Models</head><p>The proposed convolutional sentence model takes simple architectures such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref> (essentially the same convolutional architecture as SENNA <ref type="bibr" target="#b5">[6]</ref>), which consists of a convolution layer and a max-pooling over the entire sentence for each feature map. This type of models, with local convolutions and a global pooling, essentially do a "soft" local template matching and is able to detect local features useful for a certain task. Since the sentencelevel sequential order is inevitably lost in the global pooling, the model is incapable of modeling more complicated structures. It is not hard to see that our convolutional model degenerates to the SENNA-type architecture if we limit the number of layers to be two and set the pooling window infinitely large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional Matching Models</head><p>Based on the discussion in Section 2, we propose two related convolutional architectures, namely ARC-I and ARC-II), for matching two sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture-I (ARC-I)</head><p>Architecture-I (ARC-I), as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, takes a conventional approach: It first finds the representation of each sentence, and then compares the representation for the two sentences with a multi-layer perceptron (MLP) <ref type="bibr" target="#b2">[3]</ref>. It is essentially the Siamese architecture introduced in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, which has been applied to different tasks as a nonlinear similarity function <ref type="bibr" target="#b22">[23]</ref>. Although ARC-I enjoys the flexibility brought by the convolutional sentence model, it suffers from a drawback inherited from the Siamese architecture: it defers the interaction between two sentences (in the final MLP) to until their individual representation matures (in the convolution model), therefore runs at the risk of losing details (e.g., a city name) important for the matching task in representing the sentences. In other words, in the forward phase (prediction), the representation of each sentence is formed without knowledge of each other. This cannot be adequately circumvented in backward phase (learning), when the convolutional model learns to extract structures informative for matching on a population level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture-II (ARC-II)</head><p>In view of the drawback of Architecture-I, we propose Architecture-II (ARC-II) that is built directly on the interaction space between two sentences. It has the desirable property of letting two sentences meet before their own high-level representations mature, while still retaining the space for the individual development of abstraction of each sentence. Basically, in Layer-1, we take sliding windows on both sentences, and model all the possible combinations of them through "one-dimensional" (1D) convolutions. For segment i on S X and segment j on S Y , we have the feature map</p><formula xml:id="formula_6">z (1,f ) i,j def = z (1,f ) i,j (x, y) = g(ẑ (0) i,j ) · σ(w (1,f )ẑ (0) i,j + b (1,f ) ),<label>(3)</label></formula><p>whereẑ <ref type="formula">(0)</ref> i,j ∈ R 2k1De simply concatenates the vectors for sentence segments for S X and S Y :</p><formula xml:id="formula_7">z (0) i,j = [x i:i+k1−1 , y j:j+k1−1 ]</formula><p>. Clearly the 1D convolution preserves the location information about both segments. After that in Layer-2, it performs a 2D max-pooling in non-overlapping 2 × 2 windows (illustrated in <ref type="figure">Figure 5</ref>)</p><formula xml:id="formula_8">z (2,f ) i,j = max({z (1,f ) 2i−1,2j−1 , z (1,f ) 2i−1,2j , z (1,f ) 2i,2j−1 , z (1,f ) 2i,2j }).</formula><p>(4) In Layer-3, we perform a 2D convolution on k 3 × k 3 windows of output from Layer-2:</p><formula xml:id="formula_9">z (3,f ) i,j = g(ẑ (2) i,j ) · σ(W (3,f )ẑ (2) i,j + b (3,f ) ).</formula><p>(5) This could go on for more layers of 2D convolution and 2D max-pooling, analogous to that of convolutional architecture for image input <ref type="bibr" target="#b10">[11]</ref>.</p><p>The 2D-Convolution After the first convolution, we obtain a low level representation of the interaction between the two sentences, and from then we obtain a high level representation z ( ) i,j which encodes the information from both sentences. The general two-dimensional convolution is formulated as z ( )</p><formula xml:id="formula_10">i,j = g(ẑ ( −1) i,j ) · σ(W ( )ẑ ( −1) i,j + b ( ,f ) ), = 3, 5, · · ·<label>(6)</label></formula><p>whereẑ</p><formula xml:id="formula_11">( −1) i,j</formula><p>concatenates the corresponding vectors from its 2D receptive field in Layer-−1. This pooling has different mechanism as in the 1D case, for it selects not only among compositions on different segments but also among different local matchings. This pooling strategy resembles the dynamic pooling in <ref type="bibr" target="#b18">[19]</ref> in a similarity learning context, but with two distinctions: 1) it happens on a fixed architecture and 2) it has much richer structure than just similarity. Order Preservation Both the convolution and pooling operation in Architecture-II have this order preserving property. Generally, z ( ) i,j contains information about the words in S X before those in z ( ) i+1,j , although they may be generated with slightly different segments in S Y , due to the 2D pooling (illustrated in <ref type="figure">Figure 5)</ref>. The orders is however retained in a "conditional" sense. Our experiments show that when ARC-II is trained on the (S X , S Y ,S Y ) triples whereS Y randomly shuffles the words in S Y , it consistently gains some ability of finding the correct S Y in the usual contrastive negative sampling setting, which however does not happen with ARC-I.</p><p>Model Generality It is not hard to show that ARC-II actually subsumes ARC-I as a special case. Indeed, in ARC-II if we choose (by turning off some parameters in W ( ,·) ) to keep the representations of the two sentences separated until the final MLP, ARC-II can actually act fully like ARC-I, as illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>. More specifically, if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y (instead of taking both as in general case), the output of each segment-pair is naturally divided into two corresponding groups. As a result, the output for each filter f , denoted z</p><p>(1,f ) 1:n,1:n (n is the number of sliding windows), will be of rank-one, possessing essentially the same information as the result of the first convolution layer in ARC-I. Clearly the 2D pooling that follows will reduce to 1D pooling, with this separateness preserved. If we further limit the parameters in the second convolution units (more specifically w (2,f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side, and fully recover the functionality of ARC-I. As suggested by the order-preserving property and the generality of ARC-II, this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence, despite the fact that it is built on the interaction between two sentences. As a result, ARC-II can naturally blend two seemingly diverging processes: 1) the successive composition within each sentence, and 2) the extraction and fusion of matching patterns between them, hence is powerful for matching linguistic objects with rich structures. This intuition is verified by the superior performance of ARC-II in experiments (Section 5) on different matching tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>We employ a discriminative training strategy with a large margin objective. Suppose that we are given the following triples (x, y + , y − ) from the oracle, with x matched with y + better than with y − . We have the following ranking-based loss as objective:</p><formula xml:id="formula_12">e(x, y + , y − ; Θ) = max(0, 1 + s(x, y − ) − s(x, y + )),</formula><p>where s(x, y) is predicted matching score for (x, y), and Θ includes the parameters for convolution layers and those for the MLP. The optimization is relatively straightforward for both architectures with the standard back-propagation. The gating function (see Section 2) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function. In other words, We use stochastic gradient descent for the optimization of models. All the proposed models perform better with mini-batch (100 ∼ 200 in sizes) which can be easily parallelized on single machine with multi-cores. For regularization, we find that for both architectures, early stopping <ref type="bibr" target="#b15">[16]</ref> is enough for models with medium size and large training sets (with over 500K instances). For small datasets (less than 10k training instances) however, we have to combine early stopping and dropout <ref type="bibr" target="#b7">[8]</ref> to deal with the serious overfitting problem.</p><p>We use 50-dimensional word embedding trained with the Word2Vec <ref type="bibr" target="#b13">[14]</ref>: the embedding for English words (Section 5.2 &amp; 5.4) is learnt on Wikipedia (∼1B words), while that for Chinese words (Section 5.3) is learnt on Weibo data (∼300M words). Our other experiments (results omitted here) suggest that fine-tuning the word embedding can further improve the performances of all models, at the cost of longer training. We vary the maximum length of words for different tasks to cope with its longest sentence. We use 3-word window throughout all experiments 2 , but test various numbers of feature maps (typically from 200 to 500), for optimal performance. ARC-II models for all tasks have eight layers (three for convolution, three for pooling, and two for MLP), while ARC-I performs better with less layers (two for convolution, two for pooling, and two for MLP) and more hidden nodes. We use ReLu <ref type="bibr" target="#b6">[7]</ref> as the activation function for all of models (convolution and MLP), which yields comparable or better results to sigmoid-like functions, but converges faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We report the performance of the proposed models on three matching tasks of different nature, and compare it with that of other competitor models. Among them, the first two tasks (namely, Sentence Completion and Tweet-Response Matching) are about matching of language objects of heterogenous natures, while the third one (paraphrase identification) is a natural example of matching homogeneous objects. Moreover, the three tasks involve two languages, different types of matching, and distinctive writing styles, proving the broad applicability of the proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Competitor Methods</head><p>• WORDEMBED: We first represent each short-text as the sum of the embedding of the words it contains. The matching score of two short-texts are calculated with an MLP with the embedding of the two documents as input; • DEEPMATCH: We take the matching model in <ref type="bibr" target="#b12">[13]</ref> and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer; • URAE+MLP: We use the Unfolding Recursive Autoencoder <ref type="bibr" target="#b18">[19]</ref>  <ref type="bibr" target="#b2">3</ref> to get a 100dimensional vector representation of each sentence, and put an MLP on the top as in WORDEMBED; • SENNA+MLP/SIM: We use the SENNA-type sentence model for sentence representation;</p><p>• SENMLP: We take the whole sentence as input (with word embedding aligned sequentially), and use an MLP to obtain the score of coherence.</p><p>All the competitor models are trained on the same training set as the proposed models, and we report the best test performance over different choices of models (e.g., the number and size of hidden layers in MLP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment I: Sentence</head><p>Completion This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence. Basically, we take a sentence from Reuters <ref type="bibr" target="#b11">[12]</ref>with two "balanced" clauses (with 8∼ 28 words) divided by one comma, and use the first clause as S X and the second as S Y . The task is then to recover the original second clause for any given first clause. The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels. We deliberately make the task harder by using negative second clauses similar to the original ones <ref type="bibr" target="#b3">4</ref>  All models are trained on 3 million triples (from 600K positive pairs), and tested on 50K positive pairs, each accompanied by four negatives, with results shown in <ref type="table">Table 1</ref>. The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling. ARC-II outperforms ARC-I significantly, showing the power of joint modeling of matching and sentence meaning. As another convolutional model, SENNA+MLP performs fairly well on this task, although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence. It is a bit surprising that URAE comes last on this task, which might be caused by the facts that 1) the representation model (including word-embedding) is not trained on Reuters, and 2) the split-sentence setting hurts the parsing, which is vital to the quality of learned sentence representation.   <ref type="table" target="#tab_2">Table 2</ref>. This task is slightly easier than Experiment I , with more training instances and purely random negatives. It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns (e.g., work-overtime⇔ rest). Again ARC-II beats other models with large margins, while two convolutional sentence models ARC-I and SENNA+MLP come next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment II: Matching A Response to A Tweet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiment III: Paraphrase Identification</head><p>Paraphrase identification aims to determine whether two sentences have the same meaning, a problem considered a touchstone of natural language understanding.  is included to test our methods on matching homogenous objects. Here we use the benchmark MSRP dataset <ref type="bibr" target="#b16">[17]</ref>, which contains 4,076 instances for training and 1,725 for test. We use all the training instances and report the test performance from early stopping. As stated earlier, our model is not specially tailored for modeling synonymy, and generally requires ≥100K instances to work favorably. Nevertheless, our generic matching models still manage to perform reasonably well, achieving an accuracy and F1 score close to the best performer in 2008 based on hand-crafted features <ref type="bibr" target="#b16">[17]</ref>, but still significantly lower than the state-of-the-art (76.8%/83.6%), achieved with unfolding-RAE and other features designed for this task <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussions</head><p>ARC-II outperforms others significantly when the training instances are relatively abundant (as in Experiment I &amp; II). Its superiority over ARC-I, however, is less salient when the sentences have deep grammatical structures and the matching relies less on the local matching patterns, as in Experiment-I. This therefore raises the interesting question about how to balance the representation of matching and the representations of objects, and whether we can guide the learning process through something like curriculum learning <ref type="bibr" target="#b3">[4]</ref>.</p><p>As another important observation, convolutional models (ARC-I &amp; II, SENNA+MLP) perform favorably over bag-of-words models, indicating the importance of utilizing sequential structures in understanding and matching sentences. Quite interestingly, as shown by our other experiments, ARC-I and ARC-II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order (with around 60% accuracy for both). It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks.</p><p>We noticed that simple sum of embedding learned via Word2Vec <ref type="bibr" target="#b13">[14]</ref> yields reasonably good results on all three tasks. We hypothesize that the Word2Vec embedding is trained in such a way that the vector summation can act as a simple composition, and hence retains a fair amount of meaning in the short text segment. This is in contrast with other bag-of-words models like DEEPMATCH <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>, with few exceptions like <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>. When dealing with language objects, most methods still focus on seeking vectorial representations in a common latent space, and calculating the matching score with inner product <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. Few work has been done on building a deep architecture on the interaction space for texts-pairs, but it is largely based on a bag-of-words representation of text <ref type="bibr" target="#b12">[13]</ref>.</p><p>Our models are related to the long thread of work on sentence representation. Aside from the models with recursive nature <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19]</ref> (as discussed in Section 2.1), it is fairly common practice to use the sum of word-embedding to represent a short-text, mostly for classification <ref type="bibr" target="#b21">[22]</ref>. There is very little work on convolutional modeling of language. In addition to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>, there is a very recent model on sentence representation with dynamic convolutional neural network <ref type="bibr" target="#b8">[9]</ref>. This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map, tailored for classification problems with modest sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose deep convolutional architectures for matching natural language sentences, which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching. Empirical study shows our models can outperform competitors on a variety of matching tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The over all architecture of the convolutional sentence model. A box with dashed lines indicates all-zero padding turned off by the gating function (see top of Page 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The cat example, where in the convolution layer, gray color indicates less confidence in composition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture-I for matching two sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Architecture-II (ARC-II) of convolutional matching model 3.3 Some Analysis on ARC-II Order preserving in 2D-pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>ARC-I as a special case of ARC-II. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Tweet Matching. It is hard to find a job, better start polishing your resume. We hold out 300K original (tweet, response) pairs and test the matching model on their ability to pick the original response from four random negatives, with results reported in</figDesc><table><row><cell>We trained our model with 4.5 million original (tweet, response)</cell></row><row><cell>pairs collected from Weibo, a major Chinese microblog service</cell></row><row><cell>[26]. Compared to Experiment I, the writing style is obviously</cell></row><row><cell>more free and informal. For each positive pair, we find ten ran-</cell></row><row><cell>dom responses as negative examples, rendering 45 million triples</cell></row><row><cell>for training. One example (translated to English) is given below, with S X standing for the tweet, S + Y the original response, and S − Y the randomly selected response: S X : Damn, I have to work overtime</cell></row><row><cell>this weekend!</cell></row><row><cell>S + Y : Try to have some rest buddy.</cell></row><row><cell>S − Y :</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The results on Paraphrase.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our other experiments suggest that the performance can be further increased with wider windows.<ref type="bibr" target="#b2">3</ref> Code from: http://nlp.stanford.edu/˜socherr/classifyParaphrases.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We select from a random set the clauses that have 0.7∼0.8 cosine similarity with the original. The dataset and more information can be found from http://www.noahlab.com.hk/technology/Learning2Match.html<ref type="bibr" target="#b4">5</ref> Actually ARC-II can achieve 74+% accuracy with random negatives.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradourand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, Baltimore and USA</title>
		<meeting>ACL, Baltimore and USA</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech and time series. The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deep architecture for matching short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTER-SPEECH</title>
		<meeting>INTER-SPEECH</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Paraphrase identification with lexico-syntactic graph subsumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FLAIRS Conference</title>
		<meeting>FLAIRS Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On dataless hierarchical text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling semantic relevance for question-answer pairs in web social communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning bilinear model for matching queries and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2519" to="2548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Retrieval models for question and answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiwoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR &apos;08</title>
		<meeting>SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
