<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Extractive and Abstractive Neural Document Summarization with Transformer Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
							<email>1jonathan.pilault@elementai.com</email>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">École Polytechnique de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Montréal Institute for Learning Algorithms</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">École Polytechnique de Montréal</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">CIFAR AI Chair</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Extractive and Abstractive Neural Document Summarization with Transformer Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Note: The abstract above was not written by the au-thors, it was generated by one of the models presented in this paper based on an earlier draft of this paper.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We demonstrate that Transformer language models are extremely promising at summarizing long texts, and provide a new approach to deep summarization that can be used to generate more "abstractive" summaries. We show that our approach produces more abstractive summaries than state-of-the-art methods without a copy mechanism. We provide an application to text summarization of the arXiv and PubMed datasets, and show that our model outperforms other popular summarization techniques. We also discuss a simple neural extractive model based on pointers networks trained on documents and their salient sentences. We show that this model can be used to augment Transformer language models to generate better summarization results. Note: The abstract above was generated by one of the models presented in this paper, as a summary of this paper.</p><p>⇤Equal contribution, order determined by coin flip</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We demonstrate that Transformer language models are extremely promising at summarizing long texts, and provide a new approach to deep summarization that can be used to generate more "abstractive" summaries. We show that our approach produces more abstractive summaries than state-of-the-art methods without a copy mechanism. We provide an application to text summarization of the arXiv and PubMed datasets, and show that our model outperforms other popular summarization techniques. We also discuss a simple neural extractive model based on pointers networks trained on documents and their salient sentences. We show that this model can be used to augment Transformer language models to generate better summarization results. Note: The abstract above was generated by one of the models presented in this paper, as a summary of this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Language models (LMs) are trained to estimate the joint probability of an arbitrary sequence of words or characters using a large corpus of text. They typically factorize the joint distribution of tokens p(x 1 , x 2 . . . x n ) into a product of conditional probabilities n i p(x i |x &lt;i ). It is possible to use ngram based models to estimate these conditional probabilities via counts, relying on Markovian assumptions. However, Markovian assumptions and the curse of dimensionality make it harder for n-gram LMs to model long range dependencies and learn smooth functions that can learn similarities between words in the vocabulary. This has led to a preference for recurrent or feed-forward neural language models <ref type="bibr" target="#b1">(Bengio et al. 2003;</ref><ref type="bibr" target="#b20">Mikolov et al. 2010)</ref> in recent years due to to their ability to learn expressive conditional probability distributions <ref type="bibr" target="#b24">(Radford et al. 2019)</ref>.</p><p>The sequence-to-sequence (seq2seq) paradigm <ref type="bibr" target="#b30">(Sutskever, Vinyals, and Le 2014)</ref> uses language models that learn the conditional probability of one sequence given another. Here, a language model serves as a "decoder" that is typically conditioned on a representation of an input sequence produced by an encoder neural network. These types of encoder-decoder architectures have been particularly successful when applied to problems such as machine translation <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref> 1 Introduction</p><p>Language models (LMs) are trained to estimate the joint probability of an arbitrary sequence of words or characters using a large corpus of text. They typically factorize the joint distribution of tokens p(x1,x2 ...xn) into a product of conditional probabilities Qn i p(xi|x&lt;i). It is possible to use n-gram based models to estimate these conditional probabilities via counts, relying on Markovian assumptions. However, Markovian assumptions and the curse of dimensionality make it harder for n-gram LMs to model long range dependencies and learn smooth functions that can learn similarities between words in the vocabulary. This has led to a preference for recurrent or feed-forward neural language models <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b20">Mikolov et al., 2010)</ref> in recent years due to to their ability to learn expressive conditional probability distributions <ref type="bibr">(Merity et al., 2017;</ref><ref type="bibr" target="#b24">Radford et al., 2019)</ref>.</p><p>The sequence-to-sequence (seq2seq) paradigm <ref type="bibr" target="#b30">(Sutskever et al., 2014)</ref> uses language models that learn the conditional probability of one sequence given another. Here, a language model serves as a "decoder" that is typically conditioned on a representation of an input sequence produced by an encoder neural network. These types of encoder-decoder architectures have been particularly successful when applied to problems such as machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> and abstractive summarization <ref type="bibr" target="#b25">(Rush et al., 2015)</ref>. The encoder and conditional decoder language models are often paramaterized as recurrent neural networks (RNNs). Attention mechanisms <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> are used in the decoder to provide more informative conditioning on the representations produced by the encoder and to ease gradient flow into the encoder. RNNs however, are limited by their sequential nature, making them 1) difficult to optimize and learn for long sequences with long range dependencies <ref type="bibr" target="#b14">(Hochreiter, 1998;</ref><ref type="bibr" target="#b23">Pascanu et al., 2013)</ref>, and 2) hard to parallelize on modern hardware like GPUs, limiting their scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs) are trained to estimate the joint probability of an arbitrary sequence of words or characters using a large corpus of text. They typically factorize the joint distribution of tokens p(x1,x2 ...xn) into a product of conditional probabilities Qn i p(xi|x&lt;i). It is possible to use n-gram based models to estimate these conditional probabilities via counts, relying on Markovian assumptions. However, Markovian assumptions and the curse of dimensionality make it harder for n-gram LMs to model long range dependencies and learn smooth functions that can learn similarities between words in the vocabulary. This has led to a preference for recurrent or feed-forward neural language models <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b20">Mikolov et al., 2010)</ref> in recent years due to to their ability to learn expressive conditional probability distributions <ref type="bibr">(Merity et al., 2017;</ref><ref type="bibr" target="#b24">Radford et al., 2019)</ref>.</p><p>The sequence-to-sequence (seq2seq) paradigm <ref type="bibr" target="#b30">(Sutskever et al., 2014)</ref> uses language models that learn the conditional probability of one sequence given another. Here, a language model serves as a "decoder" that is typically conditioned on a representation of an input sequence produced by an encoder neural network. These types of encoder-decoder architectures have been particularly successful when applied to problems such as machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> and abstractive summarization <ref type="bibr" target="#b25">(Rush et al., 2015)</ref>. The encoder and conditional decoder language models are often paramaterized as recurrent neural networks (RNNs). Attention mechanisms <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> are used in the decoder to provide more informative conditioning on the representations produced by the encoder and to ease gradient flow into the encoder. RNNs however, are limited by their sequential nature, making them 1) difficult to optimize and learn for long sequences with long range dependencies <ref type="bibr" target="#b14">(Hochreiter, 1998;</ref><ref type="bibr" target="#b23">Pascanu et al., 2013)</ref>, and 2) hard to parallelize on modern hardware like GPUs, limiting their scalability.  First, a sentence pointer network extracts important sentences from the paper. Next, these sentences are provided along with the whole scientific article to be arranged in the following order: Introduction, extracted Sentences, abstract &amp; the rest of the paper. A transformer language model is trained on articles organized in this format. During inference, the introduction and the extracted sentences are given to the language model as context to generate a summary. In domains like news and patent documents, the introduction is replaced by the entire document. and abstractive summarization <ref type="bibr" target="#b25">(Rush, Chopra, and Weston 2015)</ref>. The encoder and conditional decoder language models are often parameterized as recurrent neural networks (RNNs). Attention mechanisms <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref> are used in the decoder to provide more informative conditioning on the representations produced by the encoder and to ease gradient flow into the encoder. RNNs however, are limited by their sequential nature, making them 1) difficult to optimize and learn for long sequences with long range dependencies <ref type="bibr" target="#b14">(Hochreiter 1998;</ref><ref type="bibr" target="#b23">Pascanu, Mikolov, and Bengio 2013)</ref>, and 2) hard to parallelize on modern hardware like GPUs, limiting their scalability.</p><p>There has therefore been a recent shift towards feedforward architectures for sequential data, such as convolutional models <ref type="bibr" target="#b30">Van Den Oord et al. 2016;</ref><ref type="bibr" target="#b7">Gehring et al. 2017)</ref> or fully attentive models popularized by architectures known as transformers <ref type="bibr" target="#b32">(Vaswani et al. 2017</ref>). These techniques have a logarithmic or constant path length (as opposed to linear in RNNs) between a network's output and any of its inputs, making gradient flow much easier, thereby opening up the possibility of learning very long term dependencies.</p><p>The abstractive summarization of news or scientific papers typically requires encoding and generating hundreds or thousands of words. Recent work by <ref type="bibr" target="#b24">(Radford et al. 2019</ref>) (GPT-2) has demonstrated that transformers with a large receptive field and trained on a lot of data yield language models that are capable of capturing long range dependencies.</p><p>If one is interested in creating a coherent, high-quality summary of long documents, such GPT-like architectures possess many desirable properties. Their results also show that unconditional language models can implicitly learn to perform summarization or machine translation as a consequence of the data on which it is trained. If the data is formatted sequentially into different aspects of a document (introduction, body, summary), each divided by "tl;dr", the model can be coaxed to generate one of these aspects. For example, the model can be made to solve a summarization task by presenting it similarly formatted data at test time; i.e. a document's introduction and body followed by "tl;dr" that will generate an abstract from a language model conditioned on this context.</p><p>In this work, we take this idea a step further by doing away with the sequence-to-sequence paradigm and formatting data for abstractive summarization in a manner that transformer language models can make use of all of the available data present in the documents and their summaries (akin to language model pre-training on mono-lingual data in machine translation <ref type="bibr" target="#b12">(Gulcehre et al. 2015)</ref>). Specifically, we use a single GPT-like Transformer LM (TLM) trained on documents followed by their summaries. During inference, we generate from the LM, conditioned on the document (see <ref type="figure" target="#fig_1">figure 1</ref>). Unlike most previous approaches to neural abstractive summarization, we do not use a seq2seq formulation with an explicit encoder and decoder for word generation. We split the task in two: an extractive step and an abstractive step <ref type="bibr" target="#b2">(Chen and Bansal 2018;</ref><ref type="bibr" target="#b8">Gehrmann, Deng, and Rush 2018)</ref>. To deal with extremely long documents that exceed several thousand words, we first perform sentence extraction using two different hierarchical document models -one based on pointer networks <ref type="bibr" target="#b33">(Vinyals, Fortunato, and Jaitly 2015)</ref>, similar to the variant proposed in <ref type="bibr" target="#b2">(Chen and Bansal 2018)</ref> and the other based on a sentence classifier <ref type="bibr" target="#b21">(Nallapati, Zhai, and Zhou 2017)</ref>. This extracts important sentences from the document (described in section ) that can be used to better condition the transformer LM on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results.</p><p>The contributions of this work are two fold:</p><p>• We demonstrate that transformer language models are surprisingly effective at summarizing long scientific articles and outperform typical seq2seq approaches, even without a copy mechanism.</p><p>• We show that our approach produces more "abstractive" summaries compared to prior work that employs a copy mechanism (See, Liu, and Manning 2017) while still achieving higher ROUGE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Automatic summarization systems seek to condense the size of a piece of text while preserving most of its important information content and meaning. The earliest attempts at automatic summarization focused on extractive techniques, which find words or sentences in a document that capture its most salient content. In the past, various similarity scores based on specific sentence features (keywords, position, length, frequency, linguistic) and metrics (structure-based, vector-based and graph-based) were employed to estimate salience <ref type="bibr" target="#b29">(Steinberger and Jezek 2004;</ref><ref type="bibr" target="#b6">Erkan and Radev 2004)</ref> between a sentence in a document and its reference summary. More recently, with advances in distributed representations of words, phrases and sentences, researchers have proposed to use these to compute similarity scores. Such techniques were further refined by <ref type="bibr" target="#b22">(Nallapati, Zhou, and Ma 2016;</ref><ref type="bibr" target="#b3">Cheng and Lapata 2016;</ref><ref type="bibr"></ref> Chen and Bansal 2018) with encoder-decoder architectures -the representations learned by the encoder are used to choose the most salient sentences. <ref type="bibr" target="#b3">(Cheng and Lapata 2016)</ref> and <ref type="bibr" target="#b22">(Nallapati, Zhou, and Ma 2016)</ref> trained encoder-decoder neural networks as a binary classifier to determine if each sentence in a document should belong to the extractive summary or not. ) also present an alternative that can pick an unordered set of sentences from the source document to assemble an extractive summary. (Chen and Bansal 2018) use a pointer network <ref type="bibr" target="#b33">(Vinyals, Fortunato, and Jaitly 2015)</ref> to sequentially pick sentences from the document that comprise its extractive summary. Human summarizers have four common characteristics. They are able to (1) interpret a source document, (2) prioritize the most important parts of the input text, (3) paraphrase key concepts into coherent paragraphs and (4) generate diverse output summaries. While extractive methods are arguably well suited for identifying the most relevant information, such techniques may lack the fluency and coherency of human generated summaries. Abstractive summarization has shown the most promise towards addressing points (3) and (4) above. Abstractive generation may produce sentences not seen in the original input document. Motivated by neural network success in machine translation experiments, the attention-based encoder decoder paradigm has recently been widely studied in abstractive summarization <ref type="bibr" target="#b25">(Rush, Chopra, and Weston 2015;</ref><ref type="bibr" target="#b22">Nallapati et al. 2016;</ref><ref type="bibr" target="#b4">Chopra, Auli, and Rush 2016)</ref>. By dynamically accessing the relevant pieces of information based on the hidden states of the decoder during generation of the output sequence, the model revisits the input and attends to important information. The advantages of extractive, abstractive and attention-based models were first combined in <ref type="bibr" target="#b11">(Gu et al. 2016</ref>) with a copy mechanism for out-of-vocabulary words present in the source document. Similarly, <ref type="bibr" target="#b26">(See, Liu, and Manning 2017)</ref> used the attention scores to calculate the probability of generating vs copying a word. A coverage mechanism was also added to penalize the attention score of previously attended words, diminishing the model's tendency to repeat itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework</head><p>Our model comprises two distinct and independently trainable components 1) a hierarchical document representation model that either points to or classifies sentences in a document to build an extractive summary 2) a transformer language model that conditions on the extracted sentences as well as a part of or the entire document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extractive Models</head><p>We describe the two neural extractive models used in this work in this section.</p><p>Hierarchical Seq2seq Sentence Pointer Our extractive model is similar to the sentence pointer architecture developed by <ref type="bibr" target="#b2">(Chen and Bansal 2018)</ref> with the main difference being the choice of encoder. We use a hierarchical bidirectional LSTM encoder with word and sentence level LSTMs while (Chen and Bansal 2018) use a convolutional word level encoder for faster training and inference. The decoder is in both cases is an LSTM.</p><p>The extractive model considers the document as a list of N sentences D = (S 1 , . . . , S N ), and each sentence as a list of tokens. We are given a ground-truth extracted summary of M sentences (S i1 , . . . , S i M ), where the i 1 &lt; . . . &lt; i M are the indices of the extracted sentences. The procedure to determine ground-truth extraction targets are identical to previous work -finding two sentences in the document that have the highest ROUGE score with each sentence in the summary.</p><p>We use an encoder-decoder architecture for this extractor. The encoder has a hierarchical structure that combines a token and sentence-level RNN. First, the "sentence-encoder" or token-level RNN is a bi-directional LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber 1997)</ref> encoding each sentence. The last hidden state of the last layer from the two directions produces sentence embeddings: (s 1 , . . . , s N ), where N is the number of sentences in the document. The sentence-level LSTM or the "document encoder", another bi-directional LSTM, encodes this sequence of sentence embeddings to produce document representations:</p><formula xml:id="formula_0">(d 1 , . . . , d N ).</formula><p>The decoder is an autoregressive LSTM taking the sentence-level LSTM hidden state of the previously extracted sentence as input and predicting the next extracted sentence. Let i t the index of the previous extracted sentence at time step t. The input to the decoder is s it , or a zero vector at time-step t = 0. The decoder's output is computed by an attention mechanism from the decoder's hidden state h t over the document representations (d 1 , . . . , d N ). We used the dot product attention method from <ref type="bibr" target="#b17">(Luong, Pham, and Manning 2015)</ref>. The attention weights a t produce a context vector c t , which is then used to compute an attention aware hidden stateh t . Following the input-feeding approach from <ref type="bibr" target="#b17">(Luong, Pham, and Manning 2015)</ref>, the attention aware hidden statẽ h t is concatenated to the input in the next time step, giving the following recurrence</p><formula xml:id="formula_1">h t = LSTM [s T ith T t−1 ] T , h t−1 , with h t = Wh c t h t , c t = N i=1 a t (i)d i , α t (i) = d T i h t , (1) a t (i) = exp (α t (i)) i exp (α t (i )) , for i = 1..N. (2)</formula><p>The attention weights a t are used as output probability distribution over the document sentences, of the choice for the next extracted sentence. We choose the convention to signal the end of the extraction by putting the same index twice in a row. Thus, the input to the decoder is the following sequence: 0, s i1 , ..., s i M , and the target: i 1 , ..., i M , i M , where M is the length of the ground-truth extracted summary and both sequences have M + 1 elements. The model is trained to minimize the cross-entropy of picking the correct sentence at each decoder time step. At inference, we use beam-search to generate the extracted summary.</p><p>Sentence Classifier As with the pointer network, we use a hierarchical LSTM to encode the document and produce a sequence of sentence representations d 1 , ..., d N where N is the number of sentences in the document. We compute a final document representation as follows:</p><formula xml:id="formula_2">d = tanh b d + W d 1 N N i=1 d i<label>(3)</label></formula><p>where b d and W d are learnable parameters. Finally, the probability of each sentence belonging to the extractive summary is given by:</p><formula xml:id="formula_3">o i =σ W o d i d + b o<label>(4)</label></formula><p>where σ is the sigmoid activation function. The model is trained to minimize the binary cross-entropy loss with respect to the sentences in the gold-extracted summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Details</head><p>The model uses word embeddings of size 300. The token-level LSTM (sentence encoder), sentencelevel LSTM (document encoder) and decoder each have 2 layers of 512 units and a dropout of 0.5 is applied at the output of each intermediate layer. We trained it with Adam, a learning rate 0.001, a weight decay of 10 −5 , and using batch sizes of 32. We evaluate the model every 200 updates, using a patience of 50. At inference, we decode using beam search with a beam size of 4 for the pointer model and pick the k most likely sentences from the sentence classifier, where k is the average number of sentences in the summary across the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Language Models (TLM)</head><p>Instead of formulating abstractive summarization as a seq2seq problem using an encoder-decoder architecture, we only use a single transformer language model that is trained from scratch, with appropriately "formatted" data (see <ref type="figure" target="#fig_1">figure  1</ref>, we also describe the formatting later in this section).</p><p>We use a transformer <ref type="bibr" target="#b32">(Vaswani et al. 2017</ref>) language model (TLM) architecture identical to <ref type="bibr" target="#b24">(Radford et al. 2019)</ref>. Our model has 220M parameters with 20 layers, 768 dimensional embeddings, 3072 dimensional position-wise MLPs and 12 attention heads. The only difference in our architectures (to our knowledge) is that we do not scale weights at initialization. We trained the language model for 5 days on 16 V100 GPUs on a single Nvidia DGX-2 box. We used a linear ramp-up learning rate schedule for the first 40, 000 updates, to maximum learning rate of 2.5 × e −4 followed by a cosine annealing schedule to 0 over the next 200, 000 steps with the Adam optimizer. We used mixed-precision training <ref type="bibr" target="#b19">(Micikevicius et al. 2017</ref>) with a batch size of 256 sequences of 1024 tokens each.</p><p>In order to get an unconditional language model to do abstractive summarization, we can use the fact that LMs are trained by factorizing the joint distribution over words autoregressively. We organized the training data for the LM such that the ground-truth summary follows the information used by the model to generate a system summary. This way, we model the joint distribution of document and summary during training, and sample from the conditional distribution of summary given document at inference.</p><p>When dealing with extremely long documents that may not fit into a single window of tokens seen by a transformer language model, such as an entire scientific article, we use its introduction as a proxy for having enough information to generate an abstract (summary) and use the remainder of the paper as in domain language model training data <ref type="figure" target="#fig_1">(Fig 1)</ref>. In such cases, we organize the arXiv and PubMed datasets as follows: 1) paper introduction 2) extracted sentences from the sentence pointer model 3) abstract 4) rest of the paper. On other datasets, the paper introduction would be the entire document and there would no rest of the paper. This ensures that at inference, we can provide the language model the paper introduction and the extracted sentences as conditioning to generate its abstract. We found that using the ground truth extracted sentences during training and the model extracted sentences at inference performed better than using the model extracted sentences everywhere.</p><p>We use a special token to indicate the start of the summary and use it at test time to signal to the model to start generating the summary. The rest of the article is provided as additional in-domain training data for the LM. The entire dataset is segmented into non-overlapping examples of 1, 024 tokens each. We use "topk" sampling at inference (?; <ref type="bibr" target="#b24">Radford et al. 2019)</ref>, with k = 30 and a softmax temperature of 0.7 to generate summaries. <ref type="figure">Figure 2</ref>: n-gram overlaps between the abstracts generated by different models and the input article on the arXiv dataset. We show in detail which part of the input was copied for our TLM conditioned on intro + extract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>Experimental setup <ref type="bibr">Datasets</ref> We experiment with four different large-scale and long document summarization datasets -arXiv, PubMed <ref type="bibr" target="#b5">(Cohan et al. 2018)</ref>, bigPatent <ref type="bibr" target="#b28">(Sharma, Li, and Wang 2019)</ref> and Newsroom <ref type="bibr" target="#b10">(Grusky, Naaman, and Artzi 2018)</ref>. Statistics are reported in <ref type="table" target="#tab_0">Table 1</ref>. Evaluation We evaluate our method using full-length F-1 ROUGE scores <ref type="bibr" target="#b16">(Lin 2004</ref>) and re-used the code from (Cohan et al. 2018) for this purpose. All ROUGE numbers reported in this work have a 95% confidence interval of at most 0.24.</p><p>Comparison We compare our results to several previously proposed extractive and abstractive models. All prior results reported on the arXiv and Pubmed benchmark are obtained from <ref type="bibr" target="#b5">(Cohan et al. 2018)</ref>. Similarly, prior results for the Big-Patent dataset are obtained from <ref type="bibr" target="#b28">(Sharma, Li, and Wang 2019)</ref> and Newsroom from <ref type="bibr" target="#b10">(Grusky, Naaman, and Artzi 2018)</ref> and <ref type="bibr" target="#b18">(Mendes et al. 2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We present our main results on summarizing arXiv and PubMed papers in tables 2, 4. Our extractive models are able to outperform previous extractive baselines on both the arXiv and Pubmed datasets. Our TLM conditioned on the extractive summary produced by our best extractive model (TLM-I+E (G,M)) outperforms prior abstractive/mixed results on the arXiv, Pubmed and bigPatent datasets, except on ROUGE-L. On Newsroom, we do better than the only other abstractive model (Seq2Seq with attention) by a massive margin and achieve better performance than the pointer generator even on the abstractive and mixed which their model should be better suited to since it has a copy mechanism. The Exconsumm model <ref type="bibr" target="#b18">(Mendes et al. 2019</ref>) however, which is primarily an extractive model does better on this dataset. We suspect the poor ROUGE-L result is due to the absence of a copy mechanism that makes it hard to get exact large ngram matches. <ref type="figure">Figure 2</ref> further supports this hypothesis, it is evident that a model with a copy mechanism is often able to copy even upto 25-grams from the article. Further, (Graham 2015) finds that ROUGE-L is poorly correlated with human judgements when compared to ROUGE-1,2,3. In table 7 and <ref type="table">Table 8</ref>, we present qualitative results of abstracts of notable papers in our field and of our TLM conditioned on the introductions and extracted summaries of a random example from the arXiv test set.  <ref type="figure">I+E (G,G)</ref>). We also experiment with using either the ground-truth extracted sentences (TLM-I+E (G,M)) or the model extracted sentences (TLM-I+E (M,M)) during training and find that latter slightly impairs performance. Finally, <ref type="figure">figure 3</ref>, presents a visualization of the word embeddings learned by our TLM.</p><p>Abstractiveness of generated abstracts <ref type="bibr" target="#b34">(Weber et al. 2018)</ref> argued that state-of-the-art abstractive summarization systems that use a copy mechanism effectively generate the summary by copying over large chunks  Document -A new plan from the government of the Philippines would offer free wireless internet to people across the country while also likely eating into the annual revenue of the nations telecoms. Bloomberg reports that the Philippines government plans to roll-out its free Wi-Fi services to roughly half of the countrys municipalities over the next few months and the country has its sights set on nationwide coverage by the end of 2016. The free wireless internet service will be made available in public areas such as schools, hospitals, airports and parks, and is expected to cost the government roughly $32 million per year. [...] Abstractive -: The government is reportedly considering a nationwide service plan to give free Wi-Fi access to rural areas.</p><p>Mixed -The government of the Philippines is considering a new plan to provide free wireless internet to the nation's largest cities and towns.</p><p>Extractive -The new plan will include free wireless internet to residents across the country while also probably eating into the annual revenue of the country's telecoms. Document -(CBS) -Controversy over a new Microsoft patent has people questioning whether or not the intention has racist undertones. CNET reported that Microsoft has been granted a U.S. patent that will steer pedestrians away from areas that are high in crime. [...] Absractive Summary -The new Microsoft patent claims a device could provide pedestrian navigation directions from a smartphone. Mixed Summary Microsoft won a U.S. patent for a new way to steer pedestrians out of areas that are high in crime from the article, essentially doing "extractive" summarization. Following this work, we measure how much a model copies from the article by counting the proportion of ngrams from the generated abstract that are also found in the article. These statistics measured on the arXiv dataset are presented in <ref type="figure">figure 2</ref>. First, the original abstract and our TLM conditioned on the intro have small and very similar overlap fractions with the original article. A model using a pointing mechanism (we used our own implementation of the model developed by <ref type="bibr" target="#b5">(Cohan et al. 2018</ref>)) 1 copies more than our transformer model, especially for higher n-grams.</p><p>In particular, more than 10% of the 20-grams from the abstracts generated by the pointing model are also found in the article, showing that it tends to copy long sequences of words. On the other hand, our proposed model produces more "abstractive" summaries, demonstrating its ability to paraphrase. Our model tends to copy longer sequences when conditioned on the introduction and the sentences from the extractor. We hypothesize that providing extracted sentences from the article that already contain a lot of words present in the reference abstract, makes the transformer's task easier, by allowing it to copy words and phrases from the extracted sentences. We find empirical evidence of this in <ref type="figure">figure 2</ref>, showing that the majority of n-gram copies come from the extracted sentences. For 5-grams, close to 2/3rd of the words copied are from the extracted sentences. As the number of grams increases to 25-grams, 4/5th of the words copied are from the extracted sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have demonstrated that Transformer language models can generate high-quality summaries of long sequences of text via an extractive step followed by an abstractive step. We quantitatively measure the positive impact of the ex-  tractive step, by comparing it to a abstractive model variant that only sees the input text itself. Our approach outperforms previous extractive and abstractive summarization methods on the arXiv, PubMed and bigPatent datasets and is less prone to copying entire phrases or sentences from the input text. The fluency and coherency of the sample summaries suggests that these models are ready for comprehensive human evaluation studies. As with other problem domains, we have observed that abstractive summaries generated by transformers can generate imaginary content. We ad- Generated abstract for "the attention model paper" (Bahdanau, Cho, and Bengio 2014) -In this paper, we propose a neural machine translation model, which learns to align and translate a source sentence without encoding it into a fixed length vector. The proposed model allows the neural network to align and translate jointly, and in addition, predicts a target word based on the context vectors associated with these source positions. We show that the proposed model achieves significantly improved translation performance than the conventional encoder decoder neural network approach, when the sentences in the training corpus are long . Quantitative analysis reveals that the proposed model finds a linguistically plausible soft alignment between a source sentence and the target sentence. Finally, we compare the performance of the proposed model with the state-of-the-art standard neural network translation models and show that the proposed approach outperforms the conventional approach in terms of both translation performance and the quality of the translations.</p><p>Original abstract -Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.</p><p>Generated abstract for "the GPT-2 paper" (?) -Language models have been successful in creating rich representations of complex systems, including tasks such as sentiment analysis. In this paper we demonstrate their ability to generalize well across domains and tasks. We demonstrate the utility of language models by demonstrating that a variety of tasks can be modeled using language models. We demonstrate the potential of language models in downstream tasks by highlighting the ability of these models to perform a wide range of tasks in a zero shot setting.</p><p>Original abstract -Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset -matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.</p><p>Generated abstract for "the BERT paper" <ref type="bibr" target="#b6">(Devlin et al. 2018</ref>) -We develop an architecture for deep bidirectional language models that allows the representation to fuse the left and the right context of a given token. Our framework uses bidirectional transformer encoder representations that enable the model to fuse the context representations of a token. We show that our architecture achieves state-of-the-art performance on a large suite of tasks, outperforming many systems with taskspecific architectures. Our results are based on a set of eleven task specific tasks that are benchmarks for a large suite of nlp tasks.</p><p>Original abstract -We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p><p>vise that such evaluations should probe multiple aspects of the summarization results including both factual correctness and coherency. We also note that for evaluating the correctness of the summaries of scientific articles and patents one must have highly trained evaluators who are willing to invest significant amounts of time to read the underlying papers and patents. Such studies could therefore require significant investments of resources. We have also presented an upper bound on extractive + abstractive models, by conditioning the abstractive step on gold-extracted sentences. In future work, we are also interested in exploring the possibility of training the extractive and abstractive steps in an end-to-end manner. While we believe that this work is a step forward towards generating more abstractive summaries, it remains an open challenge to develop models that respect the underlying facts of the content being summarized while matching the creative ability of humans to coherently and concisely synthesize summaries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>⇤Equal contribution, order determined by coin flip Preprint. Sent to peer review, May 2019.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Proposed model for abstractive summarization of a scientific article. An older version of this paper is shown as the reference document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics from<ref type="bibr" target="#b28">(Sharma, Li, and Wang 2019)</ref> for the datasets used in this work -The number of document/summary pairs, the ratio of the number of words in the document to the abstract and the number of words in the summary and document.</figDesc><table><row><cell>Dataset arXiv PubMed Newsroom BigPatent</cell><cell>#Documents 215,913 133,215 1,212,726 1,341,362</cell><cell>Comp Ratio 39.8 16.2 43.0 36.4</cell><cell>Sum Len 292.8 6,913.8 Doc Len 214.4 3,224.4 30.4 750.9 116.5 3,572.8</cell></row><row><cell cols="4">Data preprocessing Both our extractive and abstractive models use sub-word units computed using byte pair en-coding (Sennrich, Haddow, and Birch 2015) with 40, 000 replacements. To address memory issues in the sentence pointer network, we only keep 300 sentences per article, and 35 tokens per sentence.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows similar qual-itative examples on the Newsroom dataset. Tables 2, 4 and 5 also provide different train / test settings for our TLM conditioned on extracted sentences. We show a performance upper bound conditioning the Transformer LM on oracle / ground-truth extracted sentences at both train and test time (TLM-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Summarization results on the arXiv dataset. Previous work results from<ref type="bibr" target="#b5">(Cohan et al. 2018</ref>). The following lines are a simple baseline Lead-10 extractor and the pointer and classifier models. Our transformer LMs (TLM) are conditioned either on the Introduction (I) or along with extracted sentences (E) either from ground-truth (G) or model (M) extracts.</figDesc><table><row><cell>Model SumBasic LexRank LSA Seq2Seq Pointer-gen Discourse Lead-10 Sent-CLF Sent-PTR TLM-I TLM-I+E (M,M) TLM-I+E (G,M) Gold Ext TLM-I+E (G,G)</cell><cell>Type Previous Work 1 Ext 29.47 Ext 33.85 10.73 4.54 28.99 ROUGE 2 3 L 6.95 2.36 26.3 Ext 29.91 7.42 3.12 25.67 Abs 29.3 6.00 1.77 25.56 Mix 32.06 9.04 2.15 25.16 Mix 35.80 11.05 3.62 31.80 Our Models Ext 35.52 10.33 3.74 31.44 Ext 34.01 8.71 2.99 30.41 Ext 42.32 15.63 7.49 38.06 Abs 39.65 12.15 4.40 35.76 Mix 41.15 13.98 5.63 37.40 Mix 41.62 14.69 6.16 38.03 Oracle Oracle 44.25 18.17 9.14 35.33 Oracle 46.40 18.15 8.71 42.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Qualitative Results -News articles and our model gener- ated summaries on the NewsRoom dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Summarization results on the PubMed dataset. Previous work results from<ref type="bibr" target="#b5">(Cohan et al. 2018</ref>). The following lines are a simple baseline Lead-10 extractor and the pointer and classifier models. Our transformer LMs (TLM) are conditioned either on the Introduction (I) or along with extracted sentences (E) either from ground-truth (G) or model (M) extracts.</figDesc><table><row><cell>Model SumBasic LexRank LSA Seq2seq Pointer-gen Discourse Lead-10 Sent-CLF Sent-PTR TLM-I TLM-I+E (G,M) Gold Ext TLM-I+E (G,G)</cell><cell>Type Previous Work 1 Ext 37.15 11.36 ROUGE 2 3 5.42 Ext 39.19 13.89 7.27 Ext 33.89 9.93 5.04 Abs 31.55 8.52 7.05 Mix 35.86 10.22 7.60 Mix 38.93 15.37 9.97 Our Models Ext 37.45 14.19 8.26 Ext 45.01 19.91 12.13 41.16 L 33.43 34.59 29.70 27.38 29.69 35.21 34.07 Ext 43.30 17.92 10.67 39.47 Abs 37.06 11.69 5.31 34.27 Mix 42.13 16.27 8.82 39.21 Oracle Oracle 47.76 20.36 11.52 39.19 Oracle 46.32 20.15 11.75 43.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Summarization results on the bigPatent dataset. Previous work results from<ref type="bibr" target="#b28">(Sharma, Li, and Wang 2019)</ref>. Our transformer LMs (TLM) are conditioned on the whole document or additionally with extracted sentences (E) either from ground-truth (G) or model (M) extracts.</figDesc><table><row><cell>Model Lead-3 TextRank SumBasic LexRank RNN-Ext Seq2Seq Pointer-gen Pointer-gen (Cov) Sent-rewriting Gold Ext OracleFrag Sent-CLF Sent-PTR TLM TLM+E (G,M) TLM+E (G,G)</cell><cell>Type Previous Work 1 Ext 31.27 Ext 35.99 11.14 29.60 ROUGE 2 L 8.75 26.18 Ext 27.44 7.08 23.66 Ext 35.57 10.47 29.03 Ext 34.63 10.62 29.43 Abs 28.74 7.87 24.66 Mix 30.59 10.01 25.65 Mix 33.14 11.63 28.55 Mix 37.12 11.87 32.45 Oracle Oracle 43.56 16.91 36.52 Oracle 91.85 78.66 91.85 Our Models Ext 36.20 10.99 31.83 Ext 34.21 10.78 30.07 Abs 36.41 11.38 30.88 Mix 38.65 12.31 34.09 Oracle 39.99 13.79 35.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Summarization results on the Newsroom dataset.</figDesc><table><row><cell cols="10">Previous work results from (Grusky, Naaman, and Artzi 2018) and (Mendes et al. 2019).</cell></row><row><cell>Model</cell><cell>Type</cell><cell></cell><cell cols="2">Extractive</cell><cell></cell><cell>Mixed</cell><cell></cell><cell cols="2">Abstractive</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ROUGE</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>L</cell><cell>1</cell><cell>2</cell><cell>L</cell><cell>1</cell><cell>2</cell><cell>L</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Previous Work</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seq2Seq</cell><cell>Abs</cell><cell cols="3">6.1 0.2 5.4</cell><cell cols="3">5.7 0.2 5.1</cell><cell cols="2">6.2 1.1 5.7</cell></row><row><cell>TextRank</cell><cell>Ext</cell><cell cols="8">32.4 19.7 28.7 22.3 7.9 17.7 13.5 1.9 10.5</cell></row><row><cell>Pointer-gen</cell><cell>Mix</cell><cell cols="8">39.1 27.9 36.2 25.5 11.0 21.1 14.7 2.3 11.4</cell></row><row><cell>Lead-3</cell><cell>Ext</cell><cell cols="8">53.0 49.0 52.4 25.1 12.9 22.1 13.7 2.4 11.2</cell></row><row><cell>Exconsumm</cell><cell>Mix</cell><cell cols="8">68.4 62.9 67.3 31.7 16.1 27.0 17.1 3.1 14.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Our Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sent-CLF</cell><cell>Ext</cell><cell cols="8">53.0 47.0 52.1 26.8 12.6 23.6 15.4 2.7 12.8</cell></row><row><cell>Sent-PTR</cell><cell>Ext</cell><cell cols="8">60.7 55.2 59.7 28.9 14.1 25.1 15.9 2.8 13.0</cell></row><row><cell>TLM</cell><cell>Abs</cell><cell cols="8">49.8 39.7 47.4 27.1 11.6 22.8 20.4 6.9 17.1</cell></row><row><cell>TLM+E (G,M)</cell><cell>Mix</cell><cell cols="8">63.3 57.3 61.8 31.9 16.6 27.4 20.1 6.5 16.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Oracle</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gold Ext</cell><cell>Oracle</cell><cell cols="8">68.1 64.5 67.3 40.8 24.6 34.2 21.9 5.2 16.3</cell></row><row><cell>TLM+E (G,G)</cell><cell>Oracle</cell><cell cols="8">78.8 74.0 77.8 38.6 22.0 33.6 24.5 9.6 20.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Qualitative Results -Generated abstracts of select papers using our Intro Only TLM.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This model achieved the following ROUGE-1, 2, 3 and L on the arXiv dataset: 41.33, 14.73, 6.80, 36.34   </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Samples from the arXiv test set</head><p>Samples from the TLM-I and TLM-I+E (G,M) models on a random example from the arXiv test set. <ref type="table">Table 8</ref>: Qualitative Results -Generated abstracts from our models on a random example from the test set of <ref type="bibr" target="#b5">(Cohan et al. 2018)</ref> TLM conditioned on intro -deep convolutional neural networks ( cnns ) have created new benchmarks in the object recognition challenge . most of these benchmark datasets contain thousands of images of interest . in addition to these datasets , several standard data -sets , such as pascal voc 2012 and the ms coco , have been used in conjunction with the same network in order to improve the object detection task . in this paper , we present a method for object localization using the deep convolutional neural networks trained for the simpler task of image -level classification . we propose localization candidates ( or bounding boxes ) which maximize classification scores for a class of interest . we explore the beam search strategy where the number of candidate bounding boxes are restricted as we traverse from the root node towards the leaf nodes . we then gradually explore smaller and smaller regions of interest till a point is reached where the classifier is not able to discriminate among the classes of interest . the detection candidates are organized in a search tree , the root node being represented by the entire test image . we then gradually explore smaller and smaller regions of interest till a point is reached where the classifier is not able to distinguish among the classes of interest . TLM conditioned on intro + pointer network extracted sentences -deep convolutional neural networks ( cnns ) are widely used in image recognition applications . however , they face a number of challenges . first , cnn models are computationally expensive to train and to perform . second , they are not easy to implement and can not exploit global features . to alleviate these issues , we present a method that leverages the correlation between semantic information present in feature maps and localization of an object of interest . the method leverages feature map activations for object localization . we demonstrate that this method is significantly faster than state -of -the -art in localization accuracy by a significant margin of up to 8 map on two standard data -sets with complex scenes , pascal voc 2012 and the much larger ms coco . Ground truth abstract -object localization is an important computer vision problem with a variety of applications . the lack of large scale object -level annotations and the relative abundance of image -level labels makes a compelling case for weak supervision in the object localization task . deep convolutional neural networks are a class of state-of-the-art methods for the related problem of object recognition . in this paper , we describe a novel object localization algorithm which uses classification networks trained on only image labels . this weakly supervised method leverages local spatial and semantic patterns captured in the convolutional layers of classification networks . we propose an efficient beam search based approach to detect and localize multiple objects in images . the proposed method significantly outperforms the state-of-the-art in standard object localization data -sets with a 8 point increase in map scores .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T-SNE of learned word embeddings</head><p>We visualize the word embeddings learned by our TLM model using t-sne. We find that words that are often associated with computer science are clustered in a different part of space when compared to words associated with physics. We use the arXiv REST API to find the submission category of each paper in the training set and then find the ∼300 most representative words for each category, using TF-IDF scores and plot them. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11080</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07252</idno>
		<title level="m">Neural summarization by extracting sentences and words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CoRR abs/1804.05685</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Lexrank: Graph-based lexical centrality as salience in text summarization</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10792</idno>
		<title level="m">Bottom-up abstractive summarization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Re-evaluating automatic summarization with bleu and 192 shades of rouge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="128" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11283</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Looking for a few good metrics: Automatic summarization evaluation-how many samples are enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NTCIR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02020</idno>
		<title level="m">Jointly extracting and compressing documents with summary state representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Mixed precision training</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
	</analytic>
	<monogr>
		<title level="m">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Classify or select: Neural architectures for extractive document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03741</idno>
		<title level="m">Bigpatent: A largescale dataset for abstractive and coherent summarization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using latent semantic analysis in text summarization and summary evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jezek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ISIM</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="93" to="100" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>Wavenet: A generative model for raw audio. SSW 125</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1606" to="1618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CoRR abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Controlling decoding for more abstractive summaries with copy-based networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07038</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
