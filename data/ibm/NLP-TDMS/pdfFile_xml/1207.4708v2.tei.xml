<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<region>Alberta</region>
									<country>Canada Yavar Naddaf</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
							<email>veness@cs.ualberta.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Empirical Results Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>British Columbia</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
							<email>bowling@cs.ualberta.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<region>Alberta</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Journal of Artificial Intelligence Research</title>
						<imprint>
							<biblScope unit="volume">47</biblScope>
							<biblScope unit="page" from="253" to="279"/>
							<date type="published" when="2013">2013</date>
						</imprint>
					</monogr>
					<note type="submission">Submitted 02/13; published 06/13</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A longstanding goal of artificial intelligence is the development of algorithms capable of general competency in a variety of tasks and domains without the need for domain-specific tailoring. To this end, different theoretical frameworks have been proposed to formalize the notion of "big" artificial intelligence (e.g., <ref type="bibr" target="#b22">Russell, 1997;</ref><ref type="bibr" target="#b11">Hutter, 2005;</ref><ref type="bibr" target="#b14">Legg, 2008)</ref>. Similar ideas have been developed around the theme of lifelong learning: learning a reusable, highlevel understanding of the world from raw sensory data <ref type="bibr" target="#b28">(Thrun &amp; Mitchell, 1995;</ref><ref type="bibr" target="#b21">Pierce &amp; Kuipers, 1997;</ref><ref type="bibr" target="#b25">Stober &amp; Kuipers, 2008;</ref><ref type="bibr" target="#b27">Sutton et al., 2011)</ref>. The growing interest in competitions such as the General Game Playing competition <ref type="bibr" target="#b6">(Genesereth, Love, &amp; Pell, 2005)</ref>, Reinforcement Learning competition <ref type="bibr" target="#b31">(Whiteson, Tanner, &amp; White, 2010)</ref>, and the International Planning competition <ref type="bibr" target="#b3">(Coles et al., 2012)</ref> also suggests the artificial intelligence community's desire for the emergence of algorithms that provide general competency.</p><p>Designing generally competent agents raises the question of how to best evaluate them. Empirically evaluating general competency on a handful of parametrized benchmark problems is, by definition, flawed. Such an evaluation is prone to method overfitting <ref type="bibr" target="#b30">(Whiteson, Tanner, Taylor, &amp; Stone, 2011)</ref> and discounts the amount of expert effort necessary to transfer the algorithm to new domains. Ideally, the algorithm should be compared across and 210 pixels high, with a 128-colour palette; 18 "actions" can be input to the game via a digital joystick: three positions of the joystick for each axis, plus a single button. The Atari 2600 hardware limits the possible complexity of games, which we believe strikes the perfect balance: a challenging platform offering conceivable near-term advancements in learning, modelling, and planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interface</head><p>ALE is built on top of Stella 1 , an open-source Atari 2600 emulator. It allows the user to interface with the Atari 2600 by receiving joystick motions, sending screen and/or RAM information, and emulating the platform. ALE also provides a game-handling layer which transforms each game into a standard reinforcement learning problem by identifying the accumulated score and whether the game has ended. By default, each observation consists of a single game screen (frame): a 2D array of 7-bit pixels, 160 pixels wide by 210 pixels high. The action space consists of the 18 discrete actions defined by the joystick controller. The game-handling layer also specifies the minimal set of actions needed to play a particular game, although none of the results in this paper make use of this information. When running in real-time, the simulator generates 60 frames per second, and at full speed emulates up to 6000 frames per second. The reward at each time-step is defined on a game by game basis, typically by taking the difference in score or points between frames. An episode begins on the first frame after a reset command is issued, and terminates when the game ends. The game-handling layer also offers the ability to end the episode after a predefined number of frames 2 . The user therefore has access to several dozen games through a single common interface, and adding support for new games is relatively straightforward.</p><p>ALE further provides the functionality to save and restore the state of the emulator. When issued a save-state command, ALE saves all the relevant data about the current game, including the contents of the RAM, registers, and address counters. The restorestate command similarly resets the game to a previously saved state. This allows the use of ALE as a generative model to study topics such as planning and model-based reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Source Code</head><p>ALE is released as free, open-source software under the terms of the GNU General Public License. The latest version of the source code is publicly available at:</p><p>http://arcadelearningenvironment.org</p><p>The source code for the agents used in the benchmark experiments below is also available on the publication page for this article on the same website. While ALE itself is written in C++, a variety of interfaces are available that allow users to interact with ALE in the programming language of their choice. Support for new games is easily added by implementing a derived class representing the game's particular reward and termination functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Benchmark Results</head><p>Planning and reinforcement learning are two different AI problem formulations that can naturally be investigated within the ALE framework. Our purpose in presenting benchmark results for both of these formulations is two-fold. First, these results provide a baseline performance for traditional techniques, establishing a point of comparison with future, more advanced, approaches. Second, in describing these results we illustrate our proposed methodology for doing empirical validation with ALE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reinforcement Learning</head><p>We begin by providing benchmark results using SARSA(λ), a traditional technique for model-free reinforcement learning. Note that in the reinforcement learning setting, the agent does not have access to a model of the game dynamics. At each time step, the agent selects an action and receives a reward and an observation, and the agent's aim is to maximize its accumulated reward. In these experiments, we augmented the SARSA(λ) algorithm with linear function approximation, replacing traces, and -greedy exploration. A detailed explanation of SARSA(λ) and its extensions can be found in the work of <ref type="bibr" target="#b26">Sutton and Barto (1998)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Feature Construction</head><p>In our approach to the reinforcement learning setting, the most important design issue is the choice of features to use with linear function approximation. We ran experiments using five different sets of features, which we now briefly explain; a complete description of these feature sets is given in Appendix A. Of these sets of features, BASS, DISCO and RAM were originally introduced by Naddaf (2010), while the rest are novel.</p><p>Basic. The Basic method, derived from <ref type="bibr">Naddaf's BASS (2010)</ref>, encodes the presence of colours on the Atari 2600 screen. The Basic method first removes the image background by storing the frequency of colours at each pixel location within a histogram. Each game background is precomputed offline, using 18,000 observations collected from sample trajectories. The sample trajectories are generated by following a human-provided trajectory for a random number of steps and subsequently selecting actions uniformly at random. The screen is then divided into 16 × 14 tiles. Basic generates one binary feature for each of the 128 colours and each of the tiles, giving a total of 28,672 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BASS.</head><p>The BASS method behaves identically to the Basic method save in two respects. First, BASS augments the Basic feature set with pairwise combinations of its features. Second, BASS uses a smaller, 8-colour encoding to ensure that the number of pairwise combinations remains tractable.</p><p>DISCO. The DISCO method aims to detect objects within the Atari 2600 screen. To do so, it first preprocesses 36,000 observations from sample trajectories generated as in the Basic method. DISCO also performs the background subtraction steps as in Basic and BASS. Extracted objects are then labelled into classes. During the actual training, DISCO infers the class label of detected objects and encodes their position and velocity using tile coding <ref type="bibr" target="#b26">(Sutton &amp; Barto, 1998)</ref>.</p><p>LSH. The LSH method maps raw Atari 2600 screens into a small set of binary features using Locally Sensitive Hashing <ref type="bibr" target="#b7">(Gionis, Indyk, &amp; Motwani, 1999)</ref>. The screens are mapped using random projections, such that visually similar screens are more likely to generate the same features.</p><p>RAM. The RAM method works on an entirely different observation space than the other four methods. Rather than receiving in Atari 2600 screen as an observation, it directly observes the Atari 2600's 1024 bits of memory. Each bit of RAM is provided as a binary feature together with the pairwise logical-AND of every pair of bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Evaluation Methodology</head><p>We first constructed two sets of games, one for training and the other for testing. We used the training games for parameter tuning as well as design refinements, and the testing games for the final evaluation of our methods. Our training set consisted of five games: Asterix, Beam Rider, Freeway, Seaquest and Space Invaders. The parameter search involved finding suitable values for the parameters to the SARSA(λ) algorithm, i.e. the learning rate, exploration rate, discount factor, and the decay rate λ. We also searched the space of feature generation parameters, for example the abstraction level for the BASS agent. The results of our parameter search are summarized in Appendix C. Our testing set was constructed by choosing semi-randomly from the 381 games listed on Wikipedia 3 at the time of writing. Of these games, 123 games have their own Wikipedia page, have a single player mode, are not adult-themed or prototypes, and can be emulated in ALE. From this list, 50 games were chosen at random to form the test set.</p><p>Evaluation of each method on each game was performed as follows. An episode starts on the frame that follows the reset command, and terminates when the end-of-game condition is detected or after 5 minutes of real-time play (18,000 frames), whichever comes first. During an episode, the agent acts every 5 frames, or equivalently 12 times per second of gameplay. A reinforcement learning trial consists of 5,000 training episodes, followed by 500 evaluation episodes during which no learning takes place. The agent's performance is  measured as the average score achieved during the evaluation episodes. For each game, we report our methods' average performance across 30 trials. For purposes of comparison, we also provide performance measures for three simple baseline agents -Random, Const and Perturb -as well as the performance of a non-expert human player. The Random agent picks a random action on every frame. The Const agent selects a single fixed action throughout an episode; our results reflect the highest score achieved by any single action within each game. The Perturb agent selects a fixed action with probability 0.95 and otherwise acts uniformly randomly; for each game, we report the performance of the best policy of this type. Additionally, we provide human player results that report the five-episode average score obtained by a beginner (who had never previously played Atari 2600 games) playing selected games. Our aim is not to provide exhaustive or accurate human-level benchmarks, which would be beyond the scope of this paper, but rather to offer insight into the performance level achieved by our agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Results</head><p>A complete report of our reinforcement learning results is given in Appendix D. <ref type="table" target="#tab_1">Table 1</ref> shows a small subset of results from two training games and three test games. In 40 games out of 55, learning agents perform better than the baseline agents. In some games, e.g., Double Dunk, Journey Escape and Tennis, the no-action baseline policy performs the best by essentially refusing to play and thus incurring no negative reward. Within the 40 games for which learning occurs, the BASS method generally performs best. DISCO performed particularly poorly compared to the other learning methods. The RAM-based agent, surprisingly, did not outperform image-based methods, despite building its representation from raw game state. It appears the screen image carries structural information that is not easily extracted from the RAM bits.</p><p>Our reinforcement learning results show that while some learning progress is already possible in Atari 2600 games, much more work remains to be done. Different methods perform well on different games, and no single method performs well on all games. Some games are particularly challenging. For example, platformers such as Montezuma's Revenge seem to require high-level planning far beyond what our current, domain-independent methods provide. Tennis requires fairly elaborate behaviour before observing any positive reward, but simple behaviour can avoid negative rewards. Our results also highlight the value of ALE as an experimental methodology. For example, the DISCO approach performs rea-sonably well on the training set, but suffers a dramatic reduction in performance when applied to unseen games. This suggests the method is less robust than the other methods we studied. After a quick glance at the full table of results in Appendix D, it is clear that summarizing results across such varied domains needs further attention; we explore this issue further in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Planning</head><p>The Arcade Learning Environment can naturally be used to study planning techniques by using the emulator itself as a generative model. Initially it may seem that allowing the agent to plan into the future with a perfect model trivializes the problem. However, this is not the case: the size of state space in Atari 2600 games prohibits exhaustive search. Eighteen different actions are available at every frame; at 60 frames per second, looking ahead one second requires 18 60 ≈ 10 75 simulation steps. Furthermore, rewards are often sparsely distributed, which causes significant horizon effects in many search algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Search Methods</head><p>We now provide benchmark ALE results for two traditional search methods. Each method was applied online to select an action at every time step (every five frames) until the game was over.</p><p>Breadth-first Search. Our first approach builds a search tree in a breadth-first fashion until a node limit is reached. Once the tree is expanded, node values are updated recursively from the bottom of the tree to the root. The agent then selects the action corresponding to the branch with the highest discounted sum of rewards. Expanding the full search tree requires a large number of simulation steps. For instance, selecting an action every 5 frames and allowing a maximum of 100,000 simulation steps per frame, the agent can only look ahead about a third of a second. In many games, this allows the agent to collect immediate rewards and avoid death but little else. For example, in Seaquest the agent must collect a swimmer and return to the surface before running out of air, which involves planning far beyond one second.</p><p>UCT: Upper Confidence Bounds Applied to Trees. A preferable alternative to exhaustively expanding the tree is to simulate deeper into the more promising branches. To do this, we need to find a balance between expanding the higher-valued branches and spending simulation steps on the lower-valued branches to get a better estimate of their values. The UCT algorithm, developed by <ref type="bibr" target="#b13">Kocsis and Szepesvári (2006)</ref>, deals with the exploration-exploitation dilemma by treating each node of a search tree as a multi-armed bandit problem. UCT uses a variation of UCB1, a bandit algorithm, to choose which child node to visit next. A common practice is to apply a t-step random simulation at the end of each leaf node to obtain an estimate from a longer trajectory. By expanding the more valuable branches of the tree and carrying out a random simulation at the leaf nodes, UCT is known to perform well in many different settings <ref type="bibr" target="#b1">(Browne et al., 2012)</ref>.</p><p>Our UCT implementation was entirely standard, except for one optimization. Few Atari games actually distinguish between all 18 actions at every time step. In Beam Rider, for example, the down action does nothing, and pressing the button when a bullet has already  been shot has no effect. We exploit this fact as follows: after expanding the children of a node in the search tree, we compare the resulting emulator states. Actions that result in the same state are treated as duplicates and only one of the actions is considered in the search tree. This reduces the branching factor, thus allowing deeper search. At every step, we also reuse the part of our search tree corresponding to the selected action. Pseudocode for our implementation of the UCT algorithm is given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Experimental Setup</head><p>We designed and tuned our algorithms based on the same five training games used in Section 3.1, and subsequently evaluated the methods on the fifty games of the testing set.</p><p>The training games were used to determine the length of the search horizon as well as the constant controlling the amount of exploration at internal nodes of the tree. Each episode was set to last up to 5 minutes of real-time play (18,000 frames), with actions selected every 5 frames, matching our settings in Section 3.1.2. On average, each action selection step took on the order of 15 seconds. We also used the same discount factor as in Section 3.1. We ran our algorithms for 10 episodes per game. Details of the algorithmic parameters can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Results</head><p>A complete report of our search results is given in Appendix D. <ref type="table" target="#tab_3">Table 2</ref> shows results on a selected subset of games. For reference purposes, we also include the performance of the best learning agent and the best baseline policy from <ref type="table" target="#tab_1">Table 1</ref>. Together, our two search methods performed better than both learning agents and the baseline policies on 49 of 55 games. In most cases, UCT performs significantly better than breadth-first search. Four of the six games for which search methods do not perform best are games where rewards are sparse and require long-term planning. These are Freeway, Private Eye, Montezuma's Revenge and Venture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Metrics for General Atari 2600 Agents</head><p>Applying algorithms to a large set of games as we did in Sections 3.1 and 3.2 presents difficulties when interpreting the results. While the agent's goal in all games is to maximize its score, scores for two different games cannot be easily compared. Each game uses its own scale for scores, and different game mechanics make some games harder to learn than others. The challenges associated with comparing general agents has been previously highlighted by <ref type="bibr" target="#b30">Whiteson et al. (2011)</ref>. Although we can always report full performance tables, as we did in Appendix D, some more compact summary statistics are also desirable. We now introduce some simple metrics that help compare agents across a diverse set of domains, such as our test set of Atari 2600 games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Normalized Scores</head><p>Consider the scores s g,1 and s g,2 achieved by two algorithms in game g. Our goal here is to explore methods that allow us to compare two sets of scores S 1 = {s g 1 ,1 , . . . , s gn,1 } and S 2 = {s g 1 ,2 , . . . , s gn,2 }. The approach we take is to transform s g,i into a normalized score z g,i with the aim of comparing normalized scores across games; in the ideal case, z g,i = z g ,i implies that algorithm i performs as well on game g as on game g . In order to compare algorithms over a set of games, we aggregate normalized scores for each game and each algorithm.</p><p>The most natural way to compare games with different scoring scales is to normalize scores so that the numerical values become comparable. All of our normalization methods are defined using the notion of a score range [r g,min , r g,max ] computed for each game. Given such a score range, score s g,i is normalized by computing z g,i := (s g,i − r g,min ) / (r g,max − r g,min ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Normalization to a Reference Score</head><p>One straightforward method is to normalize to a score range defined by repeated runs of a random agent across each game. Here, r g,max is the absolute value of the average score achieved by the random agent, and r g,min = 0. <ref type="figure" target="#fig_1">Figure 2a</ref> depicts the random-normalized scores achieved by BASS and RAM on three games. Two issues arise with this approach: the scale of normalized scores may be excessively large and normalized scores are generally not translation invariant. The issue of scale is best seen in a game such as Freeway, for which the random agent achieves a score close to 0: scores achieved by learning agents, in the 10-20 range, are normalized into thousands. By contrast, no learning agent achieves a random-normalized score greater than 1 in Asteroids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Normalizing to a Baseline Set</head><p>Rather than normalizing to a single reference we may normalize to the score range implied by a set of references. Let b g,1 , . . . , b g,k be a set of reference scores. A method's baseline score is computed using the score range [min i∈{1,...,k} b g,i , max i∈{1,...,k} b g,i ].</p><p>Given a sufficiently rich set of reference scores, baseline normalization allows us to reduce the scores for most games to comparable quantities, and lets us know whether meaningful performance was obtained. <ref type="figure" target="#fig_1">Figure 2b</ref> shows example baseline scores. The score range for these scores corresponds to the scores achieved by 37 baseline agents (Section 3.1.2): Random, Const (one policy per action), and Perturb (one policy per action).</p><p>A natural idea is to also include scores achieved by human players into the baseline set. For example, one may include the score achieved by an expert as well as the score achieved by a beginner. However, using human scores raises its own set of issues. For example, humans often play games without seeking to maximize score; humans also benefit from prior knowledge that is difficult to incorporate into domain-independent agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Inter-Algorithm Normalization</head><p>A third alternative is to normalize using the scores achieved by the algorithms themselves. Given n algorithms, each achieving score s g,i on game g, we define the inter-algorithm score using the score range [min i∈{1,...,n} s g,i , max i∈{1,...,n} s g,i ]. By definition, z g,i ∈ [0, 1]. A special case of this is when n=2, where z g,i ∈ {0, 1} indicates which algorithm is better than the other. <ref type="figure" target="#fig_1">Figure 2c</ref> shows example inter-algorithm scores; the relevant score ranges are constructed from the performance of all five learning agents.</p><p>Because inter-algorithm scores are bounded, this type of normalization is an appealing solution to compare the relative performance of different methods. Its main drawback is that it gives no indication of the objective performance of the best algorithm. A good example of this is Venture: the inter-algorithm score of 1.0 achieved by BASS does not reflect the fact that none of our agents achieved a score remotely comparable to a human's performance. The lack of objective reference in inter-algorithm normalization suggests that it should be used to complement other scoring metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Aggregating Scores</head><p>Once normalized scores are obtained for each game, the next step is to produce a measure that reflects how well each agent performs across the set of games. As illustrated by <ref type="table" target="#tab_8">Table  4</ref>, a large table of numbers does not easily permit comparison between algorithms. We now describe three methods to aggregate normalized scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Average Score</head><p>The most straightforward method of aggregating normalized scores is to compute their average. Without perfect score normalization, however, score averages tend to be heavily influenced by games such as Zaxxon for which baseline scores are high. Averaging interalgorithm scores obviates this issue as all scores are bounded between 0 and 1. <ref type="figure" target="#fig_2">Figure 3</ref> displays average baseline and inter-algorithm scores for our learning agents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Median Score</head><p>Median scores are generally more robust to outliers than average scores. The median is obtained by sorting all normalized scores and selecting the middle element (the average of the two middle elements is used if the number of scores is even). <ref type="figure" target="#fig_2">Figure 3</ref> shows median baseline and inter-algorithm scores for our learning agents. Comparing medians and averages in the baseline score (upper two graphs) illustrates exactly the outlier sensitivity of the average score, where the LSH method appears dramatically superior due entirely to its performance in Zaxxon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Score Distribution</head><p>The score distribution aggregate is a natural generalization of the median score: it shows the fraction of games on which an algorithm achieves a certain normalized score or better. It is essentially a quantile plot or inverse empirical CDF. Unlike the average and median scores, the score distribution accurately represents the performance of an agent irrespective of how individual scores are distributed. <ref type="figure" target="#fig_3">Figure 4</ref> shows baseline and inter-algorithm score distributions. Score distributions allow us to compare different algorithms at a glance -if one curve is above another, the corresponding method generally obtains higher scores.</p><p>Using the baseline score distribution, we can easily determine the proportion of games for which methods perform better than the baseline policies (scores above 1). The interalgorithm score distribution, on the other hand, effectively conveys the relative performance of each method. In particular, it allows us to conclude that BASS performs slightly better than Basic and RAM, and that DISCO performs significantly worse than the other methods.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Paired Tests</head><p>An alternate evaluation metric, especially useful when comparing only a few algorithms, is to perform paired tests over the raw scores. For each game, we performed a two-tailed Welsh's t-test with 99% confidence intervals to determine whether one algorithm's score was statistically different than the other's. <ref type="table" target="#tab_5">Table 3</ref> provides, for each pair of algorithms, the number of games for which one algorithm performs statistically better or worse than the other. Because of their ternary nature, paired tests tend to magnify small but significant differences in scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>We now briefly survey recent research related to Atari 2600 games and some prior work on the construction of empirical benchmarks for measuring general competency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Atari Games</head><p>There has been some attention devoted to Atari 2600 game playing within the reinforcement learning community. For the most part, prior work has focused on the challenge of finding good state features for this domain. <ref type="bibr" target="#b4">Diuk, Cohen, and Littman (2008)</ref> applied their DOORMAX algorithm to a restricted version of the game of Pitfall!. Their method extracts objects from the displayed image with game-specific object detection. These objects are then converted into a first-order logic representation of the world, the Object-Oriented Markov Decision Process (OO-MDP). Their results show that DOORMAX can discover the optimal behaviour for this OO-MDP within one episode. <ref type="bibr" target="#b32">Wintermute (2010)</ref> proposed a method that also extracts objects from the displayed image and embeds them into a logicbased architecture, SOAR. Their method uses a forward model of the scene to improve the performance of the Q-Learning algorithm <ref type="bibr" target="#b29">(Watkins &amp; Dayan, 1992)</ref>. They showed that by using such a model, a reinforcement learning agent could learn to play a restricted version of the game of Frogger. Cobo, Zang, Isbell, and Thomaz (2011) investigated automatic feature discovery in the games of Pong and Frogger, using their own simulator. Their proposed method takes advantage of human trajectories to identify state features that are important for playing console games. Recently, <ref type="bibr" target="#b8">Hausknecht, Khandelwal, Miikkulainen, and Stone (2012)</ref> proposed HyperNEAT-GGP, an evolutionary approach for finding policies to play Atari 2600 games. Although HyperNEAT-GGP is presented as a general game playing approach, it is currently difficult to assess its general performance as the reported results were limited to only two games. Finally, some of the authors of this paper (Bellemare, Veness, &amp; Bowling, 2012) recently presented a domain-independent feature generation technique that attempts to focus its effort around the location of the player avatar. This work used the evaluation methodology advocated here and is the only one to demonstrate the technique across a large set of testing games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Frameworks for General Agents</head><p>Although the idea of using games to evaluate the performance of agents has a long history in artificial intelligence, it is only more recently that an emphasis on generality has assumed a more prominent role. <ref type="bibr" target="#b20">Pell (1993)</ref> advocated the design of agents that, given an abstract description of a game, could automatically play them. His work strongly influenced the design of the now annual General Game Playing competition <ref type="bibr" target="#b6">(Genesereth et al., 2005)</ref>. Our framework differs in that we do not assume to have access to a compact logical description of the game semantics. Schaul, Togelius, and Schmidhuber (2011) also recently presented an interesting proposal for using games to measure the general capabilities of an agent. <ref type="bibr" target="#b30">Whiteson et al. (2011)</ref> discuss a number of challenges in designing empirical tests to measure general reinforcement learning performance; this work can be seen as attempting to address their important concerns. Starting in 2004 as a conference workshop, the Reinforcement Learning competition <ref type="bibr" target="#b31">(Whiteson et al., 2010)</ref> was held until 2009 (a new iteration of the competition has been announced for 2013 4 ). Each year new domains are proposed, including standard RL benchmarks, Tetris, and Infinite Mario <ref type="bibr" target="#b16">(Mohan &amp; Laird, 2009</ref>). In a typical competition domain, the agent's state information is summarized through a series of high-level state variables rather than direct sensory information. Infinite Mario, for example, provides the agent with an object-oriented observation space. In the past, organizers have provided a special 'Polyathlon' track in which agents must behave in a medley of continuous-observation, discrete-action domains.</p><p>Another longstanding competition, the International Planning Competition (IPC) 5 , has been organized since 1998, and aims to "produce new benchmarks, and to gather and dis-seminate data about the current state-of-the-art" <ref type="bibr" target="#b3">(Coles et al., 2012)</ref>. The IPC is composed of different tracks corresponding to different types of planning problems, including factory optimization, elevator control and agent coordination. For example, one of the problems in the 2011 competition consists in coordinating a set of robots around a two-dimensional gridworld so that every tile is painted with a specific colour. Domains are described using either relational reinforcement learning, yielding parametrized Markov Decision Processes (MDPs) and Partially Observable MDPs, or using logic predicates, e.g. in STRIPS notation.</p><p>One indication of how much these competitions value domain variety can be seen in the time spent on finding a good specification language. The 2008-2009 RL competitions, for example, used RL-Glue 6 specifically for this purpose; the 2011 planning under uncertainty track of the IPC similar employed the Relation Dynamic Influence Diagram Language. While competitions seek to spur new research and evaluate existing algorithms through a standardized set of benchmarks, they are not independently developed, in the sense that the vast majority of domains are provided by the research community. Thus a typical competition domain reflects existing research directions: Mountain Car and Acrobot remain staples of the RL competition. These competitions also focus their research effort on domains that provide high-level state variables, for example the location of robots in the floor-painting domain described above. By contrast, the Arcade Learning Environment and the domain-independent setting force us to consider the question of perceptual grounding: how to extract meaningful state information from raw game screens (or RAM information). In turn, this emphasizes the design of algorithms that can be applied to sensor-rich domains without significant expert knowledge.</p><p>There have also been a number of attempts to define formal agent performance metrics based on algorithmic information theory. The first such attempts were due to <ref type="bibr" target="#b10">Hernández-Orallo and Minaya-Collado (1998)</ref> and to <ref type="bibr" target="#b5">Dowe and Hajek (1998)</ref>. More recently, the approaches of Hernández-Orallo and <ref type="bibr" target="#b9">Dowe (2010)</ref> and of <ref type="bibr" target="#b15">Legg and Veness (2011)</ref> appear to have some potential. Although these frameworks are general and conceptually clean, the key challenge remains how to specify sufficiently interesting classes of environments. In our opinion, much more work is required before these approaches can claim to rival the practicality of using a large set of existing human-designed environments for agent evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Final Remarks</head><p>The Atari 2600 games were developed for humans and as such exhibit many idiosyncrasies that make them both challenging and exciting. Consider, for example, the game Pong. Pong has been studied in a variety of contexts as an interesting reinforcement learning domain <ref type="bibr" target="#b2">(Cobo et al., 2011;</ref><ref type="bibr" target="#b25">Stober &amp; Kuipers, 2008;</ref><ref type="bibr" target="#b17">Monroy, Stanley, &amp; Miikkulainen, 2006)</ref>. The Atari 2600 Pong, however, is significantly more complex than Pong domains developed for research. Games can easily last 10,000 time steps (compared to 200-1000 in other domains); observations are composed of 7-bit 160×210 images (compared to 300×200 black and white images in the work of <ref type="bibr" target="#b25">Stober and Kuipers (2008)</ref>, or 5-6 input features elsewhere); observations are also more complex, containing the two players' score and side walls. In sheer size, the Atari 2600 Pong is thus a larger domain. Its dynamics are also 6. http://glue.rl-community.org more complicated. In research implementations of Pong object motion is implemented using first-order mechanics. However, in Atari 2600 Pong paddle control is nonlinear: simple experimentation shows that fully predicting the player's paddle requires knowledge of the last 18 actions. As with many other Atari games, the player paddle also moves every other frame, adding a degree of temporal aliasing to the domain.</p><p>While Atari 2600 Pong may appear unnecessarily contrived, it in fact reflects the unexpected complexity of the problems with which humans are faced. Most, if not all Atari 2600 games are subject to similar programming artifacts: in Space Invaders, for example, the invaders' velocity increases nonlinearly with the number of remaining invaders. In this way the Atari 2600 platform provides AI researchers with something unique: clean, easilyemulated domains which nevertheless provide many of the challenges typically associated with real-world applications.</p><p>Should technology advance so as to render general Atari 2600 game playing achievable, our challenge problem can always be extended to use more recent video game platforms. A natural progression, for example, would be to move on to the Commodore 64, then to the Nintendo, and so forth towards current generation consoles. All of these consoles have hundreds of released games, and older platforms have readily available emulators. With the ultra-realism of current generation consoles, each console represents a natural stepping stone toward general real-world competency. Our hope is that by using the methodology advocated in this paper, we can work in a bottom-up fashion towards developing more sophisticated AI technology while still maintaining empirical rigor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This article has introduced the Arcade Learning Environment, a platform for evaluating the development of general, domain-independent agents. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. We illustrate the promise of ALE as a challenge problem by benchmarking several domain-independent agents that use well-established reinforcement learning and planning techniques. Our results suggest that general Atari game playing is a challenging but not intractable problem domain with the potential to aid the development and evaluation of general agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Feature Set Construction</head><p>This section gives a detailed description of the five feature generation techniques from Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Basic Abstraction of the ScreenShots (BASS)</head><p>The idea behind BASS is to directly encode colours present on the screen. This method is motivated by three observations on the Atari 2600 hardware and games:</p><p>1. While the Atari 2600 hardware supports a screen resolution of 160×210, game objects are usually larger than a few pixels. Overall, important game events happen at a much lower resolution.</p><p>2. Many Atari 2600 games have a static background, with a few important objects moving on the screen. While the screen matrix is densely populated, the actual interesting features on the screen are often sparse.</p><p>3. While the hardware can show up to 128 colours in the NTSC mode, it is limited to only 8 colours in the SECAM mode. Consequently, most games use a few number of colours to distinguish important objects on the screen.</p><p>The game screen is first preprocessed by subtracting its background, detected using a simple histogram method. BASS then encodes the presence of each of the eight SECAM palette colours at a low resolution, as depicted in <ref type="figure" target="#fig_4">Figure 5</ref>. Intuitively, BASS seeks to capture the presence of objects of certain colours at different screen locations. BASS also encodes relations between objects by constructing all pairwise combinations of its encoded colour features. In Asterix, for example, it is important to know if there is a green object (player character) and a red object (collectable object) in its vicinity. Pairwise features allow us to capture such object relations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Basic</head><p>The Basic method generates the same set of features as BASS, but omits the pairwise combinations. This allows us to study whether the additional features are beneficial or harmful to learning. Because the Basic method has fewer features than BASS, it encodes the presence of each of the 128 colours. In comparison to BASS, Basic therefore represents colour more accurately, but cannot represent object interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Detecting Instances of Classes of Objects (DISCO)</head><p>This feature generation method is based on detecting a set of classes representing game entities and locating instances of these classes on the screen. DISCO is motivated by the following additional observations on Atari 2600 games:</p><p>1. The game entities are often instances of a few classes of objects. For instance, as <ref type="figure" target="#fig_5">Figure 6</ref> shows, while there are many objects in a sample screen of the game Freeway, all of these objects are instances of only two classes: Chicken and Car. Similarly, all the objects on a sample screen of the game Seaquest are instances of one of these six classes: Fish, Swimmer, Player Submarine, Enemy Submarine, Player Bullet, and Enemy Bullet.</p><p>2. The interaction between two objects can often be generalized to all instances of their respective classes. As an example, consider Car -Chicken object interactions in Freeway: learning that there is lower value associated with one Chicken instance hitting a Car instance can be generalized to all instances of those two classes. DISCO first performs a series of preprocessing steps to discover classes, during which no value function learning is performed. When the agent subsequently learns to play the game, DISCO generates features by detecting objects on the screen and classifying them. The DISCO process is summarized by the following steps:</p><p>• Preprocessing:</p><p>-Background detection: The static background matrix is extracted using a histogram method, as with BASS.  -Blob extraction: A list of moving blob (foreground) objects is detected in each game screen.</p><p>-Class discovery: A set of classes is detected from the extracted blob objects.</p><p>-Class filtering: Classes that appear infrequently or are restricted to small region of the screen are removed from the set.</p><p>-Class merging: Classes that have similar shapes are merged together.</p><p>• Feature generation:</p><p>-Class instance detection: At each time step, class instances are detected from the current screen matrix.</p><p>-Feature vector generation: A feature vector is generated from the detected instances by tile-coding their absolute position as well as the relative position and velocity of every pair of instances from different classes. Multiple instances of the same objects are combined additively. <ref type="figure" target="#fig_6">Figure 7</ref> shows discovered objects in a Seaquest frame. This image illustrates the difficulties in detecting objects: although DISCO correctly classifies the different fish as part of the same class, it also detects a life icon and the oxygen bar as part of that class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Locality Sensitive Hashing (LSH)</head><p>An alternative approach to BASS and DISCO is to use well-established feature generation methods that are agnostic about the type of input they receive. Such methods include polynomial bases <ref type="bibr" target="#b24">(Schweitzer &amp; Seidmann, 1985)</ref>, sparse distributed memories <ref type="bibr" target="#b12">(Kanerva, 1988)</ref> and locality sensitive hashing (LSH) <ref type="bibr" target="#b7">(Gionis et al., 1999)</ref>. In this paper we consider the latter as a simple mean of reducing the large image space to a smaller, more manageable set of features. The input -here, a game screen -is first mapped to a bit vector of size 7 × 210 × 160. The resulting vector is then hashed down into a smaller set of features. LSH performs an additional random projection step to ensure that similar screens are more likely to be binned together. The LSH generation method is detailed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 RAM-based Feature Generation</head><p>Unlike the previous three methods, which generate feature vectors based on the game screen, the RAM-based feature generation method relies on the contents of the console memory. The Atari 2600 has only 128 × 8 = 1024 bits of random access memory 7 , which must hold the complete internal state of a game: location of game entities, timers, health indicators, etc. The RAM is therefore a relatively compact representation of the game state, and in contrast to the game screen, it is also Markovian. The purpose of our RAM-based agent is to investigate whether features generated from the RAM affect performance differently from features generated from game screens.</p><p>The first part of the generated feature vector simply includes the 1024 bits of RAM. Atari 2600 game programmers often used these bits not as individual values, but as part of 4-bit or 8-bit words. Linear function approximation on the individual bits can capture the value of these multi-bit words. We are also interested in the relation between pairs of values in memory. To capture these relations, the logical-AND of all possible bit pairs is appended to the feature vector. Note that a linear function on the pairwise AN D's can capture products of both 4-bit and 8-bit words. This is because the product of two n-bit words can be expressed as a weighted sum of the pairwise products of their bits.</p><p>7. Some games provided more RAM on the game cartridge: the Atari Super Chip, for example, offered an additional 128 bytes of memory. The current approach only considers the main memory included in the Atari 2600 console.   .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Screenshots of Pitfall! and Space Invaders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left to right: random-normalized, baseline and inter-algorithm scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Aggregate normalized scores for the five reinforcement learning agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Score distribution over all games. Basic BASS DISCO LSH RAM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Left: Freeway in SECAM colours. Right: BASS colour encoding for the same screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Left: Screenshot of the game Freeway. Although there are ten different cars, they can all be considered as instances of a single class. Right: Screenshot of the game Seaquest depicting four different object classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Left: Screenshot of the game Seaquest. Right: Objects detected by DISCO in the game Seaquest. Each colour represents a different class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Reinforcement Learning results for selected games. Asterix and Seaquest are part of the training set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results for selected games. Asterix and Seaquest are part of the training set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Paired tests over all games. Each entry shows the number of games for which the performance of the first algorithm (left) is better (-worse) than the second algorithm's.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm 1 Locally Sensitive Hashing (LSH) Feature Generation Constants. M (hash table size), n (screen bit vector size) l (number of random bit vectors), k (number of non-zero entries)Initialization (once). {v 1 . . . v l } ← generateRandomVectors(l, k, n) {hash 1 . . . hash l } ← generateHashFunctions(l, M, n) Input. A screen matrix I with elements I xy ∈ {0, . . . , 127} Initialize v 1 . . . v l ∈ R n = 0 for i = 1 . . . l do Select x 1 , x 2 ,. . . , x k distinct coordinates between 1 and n uniformly at random v i [x 1 ] = 1; v i [x 2 ] = 1; . . . ; v i [x k ] = 1 end for return {v 1 , . . . v l } generateHashFunctions(l, M, n) (hash functions are vectors of random coordinates) Initialize hash 1 . . . hash l ∈ R n = 0 for i = 1 . . . l, j = 1 . . . n do hash i [j] ← random(1, M ) (uniformly random coordinate between 1 and M) end for return {hash 1 , . . . hash l } Remark. With sparse vector operations, LSH has a O(lk + n) cost per step.</figDesc><table><row><cell>LSH(I)</cell><cell></cell></row><row><cell cols="2">s ← binarizeScreen(I) (s has length n)</cell></row><row><cell>Initialize φ ∈ R lM = 0</cell><cell></cell></row><row><cell>for i = 1 . . . l do</cell><cell></cell></row><row><cell>h = 0</cell><cell></cell></row><row><cell>for j = 1 . . . n do</cell><cell></cell></row><row><cell cols="2">h ← h + I [s j =v ij ] hash i [j] mod M</cell><cell>(hash the projection of s onto v i )</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>φ[M (i − 1) + h] = 1</cell><cell cols="2">(one binary feature per random bit vector)</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>binarizeScreen(I)</cell><cell></cell></row><row><cell>Initialize s ∈ R n = 0</cell><cell></cell></row><row><cell cols="3">for y = 1 . . . h, x = 1 . . . w (h = 210, w = 160) do</cell></row><row><cell>s[x + y  *  h + I xy ] = 1</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>return s</cell><cell></cell></row><row><cell cols="2">generateRandomVectors(l, k, n)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Appendix B. UCT Pseudocode</figDesc><table><row><cell cols="5">Appendix C. Experimental Parameters Appendix D. Detailed Results</cell><cell></cell><cell></cell></row><row><cell cols="3">D.1 Reinforcement Learning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Algorithm 2 UCT General</cell><cell cols="3">All experiments</cell><cell></cell><cell cols="3">Maximum frames per episode 18,000</cell></row><row><cell cols="8">Frames per action 5 Training episodes per trial 5,000 Constants. m (search horizon), k (simulations per step) Reinforcement learning Game Basic BASS DISCO LSH RAM Random Const Perturb Variables. Ψ (search tree) Asterix 862.3 859.8 754.6 987.3 943.0 288.1 650.0 337.8 Evaluation episodes per trial 500 Beam Rider 929.4 872.7 563.0 793.6 729.8 434.7 996.0 754.8 Input. s (current state) Number of trials per result 30 Freeway 11.3 16.4 12.8 15.4 19.1 0.0 21.0 22.5</cell></row><row><cell cols="4">Preprocessing Seaquest Space Invaders UCT(s) if Ψ is empty or root(Ψ) = s then Background detection 579.0 664.8 421.9 203.6 250.1 239.1 Ψ ← empty search tree Algorithm 3 UCT Routines Class discovery Alien 939.2 893.4 623.6 Amidar 64.9 103.4 67.9 Assault 465.8 378.4 371.7 Constants. γ : discount factor Asteroids 829.7 800.3 744.5 Atlantis 62687.0 25375.0 20857.3</cell><cell cols="4">508.5 222.2 510.2 45.1 628.0 590.7 Maximum object velocity (pixels) 8 593.7 107.9 160.0 451.1 Sample screens per game 18,000 226.5 156.1 245.0 270.5 726.4 102.0 140.0 313.9 Sample screens per game 36,000 71.4 0.8 31.0 37.8 383.6 334.3 357.0 497.8 Maximum number of classes 10 907.3 1526.7 140.0 539.9 17593.9 19932.7 33058.4 1500.0 12089.1</cell></row><row><cell cols="8">root(Ψ) ← s end if repeat sample(Ψ, m) until visits(root(Ψ)) = k a ← bestAction(Ψ) prune(Ψ, a) (optional) return a sample(Ψ, m) Bank Heist 98.8 Battle Zone 15534.3 selectAction(n) for all c children of n do Reinforcement Berzerk 329.2 learning Bowling 28.5 -2.8 Boxing V (c) ← average-return(c) + log[visits(c)] 71.1 51.4 Minimum frequency of class appearance 20% 64.6 190.8 15.0 0.0 13.5 12750.8 0.0 14548.1 15819.7 2920.0 13000.0 5772.0 All agents 491.3 329.0 441.0 501.3 233.8 670.0 552.9 Discount factor γ 0.999 Exploration rate 43.9 35.2 26.1 29.3 24.6 30.0 30.0 0.05 15.5 12.4 10.5 44.0 -1.5 -25.0 -10.1 visits(n) BASS and Breakout 3.3 5.2 3.9 2.5 4.0 1.5 3.0 2.9 Learning rate α 0.5 Basic 2323.9 1574.2 1646.3 1147.2 765.4 869.2 0.0 485.4 Carnival Eligibility traces decay rate λ 0.9 Centipede 7725.5 8803.8 6210.6 6161.6 7555.4 2805.1 16527.0 8937.2 end for Chopper Command 1191.4 1581.5 1349.0 943.0 1397.8 698.2 1000.0 973.7 Grid width 16 Crazy Climber 6303.1 7455.6 4552.9 20453.7 23410.6 2335.4 0.0 2235.0 return arg max a V (child(n, a)) Demon Attack 520.5 318.5 208.8 355.8 324.8 289.3 130.0 776.2 Grid height 14 BASS only Double Dunk -15.8 -13.1 -23.2 -21.6 -20.3 -15.6 0.0 -20.3 Elevator Action 3025.2 2377.6 4.6 3220.6 507.9 1040.9 0.0 562.9 Number of different colours 8 rollout(n, m) R = 0 Basic only Enduro 111.8 129.1 0.0 95.8 112.3 0.0 9.0 25.9 Number of different colours 128 Fishing Derby -92.6 -92.1 -89.5 -93.2 -91.6 -93.8 -99.0 -97.2 (Initialize Monte-Carlo return to 0) g = 1 DISCO Frostbite 161.0 161.1 176.6 216.9 147.9 70.3 160.0 175.2 Learning rate α 0.1 Gopher 545.8 1288.3 295.7 941.8 722.5 243.7 0.0 286.8 Eligibility traces decay rate λ 0.9 Gravitar 185.3 251.1 197.4 105.9 387.7 205.4 0.0 106.0</cell></row><row><cell cols="8">n ← root(Ψ) while n is not a leaf, m &gt; depth(n) do if some action a was never taken in n then (c, reward) ← emulate(n, a) (run model for one step) immediate-return(c) ← reward child(n, a) ← c n ← c (c is necessarily a leaf) while m &gt; 0 do H.E.R.O. 6053.1 6458.8 2719.8 3835.8 3281.1 Tile coding, number of tilings 8 712.0 0.0 147.5 -13.9 -14.8 -18.9 -15.1 -9.5 -14.8 -1.0 -6.5 Ice Hockey Tile coding, grid size 8 James Bond 197.3 202.8 17.3 77.1 133.8 23.3 0.0 82.0 Select a according to some rollout policy (e.g. uniformly randomly) RAM-based Journey Escape -8441.0 -14730.7 -9392.2 -13898.9 -8713.5 -18201.7 0.0 -10693.9 Learning rate α 0.2 Kangaroo 962.4 1622.1 457.9 256.4 481.7 44.4 200.0 498.4 (n, reward) ← emulate(n, a) R ← R + g × reward Krull 2823.3 3371.5 2350.9 2798.1 2901.3 1880.1 0.0 1690.1 Eligibility traces decay rate λ 0.5 LSH Kung-Fu Master 16416.2 19544.0 3207.0 8715.6 10361.1 488.2 0.0 578.4 Learning rate α 0.5 Montezuma's Revenge 10.7 0.1 0.0 0.1 0.3 0.3 0.0 0.0 m ← m − 1 Ms. Pac-Man 1537.2 1691.8 999.6 1070.8 1021.1 163.3 210.0 505.5 Eligibility traces decay rate λ 0.5 Name This Game 1818.9 2386.8 1951.0 2029.8 2500.1 2012.3 3080.0 1854.3 g ← g × γ end while Pooyan 800.3 1018.9 402.7 1225.3 1210.9 501.1 30.0 540.8 Number of random vectors l 2000 Pong -19.2 -19.0 -19.6 -19.9 -19.9 -20.9 -21.0 -20.8 Number of non-zero vector entries k 1000 Private Eye 81.9 100.7 -23.0 684.3 111.9 -754.0 0.0 1947.3</cell></row><row><cell>else return R River Raid Q*Bert</cell><cell>613.5 1708.9</cell><cell>497.2 1438.0</cell><cell>326.3 0.0</cell><cell>529.1 1904.3</cell><cell cols="3">565.8 Per-vector hash table size M 50 169.0 150.0 157.4 1309.9 1608.6 1070.0 1455.5</cell></row><row><cell cols="8">a ← selectAction(n) n ← child(n, a) end if end while Planning Road Runner 67.7 Robotank 12.8 Skiing -1.1 update-value(n, R) Star Gunner 850.2 Tennis -0.2 R ← R + immediate-reward(n) 65.2 10.1 -0.7 1069.5 -0.1 average-return(n) ← average-return(n) visits(n) UCT 21.4 42.0 9.3 10.8 Maximum search depth (frames) 300 41.0 36.2 900.0 857.9 Simulations per action 500 28.7 1.6 17.0 11.3 -0.1 -0.0 0.0 0.0 0.0 0.0 Exploration constant 0.1 1002.2 722.9 769.3 638.1 600.0 509.8 -0.1 -0.1 -0.1 -24.0 0.0 -0.3 visits(n)+1 + R Full-tree search Time Pilot 1728.2 2299.5 0.0 2429.2 3741.2 3458.8 500.0 718.7 Maximum frames emulated per action 133,000 Tutankham 40.7 52.6 0.0 85.2 114.3 23.1 0.0 17.3 visits(n)+1 Up and Down 3532.7 3351.0 2473.4 2475.1 3412.6 131.6 550.0 2962.9 R = rollout(n, m − depth(n)) update-value(n, R) (propagate values back up) Venture 0.0 66.0 0.0 0.0 0.0 0.0 0.0 0.0 visits(n) ← visits(n) + 1 Video Pinball 15046.8 12574.2 10779.5 9813.9 16871.3 20021.1 705.0 9527.9 if n is not the root of Ψ, i.e. parent(n) = null then Wizard of Wor 1768.8 1981.3 935.6 945.5 1096.2 772.4 300.0 470.3</cell></row><row><cell cols="3">end if bestAction(Ψ) Zaxxon update-value(parent(n), γ × R) 1392.0 2069.1 Times Best 6 17</cell><cell>69.8 1</cell><cell>3365.1 8</cell><cell>304.3 8</cell><cell>0.0 2</cell><cell>0.0 9</cell><cell>2.0 4</cell></row><row><cell cols="4">return arg max a [visits(child(root(Ψ), a))]</cell><cell cols="4">(action most frequently taken at root)</cell></row><row><cell>prune(Ψ, a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">root(Ψ) ← child(root(Ψ), a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Reinforcement Learning results. The first five games constitute our training set. See Section 3.1 for details.</figDesc><table><row><cell>D.2 Planning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Game</cell><cell>Full Tree</cell><cell>UCT</cell><cell>Best Learner</cell><cell>Best Baseline</cell></row><row><cell>Asterix</cell><cell>2135.7</cell><cell>290700.0</cell><cell>987.3</cell><cell>650.0</cell></row><row><cell>Beam Rider</cell><cell>693.5</cell><cell>6624.6</cell><cell>929.4</cell><cell>996.0</cell></row><row><cell>Freeway</cell><cell>0.0</cell><cell>0.4</cell><cell>19.1</cell><cell>22.5</cell></row><row><cell>Seaquest</cell><cell>288.0</cell><cell>5132.4</cell><cell>664.8</cell><cell>451.1</cell></row><row><cell>Space Invaders</cell><cell>112.2</cell><cell>2718.0</cell><cell>250.1</cell><cell>270.5</cell></row><row><cell>Alien</cell><cell>784.0</cell><cell>7785.0</cell><cell>939.2</cell><cell>313.9</cell></row><row><cell>Amidar</cell><cell>5.2</cell><cell>180.3</cell><cell>103.4</cell><cell>37.8</cell></row><row><cell>Assault</cell><cell>413.7</cell><cell>1512.2</cell><cell>628.0</cell><cell>497.8</cell></row><row><cell>Asteroids</cell><cell>3127.4</cell><cell>4660.6</cell><cell>907.3</cell><cell>1526.7</cell></row><row><cell>Atlantis</cell><cell>30460.0</cell><cell>193858.0</cell><cell>62687.0</cell><cell>33058.4</cell></row><row><cell>Bank Heist</cell><cell>21.5</cell><cell>497.8</cell><cell>190.8</cell><cell>15.0</cell></row><row><cell>Battle Zone</cell><cell>6312.5</cell><cell>70333.3</cell><cell>15819.7</cell><cell>13000.0</cell></row><row><cell>Berzerk</cell><cell>195.0</cell><cell>553.5</cell><cell>501.3</cell><cell>670.0</cell></row><row><cell>Bowling</cell><cell>25.5</cell><cell>25.1</cell><cell>43.9</cell><cell>30.0</cell></row><row><cell>Boxing</cell><cell>100.0</cell><cell>100.0</cell><cell>44.0</cell><cell>-1.5</cell></row><row><cell>Breakout</cell><cell>1.1</cell><cell>364.4</cell><cell>5.2</cell><cell>3.0</cell></row><row><cell>Carnival</cell><cell>950.0</cell><cell>5132.0</cell><cell>2323.9</cell><cell>869.2</cell></row><row><cell>Centipede</cell><cell>125123.0</cell><cell>110422.0</cell><cell>8803.8</cell><cell>16527.0</cell></row><row><cell>Chopper Command</cell><cell>1827.3</cell><cell>34018.8</cell><cell>1581.5</cell><cell>1000.0</cell></row><row><cell>Crazy Climber</cell><cell>37110.0</cell><cell>98172.2</cell><cell>23410.6</cell><cell>2335.4</cell></row><row><cell>Demon Attack</cell><cell>442.6</cell><cell>28158.8</cell><cell>520.5</cell><cell>776.2</cell></row><row><cell>Double Dunk</cell><cell>-18.5</cell><cell>24.0</cell><cell>-13.1</cell><cell>0.0</cell></row><row><cell>Elevator Action</cell><cell>730.0</cell><cell>18100.0</cell><cell>3220.6</cell><cell>1040.9</cell></row><row><cell>Enduro</cell><cell>0.6</cell><cell>286.3</cell><cell>129.1</cell><cell>25.9</cell></row><row><cell>Fishing Derby</cell><cell>-91.6</cell><cell>37.8</cell><cell>-89.5</cell><cell>-93.8</cell></row><row><cell>Frostbite</cell><cell>137.2</cell><cell>270.5</cell><cell>216.9</cell><cell>175.2</cell></row><row><cell>Gopher</cell><cell>1019.0</cell><cell>20560.0</cell><cell>1288.3</cell><cell>286.8</cell></row><row><cell>Gravitar</cell><cell>395.0</cell><cell>2850.0</cell><cell>387.7</cell><cell>205.4</cell></row><row><cell>H.E.R.O.</cell><cell>1323.8</cell><cell>12859.5</cell><cell>6458.8</cell><cell>712.0</cell></row><row><cell>Ice Hockey</cell><cell>-9.2</cell><cell>39.4</cell><cell>-9.5</cell><cell>-1.0</cell></row><row><cell>James Bond</cell><cell>25.0</cell><cell>330.0</cell><cell>202.8</cell><cell>82.0</cell></row><row><cell>Journey Escape</cell><cell>1327.3</cell><cell>7683.3</cell><cell>-8441.0</cell><cell>0.0</cell></row><row><cell>Kangaroo</cell><cell>90.0</cell><cell>1990.0</cell><cell>1622.1</cell><cell>498.4</cell></row><row><cell>Krull</cell><cell>3089.2</cell><cell>5037.0</cell><cell>3371.5</cell><cell>1880.1</cell></row><row><cell>Kung-Fu Master</cell><cell>12127.3</cell><cell>48854.5</cell><cell>19544.0</cell><cell>578.4</cell></row><row><cell>Montezuma's Revenge</cell><cell>0.0</cell><cell>0.0</cell><cell>10.7</cell><cell>0.3</cell></row><row><cell>Ms. Pacman</cell><cell>1708.5</cell><cell>22336.0</cell><cell>1691.8</cell><cell>505.5</cell></row><row><cell>Name This Game</cell><cell>5699.0</cell><cell>15410.0</cell><cell>2500.1</cell><cell>3080.0</cell></row><row><cell>Pooyan</cell><cell>909.7</cell><cell>17763.4</cell><cell>1225.3</cell><cell>540.8</cell></row><row><cell>Pong</cell><cell>-20.7</cell><cell>21.0</cell><cell>-19.0</cell><cell>-20.8</cell></row><row><cell>Private Eye</cell><cell>57.9</cell><cell>100.0</cell><cell>684.3</cell><cell>1947.3</cell></row><row><cell>Q*Bert</cell><cell>132.8</cell><cell>17343.4</cell><cell>613.5</cell><cell>169.0</cell></row><row><cell>River Raid</cell><cell>2178.5</cell><cell>4449.0</cell><cell>1904.3</cell><cell>1608.6</cell></row><row><cell>Road Runner</cell><cell>245.0</cell><cell>38725.0</cell><cell>67.7</cell><cell>900.0</cell></row><row><cell>Robotank</cell><cell>1.5</cell><cell>50.4</cell><cell>28.7</cell><cell>17.0</cell></row><row><cell>Skiing</cell><cell>0.0</cell><cell>-0.8</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Star Gunner</cell><cell>1345.0</cell><cell>1207.1</cell><cell>1069.5</cell><cell>638.1</cell></row><row><cell>Tennis</cell><cell>-23.8</cell><cell>2.8</cell><cell>-0.1</cell><cell>0.0</cell></row><row><cell>Time Pilot</cell><cell>4063.6</cell><cell>63854.5</cell><cell>3741.2</cell><cell>3458.8</cell></row><row><cell>Tutankham</cell><cell>64.1</cell><cell>225.5</cell><cell>114.3</cell><cell>23.1</cell></row><row><cell>Up and Down</cell><cell>746.0</cell><cell>74473.6</cell><cell>3532.7</cell><cell>2962.9</cell></row><row><cell>Venture</cell><cell>0.0</cell><cell>0.0</cell><cell>66.0</cell><cell>0.0</cell></row><row><cell>Video Pinball</cell><cell>55567.3</cell><cell>254748.0</cell><cell>16871.3</cell><cell>20021.1</cell></row><row><cell>Wizard of Wor</cell><cell>3309.1</cell><cell>105500.0</cell><cell>1981.3</cell><cell>772.4</cell></row><row><cell>Zaxxon</cell><cell>0.0</cell><cell>22610.0</cell><cell>3365.1</cell><cell>2.0</cell></row><row><cell>Times Best</cell><cell>4</cell><cell>45</cell><cell>3</cell><cell>3</cell></row></table><note>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Search results. The first five games constitute our training set. See Section 3.2 for details.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. This functionality is needed for a small number of games to ensure that they always terminate. This prevents situations such as in Tennis, where a degenerate agent could choose to play indefinitely by refusing to serve.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. http://en.wikipedia.org/wiki/List of Atari 2600 games(July 12, 2012)   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. http://www.rl-competition.org 5. http://ipc.icaps-conference.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Marc Lanctot, Erik Talvitie, and Matthew Hausknecht for providing suggestions on helping debug and improving the Arcade Learning Environment source code. We would also like to thank our reviewers for their helpful feedback and enthusiasm about the Atari 2600 as a research platform. The work presented here was supported by the Alberta Innovates Technology Futures, the Alberta Innovates Centre for Machine Learning at the University of Alberta, and the Natural Science and Engineering Research Council of Canada. Invaluable computational resources were provided by Compute/Calcul Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigating contingency awareness using Atari 2600 games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the the 26th Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the the 26th Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of Monte Carlo tree search methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic state abstraction from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Second International Joint Conference on Articial Intelligence (IJCAI)</title>
		<meeting>the 22nd Second International Joint Conference on Articial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey of the seventh international planning competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Olaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="83" to="88" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An object-oriented representation for efficient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning (ICML)</title>
		<meeting>the 25th International Conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A non-behavioural, computational extension to the Turing Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Dowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Intelligence and Multimedia Applications (ICCIMA)</title>
		<meeting>the International Conference on Computational Intelligence and Multimedia Applications (ICCIMA)</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">General Game Playing: Overview of the AAAI competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Genesereth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Very Large Databases</title>
		<meeting>the International Conference on Very Large Databases</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HyperNEAT-GGP: A HyperNEAT-based Atari general game player</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)</title>
		<meeting>the Genetic and Evolutionary Computation Conference (GECCO)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring universal intelligence: Towards an anytime intelligence test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Dowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1508" to="1539" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A formal definition of intelligence based on an intensional variant of Kolmogorov complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Minaya-Collado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium of Engineering of Intelligent Systems (EIS)</title>
		<meeting>the International Symposium of Engineering of Intelligent Systems (EIS)</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<title level="m">Sparse Distributed Memory</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Conference on Machine Learning (ECML)</title>
		<meeting>the 15th European Conference on Machine Learning (ECML)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Machine Super Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Lugano</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An approximation of the universal intelligence measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ray Solomonoff Memorial Conference</title>
		<meeting>the Ray Solomonoff Memorial Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to play Mario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Laird</surname></persName>
		</author>
		<idno>rep. CCA-TR-2009-03</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Center for Cognitive Architecture, University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coevolution of neural networks using a layered pareto archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Monroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Genetic and Evolutionary Computation Conference (GECCO)</title>
		<meeting>the 8th Genetic and Evolutionary Computation Conference (GECCO)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Racing the Beam: The Atari Video Computer System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bogost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Game-Independent AI Agents for Playing Atari 2600 Console Games. Master&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Strategy Generation and Evaluation for Meta-Game Playing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Map learning with uninterpreted sensors and effectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="169" to="227" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rationality and intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="77" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Measuring intelligence through games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1109.1314</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalized polynomial approximations in Markovian decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seidmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical analysis and applications</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="568" to="582" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From pixels to policies: A bootstrapping agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Development and Learning (ICDL)</title>
		<meeting>the 7th IEEE International Conference on Development and Learning (ICDL)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Autonomous Agents and Multiagents Systems (AAMAS)</title>
		<meeting>the 10th International Conference on Autonomous Agents and Multiagents Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lifelong robot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="46" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Protecting against evaluation overfitting in empirical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)</title>
		<meeting>the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The reinforcement learning competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AI Magazine</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="81" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using imagery to simplify perceptual abstraction in reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wintermute</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the the 24th Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the the 24th Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
