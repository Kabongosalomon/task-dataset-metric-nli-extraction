<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Luan</surname></persName>
							<email>sitao.luan@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
							<email>mingde.zhao@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
							<email>chang@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<email>dprecup@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DeepMind * Equal Contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, neural network based approaches have achieved significant performance improvement for solving large, complex, graph-structured problems. However, the advantages of multi-scale information and deep architectures have not been sufficiently exploited. In this paper, we analyze how existing Graph Convolutional Networks (GCNs) have limited expressive power due to the constraint of the activation functions and their architectures. We generalize spectral graph convolution and deep GCN in block Krylov subspace forms and devise two architectures, both with the potential to be scaled deeper but each making use of the multi-scale information differently. On several node classification tasks, with or without validation set, the two proposed architectures achieve state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>Many real-world problems can be modeled as graphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref>. Among the recent focus of applying machine learning algorithms on these problems, graph convolution in Graph Convolutional Networks (GCNs) stands out as one of the most powerful tools and the key operation, which was inspired by the success of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b18">[19]</ref> in computer vision <ref type="bibr" target="#b20">[21]</ref>. In this paper, we focus on spectrum-free Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>, which have obtained state-of-the-art performance on multiple transductive and inductive learning tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>One big challenge for the existing GCNs is the limited expressive power of their shallow learning mechanisms <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35]</ref>. The difficulty of extending GCNs to richer architectures leads to several possible explanations and even some opinions that express the unnecessities of addressing such a problem:</p><p>1. Graph convolution can be considered as a special form of Laplacian smoothing <ref type="bibr" target="#b19">[20]</ref>. A network with multiple convolutional layers will suffer from an over-smoothing problem that makes the representation of the nodes indistinguishable even for the nodes that are far from each other <ref type="bibr" target="#b36">[37]</ref>. 2. For many cases, it is not necessary for the label information to totally traverse the entire graph. Moreover, one can operate on the multi-scale coarsening of the input graph and obtain the same flow of information as GCNs with more layers <ref type="bibr" target="#b1">[2]</ref>.</p><p>Nevertheless, shallow learning mechanisms violate the compositionality principle of deep learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15]</ref> and restrict label propagation <ref type="bibr" target="#b29">[30]</ref>. In this paper, we first give analyses of the lack of scalability of the existing GCN. Then we show that any graph convolution with a well-defined analytic spectral filter can be written as a product of a block Krylov matrix and a learnable parameter matrix in a special form. Based on the analyses, we propose two GCN architectures that leverage multi-scale information with differently and are scalable to deeper and richer structures, with the expectation of having stronger expressive powers and abilities to extract richer representations of graph-structured data. We also show that the equivalence of the two architectures can be achieved under certain conditions. For validation, we test the proposed architectures on multiple transductive tasks using their different instances. The results show that even the simplest instantiation of the proposed architectures yields state-of-the-art performance and the complex ones achieve surprisingly higher performance, both with or without the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We use bold font for vectors v, block vectors V and matrix blocks V i as in <ref type="bibr" target="#b9">[10]</ref>. Suppose we have an undirected graph G = (V, E, A), where V is the node set with |V| = N, E is the edge set with |E| = E, and A ∈ R N×N is a symmetric adjacency matrix. Let D denote the diagonal degree matrix, i.e. D ii = j A i j . A diffusion process on G can be defined by a diffusion operator L <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> which is a symmetric positive semi-definite matrix, e.g. graph Laplacian L = D − A, normalized graph Laplacian L = I − D −1/2 AD −1/2 and affinity matrix L = A + I, etc.</p><p>We use L to denote a general diffusion operator in this paper. The eigendecomposition of L gives us L = UΛU T , where Λ is a diagonal matrix whose diagonal elements are eigenvalues and the columns of U are the orthonormal eigenvectors and named graph Fourier basis. We also have a feature matrix (graph signals, can be regarded as a block vector) X ∈ R N×F defined on V and each node i has a feature vector X i,: , which is the i th row of X.</p><p>Graph convolution is defined in graph Fourier domain s.t. x * G y = U((U T x) (U T y)), where x, y ∈ R N and is the Hadamard product <ref type="bibr" target="#b6">[7]</ref>. Following from this definition, a graph signal x filtered by g θ can be written as</p><formula xml:id="formula_0">y = g θ (L)x = g θ (UΛU T )x = Ug θ (Λ)U T x<label>(1)</label></formula><p>where g θ can be any function which is analytic inside a closed contour which encircles λ(L), e.g. Chebyshev polynomial <ref type="bibr" target="#b6">[7]</ref>. GCN generalizes this definition to signals with F input channels and O output channels and the network structure is</p><formula xml:id="formula_1">Y = softmax(L ReLU(LXW 0 ) W 1 )<label>(2)</label></formula><p>where L = D −1/2Ã D −1/2 andÃ = A + I. This is called spectrum-free method <ref type="bibr" target="#b1">[2]</ref> that requires no explicit computation of eigendecomposition <ref type="bibr" target="#b36">[37]</ref> and operations on the frequency domain. We will focus on the analysis of GCN in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Why GCN is not Scalable?</head><p>Suppose we scale GCN to a deeper architecture in the same way as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>, it becomes</p><formula xml:id="formula_2">Y = softmax(L ReLU(· · · L ReLU(L ReLU(LXW 0 ) W 1 ) W 2 · · · ) W n ) ≡ softmax(Y ) (3)</formula><p>For this, we have the following theorems. Theorem 1 shows that if we simply increase the depth based on GCN architecture, the extracted features Y will at most encode stationary information of graph structure and lose all the information in node features. In addition, from the proof we see that the point-wise ReLU transformation is a conspirator. Theorem 2 tells us that Tanh is better in keeping linear independence among column features. We design a numerical experiment on synthetic data (see Appendix) to test, under a 100-layer GCN architecture, how activation functions affect the rank of the output in each hidden layer during the feed-forward process. As <ref type="figure" target="#fig_0">Figure  1</ref>(a) shows, the rank of hidden features decreases rapidly with ReLU, while having little fluctuation under Tanh, and even the identity function performs better than ReLU (see Appendix for more comparisons). So we propose to replace ReLU by Tanh. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notation and Backgrounds</head><p>Let S be a block vector subspace of R F×F containing the identity matrix I F that is closed under matrix multiplication and transposition. We define an inner product ·, · S in the block vector space R N×F as follows <ref type="bibr" target="#b9">[10]</ref>, Definition 1 A mapping ·, · S from R N×F × R N×F to S is called a block inner product onto S if it satisfies the following conditions for all X, Y, Z ∈ R N×F and C ∈ S: There are mainly three ways to define ·, · S , we use the classical one: S Cl = R F×F and X, Y Cl S = X T Y. We define a block vector subspace of R N×F , which will be used later. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spectral Graph Convolution in Block Krylov Subspace Form</head><p>In this section, we will show that any graph convolution with well-defined analytic spectral filter defined on L ∈ R N×N can be written as the product of a block Krylov matrix with a learnable parameter matrix in a specific form.</p><p>For any real analytic scalar function g, its power series expansion around center 0 is</p><formula xml:id="formula_3">g(x) = ∞ n=0 a n x n = ∞ n=0 g (n) (0) n! x n , |x| &lt; R</formula><p>where R is the radius of convergence. We can define a filter by g. Let ρ(L) denote the spectrum radius of L and suppose ρ(L) &lt; R. The spectral filter g(L) ∈ R N×N can be defined as</p><formula xml:id="formula_4">g(L) := ∞ n=0 a n L n = ∞ n=0 g (n) (0) n! L n , ρ(L) &lt; R</formula><p>According to the definition of spectral graph convolution in (1), graph signal X is filtered by g(L) in the following way,</p><formula xml:id="formula_5">g(L)X = ∞ n=0 g (n) (0) n! L n X = X, LX, L 2 X, · · · g (0) (0) 0! I F , g (1) (0) 1! I F , g (2) (0) 2! I F , · · · T = A B</formula><p>where A ∈ R N×∞ and B ∈ R ∞×F . It is easy to see that A is a block Krylov matrix and Range(A B ) ⊆ Range(A ). We know that there exists a smallest m such that <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref> span S {X, LX, L 2 X, · · · } = span S {X, LX, L 2 X, . . . , L m−1 X},</p><p>i.e. for any k ≥ m, L k X ∈ K S m (L, X). m depends on L and X, so we will write it as m(L, X) later, yet here we still use m for simplicity. From (4), the convolution can be written as</p><formula xml:id="formula_7">g(L)X = ∞ n=0 g (n) (0) n! L n X = X, LX, L 2 X, . . . , L m−1 X (Γ 0 S ) T , (Γ 1 S ) T , (Γ 2 S ) T , · · · , (Γ S m−1 ) T T ≡ K m (L, X)Γ S (5) where Γ S i , i = 1, .</formula><p>. . , m − 1 are parameter matrix blocks and Γ S i ∈ R F×F under classical definition of inner product. Then</p><formula xml:id="formula_8">g(L)XW = K m (L, X)Γ S W = K m (L, X)W S<label>(6)</label></formula><p>where W S ≡ Γ S W ∈ R mF×O . The essential number of learnable parameters is mF × O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deep GCN in Block Krylov Subspace Form</head><p>Since the spectral graph convolution can be simplified as <ref type="formula" target="#formula_8">(5)(6)</ref>, we can build deep GCN in the following way.</p><p>Suppose we have a sequence of analytic spectral filters G = {g 0 , g 1 , . . . , g n } and a sequence of point-wise nonlinear activation functions H = {h 0 , h 1 , . . . , h n }. Then,</p><formula xml:id="formula_9">Y = softmax g n (L) h n−1 · · · g 2 (L) h 1 g 1 (L) h 0 g 0 (L)XW 0 W 1 W 2 · · · W n<label>(7)</label></formula><p>Let us define H 0 = X and</p><formula xml:id="formula_10">H i+1 = h i {g i (L)H i W i }, i = 0, . . . , n−1. Then Y = softmax{g n (L)H n W n }. From (6)(7), we have an iterative relation that H i+1 = h i {K m i (L, H i )W S i }, where m i = m(L, H i ).</formula><p>It is easy to see that, when g i (L) = I, <ref type="bibr" target="#b6">(7)</ref> is fully connected network <ref type="bibr" target="#b19">[20]</ref>; when g i (L) =Ã, n = 1, it is just GCN <ref type="bibr" target="#b16">[17]</ref>; when g i (L) is defined by Chebyshev polynomial <ref type="bibr" target="#b13">[14]</ref>, W i = I and under the global inner product, <ref type="bibr" target="#b6">(7)</ref> is ChebNet <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Difficulties in Computation</head><p>In the last subsection, we gave a general form of deep GCN in block Krylov form. Following this idea, we can leverage the existing block Arnoldi (Lanczos) algorithm <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> to compute orthogonal basis of K S m i (L, H i ) and find m i . But there are some difficulties in practice:</p><p>1. During the training phase, H i changes every time that parameters are updated. This makes m i become a variable and thus requires adaptive size for parameter matrices. 2. For classical inner product, the QR factorization that is needed in block Arnoldi algorithm <ref type="bibr" target="#b9">[10]</ref> is difficult to be put into backpropagation framework.</p><p>Although direct implementation of block Krylov methods in GCN is hard, it inspires us that if we have a good way to stack multi-scale information in each hidden layer, the network will have the ability to be extended to deep architectures. We propose a way to alleviate the difficulties in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Deep GCN Architectures</head><p>Upon the analyses in the last section, we propose two architectures: snowball and truncated Krylov. These methods concatenate multi-scale feature information in hidden layers differently while both having the potential to be scaled to deeper architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Snowball</head><p>In order to concatenate multi-scale features together and get a richer representation for each node, we design a densely connected graph network <ref type="figure" target="#fig_2">(Figure 2</ref>(a)) as follows,</p><formula xml:id="formula_11">H 0 = X, H l+1 = f (L [H 0 , H 1 , . . . , H l ] W l ) , l = 0, 1, 2, . . . , n − 1 C = g ([H 0 , H 1 , . . . , H n ] W n ) (8) output = softmax (L p CW C ) where W l ∈ R l i=0 F i ×F l+1 , W n ∈ R n i=0 F i ×F C , W C ∈ R F C ×F O are learnable parameter matrices, F l+1</formula><p>is the number of output channels in layer l; f, g are point-wise activation functions; C is a classifier of any kind; p ∈ {0, 1}. H 0 , H 1 , . . . , H n are extracted features. C can be a fully connected neural network or even an identity layer with C = [H 0 , H 1 , . . . , H n ]. When p = 0, L p = I and when p = 1, L P = L, which means that we project C back onto graph Fourier basis which is necessary when graph structure encodes much information. Following this construction, we can stack all learned features as the input of the subsequent hidden layer, which is an efficient way to concatenate multi-scale information. The size of input will grow like a snowball and this construction is similar to DenseNet <ref type="bibr" target="#b15">[16]</ref>, which is designed for regular grids (images). Thus, some advantages of DenseNet are naturally inherited, e.g. alleviate the vanishing-gradient problem, encourage feature reuse, increase the variation of input for each hidden layer, reduce the number of parameters, strengthen feature propagation and improve model compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Truncated Krylov</head><p>As stated in Section 4.4, the fact that m i is a variable makes GCN difficult to be merged into the block Krylov framework. But we can make a compromise and set m i as a hyperparameter. Then we can get a truncated block Krylov network <ref type="figure" target="#fig_2">(Figure 2(b)</ref>) as shown below, There are many works on the analysis of error bounds of doing truncation in block Krylov methods <ref type="bibr" target="#b9">[10]</ref>. But the results need many assumptions either on X, e.g. X is a standard Gaussian matrix <ref type="bibr" target="#b32">[33]</ref>, or on L, e.g. some conditions on the smallest and largest eigenvalues of L have to be satisfied <ref type="bibr" target="#b26">[27]</ref>. Instead of doing truncation for a specific function or a fixed X, we are dealing with variable X during training. So we cannot put any restriction on X and its relation to L to get a practical error bound. Here we would like to mention <ref type="bibr" target="#b23">[24]</ref>, which proposes to do low-rank approximation of L by the Lanczos algorithm. The problem with this technique is that information in L will be lost if L is actually not low-rank. If we increase the Lanczos step to keep more information, it will hurt the efficiency of the algorithm. Since most of the graphs we are dealing with have sparse connectivity structures, they are actually not low-rank, e.g. Erdős-Rényi graph G(n, p) with p = ω( 1 n ) <ref type="bibr" target="#b30">[31]</ref> and Appendix IV. Thus we did not do low-rank approximation in our computations.</p><formula xml:id="formula_12">H 0 = X, H l+1 = f H l , LH l . . . , L m l −1 H l W l , l = 0, 1, 2, . . . , n − 1 C = g (H n W n ) (9) output = softmax (L p CW C ) where W l ∈ R (m l F l )×F l+1 , W n ∈ R F n ×F C , W C ∈ R F C ×F O</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Equivalence of Linear Snowball GCN and Truncated Block Krylov Network</head><p>Under the snowball GCN architecture, the identity function outperforms ReLU as shown in <ref type="figure" target="#fig_0">Figure 1</ref> and it is easier to train than Tanh. In this part, we will show that a multi-layer linear snowball GCN with identity function as f , identity layer as C and p = 1 is equivalent to a 1-layer block Krylov network with identity layer C, p = 1 and a special parameter matrix.</p><p>We write W i as W i = (W 1 i ) T , · · · , (W i+1 i ) T T and follow <ref type="bibr" target="#b7">(8)</ref> we have <ref type="formula">(8)</ref>, we have LCW C = L[H 0 , H 1 , . . . , H n ]W C . Thus we can write</p><formula xml:id="formula_13">H 0 = X, H 1 = LXW 0 , H 2 = L[X, H 1 ]W 1 = LXW 1 1 + L 2 XW 0 W 2 1 = L[X, LX] I 0 0 W 0        W 1 1 W 2 1        , . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As in</head><formula xml:id="formula_14">[H 0 , H 1 · · · , H n ] = [X, LX, · · · , L n X]                   I 0 · · · 0 0 I · · · 0 . . . . . . . . . . . . 0 0 · · · W 0                                     I 0 · · · 0 0 I · · · 0 . . . . . . . . . . . . 0 0 · · · W 1 1                   · · ·                   I 0 · · · 0 0 W n n−1 · · · 0 . . . . . . . . . . . . 0 0 · · · W 1 n−1                  </formula><p>which is in the form of (6), where the parameter matrix is the multiplication of a sequence of block diagonal matrices whose entries consist of identity blocks and blocks from other parameter matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Intuition behind Multi-scale Information Concatenation</head><p>For each node v, denote N(v) as the set of its neighbors. Then LX(v, :) can be interpreted as a weighted mean of v and N(v). If the networks goes deep as <ref type="formula">(3)</ref>, Y (v, :) becomes the weighted mean of v and its n−hop neighbors (not exactly mean because we have ReLU in each layer). As the scope grows, the nodes in the same connected component tend to have the same (global) features, while losing their individual (local) features, which makes them indistinguishable. Although it is reasonable to assume that the nodes in the same cluster share many similar properties, it will be harmful if we ignore the "personal" differences between each node.</p><p>Therefore, to get a richer representation of each node, we propose to concatenate the multi-scale information together and the most naive architecture is the densely connected one. Truncated Krylov network works because in each layer i, we start the concatenation from L 0 H i . By this way, the local information will not be diluted in each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We test linear snowball GCN ( f = g=identity, p = 1), snowball GCN ( f =Tanh, g=identity, p = 1) and truncated block Krylov network ( f = g=Tanh, p = 0) on public splits <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref> of Cora, Citeseer and PubMed 1 , as well as several smaller splits to increase the difficulty of the tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. We compare against several methods which allow validation, including graph convolutional networks for fingerprint (GCN-FP) <ref type="bibr" target="#b7">[8]</ref>, gated graph neural networks (GGNN) <ref type="bibr" target="#b21">[22]</ref>, diffusion convolutional neural networks (DCNN) <ref type="bibr" target="#b0">[1]</ref>, Chebyshev networks (Cheby) <ref type="bibr" target="#b6">[7]</ref>, graph convolutional networks (GCN) <ref type="bibr" target="#b16">[17]</ref>, message passing neural networks (MPNN) <ref type="bibr" target="#b10">[11]</ref>, graph sample and aggregate (GraphSAGE) <ref type="bibr" target="#b12">[13]</ref>, graph partition neural networks (GPNN) <ref type="bibr" target="#b22">[23]</ref>, graph attention networks (GAT) <ref type="bibr" target="#b31">[32]</ref>, LanczosNet (LNet) <ref type="bibr" target="#b23">[24]</ref> and AdaLanczosNet (AdaLNet) <ref type="bibr" target="#b23">[24]</ref>. We also compare against some methods that do no use validation, including label propagation using ParWalks (LP) <ref type="bibr" target="#b33">[34]</ref>, Co-training <ref type="bibr" target="#b19">[20]</ref>, Self-training <ref type="bibr" target="#b19">[20]</ref>, Union <ref type="bibr" target="#b19">[20]</ref>, Intersection <ref type="bibr" target="#b19">[20]</ref>, GCN without validation <ref type="bibr" target="#b19">[20]</ref>, Multi-stage training <ref type="bibr" target="#b29">[30]</ref>, Multi-stage self-supervised (M3S) training <ref type="bibr" target="#b29">[30]</ref>, GCN with sparse virtual adversarial training (GCN-SVAT) <ref type="bibr" target="#b28">[29]</ref> and GCN with dense virtual adversarial training (GCN-DVAT) <ref type="bibr" target="#b28">[29]</ref>. For each test case, we use the best hyperparameters to run 10 independent times to get the average accuracy. The hyperparameters include learning rate and weight decay for the optimizer RMSprop or Adam, taking values in the intervals [10 −6 , 5 × 10 −3 ] and [10 −5 , 10 −2 ], respectively, width of hidden layers taking value in the set {50, · · · , 5000}, number of hidden layers in the set {1, 2, . . . , 15}, dropout in (0, 0.99], and the number of Krylov blocks taking value in {4, 5, . . . , 30}. The hyperparameter values of the test cases will be presented in the appendix.</p><p>To get achieve good training, we use adaptive number of episodes (but no more than 3000): the early stopping counter is set to be 100.</p><p>We see that the proposed architectures achieve overwhelming performance in ALL test cases. It is particularly worth noting that when the training sets are small, the proposed architectures perform astonishingly better than the existing methods. From the t-SNE <ref type="bibr" target="#b24">[25]</ref> visualization of the output layers in <ref type="figure" target="#fig_5">Figure 3</ref>, we see that the architectures extract good features with small training data, especially for truncated Krylov. What also impresses us is that linear Snowball GCN can achieve state-of-the-art performance with much less computational cost.  For each (column), the greener the cell, the better the performance. The redder, the worse. If our methods achieve better performance than all others, the corresponding cell will be in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Appendix I: Proof of Theorem 1, 2</head><p>We extend Theorem 1 in <ref type="bibr" target="#b19">[20]</ref> to a general diffusion operator L in the following lemma. Lemma 1. Suppose that a graph G has k connected components {C i } k i=1 and the diffusion operator L is defined as that in <ref type="bibr" target="#b1">(2)</ref>. L has k linearly independent eigenvectors {v 1 , . . . , v k } corresponding to its largest eigenvalue λ max . If G has no bipartite components, then for any</p><formula xml:id="formula_15">x ∈ R N lim m→∞ ( 1 λ max L) m x = [v 1 , . . . , v k ]θ,<label>(10)</label></formula><p>for some θ ∈ R k .  </p><formula xml:id="formula_16">P(x + = dy + , x − dy − | x, y ∈ R N , d 0) = 1 2 N 1 + N 2 N Proof P(x + = dy + , x − dy − ) = P(x + = dy + , x − dy − | d f (x + ) ≤ 1) P(d f (x + ) ≤ 1) + P(x + = dy + , x − dy − | d f (x + ) &gt; 1) P(d f (x + ) &gt; 1) = 1 2 N 1 + N 2 N + 0 · 2 N − 1 − N 2 N = 1 2 N 1 + N 2 N</formula><p>where d f denotes degree of freedom. d f (x + ) ≤ 1 means that x can at most have one dimension to be positive and there are 1 + N out of 2 N hyperoctants that satisfies this condition. The set of y that can make x + = dy + , x − dy − hold has an area of 1 2 N , i.e. when y is in the same hyperoctant as x. If x lies in other hyperoctants, d f (y − ) ≤ N − 2. And since x + = dy + , y is just a low dimensional surface in R N with area 0. Theorem 1. Suppose that G has k connected components and the diffusion operator L is defined as that in <ref type="bibr" target="#b1">(2)</ref>. Let XR N×F be any block vector that randomly sampled under a continuous distribution and {W 0 , W 1 , . . . , W n } be any set of parameter matrices, if G has no bipartite components, then in (3), as n → ∞, rank(Y ) ≤ k almost surely.</p><p>Proof Upon the conclusions in Lemma 2-3, we have rank(ReLU(LX)) ≤rank(LX) with probability 1 and it is obvious rank(LXW 0 ) ≤rank(LX). Using these two inequality iteratively for (3), we have rank(Y ) ≤ rank(L n+1 X). Based on Lemma 1, we have probability 1 to get The goal of the experiments is to test which network structure with which kind of activation function has the potential to be extended to deep architecture. We measure this potential by the numerical rank of the output features in each hidden layer of the networks using synthetic data. The reason of choosing this measure can be explained by Theorem 3. We build the certain networks with depth 100 and the data is generated as follows.</p><formula xml:id="formula_17">lim n→∞ rank(Y ) ≤ lim n→∞ rank(L n+1 X) = rank([v 1 , . . . , v k ][θ 1 , . . . , θ F ]) ≤ rank([v 1 , . . . , v k ]) = k, where θ i ∈ R k , i = 1, . . . , F. Thus, rank(Y ) ≤ k</formula><p>We first randomly generate edges of an Erdős-Rényi graph G(1000, 0.01), i.e. the existence of the edge between any pair of nodes is a Bernoulli random variable with p = 0.01. Then, we construct the corresponding adjacency matrix A of the graph which is a R 1000×1000 matrix. We generate a R 1000×500 feature matrix X and each of its element is drawn from N(0, 1). We normalize A and X as <ref type="bibr" target="#b16">[17]</ref> and abuse the notation A, X to denote the normalized matrices. We keep 3 blocks in each layer of truncated block Krylov network. The number of input channel in each layer depends on the network structures and the number of output channel is set to be 128 for all networks. Each element in every parameter matrix W i , i = 1, . . . , 100 is randomly sampled from N(0, 1) and the size is R #input×#output . With the synthetic A, X, W i , we simulate the feedforward process according to the network architecture and collect the numerical rank (at most 128) of the output in each of the 100 hidden layers. For each activation function under each network architecture, we repeat the experiments for 20 times and plot the mean results with standard deviation bars.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix III: Rank Comparison of Activation Functions and Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix V: Experiment Settings and Hyperparameters</head><p>The so-called public splits in <ref type="bibr" target="#b23">[24]</ref> and the setting that randomly sample 20 instances for each class as labeled data in <ref type="bibr" target="#b35">[36]</ref> is actually the same. Most of the results for the algorithms with  <ref type="figure">Figure 6</ref>: Spectrum of the renormalized adjacency matrices for several datasets validation are cited from <ref type="bibr" target="#b23">[24]</ref>, where they are reproduced with validation. However, some of them actually do not use validation in original papers and can achieve better results. In the paper, We compare with their best results.</p><p>We use NVIDIA apex amp mixed-precision plugin for PyTorch to accelerate our experiments. Most of the results were obtained from NVIDIA V100 clusters on Beluga of Compute-Canada, with minor part of them obtained from NVIDIA K20, K80 clusters on Helios Compute-Canada. The hyperparameters are searched using Bayesian optimization.</p><p>A useful tip is the smaller your training set is, the larger dropout probability should be set and the larger early stopping you should have. <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table" target="#tab_3">Table 3</ref> shows the hyperparameters to achieve the performance in the experiments, for cases without and with validation, respectively. When conducting the hyperparameter search, we encounter memory problems: current GPUs cannot afford deeper and wider structures. But we do observe better performance with the increment of the network size. It is expected to achieve better performance with more advanced deep learning devices.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Number of independent column features 4 Spectral Graph Convolution and Block Krylov Subspace Methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>S-linearity: X, YC S = X, Y S C and X + Y, Z S = X, Z S + Y, Z S 2. symmetry: X, Y S = Y, X T S 3. definiteness: X, X S is positive definite if X has full rank, and X, X S = 0 F if and only if X = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 2</head><label>2</label><figDesc>Given a set of block vectors {X k } m k=1 ⊂ R N×F , the S-span of {X k } m k=1 is defined as span S {X 1 , . . . , X m } := { m k=1 X k C k : C k ∈ S} Given the above definition, the order-m block Krylov subspace with respect to A ∈ R N×N , B ∈ R N×F , and S can be defined as K S m (A, B) := span S {B, AB, . . . , A m−1 B}. The corresponding block Krylov matrix is defined as K m (A, B) := [B, AB, . . . , A m−1 B].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>are learnable parameter matrices, f and g are activation functions, and p ∈ {0, 1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Deep GCN Architectures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>t-SNE for the extracted features trained on Cora (7 classes) public (5.2%) using the best hyperparameter settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma 2 . 1 Proof1+N 2 N× 1 2 + 1 × 1 = 1 .</head><label>2122111</label><figDesc>Suppose we randomly sample x, y ∈ R N under a continuous distribution. Suppose we have point-wise function ReLU(z) = max(0, z), we have P(rank ReLU([x, y]) ≤ rank([x, y]) | x, y ∈ R N ) = We generalize ReLU onto multi-dimensional case by applying it element-wise on every element of the matrix. Any x ∈ R N can be represented as x = x + + x − , where x + and x − are the nonnegative and nonpositive components of x, respectively. We can see that ReLU(x) = x + . It is trivial when y = 0. We only discuss for all nonzero y ∈ R N . Suppose x and y are linearly independent (with probability 1), then c 0 that x = cy. If ∃ d 0 that ReLU(x)= d ReLU(y), then x + = dy + and x − dy − and the existence of this kind of x, y has a nonzero probability 1 2 N (See Lemma 3); other than these cases, the independency will be kept after the ReLU transformation. Suppose y is linearly dependent of x (with probability 0), i.e., ∃ c 0 that x = cy. If c &gt; 0, we have x − = cy − and x + = cy + . Since ReLU(x)=x + , ReLU(y)=y + , then ReLU(x)= cReLU(y). If c &lt; 0, we have x − = cy + and x + = cy − . If ∃ d 0 that ReLU(x)= d ReLU(y), this means x + = dy + = d c x − . This d exists when x + and x − are linearly dependent, which only holds when x = 0. This happens with probability 0. Then, ReLU(x) and d ReLU(y) will be independent. So whether ReLU keep the dependency between to vectors or not depends on the sign of c. Thus, under the assumption, we have probability 1 2 that ReLU keeps the dependency. According to the discussion above, we have P rank ReLU([x, y]) ≤ rank([x, y]) | x, y ∈ R N = P rank ReLU([x, y]) ≤ rank([x, y]), x, y ∈ R N P x, y ∈ R N = P rank ReLU([x, y]) ≤ rank([x, y]) | rank([x, y]) = 1, x, y ∈ R N P rank([x, y]) = 1, x, y ∈ R N + P rank ReLU([x, y]) ≤ rank([x, y]) | rank([x, y]) = 2, x, y ∈ R N P rank([x, y]) = 2, x, y ∈ R N = 0 Lemma proved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Lemma 3 .</head><label>3</label><figDesc>Suppose we randomly sample x, y ∈ R N under a continuous distribution, then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Theorem 2 . 1 Proof 1 x 2 covers</head><label>2112</label><figDesc>Suppose we randomly sample x, y ∈ R N under a continuous distribution and we have the point-wise function Tanh(z) = e z −e −z e z +e −z , we have P(rank Tanh([x, y]) ≥ rank([x, y]) | x, y ∈ R N ) = We first prove it for N = 2. It is trivial when x, y have 0 elements. Suppose x = [x 1 , x 2 ], y = [y 1 , y 2 ] are linearly dependent and all of their elements are nonzero. If Tanh(x) and Tanh(y) are still linearly dependent, we must have Tanh(point-wise Tanh transformation will break the dependency of x, y with probability 1. If x, y are linearly independent, suppose Tanh(x) = x = [x 1 , x 2 ] is a vector in R 2 , and the set of vectors in R 2 that can be transformed by point-wise Tanh to the same line as x covers an area of probability 0. That is for any fixed x ∈ R 2 , the solution of Tanh(y 1 ) Tanh(y 2 ) = x an area of 0. Thus, P(rank Tanh([x, y]) ≥ rank([x, y]) | x, y ∈ R N ) = 1 which means point-wise Tanh transformation will increase the independency between vectors in R 2 . Similar to R 2 , suppose x = [x 1 , x 2 , . . . , x N ], y = [y 1 , y 2 , . . . , y N ] are linearly dependent, if all elements in x, y are nonzero and Tanh(x) and Tanh(y) are still linearly dependent, we must have TanhThe solution of any pair of equations covers an area of probability 0 in R N . Actually, this still holds when x, y have some 0 elements, i.e. for any subset of the above equations, the area of the solution is still 0. If x, y are linearly independent, suppose Tanh(x) = x is a vector in R N , and the space in R N that can be transformed by point-wise Tanh to the same line as x covers an area of probability 0. Therefore, Lemma 2 still holds in R N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Column ranks of different activation functions with the same architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>Column ranks of different architectures with the same activation function Appendix IV: Spectrum of the Datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Theorem 1 .</head><label>1</label><figDesc>Suppose that G has k connected components and the diffusion operator L is defined as that in<ref type="bibr" target="#b1">(2)</ref>. Let X be any block vector sampled from space R N×F according to a continuous distribution and {W 0 , W 1 , . . . , W n } be any set of parameter matrices, if G has no bipartite components, then in (3), as n → ∞, rank(Y ) ≤ k almost surely.</figDesc><table><row><cell>Proof See Appendix.</cell></row></table><note>Theorem 2. Suppose we randomly sample x, y ∈ R N under a continuous distribution and point-wise function Tanh(z) = e z −e −z e z +e −z , we have P(rank Tanh([x, y]) ≥ rank([x, y]) | x, y ∈ R N ) = 1 Proof See Appendix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy Without Validation</figDesc><table><row><cell>Algorithms</cell><cell cols="3">Cora 0.5% 1% 2% 3% 4% 5% 0.5% 1% 2% 3% 4% 5% 0.03% 0.05% 0.1% 0.3% CiteSeer PubMed</cell></row><row><cell>LP</cell><cell cols="3">56.4 62.3 65.4 67.5 69.0 70.2 34.8 40.2 43.6 45.3 46.4 47.3 61.4 66.4 65.4 66.8</cell></row><row><cell>Cheby</cell><cell cols="3">38.0 52.0 62.4 70.8 74.1 77.6 31.7 42.8 59.9 66.2 68.3 69.3 40.4 47.3 51.2 72.8</cell></row><row><cell>Co-training</cell><cell cols="3">56.6 66.4 73.5 75.9 78.9 80.8 47.3 55.7 62.1 62.5 64.5 65.5 62.2 68.3 72.7 78.2</cell></row><row><cell>Self-training</cell><cell cols="3">53.7 66.1 73.8 77.2 79.4 80.0 43.3 58.1 68.2 69.8 70.4 71.0 51.9 58.7 66.8 77.0</cell></row><row><cell>Union</cell><cell cols="3">58.5 69.9 75.9 78.5 80.4 81.7 46.3 59.1 66.7 66.7 67.6 68.2 58.4 64.0 70.7 79.2</cell></row><row><cell>Intersection</cell><cell cols="3">49.7 65.0 72.9 77.1 79.4 80.2 42.9 59.1 68.6 70.1 70.8 71.2 52.0 59.3 69.7 77.6</cell></row><row><cell>MultiStage</cell><cell>61.1 63.7 74.4 76.1 77.2</cell><cell>53.0 57.8 63.8 68.0 69.0</cell><cell>57.4 64.3 70.2</cell></row><row><cell>M3S</cell><cell>61.5 67.2 75.6 77.8 78.0</cell><cell>56.1 62.1 66.4 70.3 70.5</cell><cell>59.2 64.4 70.6</cell></row><row><cell>GCN</cell><cell cols="3">42.6 56.9 67.8 74.9 77.6 79.3 33.4 46.5 62.6 66.9 68.7 69.6 46.4 49.7 56.3 76.6</cell></row><row><cell>GCN-SVAT</cell><cell cols="3">43.6 53.9 71.4 75.6 78.3 78.5 47.0 52.4 65.8 68.6 69.5 70.7 52.1 56.9 63.5 77.2</cell></row><row><cell>GCN-DVAT</cell><cell cols="3">49 61.8 71.9 75.9 78.4 78.6 51.5 58.5 67.4 69.2 70.8 71.3 53.3 58.6 66.3 77.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :80.7 83.6 62.1 64.2 72.6 70.8 73.2 76.5 79.5 truncated Krylov 73.9 77.4 82.2 83.5 63.7 68.4 73.9 71.1 72.9 75.7 79.9</head><label>2</label><figDesc>Accuracy With Validation</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell cols="2">CiteSeer</cell><cell>PubMed</cell><cell></cell></row><row><cell>Algorithms</cell><cell>0.5% 1% 3%</cell><cell>5.2% public</cell><cell>0.5% 1%</cell><cell>3.6% public</cell><cell>0.03% 0.05% 0.1%</cell><cell>0.3% public</cell></row><row><cell>Cheby</cell><cell cols="6">33.9 44.2 62.1 78.0 45.3 59.4 70.1 45.3 48.2 55.2 69.8</cell></row><row><cell>GCN-FP</cell><cell cols="6">50.5 59.6 71.7 74.6 43.9 54.3 61.5 56.2 63.2 70.3 76.0</cell></row><row><cell>GGNN</cell><cell cols="6">48.2 60.5 73.1 77.6 44.3 56.0 64.6 55.8 63.3 70.4 75.8</cell></row><row><cell>DCNN</cell><cell cols="6">59.0 66.4 76.7 79.7 53.1 62.2 69.4 60.9 66.7 73.1 76.8</cell></row><row><cell>MPNN</cell><cell cols="6">46.5 56.7 72.0 78.0 41.8 54.3 64.0 53.9 59.6 67.3 75.6</cell></row><row><cell>GraphSAGE</cell><cell cols="6">37.5 49.0 64.2 74.5 33.8 51.0 67.2 45.4 53.0 65.4 76.8</cell></row><row><cell>GAT</cell><cell cols="6">41.4 48.6 56.8 83.0 38.2 46.5 72.5 50.9 50.4 59.6 79.0</cell></row><row><cell>GCN</cell><cell cols="6">50.9 62.3 76.5 80.5 43.6 55.3 68.7 57.9 64.6 73.0 77.8</cell></row><row><cell>LNet</cell><cell cols="6">58.1 66.1 77.3 79.5 53.2 61.3 66.2 60.4 68.8 73.4 78.3</cell></row><row><cell>AdaLNet</cell><cell cols="6">60.8 67.5 77.7 80.4 53.8 63.3 68.7 61.0 66.0 72.8 78.1</cell></row><row><cell cols="7">linear Snowball 70.0 73.1 81.0 83.2 59.4 65.9 73.5 68.1 70.0 73.8 79.2</cell></row><row><cell>Snowball</cell><cell>73.0 76.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters for Tests with Validation</figDesc><table><row><cell>Architecture</cell><cell cols="2">Dataset Percentage</cell><cell>Accuracy Ours SOTA</cell><cell>lr</cell><cell cols="3">Corresponding Hyperparameters weight_decay hidden layers/n_blocks dropout optimizer</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">69.99 60.8 1.0689E-03 1.4759E-02</cell><cell>128</cell><cell>6</cell><cell>0.66987 RMSprop</cell></row><row><cell></cell><cell>Cora</cell><cell>1% 3%</cell><cell cols="3">73.10 67.5 1.4795E-03 2.3764E-02 80.96 77.7 2.6847E-03 5.1442E-03</cell><cell>128 64</cell><cell>9 9</cell><cell>0.64394 RMSprop 0.23648 RMSprop</cell></row><row><cell></cell><cell></cell><cell cols="4">5.2% (public) 83.19 83.0 1.6577E-04 1.8606E-02</cell><cell>1024</cell><cell>3</cell><cell>0.65277 RMSprop</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">59.41 53.8 4.9284E-04 6.9420E-03</cell><cell>512</cell><cell>11</cell><cell>0.90071 RMSprop</cell></row><row><cell>linear Snowball</cell><cell>CiteSeer</cell><cell>1%</cell><cell cols="3">65.85 63.3 3.2628E-03 1.6374E-02</cell><cell>512</cell><cell>3</cell><cell>0.97331 RMSprop</cell></row><row><cell></cell><cell></cell><cell cols="4">3.6% (public) 73.54 72.5 2.8218E-03 1.9812E-02</cell><cell>5000</cell><cell>1</cell><cell>0.98327 Adam</cell></row><row><cell></cell><cell></cell><cell>0.03%</cell><cell cols="3">68.12 61.0 2.1124E-03 4.4161E-02</cell><cell>128</cell><cell>7</cell><cell>0.78683 RMSprop</cell></row><row><cell></cell><cell>Pubmed</cell><cell>0.05% 0.1%</cell><cell cols="3">70.04 68.8 4.9982E-03 2.6460E-02 73.83 73.4 1.2462E-03 4.9303E-02</cell><cell>128 128</cell><cell>4 6</cell><cell>0.86788 RMSprop 0.3299 RMSprop</cell></row><row><cell></cell><cell></cell><cell cols="4">0.3% (public) 79.23 79.0 2.4044E-03 2.3157E-02</cell><cell>4000</cell><cell>1</cell><cell>0.98842 Adam</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">72.96 60.8 2.3228E-04 2.1310E-02</cell><cell>950</cell><cell>7</cell><cell>0.88945 RMSprop</cell></row><row><cell></cell><cell>Cora</cell><cell>1% 3%</cell><cell cols="3">76.76 67.5 1.5483E-04 1.3963E-02 80.72 77.7 1.6772E-03 1.0725E-02</cell><cell>250 64</cell><cell>15 14</cell><cell>0.55385 RMSprop 0.80611 RMSprop</cell></row><row><cell></cell><cell></cell><cell cols="4">5.2% (public) 83.60 83.0 1.2994E-05 9.4469E-03</cell><cell>5000</cell><cell>3</cell><cell>0.025052 RMSprop</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">62.05 53.8 2.0055E-03 3.1340E-02</cell><cell>512</cell><cell>5</cell><cell>0.88866 RMSprop</cell></row><row><cell>Snowball</cell><cell>CiteSeer</cell><cell cols="4">1% 3.6% (public) 72.61 72.5 2.5527E-03 6.2812E-03 64.23 63.3 1.8759E-03 9.3636E-03</cell><cell>128 256</cell><cell>7 1</cell><cell>0.77334 RMSprop 0.56755 RMSprop</cell></row><row><cell></cell><cell></cell><cell>0.03%</cell><cell cols="3">70.78 61.0 1.1029E-03 1.8661E-02</cell><cell>100</cell><cell>15</cell><cell>0.83381 RMSprop</cell></row><row><cell></cell><cell>Pubmed</cell><cell>0.05% 0.1%</cell><cell cols="3">73.23 68.8 3.7159E-03 2.2088E-02 76.52 73.4 4.9106E-03 3.0777E-02</cell><cell>400 100</cell><cell>9 15</cell><cell>0.9158 RMSprop 0.79133 RMSprop</cell></row><row><cell></cell><cell></cell><cell cols="4">0.3% (public) 79.54 79.0 4.9867E-03 3.5816E-03</cell><cell>3550</cell><cell>1</cell><cell>0.98968 Adam</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">73.89 60.8 1.6552E-04 4.4330E-02</cell><cell>4950</cell><cell>27</cell><cell>0.97726 Adam</cell></row><row><cell></cell><cell>Cora</cell><cell>1% 3%</cell><cell cols="3">77.38 67.5 2.8845E-04 4.8469E-02 82.23 77.7 8.6406E-04 4.0126E-03</cell><cell>4950 2950</cell><cell>30 16</cell><cell>0.93928 Adam 0.98759 Adam</cell></row><row><cell></cell><cell></cell><cell cols="4">5.2% (public) 83.51 83.0 1.0922E-03 3.5966E-02</cell><cell>1950</cell><cell>10</cell><cell>0.98403 Adam</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">63.65 53.8 2.8208E-03 4.3395E-02</cell><cell>1150</cell><cell>30</cell><cell>0.92821 Adam</cell></row><row><cell>truncated Krylov</cell><cell>CiteSeer</cell><cell cols="4">1% 3.6% (public) 73.89 72.5 1.8292E-03 4.2295E-02 68.36 63.3 3.9898E-03 3.8525E-03</cell><cell>100 600</cell><cell>27 11</cell><cell>0.71951 Adam 0.98865 Adam</cell></row><row><cell></cell><cell></cell><cell>0.03%</cell><cell cols="3">71.11 61.0 3.6759E-03 1.2628E-02</cell><cell>512</cell><cell>8</cell><cell>0.95902 RMSprop</cell></row><row><cell></cell><cell>Pubmed</cell><cell>0.05% 0.1%</cell><cell cols="3">72.86 68.8 4.0135E-03 4.8831E-02 75.68 73.4 4.7562E-03 3.7134E-02</cell><cell>4250 950</cell><cell>5 7</cell><cell>0.95911 Adam 0.96569 Adam</cell></row><row><cell></cell><cell></cell><cell cols="4">0.3% (public) 79.88 79.0 3.9673E-04 2.2931E-02</cell><cell>1900</cell><cell>4</cell><cell>0.000127 Adam</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for Tests without Validation</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell><cell cols="3">Correspondong Hyperparameters</cell></row><row><cell>Architecture</cell><cell cols="2">Dataset Percentage</cell><cell>Ours SOTA</cell><cell>lr</cell><cell cols="3">weight_decay hidden layers/n_blocks dropout Optimizer</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">69.53 61.5 4.4438E-05 1.7409E-02</cell><cell>550</cell><cell>12</cell><cell>0.007753 Adam</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell cols="3">74.12 69.9 1.0826E-03 3.3462E-03</cell><cell>1250</cell><cell>3</cell><cell>0.50426</cell><cell>Adam</cell></row><row><cell></cell><cell>Cora</cell><cell>2% 3%</cell><cell cols="3">79.43 75.9 2.4594E-06 9.6734E-03 80.41 78.5 2.8597E-05 3.4732E-02</cell><cell>1650 900</cell><cell>12 15</cell><cell>0.34073 0.039034 Adam Adam</cell></row><row><cell></cell><cell></cell><cell>4%</cell><cell cols="3">81.3 80.4 3.6830E-05 1.5664E-02</cell><cell>3750</cell><cell>4</cell><cell>0.93797</cell><cell>Adam</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell cols="3">82.19 81.7 5.8323E-06 8.5940E-03</cell><cell>2850</cell><cell>5</cell><cell>0.14701</cell><cell>Adam</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">56.76 56.1 4.5629E-03 2.0106E-03</cell><cell>300</cell><cell>3</cell><cell>0.038225 Adam</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell cols="3">65.44 62.1 3.5530E-05 4.9935E-02</cell><cell>600</cell><cell>6</cell><cell>0.03556</cell><cell>Adam</cell></row><row><cell>linear Snowball</cell><cell>CiteSeer</cell><cell>2% 3%</cell><cell cols="3">68.78 68.6 6.1176E-06 3.0101E-02 71 70.3 2.1956E-05 4.3569E-02</cell><cell>1950 3350</cell><cell>3 3</cell><cell>0.040484 Adam 0.30207 Adam</cell></row><row><cell></cell><cell></cell><cell>4%</cell><cell cols="3">72.23 70.8 9.1952E-05 4.6407E-02</cell><cell>3350</cell><cell>2</cell><cell>0.018231 Adam</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell cols="3">72.21 71.3 3.7173E-03 1.9605E-03</cell><cell>2950</cell><cell>1</cell><cell>0.96958</cell><cell>Adam</cell></row><row><cell></cell><cell></cell><cell>0.03%</cell><cell cols="3">64.133 62.2 1.0724E-03 8.1097E-03</cell><cell>64</cell><cell>4</cell><cell>0.8022 RMSProp</cell></row><row><cell></cell><cell>Pubmed</cell><cell>0.05% 0.1%</cell><cell cols="3">69.48 68.3 1.5936E-03 3.0236E-03 72.93 72.7 4.9733E-03 1.3744E-03</cell><cell>6 128</cell><cell>10 3</cell><cell>0.73067 RMSProp 0.91214 RMSProp</cell></row><row><cell></cell><cell></cell><cell>0.3%</cell><cell cols="3">79.33 79.2 1.7998E-03 9.6753E-04</cell><cell>512</cell><cell>1</cell><cell>0.97483 RMSProp</cell></row><row><cell></cell><cell></cell><cell>0.5%</cell><cell cols="3">67.15 61.5 9.8649E-04 1.0305E-02</cell><cell>1600</cell><cell>3</cell><cell>0.92785</cell><cell>Adam</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell cols="3">73.47 69.9 1.4228E-04 1.3472E-02</cell><cell>100</cell><cell>13</cell><cell>0.68601</cell><cell>Adam</cell></row><row><cell></cell><cell>Cora</cell><cell>2% 3%</cell><cell cols="3">78.54 75.9 5.7111E-06 1.5544E-02 79.97 78.5 4.0278E-05 2.7287E-02</cell><cell>600 4350</cell><cell>13 5</cell><cell>0.022622 Adam 0.57173 Adam</cell></row><row><cell></cell><cell></cell><cell>4%</cell><cell cols="3">81.49 80.4 1.4152E-05 2.3359E-02</cell><cell>2500</cell><cell>13</cell><cell>0.018578 Adam</cell></row><row><cell></cell><cell></cell><cell>5%</cell><cell cols="3">81.82 81.7 1.2621E-03 1.5323E-02</cell><cell>3550</cell><cell>2</cell><cell>0.87352</cell><cell>Adam</cell></row><row><cell>Snowball</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code to be found at https://github.com/PwnerHarry/Stronger_GCN</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
		<idno>abs/1511.02136</idno>
		<title level="m">Diffusion-convolutional neural networks. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond euclidean data. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10568</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Diffusion maps. Applied and computational harmonic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="5" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="94" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The radau-lanczos method for matrix functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Szyld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="710" to="732" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Block Krylov subspace methods for functions of matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Szyld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Transactions on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="100" to="126" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The block grade of a block krylov space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gutknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">430</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="185" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Inductive representation learning on large graphs. arXiv, abs/1706.02216</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1801.07606</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph partition neural networks for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06272</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Lanczosnet: Multi-scale deep graph convolutional networks. arXiv, abs</title>
		<imprint>
			<date type="published" when="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stability of the lanczos method for matrix function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1605" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.0053</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Virtual adversarial training on graph convolutional networks in node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11045</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-stage self-supervised learning for graph convolutional networks. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Sparse random graphs: Eigenvalues and eigenvectors. Random Structures &amp; Algorithms</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="110" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
		<title level="m">Graph attention networks. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved analyses of the randomized power method and block lanczos method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06429</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning with partially absorbing random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3077" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph convolutional networks: Algorithms, applications and open challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Social Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="91" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
