<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Self-Supervised Regularization for Supervised and Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phi</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Flyreel AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Flyreel AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Self-Supervised Regularization for Supervised and Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in semi-supervised learning have shown tremendous potential in overcoming a major barrier to the success of modern machine learning algorithms: access to vast amounts of human-labeled training data. Previous algorithms based on consistency regularization can harness the abundance of unlabeled data to produce impressive results on a number of semi-supervised benchmarks, approaching the performance of strong supervised baselines using only a fraction of the available labeled data. In this work, we challenge the long-standing success of consistency regularization by introducing self-supervised regularization as the basis for combining semantic feature representations from unlabeled data. We perform extensive comparative experiments to demonstrate the effectiveness of self-supervised regularization for supervised and semi-supervised image classification on SVHN, CIFAR-10, and CIFAR-100 benchmark datasets. We present two main results: (1) models augmented with self-supervised regularization significantly improve upon traditional supervised classifiers without the need for unlabeled data; (2) together with unlabeled data, our models yield semi-supervised performance competitive with, and in many cases exceeding, prior state-of-the-art consistency baselines. Lastly, our models have the practical utility of being efficiently trained end-to-end and require no additional hyper-parameters to tune for optimal performance beyond the standard set for training neural networks. Reference code and data are available at https://github.com/vuptran/sesemi.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Contemporary approaches to supervised representation learning, such as the convolutional neural network (CNN), continue to push the boundaries of research across a number of domains including speech recognition, visual understanding, and language modeling. However, such progresses usually require massive amounts of human-labeled training data. The process of collecting, curating, and hand-labeling large amounts of training data is often tedious, time-consuming, and costly to scale. Thus, there is a growing body of research dedicated to learning with limited labels, enabling machines to do more with less human supervision, in order to fully harness the benefits of deep learning in real-world settings. Such emerging research directions include domain adaptation <ref type="bibr" target="#b50">[51]</ref>, low-shot learning <ref type="bibr" target="#b46">[47]</ref>, self-supervised learning <ref type="bibr" target="#b19">[20]</ref>, and multi-task learning <ref type="bibr" target="#b35">[36]</ref>. In this work, we propose to combine multiple modes of supervision on sources of labeled and unlabeled data for enhanced learning and generalization. Specifically, our work falls within the framework of semi-supervised learning (SSL) <ref type="bibr" target="#b4">[5]</ref>, in the context of image classification, which can leverage abundant unlabeled data to significantly improve upon supervised classifiers in the limited labeled data setting. Indeed, in many cases, state-of-the-art semi-supervised algorithms have been shown to approach the performance of strong supervised baselines using only a fraction of the available labeled data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b25">26]</ref>. Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b <ref type="bibr" target="#b0">(1)</ref> ⌘ + b <ref type="bibr" target="#b1">(2)</ref> ⌘ .</p><p>Decoderâi = V T · ReLU ⇣ W T zi + b <ref type="bibr" target="#b2">(3)</ref> ⌘ + b <ref type="bibr" target="#b3">(4)</ref> . Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) ,</p><formula xml:id="formula_0">Lmbce = P i mi Lbce P i mi .</formula><p>Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi2D L yi2D L xi2D UL yi2D UL Lfully-supervised Lself-supervised Lsemi-supervised g(x) h(x) h(x) f✓(x)</p><formula xml:id="formula_1">1 Neural Network Lmbce = mi Lbce P mi ,</formula><p>Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b <ref type="bibr" target="#b0">(1)</ref> ⌘ + b <ref type="bibr" target="#b1">(2)</ref> ⌘ .</p><p>Decoderâi = V T · ReLU ⇣ W T zi + b <ref type="bibr" target="#b2">(3)</ref> ⌘ + b <ref type="bibr" target="#b3">(4)</ref> . Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) ,</p><formula xml:id="formula_2">Lmbce = P i mi Lbce P i mi .</formula><p>Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi2D L yi2D L xi2D UL yi2D UL Lfully-supervised Lself-supervised Lsemi-supervised g(x) h(x) h(x) f✓(x) 1 Data Augmentation Lmbce = mi Lbce P mi ,</p><p>Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b <ref type="bibr" target="#b0">(1)</ref> ⌘ + b <ref type="bibr" target="#b1">(2)</ref> ⌘ .</p><p>Decoderâi = V T · ReLU ⇣ W T zi + b <ref type="bibr" target="#b2">(3)</ref> ⌘ + b <ref type="bibr" target="#b3">(4)</ref> . Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) ,</p><formula xml:id="formula_3">Lmbce = P i mi Lbce P i mi .</formula><p>Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi2D L yi2D L xi2D UL yi2D UL Lfully-supervised Lself-supervised Lsemi-supervised g(x) h(x) h(x) f✓(x)ŷiỹi (xi, yi) 2 DL (xi, yi) 2 DUL 1 Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b <ref type="bibr" target="#b0">(1)</ref> ⌘ + b <ref type="bibr" target="#b1">(2)</ref> ⌘ .</p><p>Decoderâi = V T · ReLU ⇣ W T zi + b <ref type="bibr" target="#b2">(3)</ref> ⌘ + b <ref type="bibr" target="#b3">(4)</ref> . Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) , Lmbce = P i mi Lbce P i mi .</p><p>Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi 2 DL yi2D L xi 2 DUL yi2D UL Lfully-supervised Lself-supervised Lsemi-supervised g(x) h(x)h(x) f ✓ (x)ŷiỹi (xi, yi) 2 DL (xi, yi) 2 DULzi ziỹi 2 DUL 1 Lmbce = P mi ,</p><p>Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b <ref type="bibr" target="#b0">(1)</ref> ⌘ + b <ref type="bibr" target="#b1">(2)</ref> ⌘ .</p><p>Decoderâi = V T · ReLU ⇣ W T zi + b <ref type="bibr" target="#b2">(3)</ref> ⌘ + b <ref type="bibr" target="#b3">(4)</ref> . Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) ,</p><formula xml:id="formula_4">Lmbce = P i mi Lbce P i mi .</formula><p>Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0</p><formula xml:id="formula_5">F 0 (x) xi 2 D L y i2DL xi 2 D UL y i2DUL Lfully-supervised Lself-supervised Lsemi-supervised g(x) h(x)h(x) f ✓ (x)ŷiỹi (xi, yi) 2 D L (xi, yi) 2 D ULzi ziỹi 2 D UL 1 Supervised</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head><p>Lbce = ai log ( (â i)) · ⇣ (1 ai) log (1 (â i)) ,</p><formula xml:id="formula_6">Lmbce = mi Lbce P mi , Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b (1) ⌘ + b (2)</formula><p>⌘ . <ref type="bibr" target="#b3">(4)</ref> .</p><formula xml:id="formula_7">Decoderâ i = V T · ReLU ⇣ W T zi + b (3) ⌘ + b</formula><formula xml:id="formula_8">Lbce = ai log ( (â i)) · ⇣ (1 ai) log (1 (â i)) , Lmbce = P i mi Lbce P i mi . Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi 2 DL yi2D L xi 2 DUL yi2D UL L(yi, zi) Lself-supervised Lsemi-supervised g(x) h(x)h(x) f✓(x)ŷiỹi (xi, yi) 2 DL (xi, yi) 2 DULzi ziỹi 2 DUL 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Cross-Entropy Loss</head><p>Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) , <ref type="bibr" target="#b3">(4)</ref> .</p><formula xml:id="formula_9">Lmbce = mi Lbce P mi , Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b (1) ⌘ + b (2) ⌘ . Decoderâi = V T · ReLU ⇣ W T zi + b (3) ⌘ + b</formula><formula xml:id="formula_10">Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) , Lmbce = P i mi Lbce P i mi . Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi 2 DL yi2D L xi 2 DUL yi2D UL L(yi, zi) L(ỹi,zi) Lsemi-supervised g(x) h(x)h(x) f✓(x) yiỹi (xi, yi) 2 DL (xi, yi) 2 DULzi ziỹi 2 DUL 1 Geometric Transformation (âi)) · ⇣ (1 ai) log (1 (âi)) , e , ⇣ W · ReLU ⇣ Vai + b (1) ⌘ + b (2)</formula><p>⌘ .</p><p>ReLU <ref type="bibr" target="#b3">(4)</ref> .</p><formula xml:id="formula_11">⇣ W T zi + b (3) ⌘ + b</formula><p>(âi)) · ⇣ (1 ai) log (1 (âi)) ,</p><formula xml:id="formula_12">= P i mi Lbce P i mi . ly-supervised Lself-supervised Lsemi-supervised g(x) h(x) 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) , <ref type="bibr" target="#b3">(4)</ref> .</p><formula xml:id="formula_13">Lmbce = mi Lbce P mi , Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b (1) ⌘ + b (2) ⌘ . Decoderâi = V T · ReLU ⇣ W T zi + b (3) ⌘ + b</formula><formula xml:id="formula_14">Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) , Lmbce = P i mi Lbce P i mi . Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi 2 DL yi2D L xi 2 DUL yi2D UL L(yi, zi) L(ỹi,zi) Lsemi-supervised g(x) h(x)h(x) f✓(x) yiỹi (xi, yi) 2 DL (xi, yi) 2 DULzi ziỹi 2 DULg(x) 1 Lbce = ai log ( (â i)) · ⇣ (1 ai) log (1 (â i)) , Lmbce = mi Lbce P mi , Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b (1) ⌘ + b (2)</formula><p>⌘ . <ref type="bibr" target="#b3">(4)</ref> . <ref type="bibr" target="#b4">(5)</ref> , y i = softmax(zi). Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) , <ref type="bibr" target="#b3">(4)</ref> .</p><formula xml:id="formula_15">Decoderâ i = V T · ReLU ⇣ W T zi + b (3) ⌘ + b</formula><formula xml:id="formula_16">Lbce = ai log ( (â i)) · ⇣ (1 ai) log (1 (â i)) , Lmbce = P i mi Lbce P i mi . Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi 2 DL xi 2 DUỹi 2 DU yi2D L xi 2 DUL yi2D UL L(yi, zi) L(ỹi,zi) Lsemi-supervised g(x) h(x)h(x) f✓(x)ŷiỹi (xi, yi) 2 DL (xi, yi) 2 DULzi ziỹi 2 DULg(x) 1 zi = U · ReLU W T zi + b (3) + b</formula><formula xml:id="formula_17">Lmbce = mi Lbce P mi , Encoder zi = ReLU ⇣ W · ReLU ⇣ Vai + b (1) ⌘ + b (2) ⌘ . Decoderâi = V T · ReLU ⇣ W T zi + b (3) ⌘ + b</formula><formula xml:id="formula_18">Lbce = ai log ( (âi)) · ⇣ (1 ai) log (1 (âi)) , Lmbce = P i mi Lbce P i mi . Lmulti-task = Lmbce + Lssc mi = 1 if ai 6 = ?, else mi = 0 F 0 (x) xi 2 DL xi 2 DUỹi 2 DU yi2D L xi 2 DUL yi2D UL L(yi, zi) L(ỹi,zi) Lsemi-supervised g(x) h(x)h(x) f✓(x)ŷiỹi (xi, yi) 2 DL (xi, yi) 2 DULzi ziỹi 2 DULg(x) 1 Supervised Branch</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Branch</head><p>(a) SESEMI architecture for supervised and semi-supervised image classification, with the self-supervised task of recognizing geometric transformations. The function h(x) produces six proxy labels defined as image rotations belonging in the set of {0, 90, 180, 270} degrees along with horizontal (left-right) and vertical (up-down) flips.   Our approach to SSL belongs to a class of methods that produce proxy, or surrogate, labels from unlabeled data without requiring human annotations, which are used as targets together with labeled data. Although proxy labels may not reflect the ground truth, they provide surprisingly strong supervisory signals for learning the underlying structure of the data manifold. The training protocol for this class of SSL algorithms simply imposes an additional loss term to the overall objective function of an otherwise supervised algorithm. The auxiliary loss describes the contribution of unlabeled data and is referred to as the unsupervised loss component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Too many methods have been proposed to learn effective representations from abundant unlabeled data to augment limited ground truth labels for SSL. We summarize three particular categories most related to this work: consistency regularization, adversarial training, and self-supervised learning.</p><p>Consistency Regularization Models belonging to this class of SSL algorithms assume a dual role. On one hand, the model learns from labeled data in the conventional supervised manner; simultaneously, the model generates proxy targets for unlabeled data to be learned in conjunction with ground truth labels. During model training, sources of randomness such as dropout <ref type="bibr" target="#b39">[40]</ref> and stochastic data augmentation, along with varying levels of Gaussian noise in the input data can produce drastically different output predictions. The objective is to stabilize ensembles of predictions by minimizing their mean squared error over randomly perturbed and augmented training examples. The motivation behind this approach is to further regularize the model through the consistency principle that perturbations in the input data and/or data augmentation techniques should not significantly change the output of the model <ref type="bibr" target="#b36">[37]</ref>. Models predicated on the consistency principle, such as Pseudo-Ensembles <ref type="bibr" target="#b2">[3]</ref>, Π model <ref type="bibr" target="#b36">[37]</ref>, Temporal Ensembling <ref type="bibr" target="#b21">[22]</ref> and Mean Teacher <ref type="bibr" target="#b42">[43]</ref>, produce stable predictions on unlabeled data, which are used as unsupervised targets. The auxiliary, unsupervised consistency loss term is thus formulated as a regularizer to be jointly trained with the supervised objective on both unlabeled and labeled data for SSL.</p><p>Adversarial Training Rather than relying on the model to randomly perturb the input data by way of dropout or data augmentation, <ref type="bibr" target="#b12">Goodfellow et al. (2015)</ref>  <ref type="bibr" target="#b12">[13]</ref> proposed the concept of adversarial training to approximate the perturbations in the direction that would significantly alter the output of the model. While adversarial training requires access to ground truth labels to perform adversarial perturbations, the Virtual Adversarial Training (VAT) mechanism proposed by Miyato et al. (2017) <ref type="bibr" target="#b29">[30]</ref> can be applied to unlabeled data to produce an auxiliary unsupervised loss term that is compatible with the consistency regularization framework for SSL. Adversarial training is closely related to generative adversarial networks (GANs) <ref type="bibr" target="#b11">[12]</ref>, which have been proposed for SSL with promising results <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref>. Most recently, the self-supervised GANs with auxiliary rotation loss <ref type="bibr" target="#b25">[26]</ref> have been shown to synthesize diverse images at high resolution using only a fraction of the available labels.</p><p>Self-Supervised Learning Self-supervised learning is similar in flavor to unsupervised learning, where the goal is to learn rich visual representations from large-scale unlabeled images or videos without using any human annotations. Self-supervised representations are learned by first defining a pretext task, an objective function, for the model to solve and then producing proxy labels to guide the pretext task based solely on the visual information present in unlabeled data. The simplest self-supervised task is minimizing reconstruction error in autoencoders <ref type="bibr" target="#b15">[16]</ref> to learn low-dimensional feature representations, where the proxy labels are the values of the image pixels. More sophisticated self-supervised tasks such as image inpainting <ref type="bibr" target="#b34">[35]</ref>, colorizing grayscale images <ref type="bibr" target="#b22">[23]</ref>, and predicting image rotations <ref type="bibr" target="#b10">[11]</ref> have shown impressive results for unsupervised visual feature learning. Previous methods utilizing self-supervision for SSL did not combine an auxiliary loss with the supervised objective function but instead followed a two-stage approach: (1) pre-train a self-supervised model on a pretext task to learn useful features from unlabeled data; and (2) transfer the learned representations to downstream applications, via supervised fine-tuning, where labeled training data is scarce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Summary of Contributions</head><p>We introduce a new algorithm to jointly train a self-supervised loss term with the traditional supervised objective for the multi-task learning of both unlabeled and labeled data in a single stage. Our work is in direct contrast to prior SSL approaches based on unsupervised or self-supervised learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, which require the sequential combination of unsupervised or self-supervised pre-training followed by supervised fine-tuning.</p><p>Our approach to utilize the self-supervised loss term both as a regularizer (applied to labeled data) and SSL method (applied to unlabeled data) is analogous to consistency regularization. Although leading approaches based on consistency regularization achieve state-of-the-art SSL results, these methods require careful tuning of many hyper-parameters and are generally not easy to implement in practice. Striving for simplicity and pragmatism, our models with self-supervised regularization require no additional hyper-parameters to tune for optimal performance beyond the standard set for training neural networks. Our work is among the first to challenge the long-standing success of consistency regularization for SSL.</p><p>We conduct extensive comparative experiments to validate the effectiveness of our models by showing semi-supervised results competitive with, and in many cases surpassing, previous state-of-the-art consistency baselines. We also demonstrate that supervised learning augmented with self-supervised regularization is a viable and attractive alternative to transfer learning without the need to pre-train a separate model on large labeled datasets. Lastly, we perform an ablation study showing our proposed algorithm is the best among a family of self-supervised regularization techniques, when comparing their relative contributions to SSL performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning with Self-Supervised Regularization</head><p>We present SESEMI, a conceptually simple yet effective algorithm for enhancing supervised and semi-supervised image classification via self-supervision. The design of SESEMI is depicted in <ref type="figure" target="#fig_3">Figure 1a</ref>. The input to SESEMI is a training set of input-target pairs (x, y) ∈ D L and (optional) unlabeled inputs x ∈ D U . Typically, we assume D L and D U are sampled from the same distribution p(x), in which case D L is a labeled subset of D U . However, that assumption may not necessarily hold true in real-world settings where there exists the potential for class-distribution mismatch <ref type="bibr" target="#b32">[33]</ref>. That is, D L is sampled from p(x) but D U may be sampled from a different, although somewhat related, distribution q(x). The goal of SESEMI is to train a prediction function f θ (x), parametrized by θ, that utilizes a combination of D L and D U to obtain significantly better predictive performance than what would be achieved by using D L alone. The prediction function f θ (x) is a multi-layer perceptron with three hidden layers, each hidden layer containing 100 leaky ReLU units <ref type="bibr" target="#b27">[28]</ref> with α = 0.1. We observe that the supervised model is unable to fully capture the underlying shape of the data manifold of two moons when trained only on the labeled examples. Together with unlabeled data, SESEMI is able to learn better decision boundaries of both two moons and three spirals that would result in fewer for each epoch over</p><formula xml:id="formula_19">D U do B L ← g (x i∈D L ) Sample mini-batches of augmented labeled inputs. B U ←g (h (x i∈D U ))</formula><p>Sample mini-batches of augmented unlabeled inputs.</p><formula xml:id="formula_20">for each mini-batch do z i∈B L ← f θ (B L ) Compute model outputs for labeled inputs. z i∈B U ← f θ (B U )</formula><p>Compute model outputs for unlabeled inputs.</p><formula xml:id="formula_21">L ← − 1 |B L | i∈B L c∈C y ic log(z ic ) Supervised cross-entropy loss. − 1 |B U | i∈B U k∈Kỹ ik log(z ik ) Self-supervised cross-entropy loss. θ ← θ − ∇ θ L Update parameters via gradient descent. end end return f θ (x)</formula><p>mis-classifications on the test set. This demonstration illustrates the applicability of SESEMI to other data modalities besides image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Architectures</head><p>In principle, the prediction function f θ (x) could be any classifier. For comparison and analysis with previous work, we experiment with three high-performance CNN architectures: (i) the 13-layer maxpooling Network-in-Network (NiN) <ref type="bibr" target="#b10">[11]</ref>; (ii) the 13-layer max-pooling ConvNet <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27]</ref>; and (iii) the more modern wide residual network with depth 28 and width 2 (WRN-28-2) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>We faithfully follow the original specifications of the NiN, WRN, and ConvNet architectures, so we refer to their papers for details. All architectures have convolutional layers followed by batch normalization <ref type="bibr" target="#b17">[18]</ref> and ReLU non-linearity <ref type="bibr" target="#b30">[31]</ref>, except the ConvNet architecture uses leaky ReLU <ref type="bibr" target="#b27">[28]</ref> with α = 0.1. The NiN, WRN, ConvNet architectures have roughly 1.01, 1.47, and 3.13 million parameters, respectively.</p><p>For both supervised and semi-supervised settings, we separate the input data into labeled and unlabeled branches, and apply the same CNN model to both. Note that the unlabeled branch consists of all available training examples, but without ground truth label information. One can view SESEMI as a multi-task architecture that has a common CNN "backbone" to learn a shared representation of both labeled and unlabeled data, and an output "head" for each task. The ConvNet backbone computes an abstract 6 × 6 × 128 dimensional feature representation from the input image, while the NiN and WRN backbones have an output of 8 × 8 × 192 and 8 × 8 × 128 dimensions, respectively. Each task has extra layers in the head, which may have a complex structure, and computes a separate loss. The head of the labeled branch has a global average pooling layer followed by softmax activation to evaluate the supervised task with standard categorical cross-entropy loss. For the unlabeled branch, we define a self-supervised pretext task to be learned in conjunction with the labeled branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recognizing Image Rotations and Flips as Self-Supervision</head><p>Following <ref type="bibr" target="#b10">[11]</ref>, we apply a set of discrete geometric transformations on the input image and train the network to recognize the resulting transformations as the self-supervised task. The network architecture for the self-supervised task shares the same CNN backbone with its supervised counterpart but has a separate output head consisting of a global average pooling layer followed by softmax activation. In their original work on self-supervised rotation recognition, Gidaris et al. <ref type="bibr" target="#b10">[11]</ref> defined the proxy labels to be image rotations belonging in the set of {0, 90, 180, 270} degrees, resulting in a four-way classification task. Their models performed well on the rotation recognition task by learning salient visual features depicted in the image, such as location of objects, type, and pose. In this work, we extend the geometric transformations to include horizontal (left-right) and vertical (up-down) flips, resulting in the self-supervised cross-entropy loss over six classes. Further, we propose to train SESEMI on both labeled and unlabeled data simultaneously, which is more efficient and yields better performance than the approach of Gidaris et al. based on the sequential combination of self-supervised pre-training on unlabeled data followed by supervised fine-tuning on labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Integrating Self-Supervised Loss as Regularization</head><p>The algorithmic overview of SESEMI is provided in Algorithm 1. At each training step, we sample two mini-batches having the same number of labeled and unlabeled examples as inputs to a shared CNN backbone f θ (x). Note that in a typical semi-supervised setting, labeled examples will repeat in a mini-batch because the number of unlabeled examples is much greater. We forward propagate f θ (x) twice, once on the labeled branch x i∈D L and another pass on the unlabeled branch x i∈D U , resulting in softmax prediction vectors z i andz i , respectively. We compute the supervised cross-entropy loss L SUPER (y i , z i ) using ground truth labels y i and compute the self-supervised cross-entropy loss L SELF (ỹ i ,z i ) using proxy labelsỹ i generated from image rotations and flips. The parameters θ are learned via backpropagation by minimizing the multi-task SESEMI objective function defined as the weighted sum of supervised and self-supervised loss components:</p><formula xml:id="formula_22">L SESEMI = L SUPER (y i , z i ) + wL SELF (ỹ i ,z i )</formula><p>. Our formulation of the SESEMI objective treats the self-supervised loss as a regularization term, and w &gt; 0 is the regularization hyper-parameter that controls the relative contribution of self-supervision in the overall objective function.</p><p>In previous SSL approaches based on consistency regularization, such as Π model and Mean Teacher, w was formulated as the consistency coefficient and was subjected to considerable tuning, on a per-dataset basis, for optimal performance. We experimented with different values for the weighting parameter w in SESEMI and found w = 1 yields consistent results across all datasets and CNN architectures, suggesting that supervised and self-supervised losses are relatively balanced and compatible for image classification. Moreover, setting w = 1 leads to a convenient benefit of having one less hyper-parameter to tune. We backpropagate gradients to both branches of the network to update θ, similar to Π model. The self-supervised loss term has a dual purpose. First, it enables SESEMI to learn additional, complementary visual features from unlabeled data that help guide its decision boundaries along the data manifold. Second, it is compatible with conventional supervised learning without unlabeled data by serving as a strong regularizer against geometric transformations for improved generalization. We refer to SESEMI models trained with self-supervised regularization on labeled data as augmented supervised learning (ASL). At inference time, we simply take the supervised branch of the network to make predictions on test data and discard the self-supervised branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Evaluation</head><p>We follow standard evaluation protocol for SSL, in which we randomly sample varying fractions of the training data as labeled examples while treating the entire training set, discarding all label information, as the source of unlabeled data. We train a model with both labeled and unlabeled data according to SESEMI (Algorithm 1) and compare its performance to that of the same model trained using only the labeled portion in the traditional supervised manner. For ASL, we train SESEMI using the ConvNet architecture on labeled data, but augment the supervised objective with self-supervised regularization. The performance metric is classification error rate. We expect a good SSL algorithm to yield better results (lower error rate) when unlabeled data is used together with labeled data. We closely follow the experimental protocols described in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b32">33]</ref> to remain consistent with previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Baselines</head><p>We evaluate our proposed SESEMI algorithm on three benchmark datasets for supervised and semi-supervised image classification: Street View House Numbers (SVHN) <ref type="bibr" target="#b31">[32]</ref>, CIFAR-10 and  CIFAR-100 <ref type="bibr" target="#b20">[21]</ref>. For details on the datasets and implementation, see Appendices A and B. We also use two auxiliary datasets to augment our experiments on supervised and semi-supervised learning: 80 million Tiny Images <ref type="bibr" target="#b43">[44]</ref> and ImageNet-32 <ref type="bibr" target="#b6">[7]</ref>. Tiny Images is the superset of CIFAR-10 and CIFAR-100 organized into 75,062 generic scene and object categories. ImageNet-32 is the full ImageNet dataset <ref type="bibr" target="#b7">[8]</ref> down-sampled to 32 × 32 pixels. We use ImageNet-32 for supervised transfer learning experiments. We use Tiny Images as a source of unlabeled extra data to augment SSL on CIFAR-100 and to evaluate SESEMI under the condition of class-distribution mismatch.</p><p>We empirically compare our SESEMI models trained with self-supervised regularization against two state-of-the-art baselines for supervised and semi-supervised learning: (a) the RotNet models of <ref type="bibr" target="#b10">Gidaris et al. (2018)</ref>  <ref type="bibr" target="#b10">[11]</ref>, which were pre-trained on unlabeled data with self-supervised rotation loss followed by a separate step of supervised fine-tuning on labeled data; and (b) models jointly trained on both unlabeled and labeled data using consistency regularization as the unsupervised loss, namely Π model and its Temporal Ensembling (TempEns) variant <ref type="bibr" target="#b21">[22]</ref>, VAT <ref type="bibr" target="#b29">[30]</ref>, and Mean Teacher <ref type="bibr" target="#b42">[43]</ref>.</p><p>The RotNet baseline uses the 13-layer max-pooling NiN architecture, whereas the consistency models use the 13-layer max-pooling ConvNet architecture. We also provide a comparison of SESEMI within the unified evaluation framework of Oliver et al. (2018) <ref type="bibr" target="#b32">[33]</ref>, in which they re-implemented the consistency models using the WRN-28-2 architecture. Thus, our experiments report results from both ConvNet and WRN backbones to evaluate the relative impact of alternative convolutional architectures on SSL performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Self-Supervised Regularization Outperforms Pre-Training + Fine-Tuning on CIFAR-10</head><p>Following the protocol of Gidaris et al. (2018) <ref type="bibr" target="#b10">[11]</ref>, we evaluate the accuracy of SESEMI using varying quantities of labeled examples from 200 to 50,000. <ref type="figure">Figure 2</ref> presents our supervised and semi-supervised results on CIFAR-10 with those previously obtained by RotNet. The best results are in boldface indicating the lowest classification error rate. For both supervised and semi-supervised learning, we find that our SESEMI models trained with self-supervised regularization significantly outperform RotNet models by as much as 23.9%. Why does SESEMI outperform RotNet when the two models ostensibly share the same architecture and self-supervised task? This question can be partly explained by empirical observations that suggest better performance on the pre-training task does not always translate to higher accuracy on downstream tasks via supervised fine-tuning <ref type="bibr" target="#b19">[20]</ref>. By solving both supervised and self-supervised objectives during training, SESEMI is able to learn complementary visual features from labeled and unlabeled data simultaneously for enhanced generalization.</p><p>We also perform an experiment to pre-train the NiN model on the large ImageNet-32 dataset containing 1.28 million images and then transfer to CIFAR-10 via supervised fine-tuning. Our motivation is  to gain insight into the potential upper bound of supervised learning with limited labels via transfer learning. We find that our SESEMI models compare favorably to supervised transfer learning without the need to pre-train a separate model on ImageNet-scale labeled dataset. The ImageNet-32 entry is regarded as the upper bound in performance, whereas the Supervised entry indicates the lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Self-Supervised Regularization Outperforms Consistency Regularization on CIFAR-10 and CIFAR-100</head><p>SVHN <ref type="table" target="#tab_1">Table 1</ref> compares our supervised and semi-supervised results with consistency baselines. In analyzing the SVHN results on the left side of <ref type="table" target="#tab_1">Table 1</ref>, we observe that SESEMI ASL surpasses the supervised baselines, including ImageNet-32 and those with strong Mixup <ref type="bibr" target="#b49">[50]</ref> and Manifold Mixup <ref type="bibr" target="#b44">[45]</ref> regularization, by a large margin for all experiments. However, the results are not satisfactory when compared against the semi-supervised baselines, especially Mean Teacher. We discuss the limitation of SESEMI for semi-supervised learning on the SVHN dataset in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>Experiments on CIFAR-10 tell a different story. The right side of <ref type="table" target="#tab_1">Table 1</ref> shows that SESEMI uniformly outperforms all supervised and semi-supervised baselines, improving on SSL results by as much as 17%. On CIFAR-10, the combination of supervised and self-supervised learning is a strength of SESEMI, but it is also a limitation in the case of SVHN. We observe that the ConvNet and WRN architectures produce comparable results across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100 and Tiny Images</head><p>The successes of SESEMI on CIFAR-10 also transfer to experiments on CIFAR-100. The left side of <ref type="table" target="#tab_2">Table 2</ref> provides a comparison of SESEMI against the Π model and TempEns baselines, where we obtain competitive semi-supervised performance using 10,000 labels and achieve state-of-the-art supervised results when all 50,000 labels are available, matching the upper bound performance of ImageNet-32 supervised fine-tuning.</p><p>Additionally, we run two experiments to evaluate the performance of SESEMI in the case of classdistribution mismatch. Following Laine and Aila (2017) <ref type="bibr" target="#b21">[22]</ref>, our first experiment utilizes all 50,000 available labels from CIFAR-100 and randomly samples 500,000 unlabeled extra Tiny Images, most belonging to categories not found in CIFAR-100. Our second experiment uses a restricted set of 237,203 Tiny Images from categories found in CIFAR-100. The right side of <ref type="table" target="#tab_2">Table 2</ref>   <ref type="bibr" target="#b32">[33]</ref> with other SSL approaches. For SESEMI with WRN-28-2 architecture, the addition of (randomly selected) unlabeled extra data from Tiny Images further  It is important to note that we do not perform any hyper-parameter search in this experiment, but use the same set of hyper-parameters described in Appendix B along with w = 1 for the weighting of the self-supervised loss term in SESEMI. In practical applications where tuning many (possibly inter-dependent) hyper-parameters can be problematic <ref type="bibr" target="#b32">[33]</ref>, especially over small validation sets, our approach to supervised and semi-supervised learning using SESEMI offers a clear and significant benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Art Supervised Methods on CIFAR-10 and CIFAR-100</head><p>Motivated by the strong performances of SESEMI ASL for supervised learning augmented with self-supervised regularization, we provide a comparative analysis of SESEMI against several previous state-of-the-art supervised methods on CIFAR-10 and CIFAR-100 in <ref type="table" target="#tab_5">Table 4</ref>. We observe that SESEMI ASL is competitive in predictive performance with advanced CNN architectures like FractalNet <ref type="bibr" target="#b23">[24]</ref>, Fractional Max-Pooling <ref type="bibr" target="#b13">[14]</ref>, ResNet-1001 <ref type="bibr" target="#b14">[15]</ref>, Wide ResNet-40-4 <ref type="bibr" target="#b47">[48]</ref>, and DenseNet <ref type="bibr" target="#b16">[17]</ref> while requiring a fraction of the computational complexity, as measured in millions of trainable parameters. For those architectures having roughly the same number of trainable parameters, our SESEMI ASL models outperform Highway Network <ref type="bibr" target="#b40">[41]</ref> and FitResNet with LSUV initialization <ref type="bibr" target="#b28">[29]</ref> by a large margin.</p><p>The effectiveness of SESEMI ASL is directly attributed to self-supervised regularization and not to the CNN architecture. The same ConvNet architecture without self-supervised regularization performs significantly worse on both CIFAR-10 (5.82% error rate) and CIFAR-100 (26.42% error rate). In principle, self-supervised regularization could be incorporated into any CNN architecture for further reduction in classification error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Several studies have provided conclusive evidence that self-supervision is an effective unsupervised pre-training technique for downstream supervised visual understanding tasks such as image classification, object detection, and semantic segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. However, the evaluation of self-supervised algorithms for SSL has not been explored, especially in the setting where the supervised and self-supervised losses are jointly trained, per Algorithm 1. We briefly describe the following self-supervised tasks based on image reconstruction and compare their SSL performances against the task of classifying image rotations and flips. All experiments use the same convolutional encoder-decoder framework, where the encoder backbone is the WRN-28-2 architecture, and the decoder head comprises a set of two deconvolutional layers <ref type="bibr" target="#b24">[25]</ref> with batch normalization and ReLU non-linearity to produce a reconstructed output with the same dimensions as the input.</p><p>Denoising Autoencoder The self-supervised objective of this simple baseline is to minimize the mean pixel-wise squared error between the reconstructed output and image input corrupted with Gaussian noise.</p><p>Image Inpainting Following <ref type="bibr" target="#b34">[35]</ref>, the input to the encoder is an image with the central square patch covering 1 /4 of the image masked out or set to zero. The decoder is trained to generate prediction for the masked region using a masked L 2 reconstruction loss as self-supervision.</p><p>Image Colorization Following <ref type="bibr" target="#b22">[23]</ref>, the input to the encoder is a grayscale image (the L* channel of the L*a*b* color space) and the decoder is trained to predict the a*b* color components at every pixel. The self-supervised loss is the mean squared error between the reconstructed a*b* color output and ground truth a*b* color components.</p><p>Ablation Results <ref type="figure" target="#fig_6">Figure 3</ref> shows the individual tasks of recognizing image flips and rotations outperform image reconstruction, inpainting, and colorization on the CIFAR-10 dataset. These results suggest that classification-based self-supervision provides a better, or perhaps more compatible, proxy label for semi-supervised image classification than reconstruction-based tasks. Our findings corroborate recent studies showing rotation-based self-supervision is the superior pre-training technique for downstream transfer learning tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. Lastly, combining horizontal and vertical flips with the rotation recognition task outperforms all other self-supervised tasks, leading to an improvement in SSL performance over rotation recognition by 8.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Limitation of SESEMI We speculate the poor performance of SESEMI on the SVHN dataset stems from our chosen self-supervised task of predicting image rotations and flips. <ref type="bibr" target="#b10">Gidaris et al. (2018)</ref>  <ref type="bibr" target="#b10">[11]</ref> showed that their self-supervised model focused its attention maps on salient parts of the images to aid in the rotation recognition task. We hypothesize similar dynamics are at play here, but the SVHN dataset presents an additional layer of complexity in which the centermost digits (the digits to be recognized) are often surrounded by "distractor" digits. When the digits are rotated and flipped, the self-supervised branch is likely picking up dominant visual features corresponding to the distractor digits and relate them to the supervised branch as belonging to the digits of interest. These "miscues" are most prominent when few labels are present, where the supervised branch is simply learning visual information from the self-supervised branch. However, when all labels are available, the supervised branch is able to correct the miscues, and our SESEMI models produce the best classification results.</p><p>Comparison with Recent Developments in SSL Recent years have seen a flurry of research on SSL techniques that are related to or concurrent with our work. The prior work of Smooth Neighbors on Teacher Graphs <ref type="bibr" target="#b26">[27]</ref>, Virtual Adversarial Dropout <ref type="bibr" target="#b33">[34]</ref>, Interpolation Consistency Training <ref type="bibr" target="#b45">[46]</ref>, Stochastic Weight Averaging <ref type="bibr" target="#b1">[2]</ref>, and Label Propagation <ref type="bibr" target="#b18">[19]</ref> advanced the field of semi-supervised learning by achieving impressive results on SVHN, CIFAR-10, and CIFAR-100 benchmarks. However, those methods all build upon strong consistency baselines by either adding a third loss (and new hyper-parameters to tune) to the overall objective of consistency models or averaging model weights. The concurrent work on self-supervised semi-supervised learning <ref type="bibr" target="#b48">[49]</ref> independently explores the contributions of self-supervised regularization for SSL in a way similar to ours, but with a different evaluation protocol and end goal. MixMatch <ref type="bibr" target="#b3">[4]</ref> combines strong Mixup <ref type="bibr" target="#b49">[50]</ref> regularization with consistency regularization to achieve state-of-the-art SSL results.</p><p>Our goal for this work is to directly compare the effectiveness of self-supervised regularization to consistency regularization, which has not been done before. Moreover, we deliberately avoid increasing complexity in favor of simplicity and pragmatism with our design choices and training protocol. In principle, our work is potentially complementary to Label Propagation and MixMatch. Label Propagation requires the Mean Teacher prediction function to work well, which can be directly replaced by our SESEMI module. For MixMatch, we can integrate a third loss term into the objective function for learning additional self-supervised features. These are viable topics for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a conceptually simple yet effective multi-task CNN architecture for supervised and semi-supervised learning based on self-supervised regularization. Our approach produces proxy labels from geometric transformations on unlabeled data, which are combined with ground truth labels for improved learning and generalization in the limited labeled data setting. We provided a comprehensive empirical evaluation of our approach using three different CNN architectures, spanning multiple benchmark datasets and baselines to demonstrate its effectiveness and wide range of applicability. We highlight two attractive benefits of SESEMI. First, SESEMI achieves state-of-the-art predictive performance for both supervised and semi-supervised image classification without introducing additional hyper-parameters to tune. Second, SESEMI does not need a separate pre-training step, but is trained end-to-end for simplicity, efficiency, and practicality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Left (a) -The SESEMI architecture. Right -Demonstration of the SESEMI algorithm on (b) two moons and (c) three spirals synthetic datasets. Labeled examples are marked with the black cross. Decision boundaries separate classes by colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figures</head><label></label><figDesc>1b and 1c illustrate the SESEMI algorithm on the two moons and three spirals synthetic datasets. Each dataset has 500 examples per class and a label rate of 0.01 (i.e., only 5 examples are labeled per class).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 :</head><label>1</label><figDesc>SESEMI mini-batch training. Require : Training set of labeled input-target pairs (x, y) ∈ D L . Training set of unlabeled inputs x ∈ D U . Geometric transformation function h(x) producing proxy labelsỹ ∈ D U . Input data augmentation functions g(x) andg(x). Neural network architecture f θ (x) with trainable parameters θ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Exploring the relative contributions of various self-supervised tasks for SSL on CIFAR-10. The Supervised baseline utilizes a random sample of 4,000 labeled examples, whereas all other tasks learn from the same set of 4,000 labels along with 50,000 unlabeled examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>51.79 32.50 18.43 11.78 6.73 35.48</head><label></label><figDesc></figDesc><table><row><cell>Classification Error Rate (%)</cell><cell>60.88</cell><cell>35.34</cell><cell>49.87</cell><cell>36.90</cell><cell>24.05</cell><cell>22.15</cell><cell>20.32</cell><cell>21.10</cell><cell>14.50</cell><cell>13.99</cell><cell cols="2">14.10 RotNet SSL -Pretrain + Finetune 7.20 13.20 8.84 11.06 6.73 12.52 10.17 SESEMI SSL ImageNet-32 Supervised Finetune SESEMI Augmented S upervised RotNet Supervised</cell><cell>5.91</cell><cell>Rotation Flip Autoencoder Colorization Inpainting Supervised</cell><cell>12.19 12.98</cell><cell>16.95 17.07 17.08 19.40</cell></row><row><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell cols="2">1,000</cell><cell></cell><cell></cell><cell cols="2">4,000</cell><cell>10,000</cell><cell>50,000</cell><cell></cell><cell>Rotation + Flip</cell><cell>11.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Number of Labels</cell><cell></cell><cell></cell><cell></cell><cell>Classification Error Rate (%)</cell></row><row><cell cols="14">Figure 2: Comparison of SESEMI with RotNet</cell><cell></cell><cell></cell></row><row><cell cols="14">for supervised and semi-supervised learning on</cell><cell></cell><cell></cell></row><row><cell cols="14">CIFAR-10. RotNet models follow the conven-</cell><cell></cell><cell></cell></row><row><cell cols="14">tional two-stage approach of self-supervised pre-</cell><cell></cell><cell></cell></row><row><cell cols="14">training on unlabeled data followed by supervised</cell><cell></cell><cell></cell></row><row><cell cols="12">fine-tuning on labeled data.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test classification error rates (%) for supervised and semi-supervised learning on SVHN (Left) and CIFAR-10 (Right) with data augmentation averaged over four runs.</figDesc><table><row><cell>Method (SVHN)</cell><cell cols="3">250 labels 73,257 images 73,257 images 73,257 images 500 labels 1,000 labels</cell><cell>73,257 labels 73,257 images</cell><cell>Method (CIFAR-10)</cell><cell cols="3">1,000 labels 50,000 images 50,000 images 50,000 images 2,000 labels 4,000 labels</cell><cell>50,000 labels 50,000 images</cell></row><row><cell>Supervised [43] Mixup [46] Manifold Mixup [46] SESEMI ASL (ConvNet) ImageNet-32 Fine-tuned Π Model SSL [22] TempEns SSL [22] VAT SSL [30] Mean Teacher SSL [43] SESEMI SSL (ConvNet) SESEMI SSL (WRN)</cell><cell>27.77 ± 3.18 33.73 ± 1.79 31.75 ± 1.39 23.60 ± 1.38 30.87 ± 1.41 ---4.35 ± 0.50 8.32 ± 0.13 10.24 ± 0.24</cell><cell>16.88 ± 1.30 21.08 ± 0.61 20.57 ± 0.63 15.45 ± 0.79 17.42 ± 0.59 6.65 ± 0.53 5.12 ± 0.13 -4.18 ± 0.27 6.50 ± 0.28 7.91 ± 0.24</cell><cell>12.32 ± 0.95 13.70 ± 0.47 13.07 ± 0.53 10.32 ± 0.16 12.41 ± 0.67 4.82 ± 0.17 4.42 ± 0.16 5.42 ± 0.22 3.95 ± 0.19 5.59 ± 0.12 5.35 ± 0.15</cell><cell>2.75 ± 0.10 --2.26 ± 0.07 2.66 ± 0.02 2.54 ± 0.04 2.74 ± 0.06 -2.50 ± 0.05 2.26 ± 0.07 2.34 ± 0.05</cell><cell>Supervised [43] Mixup [46] Manifold Mixup [46] SESEMI ASL (ConvNet) ImageNet-32 Fine-tuned Π Model SSL [22] TempEns SSL [22] VAT SSL [30] Mean Teacher SSL [43] SESEMI SSL (ConvNet) SESEMI SSL (WRN)</cell><cell>46.43 ± 1.21 36.48 ± 0.15 34.58 ± 0.37 29.44 ± 0.24 17.96 ± 0.34 ---21.55 ± 1.48 17.88 ± 0.29 18.32 ± 0.28</cell><cell>33.94 ± 0.73 26.24 ± 0.46 25.12 ± 0.52 21.53 ± 0.18 12.92 ± 0.47 ---15.73 ± 0.31 14.22 ± 0.27 14.45 ± 0.25</cell><cell>20.66 ± 0.57 19.67 ± 0.16 18.59 ± 0.18 16.15 ± 0.12 10.16 ± 0.22 12.36 ± 0.31 12.16 ± 0.24 11.36 ± 0.34 12.31 ± 0.28 11.65 ± 0.13 11.23 ± 0.22</cell><cell>5.82 ± 0.15 --4.70 ± 0.11 4.61 ± 0.11 5.56 ± 0.10 5.60 ± 0.10 5.81 ± 0.02 5.94 ± 0.15 4.70 ± 0.11 4.71 ± 0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test classification error rates (%) on CIFAR-100 with data augmentation averaged over four runs. Left -Results with 10,000 and 50,000 labels. Right -Results with unlabeled Tiny Images.</figDesc><table><row><cell>Method</cell><cell>10,000 labels 50,000 images</cell><cell>50,000 labels 50,000 images</cell><cell>Method</cell><cell>50,000 labels Tiny 500,000</cell><cell>50,000 labels Tiny 237,203</cell></row><row><cell>Supervised [22] SESEMI ASL (ConvNet) ImageNet-32 Fine-tuned Π Model SSL [22] TempEns SSL [22] SESEMI SSL (ConvNet) SESEMI SSL (WRN)</cell><cell>44.56 ± 0.30 40.57 ± 0.20 32.44 ± 0.27 39.19 ± 0.36 38.65 ± 0.51 38.71 ± 0.11 38.69 ± 0.10</cell><cell>26.42 ± 0.17 22.49 ± 0.15 22.22 ± 0.25 26.32 ± 0.04 26.30 ± 0.15 22.49 ± 0.15 23.42 ± 0.11</cell><cell>Supervised [22] SESEMI ASL (ConvNet) ImageNet-32 Fine-tuned Π Model SSL [22] TempEns SSL [22] SESEMI SSL (ConvNet) SESEMI SSL (WRN)</cell><cell>26.42 ± 0.17 22.49 ± 0.15 22.22 ± 0.25 25.79 ± 0.17 23.62 ± 0.23 22.52 ± 0.10 22.65 ± 0.30</cell><cell>26.42 ± 0.17 22.49 ± 0.15 22.22 ± 0.25 25.43 ± 0.32 23.79 ± 0.24 22.50 ± 0.26 22.62 ± 0.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test classification error rates (%) on CIFAR-10 with 4,000 labels and SVHN with 1,000 labels. All entries are trained using the WRN-28-2 architecture within the unified evaluation framework of<ref type="bibr" target="#b32">Oliver et al. (2018)</ref> <ref type="bibr" target="#b32">[33]</ref>.</figDesc><table><row><cell>Dataset</cell><cell># Labels</cell><cell>Supervised</cell><cell>Π Model</cell><cell>Mean Teacher</cell><cell>VAT</cell><cell cols="2">ImageNet32 SESEMI (ours)</cell></row><row><cell>CIFAR-10 SVHN</cell><cell cols="3">4,000 20.26 ± 0.38 16.37 ± 0.63 1,000 12.83 ± 0.47 7.19 ± 0.27</cell><cell cols="2">15.87 ± 0.28 13.86 ± 0.27 5.65 ± 0.47 5.63 ± 0.20</cell><cell>12.09 -</cell><cell>11.23 ± 0.22 5.35 ± 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test classification error rates (%) for various supervised methods on CIFAR-10 and CIFAR-100 with all 50,000 labeled training examples. For each method, we compare best-case performances along with the required number of trainable parameters.</figDesc><table><row><cell>Method</cell><cell cols="3"># Params CIFAR-10 CIFAR-100</cell></row><row><cell>FractalNet [24]</cell><cell>38.6M</cell><cell>4.60</cell><cell>23.73</cell></row><row><cell>Fractional MP [14]</cell><cell>12.0M</cell><cell>3.47</cell><cell>26.39</cell></row><row><cell>ResNet-1001 [15]</cell><cell>10.2M</cell><cell>4.62</cell><cell>22.71</cell></row><row><cell>Wide ResNet-40-4 [48]</cell><cell>8.9M</cell><cell>4.53</cell><cell>21.18</cell></row><row><cell>DenseNet (k = 12) [17]</cell><cell>7.0M</cell><cell>4.10</cell><cell>20.20</cell></row><row><cell>FitResNet (LSUV) [29]</cell><cell>2.5M</cell><cell>5.84</cell><cell>27.66</cell></row><row><cell>Highway Network [41]</cell><cell>2.3M</cell><cell>7.72</cell><cell>32.39</cell></row><row><cell>Supervised (ConvNet) [43, 22]</cell><cell>3.1M</cell><cell>5.82</cell><cell>26.42</cell></row><row><cell>SESEMI ASL (ConvNet)</cell><cell>3.1M</cell><cell>4.70</cell><cell>22.49</cell></row><row><cell>SESEMI ASL (WRN-28-2)</cell><cell>1.5M</cell><cell>4.71</cell><cell>23.42</cell></row><row><cell cols="4">reduces CIFAR-100 error rate from 23.42% to 22.62%, matching performances obtained by SESEMI</cell></row><row><cell cols="2">with ConvNet and ImageNet-32 supervised fine-tuning.</cell><cell></cell><cell></cell></row><row><cell cols="4">SESEMI with Residual Networks Table 3 provides a comparison of SESEMI for semi-supervised</cell></row><row><cell cols="4">learning on SVHN and CIFAR-10 within the unified evaluation framework of Oliver et al. (2018) [33],</cell></row><row><cell cols="4">in which they re-implemented the consistency baselines using the WRN-28-2 architecture, carried</cell></row><row><cell cols="4">out large-scale hyper-parameter optimization specific to each technique, and reported best-case</cell></row><row><cell cols="4">performances. Our SESEMI model with WRN-28-2 architecture establishes a new upper bound</cell></row><row><cell cols="4">in SSL performance by outperforming all methods, including ImageNet-32, under this evaluation</cell></row><row><cell>setting for both SVHN and CIFAR-10.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author thanks Cole Winans and Brian Keller at Flyreel for their continued support, and gracious reviewers for their constructive feedback on this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Dataset Details</head><p>The SVHN <ref type="bibr" target="#b31">[32]</ref> dataset contains 73,257 train and 26,032 test samples categorized over 10 digits (0-9) in natural scene images. The classification task is to recognize the centermost digit in each color image of 32 × 32 pixels. We only use the official train/test splits and do not utilize the provided 531,131 extra images. The CIFAR-10 <ref type="bibr" target="#b20">[21]</ref> dataset consists of 60,000 32 × 32 natural color images in 10 classes, with 6,000 images per class. The dataset is split into 50,000 train and 10,000 test samples. The CIFAR-100 <ref type="bibr" target="#b20">[21]</ref> dataset is similar to CIFAR-10, except it has 100 classes containing 600 images each. There are 500 train and 100 test images per class. CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million Tiny Images dataset <ref type="bibr" target="#b43">[44]</ref>, which is organized into 75,062 generic scene and object categories. ImageNet-32 <ref type="bibr" target="#b6">[7]</ref> is the full ImageNet dataset <ref type="bibr" target="#b7">[8]</ref> down-sampled to 32 × 32 pixels. We use ImageNet-32 for supervised transfer learning experiments. We use Tiny Images as a source of unlabeled extra data to augment semi-supervised learning on CIFAR-100 and to evaluate the performance of SESEMI under the condition of class-distribution mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We implement the SESEMI algorithm using Keras <ref type="bibr" target="#b5">[6]</ref> with GPU-enabled TensorFlow backend <ref type="bibr" target="#b0">[1]</ref>. We follow standard practice for data pre-processing, augmentation, and hyper-parameter search.</p><p>Data Pre-processing and Augmentation We apply global contrast normalization to scale all datasets to have zero mean and unit L 2 norm. We further pre-process CIFAR-10, CIFAR-100, and Tiny Images with Zero Components Analysis (ZCA) whitening <ref type="bibr" target="#b20">[21]</ref>. Standard data augmentation on CIFAR-10 and CIFAR-100 includes random translations by up to 2 pixels on each side {∆x, ∆y} ∈ [−2, 2], horizontal (left-right) flip, and additive Gaussian noise with σ = 0.15, whereas SVHN is limited to random translations and Gaussian noise. Data augmentation is applied independently to both supervised and self-supervised branches of SESEMI, except we do not apply horizontal flip on the self-supervised branch.</p><p>Hyper-parameters During model development, we use 10 percent of the provided SVHN training examples as a dev set and perform hyper-parameter tuning to find the optimal combination of minibatch size, percentage of dropout regularization, initial learning rate, and number of training epochs that minimizes classification error on said dev set. The same hyper-parameters are subsequently shared across supervised and semi-supervised settings for all datasets, and are used in all experiments featuring NiN, ConvNet and WRN architectures. These are standard hyper-parameters for tuning CNNs and are not specific to SESEMI, which is a notable advantage of our approach. By contrast, previous consistency baselines have specific hyper-parameters that must be carefully tuned for optimal performance, such as the consistency coefficient in Π model, exponential moving average decay in Mean Teacher, and norm constraint for the adversarial direction in VAT. The final models are trained on all examples from the combined training and dev sets.</p><p>Training Protocol We train our models using Nesterov accelerated gradient descent <ref type="bibr" target="#b41">[42]</ref> on minibatches of 16 examples, with an initial base learning rate of 0.05, momentum of 0.9, weight decay of 0.0005, and dropout rate of 0.5 in all experiments. Similar to <ref type="bibr" target="#b10">[11]</ref>, we implement four rotations and two flips on a given image in a mini-batch for improved training. Thus, our SESEMI models receive two effective mini-batches having the same number of 16 × 6 = 96 unlabeled and labeled examples. In the supervised setting, we train SESEMI for up to 2,000 epochs over labeled examples in D L , depending on the dataset and scarcity of labeled examples. In the semi-supervised setting, we train SESEMI over unlabeled examples in D U for 30 epochs on SVHN and 50 epochs on CIFAR-10 and CIFAR-100.</p><p>During training, we anneal the base learning rate according to the polynomial decay of the form: lr(t) ← base_lr × (1 − t /tmax) p , where base_lr = 0.05, t is the current iteration, t max is the maximum number of iterations, and p = 0.5 controls the rate of decay. With this learning rate schedule, the models gain the most performance improvement in the last few epochs, so we simply report test error after the last training iteration. We report the mean and standard deviation of four independent runs with random weight initializations on fixed data splits.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with Pseudo-Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>1st edn.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Downsampled Variant of ImageNet as an Alternative to the CIFAR Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-Task Self-Supervised Visual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarially Learned Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning by Predicting Image Rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<title level="m">Fractional Max-Pooling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Label Propagation for Deep Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting Self-Supervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal Ensembling for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Colorization as a Proxy Task for Visual Understanding&apos;, in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FractalNet: Ultra-Deep Neural Networks without Residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fully Convolutional Networks for Semantic Segmentation&apos;, in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High-Fidelity Image Generation With Fewer Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smooth Neighbors on Teacher Graphs for Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">All You Need is a Good Init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning&apos;, Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Realistic Evaluation of Semi-Supervised Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial Dropout for Supervised and Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-Supervised Multitask Learning for Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization with Stochastic Perturbations for Deep Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised and Semi-Supervised Learning with Categorical Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Training Very Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the Importance of Initialization and Momentum in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition&apos;, Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold Mixup: Better Representations by Interpolating Hidden States&apos;, in International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interpolation Consistency Training for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Zero-Shot Learning -A Comprehensive Evaluation of the Good, the Bad and the Ugly&apos;, Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">S4L: Self-Supervised Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bridging Theory and Algorithm for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
